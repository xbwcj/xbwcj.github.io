<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>《Practical Secure Aggregation for Federated Learning on User-Held Data》阅读笔记</title>
      <link href="/2022/09/09/practical-secure-aggregation-for-federated-learning-on-user-held-data-yue-du-bi-ji/"/>
      <url>/2022/09/09/practical-secure-aggregation-for-federated-learning-on-user-held-data-yue-du-bi-ji/</url>
      
        <content type="html"><![CDATA[<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>&emsp;&emsp;安全聚合(Secure Aggregation)是一类安全多方计算算法，其中相互不信任的一组 $u \in \mathcal{U}$ 各自持有一个私有值 $x_u$ ，并协作计算一个聚合值，例如总和 $\sum_{u \in \mathcal{U}} x_{u}$，除了可以从聚合值本身学到的以外，彼此不透露关于其私有值的任何信息。在这项工作中，我们考虑在联邦学习模型中训练一个深度神经网络，使用分布式梯度下降在移动设备上的用户持有训练数据，使用安全聚合来保护每个用户的模型梯度的隐私。我们确定了效率和鲁棒性要求的组合，据我们所知，这是文献中现有算法无法满足的。我们继续为高维数据设计了一种新颖的、通信高效的安全聚合协议，该协议允许多达 $1/3$ 的用户无法完成协议。对于16位输入值，我们的协议为 $210$ 个用户和 $220$ 维向量提供了 $1.73 × 通信扩展$ ，为 $214$ 个用户和 $224$ 维向量提供了 $1.98 × 通信扩展$ 。</p><h1 id="用于联邦学习的安全聚合"><a href="#用于联邦学习的安全聚合" class="headerlink" title="用于联邦学习的安全聚合"></a>用于联邦学习的安全聚合</h1><p>&emsp;&emsp;考虑训练一个深度神经网络来预测用户在写短信时将要输入的下一个单词，以提高手机屏幕键盘[11]的打字精度。建模师可能希望针对大量用户中的所有文本消息训练这样的模型。然而，短信经常包含敏感信息；用户可能不愿意将它们的副本上传到建模者的服务器。相反，我们考虑在联邦学习设置中训练这样的模型，在这种设置中，每个用户在自己的移动设备上安全地维护一个关于她的文本消息的私有数据库，并在中央服务器的协调下训练一个共享的全局模型，该服务器基于来自用户的高处理、最小范围、短暂的更新[14,17]。</p><p>&emsp;&emsp;神经网络表示函数 $f(x, \Theta)=y $，将输入 $x$ 映射到输出 $y$，其中 $f$ 由高维向量 $\Theta \in \mathbb{R}^{k}$ 参数化。对于文本消息组合建模，$x$ 可能对目前输入的单词进行编码，$y$ 可能对下一个单词进行概率分布。一个训练例子是观察这一对 $\left\langle x, y \right\rangle$，训练集是集合 $D = \\{ \left\langle x_{i}, y_{i}\right\rangle ; i=1, \ldots, m \\} $ 。我们定义训练集 $ \mathcal{L}_{f}(D, \Theta)=\frac{1}{|D|}  \sum_{ \left\langle x, y \right\rangle \in D} \mathcal{L}_{f} (x_{i}, y_{i}, \Theta)$ 上的损失，其中对于损失函数 $\ell$， $\mathcal{L}_{f} (x_{i}, y_{i}, \Theta) = \ell(y, f(x, \Theta))$，例如 $\ell(y, \hat{y})=(y-\hat{y})^{2}$。训练包括寻找实现小 $\mathcal{L}_{f}(D, \Theta)$ 的参数 $\Theta$ ，通常使用可变的小批量随机梯度下降[4,10]</p><p>&emsp;&emsp;在联邦学习设置中，每个用户 $u \in \mathcal{U}$ 都拥有一个训练样例的私有集合 $D_{u}$，其中 $D=\bigcup_{u \in \mathcal{U}} D_{u}$。为了运行随机梯度下降，对于每次更新，我们从一个随机子集 $\mathcal{U}^{\prime} \subset \mathcal{U}$ 中选择数据，并形成一个（虚拟）小批 $B=\bigcup_{u \in \mathcal{U}^{\prime}} D_{u}$ （在实践中，我们可能会说 $ | \mathcal{U}^{\prime} |=10^{4} $ 而 $|\mathcal{U}|=10^{7}$ ；我们可能只考虑每个用户本地数据集的一个子集）。小批损耗梯度 $\nabla \mathcal{L}_{f}(B, \Theta)$ 可以改写为跨用户的加权平均值： $\nabla \mathcal{L}_{f}(B, \Theta)=\frac{1}{|B|} \sum_{u \in \mathcal{U}^{\prime}} \delta_{u}^{t}$，其中 $\delta_{u}^{t}=\left|D_{u}\right| \nabla \mathcal{L}_{f}\left(D_{u}, \Theta^{t}\right)$。因此，用户可以只与服务器共享 $\left\langle |D_{u}|, \delta_{u}^{t}\right\rangle$ ，从中可以得到一个梯度下降步骤 $\Theta^{t+1} \leftarrow \Theta^{t}-\eta \frac{\sum_{u \in \mathcal{U}^{\prime}} \delta_{u}^{t}}{\sum_{u \in \mathcal{U}^{\prime}}\left|D_{u}\right|}$ 。</p><p>&emsp;&emsp;尽管每次更新 $\left\langle\left|D_{u}\right|, \delta_{u}^{t}\right\rangle$ 是短暂的，包含的信息比原始的 $D_{u}$ 少，用户可能仍然想知道还有什么信息。有证据表明，经过训练的神经网络参数有时允许重构训练示例[8,17,1]；参数更新会受到类似的攻击吗？例如，如果输入 $x$ 是编码最近键入的单词的单热词汇长度向量，那么对于每个单词 $w$，普通神经网络体系结构将在 $\Theta$ 中至少包含一个参数 $\theta_{w}$ ，使得 $\frac{\partial \mathcal{L}_{f}}{\partial \theta_{w}}$ 仅当 $x$ 编码 $w$ 时才非零。因此，通过检查 $\delta_{u}^{t}$ 的非零项，可以发现 $D_{u}$ 中最近输入的单词集。然而，服务器不需要检查任何单个用户的更新;它只需要 $\sum_{u \in \mathcal{U}}\left|D_{u}\right|$ 和 $\sum_{u \in \mathcal{U}} \delta_{u}^{t}$ 的和。使用安全聚合协议将确保服务器只知道 $\mathcal{U}$ 中的一个或多个用户写了单词 $w$ ，而不知道是哪个用户。</p><p>&emsp;&emsp;联邦学习系统面临着几个实际的挑战。移动设备只有零星的电源和网络连接，因此参与每个更新步骤的集合 $U$ 是不可预测的，系统必须对退出的用户具有健壮性。因为 $\Theta$ 可能包含数百万个参数，更新 $\delta_{u}^{t}$ 可能很大，代表了用户在计量网络计划中的直接成本。移动设备通常也不能与其他移动设备建立直接通信通道（依靠服务器或服务提供商调解这种通信），也不能对其他移动设备进行本机验证。因此，联邦学习激发了对安全聚合协议的需求：</p><ol><li>对高维向量进行操作，</li><li>通信高效，即使在每个实例化上都有一组新的用户，</li><li>对用户退出具有健壮性，</li><li>在服务器中介的、未经身份验证的网络模型的约束下提供最强的可能安全性。</li></ol><h1 id="实用的安全聚合协议"><a href="#实用的安全聚合协议" class="headerlink" title="实用的安全聚合协议"></a>实用的安全聚合协议</h1><p>&emsp;&emsp;在我们的协议中，有两种参与方:单个服务器 $S$ 和 $n$ 个用户 $\mathcal{U}$ 的集合。每个用户 $u \in \mathcal{U}$ 拥有一个维数为 $k$ 的私有向量 $x_{u}$ 。我们假设对于某个已知的 $R$ <a href="#1.1">点击跳转到该注释1</a>,  $x_{u}$ 和 $\sum_{u \in \mathcal{U}} x_{u}$ 的所有元素都是在 $[0, R)$ 范围内的整数。正确性要求，如果各方都诚实，$S$ 学习 $\bar{x}=\sum_{u \in \overline{ \mathcal{U} } } x_{u}$ 对于用户的某个子集 $\mathcal{U} \subseteq \mathcal{U}$ ，其中 $|\overline{\mathcal{U}}| \geq \frac{n}{2}$ 。安全性要求（1） $S$ 除了可以从 $\bar{x}$ 推断出的东西之外什么都不学习，（2）每个用户 $u \in \mathcal{U}$ 什么都不学习。我们考虑三种不同的威胁模型。在所有这些协议中，所有用户都诚实地遵守协议，但服务器可能试图通过不同的方式了解额外的信息<a href="#2.2">点击跳转到该注释2</a>：</p><ol><li>服务器是诚实的，但也很好奇，也就是说它诚实地遵循协议，但试图从它从用户那里接收到的消息中了解尽可能多的信息</li><li>服务器可以向用户谎报哪些其他用户退出了，包括不同用户之间不一致的退出报告</li><li>服务器可以谎报谁退出了（如2），也可以访问一些有限数量的用户的私有内存（这些用户自己诚实地遵守协议）。（在这种情况下，隐私要求只适用于其余用户的输入。）</li></ol><p><b>协议0：Masking with One-Time Pads：</b>我们通过一系列改进来开发我们的协议。我们首先假设所有各方都完成了协议，并拥有具有充足带宽的双向安全通信通道。每对用户首先就一对匹配的输入扰动达成一致，也就是说，用户 $u$ 为其他用户 $v$ 从 $[0,R)^{k}$ 中均匀抽样向量 $s_{u,v}$ 。用户 $u$ 和 $v$ 在他们的安全通道上交换 $s_{u,v}$ 和 $s_{v,u}$ ，并计算微扰 $p_{u,v} = s_{u,v} − s_{v,u} (mod R)$，注意到 $p_{u,v} = −p_{v,u} (mod R)$ ，当 $u = v$ 时取 $p_{u,v} = 0$。每个用户向服务器发送： $y_{u}=x_{u}+\sum_{v \in \mathcal{U}} p_{u, v}(\bmod R)$ 。服务器简单地对摄动值求和： $\bar{x}=\sum_{u \in \mathcal{U}} y_{u}(\bmod R)$ 。因为 $y_{u}$ 中的成对摄动抵消了:</p><script type="math/tex; mode=display">\bar{x}=\sum_{u \in \mathcal{U}} x_{u}+\sum_{u \in \mathcal{U}} \sum_{v \in \mathcal{U}} p_{u, v}=\sum_{u \in \mathcal{U}} x_{u}+\sum_{u \in \mathcal{U}} \sum_{v \in \mathcal{U}} s_{u, v}-\sum_{u \in \mathcal{U}} \sum_{v \in \mathcal{U}} s_{v, u}=\sum_{u \in \mathcal{U}} x_{u} \quad(\bmod R) .</script><p>协议0保证用户的完美隐私;因为用户添加的 $s_{u,v}$ 因子是统一抽样的，所以 $y_{u}$ 值对服务器来说是统一随机的，受限于 $\bar{x}=\sum_{u \in \mathcal{U}} y_{u}(\bmod R)$ 。事实上，即使服务器可以访问一些用户的内存，剩余的用户仍然保留隐私 <a href="#3.3">点击跳转到该注释3</a> 。</p><div id="1.1"><b>注释1：联邦学习更新 $\delta_{u} \in \mathbb{R}^{k}$ 可以通过剪切/缩放、线性变换和（随机）量化的组合映射到 $[0,R)^{k}$ 。</b></div><div id="2.2"><b>注释2：我们不分析针对任意恶意服务器和可能串通的用户的安全性。我们将这种情况和更正式的安全性分析推迟到完整版本。</b></div><div id="3.3"><b>注释3：一个更完整和正式的论证推迟到这篇论文的完整版本。</b></div><p><b>协议1：使用秘密共享的删除用户恢复：</b>不幸的是，协议0没有达到我们的几个设计标准，包括健壮性：如果任何用户 $u$ 没有通过向服务器发送 $y_{u}$ 来完成协议，那么结果和将被 $y_{u}$ 本应取消的扰动所掩盖。为了实现鲁棒性，我们首先在协议中添加一个初始轮，其中用户 $u$ 生成一个公共/私有密钥对，并在成对通道上广播公钥。所有未来从 $u$ 到 $v$ 的消息将由服务器进行中介，但使用 $v$ 的公钥加密，并由 $u$ 签名，模拟一个安全的经过身份验证的通道。这允许服务器对哪些用户成功通过了每一轮协议保持一致的视图。（这里我们暂时假设服务器忠实地在用户之间传递所有消息。）<br>&emsp;&emsp;我们还在 $s_{u,v}$ 值被选中后添加了用户之间的秘密共享轮。在这一轮中，每个用户使用 $(t,n)-$ 阈值方案 <a href="#4.4">点击跳转到该注释4</a> 计算每个摄动 $p_{u,v}$ 的 $n$ 份份额，例如Shamir的秘密共享[16]，对于某些 $t&gt;n/2$。对于每个秘密用户 $u$，她用每个用户 $v$ 的公钥加密一个共享，然后将所有这些共享发送到服务器。服务器从用户 $\mathcal{U}_{1} \subseteq \mathcal{U}$ 的子集中收集至少 $t$ 大小的共享（例如，等待一个固定的时间段），然后考虑所有其他用户被删除。服务器向每个用户 $v \in \mathcal{U}_{1}$ 提供为该用户加密的秘密共享；现在 $\mathcal{U}_{1}$ 中的所有用户都从接收到的共享集合中推断出幸存用户集 $\mathcal{U}_{1}$ 的一致视图。当用户计算 $y_{u}$ 时，她只包含那些与幸存用户相关的扰动；即 $y_{u}=x_{u}+\sum_{v \in \mathcal{U}_{1}} p_{u, v}(\bmod R)$ 。<br>&emsp;&emsp;在服务器从至少 $t$ 个用户 $\mathcal{U}_{2} \subseteq \mathcal{U}_{1}$ 接收到 $y_{u}$ 之后，它将继续进行新的解掩轮，并考虑将丢弃所有其他用户。从 $\mathcal{U}_{2}$ 中剩下的用户中，服务器请求由 $\mathcal{U}_{1} \backslash \mathcal{U}_{2}$ 中被删除的用户生成的所有秘密共享。只要是 $\left|\mathcal{U}_{2}\right|&gt;t$ ，每个用户都会用这些股份来响应。一旦服务器接收到至少 $t$ 个用户的份额，它就会重建对 $\mathcal{U}_{1} \backslash \mathcal{U}_{2}$ 的扰动，并计算聚合值：$\bar{x}=\sum_{u \in \mathcal{U}_{2}} y_{u}-\sum_{u \in \mathcal{U}_{2}} \sum_{v \in \mathcal{U}_{1} \backslash \mathcal{U}_{2}} p_{u, v} (mod R)$。只要至少 $t$ 个用户完成协议，就保证了 $\overline{\mathcal{U}}=\mathcal{U}_{2}$ 的正确性。在这种情况下，总和 $x$ 包含至少 $t &gt; n/2$ 个用户的值，所有的扰动都抵消了：</p><script type="math/tex; mode=display">\bar{x}=\left(\sum\_{u \in \mathcal{U}\_2} x_u+\sum\_{u \in \mathcal{U}\_2} \sum\_{v \in \mathcal{U}\_1} p_{u, v}\right)-\sum\_{u \in \mathcal{U}\_2} \sum\_{v \in \mathcal{U}\_1 \backslash \mathcal{U}\_2} p_{u, v}=\sum\_{u \in \mathcal{U}\_2} x_u+\sum\_{u \in \mathcal{U}\_2} \sum\_{v \in \mathcal{U}\_2} p_{u, v}=\sum\_{u \in \mathcal{U}\_2} x_u \quad(\bmod R)</script><p>&emsp;&emsp;然而，安全性已经丧失了：如果服务器错误地从 $\mathcal{U}_2$ 中遗漏了 $u$ ，无论是无意的（例如 $y_{u}$ 到达的稍微晚了一点）还是恶意的， $\mathcal{U}_2$ 中的诚实用户将向服务器提供所需的所有秘密共享，以消除 $y_{u}$ 中掩盖 $x_{u}$ 的所有扰动。这意味着即使面对诚实但好奇的服务器，我们也不能保证安全性（威胁模型T1）。</p><p><b>协议2：双重屏蔽阻止恶意服务器</b>为保证安全，我们引入了双重屏蔽结构，即使服务器可以重构 $u$ 的扰动，也可以保护 $x_{u}$ 。首先，在生成 $s_{u,v}$ 值的同一轮中，每个用户 $u$ 统一从 $[0,R)^k$ 中抽样一个额外的随机值 $b_{u}$。在秘密共享过程中，用户还生成 $b_{u}$ 的共享并将其分发给每个其他用户。在生成 $y_{u}$ 时，用户还添加了这个二次掩码：$y_u=x_u+b_u+\sum_{v \in \mathcal{U}_1} p_{u, v}(\bmod R)$ 。在解掩轮中，服务器必须对每个用户 $v \in \mathcal{U}_1$ 做出显式选择：从每个存活成员 $v \in \mathcal{U}_2$ 中，服务器可以请求与 $u$ 相关的 $p_{u、v}$ 摄动的份额，也可以请求与 $u$ 相关的 $b_{u}$ 的份额；诚实的用户 $v$ 只会在 $|\mathcal{U}_2| &gt; t$ 时才会响应，并且不会为同一用户显示两种类型的股份。在收集到所有 $u \in \mathcal{U}_1 / \mathcal{U}_2$ 的至少 $t$ 份 $p_{u,v}$ 和所有 $u \in \mathcal{U}_2$ 的 $b_{u}$ 的 $t$ 份 $b_{u}$ 后，服务器重新构造秘密，并计算聚合值：$\bar{x}=\sum_{u \in \mathcal{U}_2} y_u-\sum_{u \in \mathcal{U}_2} b_u-\sum_{u \in \mathcal{U}_2} \sum_{v \in \mathcal{U}_1 \backslash \mathcal{U}_2} p_{u, v}(\bmod R)$。</p><p>&emsp;&emsp;我们现在可以保证 $t &gt; n/2$ 在威胁模型T1中的安全性，因为 $x_{u}$ 总是被 $p_{u,v}$ s或总线掩盖。可以看出，在威胁模型T2和T3中，阈值必须相应地提高到 $2n / 3$ 和 $4n / 5$。我们将详细的分析，以及任意恶意和串通服务器和用户的情况，推迟到完整版本 <a href="#5.5">点击跳转到该注释5</a>。</p><div id="4.4"><b>注释4：一个 $(t,n)$ 的秘密共享方案允许将一个秘密分成 $n$ 份，这样 $t$ 份中的任何子集都足以恢复秘密，但如果小于 $t$ 份的任何子集，秘密仍然完全隐藏</b></div><div id="5.5"><b>注释5：安全参数涉及限制服务器可以通过伪造退出来恢复的共享数量</b></div><p><b>协议3：有效地交换秘密：</b>虽然协议2在选择正确的 $t$ 时是健壮和安全的，但它需要 $O(kn^2)$ 通信，我们在这个协议的改进中解决了这个问题。<br>&emsp;&emsp;可以观察到，通过使用单个秘密值作为加密安全伪随机生成器(PRG)的种子，可以将其扩展为一个伪随机值向量[2,9]。因此，我们可以生成标量种子 $s_{u,v}$ 和 $b_{u}$ ，并将它们展开为 $k$ 元素向量。然而，每个用户与其他用户有 $(n−1)$ 个秘密 $s_{u,v}$，并且必须发布所有这些秘密的共享。我们使用密钥协议来更有效地建立这些秘密。每个用户生成一个Diffie-Hellman密钥 $s^{SK}$ 和公钥 $s^{Pk}$。用户将自己的公钥发送给服务器（根据协议1进行认证）;然后，服务器向所有用户广播所有公钥，为自己保留一份副本。每对用户 $u,v$ 现在可以同意一个秘密 $s_{u,v} = s_{v,u} = AGREE(s_{u}^{SK}, s_{v}^{PK}) = AGREE(s_{v}^{SK}, s_{u}^{PK})$ 。为了构造摄动，我们假设 $\mathcal{U}$ 上有一个总序，当 $u<v$ 时取 $p_{u,v}="PRG(s_{u,v})$，当" $u> v$ 时取 $p_{u,v} = −PRG(s_{u,v})$，当 $u = v$ 时取 $p_{u,v} = 0$ （如前所述）。服务器现在只需要学习 $s_{u}^{SK}$ 来重构 $u$ 的所有扰动；因此你只需要在秘密分享过程中分配 $s_{u}^{SK}$ 和 $b_{u}$ 的股份。在不同的威胁模型中，协议3的安全性可以证明与协议2的安全性基本相同。</v$></p><p><img src="/2022/09/09/practical-secure-aggregation-for-federated-learning-on-user-held-data-yue-du-bi-ji/2.png" alt="图1：协议4通信图"><br><b>协议4：实践中的最小信任值：</b>协议3不能实际部署到移动设备上，因为它们缺乏双向安全通信和身份验证。我们建议通过用服务器中介的密钥协议取代协议1中描述的公钥/私钥交换来引导通信协议，其中每个用户生成一个Diffie-Hellman密钥 $c^{SK}$ 和公钥 $c^{PK}$，并将后者与 $s^{PK}$ <a href="#7.7">点击跳转到该注释7</a>一起发布。我们立即注意到，服务器现在可能进行中间人攻击，但认为这是可以容忍的，原因有几个。首先，对于缺乏身份验证机制或预先存在的公钥基础设施的用户来说，这基本上是不可避免的。仅依靠引导轮的非恶意性也构成了信任的最小化:实现这一阶段的代码很小，可以公开审计，外包给可信的第三方，或通过提供远程认证功能的可信计算平台实现[7,6,18]。此外，该协议显著地提高了安全性（通过保护服务器不受主动恶意攻击），并提供了前向保密（在密钥交换后的任何时间损害服务器，对攻击者没有任何好处，即使所有数据和通信都已完全记录）。</p><p><img src="/2022/09/09/practical-secure-aggregation-for-federated-learning-on-user-held-data-yue-du-bi-ji/1.png" alt="表1：协议4成本总结（推导延后到全文）"></p><p>&emsp;&emsp;我们在表1中总结了该协议的性能。假设密钥协议公钥和加密的秘密共享是256位，并且用户的输入都在相同的范围 <a href="#8.8">点击跳转到该注释8</a> $[0,R_{U}−1]$，每个用户传输 $\frac{256(7 n-4)+k\left\lceil\log _2\left(n\left(R_U-1\right)+1\right)\right\rceil+n}{k\left\lceil\log _2 R_U\right\rceil}$ 的数据比发送原始向量多。</p><h1 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h1><p>&emsp;&emsp;在安全聚合的受限情况下，除一个用户外所有用户都有输入0，可以表示为用餐密码网络（DC-net），它通过使用输入的成对盲法提供匿名性[3,9]，允许不可追踪地学习每个用户的输入。近年来的研究对恶意用户[5]存在下的通信效率和运行进行了研究。但是，即使有一个用户过早终止，现有的协议也必须从头开始重新启动，这可能是非常昂贵的[13]。基于模相加的加密方案中的双向盲法已经被探索过，但现有方案对向量既不高效，对单个故障也不健壮[2,12]。其他方案（如基于Paillier密码系统的[15]）是非常昂贵的计算。</p><div id="6.6"><b>注释6：通过缓存拉格朗日系数，我们从 $O(t^{2} + nt)$ 中对齐的 $(t, n)-$Shamir共享中重构了 $n$ 个秘密<div id="7.7"><b>注释7：这可以看作是在每对用户之间引导一个SSL/TLS连接</b></div><div id="8.8"><b>注释8：取 $R = n(R_{U}−1)+ 1$，保证不溢出</b></div></b></div>]]></content>
      
      
      <categories>
          
          <category> 科研学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 联邦学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《Advances and Open Problems in Federated Learning》阅读笔记</title>
      <link href="/2022/09/06/advances-and-open-problems-in-federated-learning-yue-du-bi-ji/"/>
      <url>/2022/09/06/advances-and-open-problems-in-federated-learning-yue-du-bi-ji/</url>
      
        <content type="html"><![CDATA[<h1>3 efficiency_and_effectiveness</h1><p>  在本节中，我们会探索各种技术和开放性的问题，以解决使联邦学习更加高效和有效的挑战。其中包含了无数种可能的方法，包括：探索更好的优化算法；提供不同的模型给不同的客户端；让机器学习任务，例如：参数搜索、结构搜索和调试在联邦学习场景中更加容易；提高通信效率等等。<br>  解决这些目标的一个基本挑战就是non-IID（非独立同分布）数据的存在，因此，我们首先分析这个问题，并强调潜在的解决方案。</p><h2 id="3-1联邦学习中的Non-IID数据"><a class="header-anchor" href="#3-1联邦学习中的Non-IID数据">¶</a>3.1联邦学习中的Non-IID数据</h2><p>  虽然通常情况下IID的含义很清楚，但是数据non-IID则有很多种可能。在本节中，我们提供了一种对于non-IID数据的分类方法，对任何客户端分区的数据集可能出现的数据non-IID情况进行分类。最常见的独立但是非同分布的来源是每个客户端对应着一类特定的用户，一片地理区域，或者一段特定的时间。提出的分类法与数据漂移的概念有密切的关系[304, 327]，它研究训练集分布和测试集分布之间的差异。这里，我们仅考虑每个客户端上数据分布的差异。<br>  对于以下内容，我们假设一个监督任务有特征$x$和其对应的标签$y$。联邦学习的统计模型涉及到两个层面的采样： 第一层是对客户端 $i \sim Q$ 进行采样（在可用客户端上的数据分布），第二层是对客户端本地数据分布进行采样 $(x,y) \sim \mathcal{P}_i(x,y)$。<br>  当在联邦学习中提到数据non-IID时，我们通常指的是客户端$i$和客户端$j$所对应的 $\mathcal{P}_i$ 与 $\mathcal{P}_j$ 不同。然而，有一点需要我们特别注意的是：$\mathcal{Q}$ 和 $\mathcal{P_i}$ 是可能随着时间的推移而改变，从而导致另一维度上的non-IID。<br>  为了完整起见，我们注意到，即使对于单个设备上的数据，如果数据没有经过充分的随机打乱顺序，比如：按照时间排列，那么本地数据的独立性也无法保证。一个简单的例子：视频中连续的帧是高度相关的。客户端内部相关性的来源通常可以通过在本地随机打乱顺序来解决。<br><b>非同分布的客户端分布：</b> 根据Hsieh et al.[205]，我们首先研究了数据偏离同分布的一些常见方式，即对于不用的客户端 $i$ 和客户端 $j$ 的分布不同 $P_i \not= P_j$。我们将 $P_i(x,y)$ 重写为 $P_i(y|x)P_i(x)$ 和 $P_i(x|y)P_i(y)$ 让我们能够更加准确地描述他们的区别。<br><I>特征分布倾斜（协变量飘移）：</I>即使共享 $\mathcal{P}(y|x)$ ，不同客户端上的边缘分布 $\mathcal{P}_i(x)$ 也可能不同 $^4$ 。比如，在手写识别领域，用户在书写同一个单词时也可能有着不同的笔画宽度、斜度等。<br><I>标签分布倾斜（先验概率飘移）：</I>即使 $\mathcal{P}(x|y)$ 是相同的，对于不同客户端上的边缘分布 $\mathcal{P}_i(y)$ 也可能不同。比如，当客户端与特定的地理区域绑定时，标签的分布在不同的客户端上是不同的。比如：袋鼠只在澳大利亚或动物园里；一个人的脸只在出现在全球的几个地方；对于手机设备的键盘，某些特定人群使用某些表情，而其他人不使用。<br><I>标签相同，特征不同（概念飘移）：</I>即使共享 $\mathcal{P}(y)$ ，不同客户端上的条件分布 $P_i(x|y)$ 也可能是不同。由于文化差异，天气影响，生活水平等因素，对于相同的标签 $y$ ，对于不同的客户端可能对应着差异非常大的特征$x$。比如：世界各地的家庭图片千差万别，衣着也千差万别。即使在美国，冬季停放的被大雪覆盖汽车的图像只会出现在某些地区。同样的品牌在不同的时间和不同的时间尺度上看起来也会有很大的不同：白天和晚上、季节效应、自然灾害、时尚设计潮流等等。<br><I>特征相同，标签不同（概念飘移）：</I>即使 $\mathcal{P}(X)$ 是相同的，对于不同客户端上的条件分布 $P_i(y |x)$ 也可能不同。由于个人偏好，训练数据项中的相同特征向量可能具有不同的标签。例如，反映情绪或单词联想的标签有着个人和地区差异。<br><I>数量倾斜或者不平衡：</I>不同的客户可以拥有着样本数量差异很大的数据。</p><p>  在现实世界中，联邦数据集可能同时包含多个上述影响，同时如何去刻画现实世界中的不同客户端之间的数据集的分布是一个重要的开放性问题。大多数关于合成的non-IID数据集的实证工作(例如[289])都集中在标签分布倾斜上，在这种情况下，non-IID数据集是通过基于标签划分现有数据集的“平面”而形成的。为了更好地理解真实世界的non-IID数据集的性质，我们允许构建受控的但真实的non-IID数据集，用于测试算法和评估它们对不同程度的客户端异构的恢复力。<br>  此外，对于不同的non-IID分布可能需要制定不同的缓解策略。例如：在特征分布倾斜的情况下，因为 $\mathcal{P}(y|x)$ 被假设是共同的，这个问题至少在理论上是很清楚的，训练一个全局模型去学习 $\mathcal{P}(y|x)$ 将是合适的。当同一个特征在不同的客户端上被映射到不同的标签上时，某种形式的个性化（详见3.3）可能对学习真正的标签函数很重要。</p><p><b>违反独立性：</b> 在训练过程中，只要概率分布$\mathcal{Q}$发生变化，就会其导致违反独立性。举一个具有代表性的例子：在跨设备联邦学习中，设备通常需要满足特定的要求才能够参与训练（详见1.1.2）。设备通常在本地的夜间时间满足这些要求（当它们大概率在充电、使用免费wi-fi和空闲时），因此设备可用性可能存在明显的昼夜不同。更进一步的，因为当地的时间直接对应着经度，因此数据的来源就存在着非常大的地理偏见。Eichner等人[151]描述了这个问题和一些缓解策略，但是仍然有许多问题是待解决的。<br><b>数据集飘移：</b> 最后，我们注意到了分布 $\mathcal{Q}$ 和 $\mathcal{P}$ 对于时间的依赖性可能引入传统意义上的数据集偏移（训练集和测试集的分布不同）。此外，其他的条件可能会使有资格训练联合模型的客户端集合与模型被部署的客户端集合不同。例如，训练比推理可能要求设备拥有更大的内存。这些问题将在第6节被更深入的探讨。采取技术来解决数据集飘移对于联邦学习来说是另一个有趣的开放性问题。</p><h3 id="3-1-1处理Non-IID数据的策略"><a class="header-anchor" href="#3-1-1处理Non-IID数据的策略">¶</a>3.1.1处理Non-IID数据的策略</h3><p>  联邦学习的最初目标是在所有客户端数据集的并集上训练单个全局模型，而non-IID的数据则使其变得更加困难。一个自然的方法就是修改现有的算法（例如：通过不同的参数选取）或者探索一种新的方法更高效地达到这个目标。本节的3.2.2将讨论这方法。<br>  对于某些应用程序，可能可以增加数据以使不同客户端的数据更加相似。一种方法是创建一个可以全局共享的小数据集。这个数据集可能来自一个公开可用的代表性数据源，一个不涉及隐私敏感的独立于客户数据的数据集，或者可能是原始数据的蒸馏结果（参考Wang等人[404]）。<br>  客户目标函数的异构性使得如何构建目标函数的问题变得更加重要——现在已经不清楚平等地对待所有的样本是否是有意义的。替代方案包括：限制任何一个用户的数据贡献（这对隐私也很重要，见第4节），并在客户端之间引入其他公平概念；参见第6节中的讨论。<br>  但是，如果我们能够在每个设备上的本地数据上运行训练（这对于全局模型的联合学习是必要的），那么训练单个全局模型是否是正确的目标呢？在许多情况下，使用单个模型是首选地，例如：为了向没有数据的客户端提供模型，或者在为了在部署之前允许进行人工验证和质量确认。然而，由于本地训练是可能的，因此每个客户都有一个定制的模型是可行的。这种方法可以把non-IID问题从一个bug变成一个特性，几乎是字面上的意思，即因为每个客户端都有自己的模型，客户端能够独立地参数化模型，看起来有些病态但缺让non-IID变得不那么重要。例如：对每一个 $i$，$\mathcal{P}_i(y)$ 只支持一个标签，那么找到一个高精度的全局模型可能是非常具有挑战性的（特别是当 $x$ 的信息相对不足时），但是训练一个高精度的局部模型是微不足道的（只需要一个持续的预测）。这种多模型方法将在第3.3节中深入讨论。除了解决非独立的客户端分布之外，使用多个模型还可以解决由于客户端可用性变化而导致的违背独立性的问题。例如，Eichner等人[151]的方法运行单个训练，但对不同的迭代进行平均，并基于时区/经度为客户端的推断提供不同的模型。</p><h2 id="3-2-联邦学习的优化算法"><a class="header-anchor" href="#3-2-联邦学习的优化算法">¶</a>3.2 联邦学习的优化算法</h2><p>  在典型的联邦学习任务中，目标是学习单个全局模型，该模型最小化整个训练数据集上的经验风险函数，训练集数据为所有客户端数据的并集。联邦优化算法和标准分布式训练方法之间的主要区别是需要处理表格 1中的特征——对于优化需要特别关注：non-IID和不平衡的数据、有限的通信带宽、不可靠和有限的可用设备。<br>  当联邦学习在设备的总数非常庞大时（如：跨移动设备），算法每轮只需要一些客户端参与（客户端采样）。此外，每个设备都可能多次参加训练给定的模型，因此算法应当是不状态依赖的。这就排除了直接应用在数据中心上下文中非常有效的各种方法，例如：ADMM之类的有状态优化算法，以及根据前几轮遗留的压缩错误修正更新的有状态压缩策略。<br>  联邦学习算法的在现实中另一个重要考虑是与其他技术的可组合性。优化算法并非在生产部署中独立运行，而是需要与其他技术结合使用，如：第4.2.1节中的加密聚合协议、第4.2.2节的差分隐私(DP)和3.5节中的模型和更新压缩。如第1.1.2节所述，这些技术中有许多可以应用于基本类型，如“对选定的客户机求和”和“向选定的客户机广播”，以这些基本形式表达的优化算法提供了一个有价值的关注点分割，但也可能排除某些技术，例如：通过异步更新。<br>  联邦学习最常用的优化方法之一是联邦平均算法[289]，它适用于本地更新或并行的SGD。在这里，每个客户机在本地运行一些SGD步骤，然后对更新后的本地模型求平均值，以在协同服务器上形成更新的全局模型。伪代码在算法1中给出。<br>  执行本地更新并减少与中央服务器的通信频率，解决了在面对数据位置限制和移动设备客户机有限通信能力情况下的核心挑战。然而，从优化理论的角度来看，这类算法也带来了一些新的算法挑战。在第3.2节中，我们分别讨论了数据跨客户端分布IID和non-IID情况下，联邦优化算法的最新进展和面临的挑战。开发专门针对联邦学习场景特征的新算法仍然是一个重要的开放问题。<br><img src="/2022/09/06/advances-and-open-problems-in-federated-learning-yue-du-bi-ji/1.png" alt="联邦平均算法"></p><h3 id="3-2-1-优化算法和IID数据集的收敛率"><a class="header-anchor" href="#3-2-1-优化算法和IID数据集的收敛率">¶</a>3.2.1 优化算法和IID数据集的收敛率</h3><p>  虽然可以对正在优化的每个客户机函数作出各种不同的假设，但最基本的划分是假设IID和non-IID数据。在形式上，在客户端上具有IID数据意味着，用于客户端本地更新的每个mini-batch数据在统计上都与整个训练数据集（所有客户端上本地数据集的并集）的均匀抽取样本（有放回）相同。由于客户独立地收集他们自己的训练数据，这些数据在大小和分布上都有所不同，而且这些数据不与其他客户或中心节点共享，因此IID的假设在实践中几乎不可能成立。但是，这个假设极大地简化了联邦优化算法的理论收敛性分析，并给出了了一个基准线，可以用来理解non-IID数据对优化率的影响。因此，第一步自然是了解IID数据情况下的优化算法。<br>  在形式上，IID的设定让我们能够标准定义随机优化问题<br>$$<br>\min _{x \in \mathbb{R}} F(x):=\underset{z \sim P}{\mathbb{E}}[f(x ; z)]<br>$$</p><p>  我们假设一个间歇性的通信模型，如Woodworth等人[411，第4.4节]，其中$M$个无状态客户端参与每一轮T，在每一轮中，每个客户端可以计算 $K$ 个样本（如mini-batch）的梯度 $z_1,…,z_K$ 从 $\mathcal{P}$ 中IID采样得到（可能使用这些来进行连续的步骤）。在IID数据的假设中，客户端是可互换的，我们可以不失一般性地假设 $M=N$。表格4中，总结了本节中的符号。<br>  对 $f$ 的不同假设会产生不同的保证。我们将首先讨论凸设置，然后观察非凸问题的结果。<br><b>凸问题的基准线和最高水准：</b> 本节中，我们总结了 $H$-平滑的收敛结果，凸（但不一定是强凸）函数的假设下的方差随机梯度的边界为 $\sigma^2$ 。更正式地说，$H$-平滑意味着对于所有 $z,f(\cdot;z)$ 都是可微的，并且具有H-Lipschitz梯度，也就是说，对于任意 $x,y$ 的<br>$$<br>|\nabla f(x, z)-\nabla f(y, z)| \leq L|x-y|<br>$$<br>我们还假设对于所有的 $x$，随机梯度 $\bigtriangledown_x f(x;z)$ 满足：<br>$$<br>\underset{z \sim P}{\mathbb{E}}|\nabla x f(x ; z)-\nabla F(x)| \leq \sigma^{2}<br>$$<br>当求算法在 $T$ 次迭代后输出 $x_T$ 的收敛率时，我们考虑公式：<br>$$<br>\mathbb{E}\left[F\left(x_{T}\right)\right]-F\left(x^{<em>}\right)<br>$$<br>其中 $x^</em>=min_xF(x)$ 。这里讨论的所有收敛速度都是这一项的上界。 表5给出了这些函数的收敛结果的汇总。<br>  联邦平均法（又称并行SGD/本地SGD）自然地需要和两个基准线进行对比：首先，我们可以在每一轮本地更新中固定 $x$，并计算当前$x$总的 $KM$ 梯度，以加速的mini-batch数据SGD的运行。令 $\bar{x}$ 表示该算法 $T$ 次迭代的平均值。对于凸问题[256, 119, 132]，我们可以得到上界：<br>$$<br>\mathcal{O}\left(\frac{H}{T^{2}}+\frac{\sigma}{\sqrt{T K M}}\right)<br>$$<br>请注意，在训练过程中我们也需要将$z$的随机性考虑到第一项期望的计算中。<br>  第二个自然的基准线是仅考虑所有 $M$ 个活动客户端中的1个客户端，这允许(加速的)SGD连续执行 $KT$ 步。结合上述相同的一般界限，此方法提供的上界为：<br>$$<br>\mathcal{O}\left(\frac{H}{(T K)^{2}}+\frac{\sigma}{\sqrt{T K}}\right)<br>$$<br>  比较这两个结果，我们可以看到mini-batch数据SGD达到了最佳的“统计”项 $(\sigma / \sqrt{T K M})$ ，而对于单客户端SGD（忽略其他设备的更更新）获得了最佳的“优化”项 $(H/ \sqrt{(HK)^2})$ 。<br>  局部更新梯度下降方法的收敛性分析是当前研究的一个非常活跃领域[370,271,428,399,334,318,233]。本地更新梯度下降的第一个收敛结果最早可以追溯到Stich[370]对于强凸目标函数和Yu等人[428]对于非凸目标函数的有界梯度范数假设。这些分析使用次优优化项从而达到目标的 $\sigma/ \sqrt{TKM}$ 的统计项（表5总结了凸函数的中间条件的结果）。<br><img src="/2022/09/06/advances-and-open-problems-in-federated-learning-yue-du-bi-ji/2.png" alt="凸函数的中间部分总结结果"></p><p>  Wang和Joshi[399]、Stich和Karimireddy[371]去除了有界梯度的假设，进一步将优化项改进为 $HM/T$ 。结果表明，当局部步数 $K$ 小于 $T/M^3$ 时，最优统计项占主导地位。然而，对于一般的跨设备应用程序，我们可能有 $T=10^6$ 和 $M=100$ (表2)，这意味着 $K = 1$。<br>  在文献中，收敛边界经常伴随着关于可以选择多大的 $K$ 以渐近地达到与mini-batch数据SGD收敛速度相同的统计项的讨论。对于强凸函数，Khaled等人[233]改进了这个边界，Stich和Karimireddy[371]进一步改进了这个边界。<br>  对于非凸目标，Yu等[428]表明，局部更新 $K$ 小于 $T^{1/3}/M$ 时，局部梯度下降可以达到 $1/ \sqrt{TKM}$ 的渐近误差边界。Wang和Joshi [399]进一步改进了收敛性保证，他们删除了有界梯度范数假设，并表明本地更新的数量可以大到 $T/M^3$。[399]中的分析也可以应用于具有局部更新的其他算法，从而第一个为具有局部更新的去中心的SGD（或周期性分散SGD）和弹性平均SGD提供收敛保证[432]。Haddadpour等[191]改进了Wang和Joshi [399]中要求Polyak-Lojasiewicz（PL）的条件[226]的函数的界线，这是强凸性质的推广。Haddadpour等[191]研究表明，对于PL函数，每轮 $T^2/M$ 的局部更新可以达到  $\mathcal{O}(1/TKM)$ 收敛。<br>  尽管以上工作着眼于随着迭代迭代错误率的收敛情况，然而从业者最关注时间上的收敛速度。评估时必须根据通信和本地计算的相对成本，考虑参数的设计对每次迭代所花费时间的影响。从这个角度来看，在保持统计速率的同时关注于可以使用的最大$K$可能不是联邦学习中的主要关注点，在联邦学习中，人们可能会假设几乎是无限的数据集（非常大的N）。增加 $M$ 的成本（至少在时间方面）很小，因此，适当地增加$M$以匹配优化项，然后调整 $K$ 来最大化时间性能可能更自然。那么如何选择 $K$ ？增加客户端在聚合之前的本地迭代的更新次数，本地模型被平均时的差异也会随之增加。这就会导致，在训练损失方面的误差收敛相比顺序SGD的$TK$步迭代更慢。然而，执行更多的本地更新可以节省大量的通信成本并减少每次迭代所花费的时间。最优的局部更新次数在这两种现象之间取得了平衡，实现了最快的误差和时钟时间的收敛。Wang和Joshi[400]提出了一种自适应的通信策略，该策略根据训练过程中的训练损失，在一定的时间间隔内对$K$进行调整。<br>  联邦学习中的另一个重要设计参数是模型聚合方法，该方法用于使用选定客户端进行的更新来更新全局模型。在最初的联邦学习论文中，McMahan等人[289]建议对本地模型进行加权平均，与本地数据集的大小成比例。对于IID数据，假定每个客户端都有一个无限大的数据集，这可以简化为对本地模型进行简单的平均。但是，尚不清楚此聚合方法是否为最快的错误收敛方法。<br>  即使在使用IID数据的情况下，联邦优化中还有许多悬而未决的问题。<br>  Woodworth等人[411]强调了与联邦学习设置的优化上限和下限之间的一些差距，特别是对于“间隔通信图”，它包含了本地SGD方法，但是不清楚这种方法的收敛速率和对应的下限。在表5中，我们展示了对于凸设定下的收敛结果。虽然大多数方案都能够达到渐近显性统计项，但没有一个方案能够与加速mini-batch SGD的收敛速度相匹配。联邦平均算法是否能够拉近这个距离仍然是一个问题。<br>  所有 $M$ 个客户端执行相同数量的本地更新的本地更新SGD方法可能会遇到一个常见的可伸缩性问题，即如果任何一个客户端意外地速度慢或失败，则它们就可能会成为瓶颈。可以使用多种方法来解决此问题，但尚不清楚哪种方法是最佳的，尤其是在考虑到潜在的偏差时（请参见第6节）。Bonawitz等[74]建议为客户提供过多的资源（例如，向130万个客户请求更新），然后接受收到的前$M$个更新，并拒绝后续掉队者的消息。稍微复杂一点的解决方案是固定一个时间窗口，并允许客户端在此期间尽可能多地进行本地更新$K_i$轮，然后由中央服务器平均其模型。解决客户端掉队者问题的另一种方法是在 $\tau$ 处固定本地更新的数量，但允许客户端以同步或无锁方式更新全局模型。尽管一些先前的工作[432，267，143]提出了类似的方法，但是误差收敛分析是一个开放且具有挑战性的问题。但是，联邦学习环境中的一个更大挑战是，从第3.2节开始讨论起，异步方法可能会变得难以与差异性隐私或安全聚合之类的互补技术结合起来。<br>  除了本地更新的数量外，每次训练选择的客户群大小的选择与本地更新的数量存在类似的折中点。更新并平均更大数量的客户端可以让每个训练回合的模型产生更好的收敛性，但是由于与客户端进行的计算/通信中的不可预测的尾部延迟，使得训练速度容易受到下降。<br>  在non-IID设定中对本地SGD/联邦平均计算的分析更具挑战性，在下一章中将讨论与其相关的结果和未解决的问题，以及直接解决non-IID问题的专用算法。</p><h3 id="3-2-2-优化算法和IID数据集的收敛率"><a class="header-anchor" href="#3-2-2-优化算法和IID数据集的收敛率">¶</a>3.2.2 优化算法和IID数据集的收敛率</h3><p>  相比中心学习中经过充分随机而得到的独立且同分布的（IID）样本，联邦学习的样本使用来自最终用户设备的本地数据，从而产生了多种non-IID数据（第3.1节）。<br>  在这种假设下，对于 $N$ 个客户端都拥有自己的本地数据分布 $\mathcal{P}<em>i$ 和本地目标函数：<br>$$<br>f</em>{i}(x)=\underset{z \sim \mathcal{P}<em>{i}}{\mathbb{E}}[f(x ; z)]<br>$$<br>其中 $f(x;z)$ 为模型 $x$ 对于样本 $z$ 的损失。我们通常希望最小化：<br>$$<br>F(x)=\frac{1}{N} \sum</em>{i=1}^{N} f_{i}(x)<br>$$<br>请注意，当 $\mathcal{P}_i$ 是同分布的时候，这就沦落为IID的设定。我们定义 $F^$ 为 $F$ 的的最小值，此时观测值为 $x^$。类似的，我们使用 $f_i^*$ 代表 $f_i$ 的最小值。<br>  像在IID设定中一样，我们假设采用间歇性通信模型（例如Woodworth等人[411,第4.4节]），其中 $M$ 个无状态客户参与$T$轮更新，并且在每个轮次中，每个客户可以计算 $K$ 个样本（例如 mini-batches）的梯度。不同之处在于，样本 $z{i,1},…,z{i,K}$ 由第 $i$ 个客户端的本地数据分布 $\mathcal{P}_i$ 采样而出。与IID设定不同，我们不必假定 $M=N$ ，因为客户端分布并不完全相等。在下文中，如果算法假定 $M=N$ ，我们将忽略 $M$ 并地写作 $N$ 。我们注意到，尽管这样的假设可能与表1中的跨数据孤岛的联邦假设兼容，但在跨设备的假设中通常是不可行的。<br>  尽管[370,428,399,371]主要针对IID假设，但可以通过对数据差异添加假设，例如通过限制客户端梯度与全局梯度[266,261,265,401]之间或客户端之间和全局最优值的差异[264,232]，将分析技巧推广到非IID场景。在这种假设下，Yu等 [429]表明，在non-IID情况下，本地SGD的错误边界变得更糟。为了达到 $1/\sqrt{TKN}$ 的比率（在非凸目标下），本地更新数 $K$ 应该小于 $T^{1/3}/N$ ，而不是像IID情况下的 $T/N^3$ [399]。Li等[261]提出在每个局部目标函数中添加一个近似项，以使该算法对局部目标之间的异质性更加鲁棒。所提出的FedProx算法从经验上提高了联邦平均的性能。但是，目前尚不清楚是否可以证明提高收敛速度。Khaled等[232]假设所有客户都参与，并在客户上使用批量梯度下降，这可能比客户上的随机梯度更快地收敛。<br>  最近，许多工作在放宽所需的假设方面取得了进展，以便更好地应用于联邦平均的实际使用。例如，Lietal[264]研究了在更现实的环境中联邦平均的收敛性，在每一轮中仅涉及一部分客户。为了保证收敛，他们假设选择的客户是随机的，或者是与本地数据集的大小成正比的概率。但是，在实践中，服务器可能无法使用这些理想的方式对客户端进行采样，特别是在跨设备设置中，只有满足资格要求的设备（例如：充电、空闲、免费无线上网）才会被选择参与计算。在一天的不同时间，客户的特征会明显不同。 Eichner等人[151]提出了这个问题，研究了半周期SGD的收敛性，其中从多个具有不同特征的客户区域中按照规则的周期性模式（例如，昼夜）进行采样。<br><img src="/2022/09/06/advances-and-open-problems-in-federated-learning-yue-du-bi-ji/3.png" alt="Non-IID假设"><br><img src="/2022/09/06/advances-and-open-problems-in-federated-learning-yue-du-bi-ji/4.png" alt="其他假设和变量"><br><img src="/2022/09/06/advances-and-open-problems-in-federated-learning-yue-du-bi-ji/5.png" alt="收敛率"><br><b>表6：</b>非iid数据假设下联邦学习中优化方法的收敛性。我们总结了对非iid数据、每个客户机上的本地函数和其他假设的关键假设。我们还介绍了该算法与联邦平均算法的比较，以及消除常数的收敛速度。<br>  我们在表6中总结了最新的理论结果。表6中的所有方法均假定客户端的局部函数具有平滑度或Lipschitz梯度。误差范围由凸函数的最优目标（1）和非凸函数的梯度范数来衡量。对于每种方法，我们展示关键的non-IID假设，每个客户端函数 $f_i(x)$ 的假设以及其他辅助假设。我们还简要地将每种方法描述为联邦平均算法的一种变体，和显示地简化收敛速率消除常数。假设客户端功能是强凸的，则可以提高收敛速度[264,227]。 当客户使用随机局部更新[266,264,265,401,227]时，经常使用有界梯度方差，这是分析随机梯度方法的一种广泛使用的假设。Li等 [264]直接分析联邦平均算法，该算法在每轮随机抽样的 $M$ 个客户端上执行 $K$ 个步骤的本地更新，并提出了本地更新（ $K &gt; 1$）可能减慢收敛速度的速率。证明 $K &gt; 1$ 可能会损害或帮助收敛的领域是一个重要的开放问题。<br><b>与去中心优化的联系：</b>近年来，在去中心优化社区中研究了联邦优化的目标函数。如Wang和Joshi [399]所示，去中心SGD的收敛分析可以与本地SGD结合使用，也可以与网络拓扑矩阵（混合矩阵）的设定适当结合使用。为了减少通信开销，Wang和Joshi [399]提出了周期性去中心SGD（PD-SGD），它允许去中心SGD使用多次本地更新作为联邦平均。Li[265]等人将此算法进行了推广到了non-IID情况。 MATCHA [401]通过随机采样客户端进行计算和通信进一步提高了PD-SGD的性能，并提供了一种收敛分析，表明本地更新可以加速收敛。<br><b>加速和方差减少技术：</b>对于一阶优化方法，动量和方差减少是改善优化和泛化性能的有效的方法。但是，关于如何将动量或减少方差的技术应用于本地SGD和联邦平均，仍未达成共识。SCAFFOLD [227]用控制变量显式地模拟客户端更新中的差异以执行方差减少，这可以快速收敛而不会限制客户端数据分布的差异。至于动量方案，Yu等[429]建议让每个客户保持一个局部动量缓冲区，并在每个通信回合中平均这些局部缓冲区以及局部模型参数。尽管此方法从经验上提高了本地SGD的最终准确性，但它需要两倍的通信成本。Wang等[402]提出了另一种称为SlowMo的动量方案，该方案可以显着提高本地SGD的优化和泛化性能，而无需牺牲吞吐量。Hsu等[206]提出了一种类似于SlowMo的动量方案。 [429,402]均证明了局部SGD的动量变体可以以与同步mini-batch SGD以相同的速率收敛到非凸目标函数的平稳点，但要证明动量能加快联邦假设下的收敛速度是充满挑战性的。</p><h2 id="3-3-多任务学习，个性化和元学习"><a class="header-anchor" href="#3-3-多任务学习，个性化和元学习">¶</a>3.3 多任务学习，个性化和元学习</h2><p>  在本节中，我们考虑各种“多模型”方法——对于不同的客户端在推断的时候可以高效地使用不同的模型。当面对non-IID数据（第3.1节）时，这些技术尤其重要，因为它们可能优于潜在的全局共享最优模型。我们注意到，个性化已经在完全去中心的设定下也得到了一定的研究[392,54,431,22]，在这种情况下，训练个体模型尤为自然。</p><h3 id="3-3-1-通过特征个性化"><a class="header-anchor" href="#3-3-1-通过特征个性化">¶</a>3.3.1 通过特征个性化</h3><p>  本节的其余部分专门考虑了不同用户在使用不同模型参数（权重）进行运行推断时的技术需求。但是，在某些应用程序中，只需将用户和上下文功能添加到模型中，即可获得相近的收益。例如，考虑一下Hard等人[196]中用于移动键盘中下一个单词预测的语言模型。不同的客户端可能使用不同的语言，实际上，模型参数的设备上个性化已为该问题带来了显着改善[403]。但是，一种更加完善的方法可能是训练一个联邦模型，该模型不仅要输入到目前为止用户输入的单词，还要输入各种其他用户和上下文特征作为输入，例如：该用户经常使用哪些单词？ 他们当前正在使用什么应用程序？ 如果他们正在聊天，他们之前曾向此人发送过哪些消息？ 适当地加以个性化，这样的输入可以允许共享的全局模型产生更好的个性化预测。 但是，由于很大程度上很少有公共数据集包含此类辅助功能，因此探索如何有效合并不同任务上下文信息的模型结构仍然是一个重要的开放问题，有可能极大地提高联邦学习训练的模型的实用性。</p><h3 id="3-3-2-多任务学习"><a class="header-anchor" href="#3-3-2-多任务学习">¶</a>3.3.2 多任务学习</h3><p>  如果人们将每个客户的本地问题（本地数据集上的学习问题）视为一项单独的任务（而不是单个数据集的一个划分），那么多任务学习[433]的技术将立即变得有意义。值得注意的是，史密斯等[362]引入了用于多任务联合学习的MOCHA算法，直接解决了通信效率、掉队者和容错的挑战。在多任务学习中，训练过程的结果是每个任务得到一个模型。 因此，大多数多任务学习算法都假设所有客户（任务）都参与每个训练周期，并且由于每个客户都在训练一个单独的模型，因此也要求客户有自己的状态。 这使得此类技术与数据孤岛联邦学习应用相关性更高，但在跨设备方案中更难应用。<br>  另一种方法是重新考虑客户（本地数据集）和学习任务（待训练的模型）之间的关系，对于每个客户端观察单个模型和全局模型的共同点。例如，可能可以应用来自多任务学习的技术（以及其他方法，如个性化，将在接下来进行讨论），其中我们将“任务”作为客户端的子集，也许是显示选择的（例如，基于地理区域与设备或者用户的特征），或者可能基于在客户端上学习到的聚类或学习到的图的连接结构[431]。这些算法的发展是一个重要的开放问题。请参阅第4.4.4节，讨论关于稀疏的联邦学习问题（例如，在这种类型的多任务问题中自然会产生的问题）的解决方式，而不必揭示每个客户端所属的客户端子集（任务）。</p><h3 id="3-3-3-本地微调和元学习"><a class="header-anchor" href="#3-3-3-本地微调和元学习">¶</a>3.3.3 本地微调和元学习</h3><p>  本地微调，我们指的是通过联邦学习训练单个模型，然后将模型部署到所有的客户端中，并在被用于推断前使用本地的数据集通过额外的训练达到个性化的效果。 这种方法自然地融入了联邦学习模型的通常的生命周期（第1.1.1节）。仍然可以在每轮（例如，100秒）中仅使用少量客户样本进行全球模型的培训；部署模型后，仅发生一次向所有客户端（例如数百万个）广播全局模型。唯一的区别是，在使用模型对客户进行实时预测之前，会进行最终的训练，从而将模型为本地数据集进行个性化。<br>  给定一的性能优异的全局模型，对其进行个性化设置的最佳方法是什么？在非联邦学习中，研究人员经常使用微调、迁移学习、域自适应[284,115,56]或者使用本地个性化的模型进行插值。 当然，例如插值等技术，关键在于联邦学习的背景下保证其相应的学习效果。此外，这些技术通常仅假设一对域（源域和目标域），因此可能会丢失联邦学习的一些较丰富的结构。<br>  另一种研究个性化和非个性化的方法是通过元学习来进行，这是一种流行的模型适应设定。 在标准的learning-to-learn（LTL）设置中[52]，它对任务上具有一个元分布，用来学习一个学习算法的样本，例如通过发现参数空间的好的约束。 这实际上很好的对应了第3.1节中讨论的统计设定，其中我们对客户端（任务） $i\sim \mathcal{Q}$ 进行采样，然后从$\mathcal{P_i}$采样该客户端（任务）的数据。<br>  最近，已经开发了一种称为模型不可知元学习（MAML）的算法，即元学习全局模型，它可以仅使用几次局部梯度迭代作为学习适合于给定任务的良好模型的起点。 最值得注意的是，流行的Reptile算法[308]的训练阶段与联邦平均[289]密切相关，即Reptile允许服务器的学习率，并且假设所有客户端都拥有相同数量的数据，但其他都是相同的。Khodaketal等人[234]和Jiang等人[217]探索了FL和MAML之间的联系，并展示了MAML的假设是一个可以被联邦学习用于性化模型的相关框架。其他和差分隐私的关系在[260]中被研究。<br>  将FL和MAML的思想相结合的总体方向是相对较新的，存在许多未解决的问题：</p><ol><li>监督任务的MAML算法评估主要集中在合成图像分类问题上[252,331]，其中可以通过对图像类别进行下采样来构造无限的人工任务。用于模拟FL实验的现有数据集建模的FL问题（附录A）可以作为MAML算法的现实基准问题。</li><li>观察到的全局准确性与个性化准确性之间的差距[217]提出了一个很好的论据，即个性化对于FL至关重要。但是，现有的工作都没有清楚地阐明用于衡量个性化表现的综合指标。例如，对于每个客户来说，小的改进是否比对一部分客户的更大改进更好？相关讨论，请参见第6节。</li><li>Jiang等[217]强调了一个事实，即具有相同结构和性能但经过不同训练的模型可以具有非常不同的个性化能力。尤其是，以最大化全局性能为目标去训模型似乎实际上可能会损害模型的后续个性化能力。理解这个问题的根本原因和FL社区与更大的ML社区都相关。</li><li>在此多任务/LTL框架中，已经开始研究包括个性化和隐私在内的几个具有挑战性的FL命题[234,217,260]。是否还可以通过这种方式分析其他例如概念漂移的问题，比如作为终身学习中的问题[359]？</li><li>非参数传递LTL算法（例如ProtoNets [363]）是否可以用于FL？</li></ol><h3 id="3-3-4-何时进行全局FL训练更好"><a class="header-anchor" href="#3-3-4-何时进行全局FL训练更好">¶</a>3.3.4 何时进行全局FL训练更好</h3><p>  哪些是联邦学习可以为你做，而在一个设备上进行本地学习是做不了的？当本地数据集很小且数据为IID时，FL显然具有优势，实际上，联邦学习[420,196,98]的应用实际受益于跨设备训练单个模型。另一方面，给non-IID的分布的类型（例如，$\mathcal{P}<em>{i}(y|x)$ 跨客户端是完全不同的），则局部模型会更好。因此，一个自然的理论上的问题是确定在什么条件下共享全局模型比独立每设备模型更好。假设我们为每个客户机 $k$ 训练一个模型 $h_k$，使用该客户机可用的大小为 $m_k$ 的样本。我们能保证 $h</em>{FL}$ 通过联邦学习学习到的模型在用于客户 $k$ 时至少和 $h_k$ 一样准确吗?我们能量化通过联邦学习可以期望多大的改进吗?我们能否在理论上保证至少与自然基线（ $h_k$ 和 $h_{FL}$ ）的表现相匹配的个性化策略?<br>  其中一些问题与先前在多源域适应和不可知联合学习方面的工作有关[284,285,203,303]。这些问题的难易程度取决于各方之间的数据分配方式。例如，如果数据是垂直切分的，则每一方都维护有关公共实体的不同功能集的私有记录，则这些问题可能需要解决联邦学习任务中的记录链接[108]。独立于私下进行记录链接的最终技术要求[348]，该任务本身在现实世界中恰好有很大的噪声倾向[347]，只有很少的结果讨论了它对训练模型的影响[198]。可以在有监督的学习中使用损失分解技巧来缓解垂直划分假设本身，但实际的好处取决于数据的分布和参与方的数量[320]。</p><h2 id="3-4-使用于联邦学习的ML工作流"><a class="header-anchor" href="#3-4-使用于联邦学习的ML工作流">¶</a>3.4 使用于联邦学习的ML工作流</h2><p>  在将标准机器学习的工作流和流水线（包括数据扩充、功能工程、神经网络结构设计、模型选择、超参数优化和调试）适应去中心数据集和资源受限的移动设备时，会遇到许多挑战。我们在将下面讨论其中一些挑战。</p><h3 id="3-4-1-超参数调整"><a class="header-anchor" href="#3-4-1-超参数调整">¶</a>3.4.1 超参数调整</h3><p>  在资源有限的移动设备上使用不同的超参数进行多轮培训可能会受到限制。对于小型设备，这可能导致过度使用有限的通信和计算资源。但是，最近的深度神经网络在很大程度上依赖于有关神经网络的结构、正则化和优化的超参数选择。对于大型模型和大规模设备上的数据集，评估可能会很昂贵。在AutoML [339,237,241]的框架下，超参数优化（HPO）历史悠久，但它主要涉及如何提高模型的准确性[59,364,321,159]，而不是针对移动设备的通信和计算效率。因此，我们期望在联邦学习的背景下，进一步的研究应考虑研发解决方案，以实现高效地超参数优化。<br>  除了通用方法来解决超参数优化问题外，对于特殊的训练空间去针对性地去发展容易调整的优化算法也是一个主要的开放领域。中心式训练已经需要调整学习率、动量、批量大小和正则化等参数。联邦学习可能会添加更多的超参数，如：分别调整聚合/全局模型更新规则和本地客户端优化程序、每轮选择的客户端数量、每轮本地步骤的数量、更新压缩算法的配置等等。除了更高维度的搜索空间之外，联邦学习通常还需要更长的训练时间并受限于有限的计算资源。应该通过对超参数设置具有鲁棒性的优化算法（相同的超参数值适用于许多不同的现实世界数据集和网络结构）以及自适应或自调整[381,75]算法来解决这一挑战。</p><h3 id="3-4-2-神经结构设计"><a class="header-anchor" href="#3-4-2-神经结构设计">¶</a>3.4.2 神经结构设计</h3><p>  我们建议研究人员和工程师在联邦学习环境中探索神经体系结构搜索（NAS）。这是由于当前使用预定的深度学习模型的方法的缺陷引起的：当用户生成的数据对模型开发人员不可见时，深度学习模型的预定网络结构可能不是最佳的设计选择。例如，神经体系结构可能具有特定数据集的某些冗余组件，这可能导致设备上不必要的计算。对于non-IID数据分布，可能会有更好的网络体系结构设计。第3.3节中讨论的个性化方法仍在所有客户端之间共享相同的模型架构。NAS的最新进展[332,154,333,55,322,273,417,154,279]提供了解决这些缺陷的潜在方法。NAS有三种主要方法，它们利用进化算法、强化学习或梯度下降来搜索特定数据集上特定任务的最佳架构。其中，基于梯度的方法利用权重共享的高效梯度反向传播，将架构搜索过程从超过3000个GPU一天减少到只用1个GPU一天。最近发表的另一篇有趣的论文涉及权重不可知神经网络[170]，声称仅神经网络架构，无需学习任何权重参数，就可以为给定任务提供编码解决方案。如果该技术进一步发展并得到广泛使用，则可以将其应用于联邦学习而无需在设备之间进行协作训练。尽管尚未针对分布式设定（例如联帮学习）开发这些方法，但将它们全部转换为联邦设定都是可行的。因此，我们认为在联邦学习环境中针对全局或个性化模型的神经体系结构搜索（NAS）是有希望的研究方向。</p><h3 id="3-4-3-联邦学习的调试和可解释性"><a class="header-anchor" href="#3-4-3-联邦学习的调试和可解释性">¶</a>3.4.3 联邦学习的调试和可解释性</h3><p>  尽管联邦模型联邦训练取得了实质性进展，但这完全是ML工作流的一部分。经验丰富的建模人员可以直接检查子数据集的任务，包括基本的健全性检查、调试错误分类\发现异常值\手动标记样本或检测训练集中的偏差。开发隐私保护技术来解决此类去中心的问题是主要的开放性问题上。最近，Augensteinetal[32]提出了使用经过联帮学习训练的差分生成模型（包括GAN）的使用，以回答此类问题。但是，仍然存在许多悬而未决的问题（请参见[32]中的讨论），特别是改进FL DP生成模型的精确度的算法的开发。</p><h2 id="3-5-通信和压缩"><a class="header-anchor" href="#3-5-通信和压缩">¶</a>3.5 通信和压缩</h2><p>  现在，众所周知，通信可能是联邦学习的主要瓶颈，因为无线连接和其他最终用户互联网连接的运行速率通常低于数据中心内或数据中心间连接的速率，并且可能昂贵且不可靠。这引起了最近对减少联邦学习的通信带宽的极大兴趣。联邦平均和模型更新的量化和量化到少量比特的方法已经证明，通信成本显着降低，并且对训练精度的影响最小[245]。但是，尚不清楚是否可以进一步降低通信成本，以及这些方法中的任何一种或其组合是否可以接近在联邦学习中的通信和准确性之间提供最佳的折衷。描述这种精确性和通信量之间的基本平衡是理论统计学最近的研究热点[434,81,195,11,47,380]。这些工作描述了在通信约束下用于分布式统计估计和学习的最佳最小极大速率。然而，由于这些理论工作通常忽略了优化算法的影响，因此很难在实践中从这些理论工作中得出具体的结论来减少通信带宽。利用这种统计方法来指导实际的训练方法仍然是一个开放的方向。<br><b>压缩目标：</b> 由于当前设备中计算机、内存和通信资源的限制，有几个不同的具有实用价值的压缩目标如下：</p><ol><li>梯度压缩，减少从客户端到服务器通信的对象的大小，该对象用于更新全局模型；</li><li>模型广播压缩，减小从服务器向客户端广播的模型的大小，客户端从该模型开始本地训练；</li><li>减少本地计算，修改整体训练算法，使本地训练过程在计算上更加高效。</li></ol><p>  这些目标在大多数情况下是互补的。其中，(1)在总运行时间方面具有最大的实际影响潜力。这是因为客户端连接的上传速度通常比下载速度慢，因此与(2)相比，可以获得更多的带宽，也因为在许多客户端上平均的效果可以实现更积极的有损压缩方案。通常，(3)与(1)和(2)通过特定的方法共同实现。<br>  许多现有的文献适用于目标(1)[245, 376, 244, 20, 204]。(2)对一般收敛性的影响直到最近才得到研究，在[231]中有一个有限的分析。Caldas等人[87]提出了一种联合处理所有(1)、(2)和(3)的方法，该方法通过约束所需的模型更新，使得只有模型变量特定的元素需要在客户端被使用。<br>  在跨设备FL中，算法通常不能假设客户端上保留了任何状态（表1）。但是，在跨数据孤岛FL设置中通常不存在这种约束，在跨设备FL设置中，相同的客户端重复参与。因此，一些更广泛的关于错误修正的思想，如[272,346,396,380,228,371]在这种情况下是相关的，其中许多可以同时处理(1)和(2)。<br>  另一个目标是修改训练程序，以使最终模型更紧凑或更有效地进行推理。这个主题在更大的ML社区中得到了很多关注[194,120,436,270,312]，但是这些方法或者没有直接对应到联邦学习，或者使训练过程更加复杂，这使得它变得很难采纳。同时产生一个紧凑的最终模型的研究，也同时解决了上述三个目标，具有产生实际影响的巨大潜力。<br>  对于梯度压缩，依据最小的最大感知量出现了一些现有的工作[376]，以表征最坏的情况。然而，通常在信息论中，压缩保证是特定于实例的，并取决于基础分布的熵[122]。换句话说，如果数据易于压缩，则可以证明它们被大量压缩。 有趣的是，是否可以为梯度压缩获得类似的实例特定结果。同样，最近的工作表明，以数据相关的方式学习压缩方案可以显着提高数据压缩[412]和梯度压缩的压缩率。因此，值得在联邦设定中评估这些与数据相关的压缩方案[171]。<br><b>差分隐私和安全聚合的兼容：</b> 联邦学习中使用的许多算法，例如安全聚合[72]和使噪声变淡以实现差分隐私[7，290]的机制，都没有设计用于压缩或量化通信。例如，Bonawitz等人的Secure Aggregation协议的直接应用。 [73]要求每个标量有一个额外的 $O(logM)$ 通信位，其中 $M$ 是要累加的客户端数，这可能会使$M$大时更新的主动量化无效（尽管更有效的方法请参见[75]）。现有的噪声添加机制假定在每个客户端上添加实值高斯或拉普拉斯噪声，这与用于减少通信的标准量化方法不兼容。我们注意到，最近的一些工作允许有偏估计，并且可以很好地与Laplacian噪声[371]一起使用，但是无论如何都不会放弃差分隐私，因为它们在两轮之间具有独立性。在增加离散噪声方面有一些工作[13]，但目前还不清楚这些方法是否最佳。 因此，联邦设定下具有兼容性和安全性的压缩方法是一个有价值的开放问题。<br><b>无线联邦学习协同设计：</b> 联邦学习中的现有文献通常忽略了模型训练期间无线通道动态的影响，这有可能破坏训练周期，从而破坏整个生产系统的可靠性。特别是，无线干扰，嘈杂的信道和信道波动会严重阻碍服务器与客户端之间的信息交换（或直接在单个客户端之间进行信息交换，如在完全分散的情况下，请参阅第2.1节）。对于任务关键应用程序而言，这是一项主要挑战，其根源在于减少延迟和增强可靠性。应对这一挑战的潜在解决方案包括联邦蒸馏（FD），其中工人交换它们的模型输出参数（logit）而不是模型参数（梯度和/或权重），并通过适当的通信和计算资源来优化工人的调度策略[ 215、316、344]。另一种解决方案是利用无线信道的独特特性（例如广播和叠加）作为自然的数据聚合器，其中，不同工作人员同时传输的模拟波会叠加在服务器上，并由无线信道进行系数权衡[8]。这样可以在服务器上更快地进行模型聚合，并且可以将训练速度加速因子提高到参与者的数量。这与传统的正交频分复用（OFDM）范式形成鲜明对比，在传统的正交频分复用（OFDM）范式中，工人在正交频率上上传其模型，而正交频率的性能会随着参与数量的增加而降低。</p><h2 id="3-6-应用到更多类型的机器学习问题和模型"><a class="header-anchor" href="#3-6-应用到更多类型的机器学习问题和模型">¶</a>3.6 应用到更多类型的机器学习问题和模型</h2><p>  迄今为止，联邦学习主要考虑了监督学习任务，其中每个客户都自然可以获得标签。 将FL扩展到其他ML范式，包括强化学习、半监督和无监督学习、主动学习和在线学习[200，435]都提出了有趣且开放的挑战。<br>  与FL高度相关的另一类重要模型是可以表征预测不确定性的模型。大多数现代深度学习模型无法表示其不确定性，也无法对参数学习进行概率解释。这推动了贝叶斯模型与深度学习相结合的工具和技术的最新发展。从概率论的角度来看，使用单点估计进行分类是不合理的。贝叶斯神经网络[358]已经被提出并显示出对过度拟合更为健壮，并且可以轻松地从小型数据集中学习。贝叶斯方法通过其参数以概率分布的形式进一步提供不确定性估计，从而防止过度拟合。此外，借助概率推理，人们可以预测不确定性如何减小，从而使网络做出的决策随着数据大小的增长变得更加准确。<br>  由于贝叶斯方法相比深度模型在置信度上拥有丰富的经验，并且在许多任务上也能达到最先进的性能，因此人们希望贝叶斯方法能够为经典的联邦学习提供理论上的改进。实际上，Lalitha等人的初步工作[254]表明，合并贝叶斯方法可用于跨non-IID数据和异构平台的模型聚合。但是，必须解决有关可伸缩性和计算可行性的诸多问题。</p><h1>4 privacy</h1><p>  机器学习过程由许多功能不同的角色参与运作。例如，用户可以通过与设备交互来生成训练数据，在机器学习训练过程中，其从这些数据中提取人机交互模式（例如，以训练后模型参数的形式），之后机器学习工程师或分析师可以评估该训练模型的质量，最后可能将该模型部署在最终用户主机上，以支持定制的用户体验（见下图1）。<br>  在一个理想的世界里，系统中的每一个参与者只会学到扮演他们角色所需的信息。例如，如果分析师只需要确定某个特定的质量度量是否超过了所需的阈值，以便授权将模型部署到最终用户，那么在理想化的世界中，该度量值是分析师可以获得的唯一信息；该分析师既不需要访问训练数据，也不需要访问模型参数。类似地，最终用户体验到的可能只需要由经过训练的模型提供的预测，而不需要其他任何内容。<br>  此外，在理想的世界中，系统中的每个参与者都能够轻松、准确地推断出自己和他人的哪些个人信息可能通过参与系统而泄露，参与者将能够利用这一推理结果，就是否参与以及如何参与做出明智的选择。<br>  创建一个具有上述所有理想隐私属性的系统本身将是一项令人望而生畏的壮举，若其还可以实现其他令人满意的属性，则更是难上加难，比如所有参与者的易用性、最终用户体验的质量和公平性（以及影响体验结果的模型），智能地使用通信和计算资源、抵御攻击和失败的能力等。<br>  与其追求无法企及的完美，我们不如另辟蹊径——整个系统由模块化单元组成，这些单元可以相对独立地进行学习和改进，同时我们也要注意，我们最终必须根据上述我们的理想隐私目标，测量整个系统的隐私属性。本节中，我们将提出目前无论是通过单个模块实现方法还是通过整个系统实现方法，还不了解如何同时实现所有目标的领域，作为开放性研究问题。<br><img src="/2022/09/06/advances-and-open-problems-in-federated-learning-yue-du-bi-ji/6.png" alt="图1：fl训练的模型的生命周期和联邦学习系统中的各种参与者。(重复第7页)"><br>  联邦学习提供了一个有吸引力的结构，可以将整个机器学习工作流程分解成我们想要的可实现的模块单元。联邦学习模型的一个主要优点是它可以通过数据最小化为参与的用户提供一定程度的隐私：设备从不发送原始用户数据，只将对模型的更新（例如梯度更新）发送到中央服务器。这些模型更新更侧重于要完成的学习任务而非关注原始数据（即，与原始数据相比，它们严格不包含关于用户的附加信息，而且通常不包含其他意义），并且单个更新只需要由服务器暂时保存。<br>  虽然这些特性可以在集中所有训练数据的基础上提供显著的实用性隐私改进，但是在这个基线联邦学习模型中仍然没有隐私的正式保证。例如，可以构造这样的场景，在该场景中，原始数据的信息从客户端泄漏到服务器。比如，知道以前的模型和用户的梯度更新将允许某一方推断该用户持有的训练示例。因此，本节调查现有的结果，并概述了设计可以提供严格的隐私保障的联邦学习系统的开放性的挑战。我们更专注于联邦学习和分析设置中特定的问题，而不考虑了在一般的机器学习设置中也会出现的问题。<br>  除了针对用户隐私的攻击之外，还有其他种类的对联邦学习的攻击；例如，对方可能会试图阻止模型被学习，或者他们可能会试图使模型产生偏向对方的训练结果。我们稍后在第5章讨论这些类型的攻击的讨论。<br>  本章的其余小节梗概如下。第4.1节讨论了我们希望提供抵御的各种威胁模型。第4.2节列出了一套核心工具和技术，可用于针对第4.1节中讨论的威胁模型提供严格的保护。第4.3节假定可信服务器的存在，并讨论在对抗对手和/或分析师提供保护方面的公开问题和挑战。第4.4节讨论了在没有完全可信服务器的情况下的开放性问题和挑战。最后，第4.5节讨论了关于用户感知的开放性问题。</p><h2 id="4-1-参与者，威胁模型与深层隐私"><a class="header-anchor" href="#4-1-参与者，威胁模型与深层隐私">¶</a>4.1 参与者，威胁模型与深层隐私</h2><p>  在联邦学习中，对隐私风险的规范处理需要一种整体的、跨学科的方法。对于一些风险类型，可以通过将现有技术扩展到指定场景中从而保护隐私和减轻风险，而其他更复杂的风险类型则需要跨学科的协同努力。<br>  隐私不是二进制量，甚至不是标量。这种规范处理的第一步是准确描述不同的参与者（见第1节图1，为方便起见，在第35页重复）及其在模型中扮演的角色，最终确定相关的威胁模型（见表7）。例如，我们希望将服务器管理员的视图与使用所学模型的分析师的视图区分开来，因为可以想象，设计用于针对恶意分析师提供强大隐私保证的系统可能不会提供任何恶意攻击的机会。这些参与者行为模式很好映射到其他文献中讨论的威胁模型上；例如，在Bittau等人中。[67，第3.1节]，“编码器”对应于客户机，“洗牌者”通常对应于服务器，“分析器”可能对应于服务器或分析师完成的后处理。<br>  例如，一个特定的系统可能提供差异性的隐私保证（差异性隐私将在4.2.2节规范化介绍，这里知道更小的ε表示更强的隐私性即可），比如向服务器管理员提供的特定参数为ε，而分析师观察到的参数结果可能具有更高的保护性ε’&lt;ε。<br>  此外，这一保证可能仅适用于能力受到特定限制的对手，例如可以观察服务器上发生的所有事情（但不能影响服务器的行为）的对手，这类对手同时完全控制占客户端总数比例为γ的客户端（“完全控制”即可以观察他们可获知的所有数据并以任意方式影响其行为）；对手也可能被认为无法破解在特定安全级别σ下，实例化的加密机制。为了对抗实力突破这些限制的对手，在服务器管理员的看来可能仍然需要差异性隐私保证，但认为分析师观察的结果在较弱的隐私保护级别ε’&gt;ε。<br>  正如我们在本例中看到的，精确地指定系统的假设和隐私目标，以及诸如差异性隐私保证、老实但好奇行为等安全性概念，可以很容易地通过到几个参数（ε、ε’、ε0、γ、σ等）进行具体实例化。<br>  实现联邦学习所需的所有隐私属性通常需要将下述的许多工具和技术组合到端到端系统中，包括以下两种：多种分层策略都是为了保护系统的同一部分（例如，在可信执行环境（TEE）中运行安全多方计算（MPC）协议的一部分，使对手更难对该组件产生足够大的损害）以及使用不同的策略来保护系统中不同的部分（例如，使用MPC保护模型更新的聚合，然后在服务器之外共享聚合更新之前使用隐私披露技术）。<br>  因此，在两种技术都无法提供其预期隐私保护能力的情况下，我们提倡构建这样一种优美的联邦系统，即尽可能降低隐私性。例如，在TEE中运行MPC协议的服务器组件可能允许维护隐私，即使TEE安全性或MPC安全性假设中的一个（但不是两个）在实践中不成立。另一个例子是，要求客户端向服务器端TEE发送渐变更新，而要求客户端将原始训练示例发送到服务器端TEE将被强烈不推荐。因为一旦TEE的安全性失效，前者的隐私性期望将更优美地降级。我们将这种优美降级的原则称为“深度隐私”，类似于成熟的深度防御网络安全原则[311]。</p><h2 id="4-2-工具与技术"><a class="header-anchor" href="#4-2-工具与技术">¶</a>4.2 工具与技术</h2><p>  一般来说，联邦学习计算的目的是让分析师或工程师通过计算请求获得结果，这可以看作是对分布式客户机数据集上的函数f的评估（通常是机器学习模型训练算法，但可能更简单，例如基本的数据统计）。有三个隐私方面需要解决。<br>  首先，我们需要考虑f是如何计算的，以及在这个过程中中间结果的信息流是什么，它主要影响对恶意客户端、服务器和管理参与者的敏感性。除了设计系统中的信息流（例如提前数据最小化）外，包括安全多方计算（MPC）和可信执行环境（TEEs）等安全计算相关技术对于解决这些问题特别重要。这些技术将在第4.2.1节中详细讨论。<br>  其次，我们必须考虑该计算哪些内容。换言之，f本身的计算结果会向分析师和域内参与者透露了参与客户的多少信息。这与隐私保护披露技术，特别是差异隐私（DP）是高度相关的，将在第4.2.2节中详细讨论。<br>  最后，可验证性也是需要考虑的问题，即客户机或服务器能够向系统中的其他人证明他们已忠实地运行了所需的指令，而不泄露他们运行过程中的潜在隐私数据。验证技术，包括远程认证和零知识证明，将在第4.2.3节中讨论。</p><h3 id="4-2-1-安全计算"><a class="header-anchor" href="#4-2-1-安全计算">¶</a>4.2.1 安全计算</h3><p>  安全计算的目标是评估计算分散输入的函数，通过判断其是否仅向预期各方显示计算结果，而不显示任何附加信息（例如各方的输入或任何中间结果）。<br><b>安全多方计算：</b>安全多方计算（MPC）是密码学的一个子领域，与这样一个问题有关：一组参与方计算其隐私输入通过共识函数得到输出，从而可以只向每个参与方显示期望的输出。这一领域在20世纪80年代由姚[422]开创。由于理论和工程上的突破，该领域已经从单纯的理论研究转向工业上的部署技术[71、70、257、29、169、209、210]。值得注意的是，MPC定义了一组技术，应该更多地被视为安全计算中的领域或安全性的一般概念，而不是技术本身。MPC的一些最新进展可以归因于低级原语的突破，例如不经意传输协议[211]和具有同态性质的加密方案（如下所述）。</p><table border="1"><caption align="top"><b>表4-1 不同的敌对参与者的多种威胁模型<b></b></b></caption><tr><td><b>数据/访问节点</b></td><td><b>参与者</b></td><td><b>危胁模型</b></td></tr><tr><td>客户端</td><td>通过系统设计或破坏设备获得客户端设备的最高访问权限者</td><td>恶意客户端可以检查所参与轮次从服务器接收的全部消息（包括模型迭代），并可以篡改训练过程。老实但好奇的客户端可以检查从服务器接收的所有消息，但不能篡改培训过程。在某些情况下，安全包围/TEEs等技术可能会限制此类攻击者的影响和信息可见性，从而削弱该模型威胁程度。</td></tr><tr><td>服务器</td><td>通过系统设计或破坏设备获得服务器设备的最高访问权限者</td><td>恶意服务器可以检查所有轮次发送到服务器的全部消息（包括梯度更新），并可以篡改训练过程。老实但好奇的客户端可以检查发送到服务器的所有消息，但不能篡改培训过程。在某些情况下，安全包围/TEEs等技术可能会限制此类攻击者的影响和信息可见性，从而削弱该模型威胁程度。</td></tr><tr><td>输出模型</td><td>工程师与分析师</td><td>恶意分析师或模型工程师可以访问系统的多组输出，例如，使用不同超参数的多个训练运行的模型迭代序列。该向这类参与者发布什么信息是一个重要的系统设计问题。</td></tr><tr><td>部署模型</td><td>其他设备</td><td>在跨设备联邦学习场景下，最终模型可能部署到数亿个设备上。访问部分受损的设备可能仍满足黑盒模型，而访问完全受损的设备可以认为是白盒模型。</td></tr></table><p>  密码学解决方案的共同点是，操作通常在一个有限的字段上完成（即，素数p都是整数），这在表示实数时会带来困难。一种常见的方法是调整机器学习模型及其训练程序，即通过标准量化操作并依赖精心设计的量化模式，以确保下（上）溢量在可控范围[172、14、182、77]。<br>  即使在恶意对手面前，任何函数都可以安全计算[183]这一点在几十年间达成了共识。虽然通用解决方案存在，但它们的性能特征常常使它们在实际设置中不适用。因此，研究显著趋势是线性和逻辑斯蒂回归[309，172，302]和神经网络训练和推理[302，14，46]等应用设计定制协议。这些协议通常在孤井互通的设置中进行，或是将计算委托给一组不相互协作的计算服务器的变体模型。将这些协议移植到跨设备设置并不简单，因为它们需要大量的通信。<br>  同态加密 同态加密（Homomorphic encryption）方案允许在密文上直接执行某些数学运算，而无需事先解密。同态加密通过使参与者计算函数值，同时保持值隐藏，是一个使MPC成为可能的强大工具。<br>  从一般的全同态加密（FHE）〔176〕到更高水平的变体[79, 160, 80，112 ]，同态加密存在多种实现[3, 350, 4 ]。一些称为部分同态的方案同样具有实际意义，例如包括ElGamal和Paillier，允许同态加法或乘法。加性同态加密称为孤井互通设置中MPC协议的一种成分[309 198]。文献[345]调研了一些同态加密软件库，并简要说明了选择库时应考虑的标准/特性。</p><table border="1"><caption align="top"><b>表4-2 不同技术及其特性描述<b></b></b></caption><tr><td><b>技术</b></td><td><b>特性描述</b></td></tr><tr><td>差异隐私（本地、中心、混编、聚合、混合模型）</td><td>从包含用户的数据集的输出分析中可以了解到的个人信息量。具有差分隐私的算法必然包含一定数量的随机性或噪声，可以对其进行调整以掩盖用户对输出的影响。</td></tr><tr><td>安全多方计算</td><td>两个或多个参与者协作，通过密码学模拟完全可信的第三方，第三方满足：   •计算所有参与者提供的输入的函数；   •向选定的部分参与者显示计算结果，同时任一方没有进一步学习。</td></tr><tr><td>同态加密</td><td>允许一方在不具有纯文本访问权限的情况下，不解密密文下对密文执行数学运算，从而计算出它们的数据的函数。尽管计算成本更高，任意复杂度的数据函数都可以通过这种方式计算（“完全同态加密”）。</td></tr><tr><td>可信执行环境（安全环境）</td><td>可信执行环境提供了在远程计算机上可靠地运行代码的能力，即使不信任计算机的所有者/管理员。这是通过限制任何一方（包括管理员）的能力来实现的。尤其是，可信执行环境具有以下性质373]：   •一致性：除非程序显式发布消息，否则程序的执行状态始终不可见；   •完整性：除非程序显式接收输入，否则程序的执行不会受到影响；   •可测量/认证性：可信执行环境可以向远程方证明什么程序（二进制）正在执行，以及它的起始状态是什么，定义了一致性和完整性的初始条件。</td></tr></table><p>  考虑在联邦学习设置中使用同态加密，会遇到谁该持有该模式的密钥这一问题。虽然每个客户机加密其数据并将其发送到服务器端进行同态计算的想法很有吸引力，但服务器不应该能够解密单个客户机的提交数据。克服这一问题的一个简单方法是依赖一个持有密钥并解密计算结果的外部非合谋方。然而，大多数同态加密方案要求密钥经常更新（例如，由于易受选择密文攻击[102]）。此外，使用信任的非共谋方不在标准的联邦学习设置中。<br>  解决此问题的另一种方法是依赖于分布式（或阈值）加密方案，其中密钥在各方之间分发。Reyzin等人。[336]和Roth等人。[341]提出在跨设备设置中计算总和的这种解决方案。他们的协议使用了加性同态方案（分别是基于ElGamal和基于格的方案的变体）。<br><b>可信执行环境：</b>可信执行环境（TEEs，也称为安全环境）可以将联邦学习过程的一部分转移到云中的可信环境中，而该环境的代码可以被证明和验证。可信执行环境拥有几个关键性质，使他人相信，一段程序已被忠实而保密地执行[373]：</p><ol><li><b>一致性：</b>除非程序明确发布消息，否则程序的执行状态仍是保密的。</li><li><b>完整性：</b>除非程序显式地接收输入，否则程序的执行不会受到影响。</li><li><b>可测量/认证性：</b>可信执行环境可以向远程方证明什么程序（二进制）正在执行，以及它的起始状态是什么，定义了一致性和完整性的初始条件。</li></ol><p>  可信执行环境已经在实现不同体系结构上被，包括英特尔的SGX处理器[208，116]、ARM的TrustZone[2，1]和Sanctumon RISC-V[117]，它们在上述关键性质上的性能各不相同。<br>  当前的安全环境在内存方面受到限制，只提供对CPU资源的访问，即它们不允许在GPU或机器学习处理器上进行处理（Tram`er和Boneh[382]探索如何将可信执行环境与GPU结合起来进行机器学习推断）。此外，对于可信执行环境（特别是那些在共享微处理器上操作的可信执行环境）来说，完全排除所有类型的侧信道攻击也是一项挑战[391]。<br>  虽然安全环境为运行在其中的所有程序提供保护，但在实践中还必须解决其他问题。例如，通常有必要将运行在该环境中的代码构造为一个不受数据影响的过程，这样它的运行时和内存访问模式就不会显示它正在计算的数据的信息（参见示例[67]）。此外，可测量/证明性通常只证明某个特定的二进制文件正在运行；系统架构师需要提供一种方法来证明该二进制文件具有所需的隐私属性，这可能需要使用来自开源代码的复现过程来重构该二进制文件。<br>  如何在安全环境、云计算资源和客户端设备之间划分联邦学习功能仍然是一个悬而未决的问题。例如，安全环境可以执行安全聚合或混编等关键功能，以限制服务器对原始客户端上传内容的访问，同时此可信计算基础不涉及大多数联合学习逻辑。<br><b>受关注的安全计算问题：</b>虽然安全多方计算和可信执行环境为分布式隐私数据上的任何函数的保密计算问题提供了一般解决方案，但许多优化可以应用到某些特定功能中。下面描述的任务就是这种情况。<br>  <b>安全聚合</b> 安全聚合是指在n个客户端和一个服务器的场景中，允许每个客户端提交一个值（通常是联邦学习设置中的向量或张量），这样服务器只学习客户端值的一个聚合函数，通常是这些值的和。<br>  大量文献对单服务器设置（通过成对加法遮蔽[12，188，73]、通过阈值同态加密[356，193，92]、通过一般安全多方计算[86]）以及在多个非合谋服务器设置[71，29，113]中的安全聚合进行了探讨。也可以使用可信执行环境（如上所述）来实现安全聚合，如[269]。<br>  <b>安全混编</b> 安全混编是指在n个客户端和服务器的场景中，允许每个客户机提交一条或多条消息，这样服务器只从所有客户端学习一个无序的消息集合（multiset），而不需要更多。具体来说，除了消息本身包含的信息之外，服务器无法判断任一条消息的发送者。安全混编可以被视为安全聚合的一个实例，其中值是多集单例，聚合操作为多集求和，尽管通常情况下，为达到安全混编和安全聚合的最佳性能，典型操作制度提供了非常不同的实现<br>  在安全多方计算的背景下，安全混编通常是在混合网络的标题下进行研究[95 251]，也有在可信计算的背景下进行的研究[67]。混合网络中已经存在了以Tor网络[138]的形式进行的大规模部署。<br>  <b>隐私信息检索</b> 隐私信息检索（PIR）是服务器为客户端提供的功能。它使客户机能够从服务器托管的数据库中下载条目，这样服务器就不会获得客户机请求的条目的任何信息。<br>  MPC方法将PIR分为两大类：基于计算的PIR（cPIR），其中一方可以执行协议的整个服务器端[249]；基于信息论的PIR（itPIR），需要其中多个非共谋方执行协议的服务器端[106]。<br>  PIR适用性的主要障碍如下：cPIR具有非常高的计算成本[361]，而非共谋方设置难以在工业场景中有说服力地实现。最近关于PIR的研究结果表明，通过使用基于点阵的密码系统，计算成本显著降低[16，313，17，25，175]。[218]已经证明了在单个服务器上如何利用用户可用的边带信息构建高效通信PIR。最近的相关工作建议利用客户机本地状态来加速PIR。帕特尔等人[319]展示了如何获取后者边带信息，并在单个服务器上实现和验证了一个实用的混合（基于计算和信息论）PIR方案。Corrigan-Gibbs和Kogan[114]在一个离线/在线模型上提出了一种压线性的在线PIR协议，在该模型的离线阶段，客户端从服务器获取信息，这些请求独立于将来要执行的查询。<br>  过去的工作[410]，进一步探索了PIR和隐私共享之间的联系，最近[139]将编码数据联系到PIR中，并且已经建立了高效通信PIR[66]。PIR也在开-关隐私的背景下进行了研究，在这种背景下，客户可以关闭他们的隐私保护以换取更好的实用性或性能[306, 423]。</p><h3 id="4-2-2-隐私保护披露"><a class="header-anchor" href="#4-2-2-隐私保护披露">¶</a>4.2.2 隐私保护披露</h3><p>  量化和限制个人信息披露的最新模型是差异隐私（DP）[147，144，145]，其目的是在发布的模型中引入一定程度的不确定性，以充分掩盖任何个人用户的贡献。差异隐私由隐私损失参数 $(\varepsilon,\delta)$ 量化，其中较小的 $(\varepsilon,\delta)$ 对应于隐私性增强。更正式地说，对于所有 $S \subseteq \operatorname{Range}(A)$，以及所有相邻数据集 $D$ 和 $D’$，如果满足下式，则称随机化算法A是 $(\varepsilon,\delta)$-差异隐私的：<br>$$<br>P(A(D) \in S) \leq \operatorname{e\varepsilon } P\left(A\left(D^{\prime}\right) \in S\right)+\delta .(3)<br>$$<br>  在联邦学习的情景中， $D$ 和 $D’$ 对应于分散的数据集，如果 $D’$ 可以通过加上或减去单个客户机（用户）的所有记录而从 $D$ 获得，则这些数据集是相邻的[290]。这种差异隐私的概念被称为用户级差异隐私。它比通常使用的相邻概念强，其中$D$ 和 $D’$ 只相差一条记录[145]，因为通常一个用户可以向数据集贡献多条记录（例如训练集）。<br>  在过去的十年中，用于差异性私有数据分析的广泛技术已经得到发展，特别是在假设集中设置的情况下，在应用实现隐私所需的扰动之前，原始数据由可信方收集。在联邦学习中，通常编排服务器将充当DP机制的可信实现者，确保只将私有化的输出发布给模型工程师或分析师。<br>  然而，在可能的情况下，我们通常希望减少对可信方的需求。近年来，人们考虑了几种减少对数据管理员信任需求的方法。<br><b>本地差异隐私：</b> 通过让每个客户机在与服务器共享数据之前对其数据进行差异隐私转换，可以在不需要可信集中服务器的情况下实现差异隐私。也就是说，我们将公式（3）应用于处理单个用户的本地数据集D的机制A，并且保证对任何可能的其他本地数据集D’保持相同的性质。该模型被称为本地差异隐私模型（LDP）[406，229]。LDP已经被谷歌、苹果和微软有效地用于收集大型用户群中热门项目的统计数据[156，135，136]。它还被Snap[325]用于垃圾邮件分类训练的联邦设置中。这些LDP部署都涉及大量的客户机和表项，在Snap中甚至高达10亿，这与DP的集中实例化形成鲜明对比，而后者可以从更小的数据集中提供高实用性。不幸的是，正如我们将在第4.4.2节中讨论的那样，在保持效用的同时实现LDP是很困难的[229，388]。因此，需要一个介于完全中心和完全本地DP之间的差分隐私模型。这可以通过分布式差异隐私或混合模型来实现，如下所述。<br><b>分布式差异隐私：</b> 为了在不依赖可信的中心服务器的情况下恢复中心DP的一些实用性，可以使用分布式差分隐私模型[146、356、67、105]。在此模型下，客户机首先计算并编码一个最小（特定应用程序）的报告，然后将编码后的报告通过安全计算函数，该功能的输出可供中央服务器访问，从而在服务器能够检查时，此输出已经满足了不同的隐私要求。编码是为了帮助维护客户端的隐私，可以包括如LDP等隐私项。安全计算功能可以有多种体现。它可以是一个MPC协议，一个在TEE上完成的标准计算，甚至是两者的结合。每种选择都有不同的假设和威胁模型。<br>  必须指出的是，分布式差异隐私和本地差异隐私从多个角度得到了不同的保证：虽然分布式DP框架可以为与LDP相同级别的差异隐私生成更准确的统计数据，但它依赖于不同的设置，并且通常会做出更有力的假设，例如作为对MPC协议的访问。下面，我们概述了两种可能的分布式差异隐私方法，依赖于安全聚合和安全混编，尽管我们强调还有许多其他方法可以使用。<br>  通过安全聚合实现分布式DP。在第4.2.1节中讨论过，在FL中，安全聚合是实现分布式DP的一种的工具。安全聚合可用于确保中心服务器获得聚合结果，同时确保单个设备和参与者的中间参数不会透露给中心服务器。为了进一步确保聚合结果不会向服务器显示附加信息，我们可以使用本地差异隐私（例如，中等ε级别）。例如，每个设备可以在安全聚合之前扰动其自身的模型参数，以实现本地差异隐私。通过正确设计噪声，我们可以确保聚合结果中的噪声与可信服务器（例如，低ε/高隐私级别）集中添加的噪声匹配[12，330，181，356，188]。<br>  通过安全混编实现分布DP。另一个分布式差异隐私模型是混编模型，它由最近引入的混编分析编码（ESA）框架[67]启动（如图3所示）。在这个框架的最简单版本中，每个客户端在其数据上运行一个LDP协议（例如，具有中等级的ε），并将其输出提供给一个安全的混编器。混编器随机排列报告表项，并将混编后报告的集合（没有任何标识信息）发送到服务器进行最终分析。直观地说，此安全计算功能的介入使得服务器更难了解参与者的任何信息，并支持差异隐私分析（例如，低ε/高隐私级别）。在更一般的多消息混编框架中，每个用户可以向混编器发送多个消息。混编器独立于服务器并专门用于混编，可以直接作为一个受信任的实体实现，也可以通过上面讨论的更复杂的加密原语来实现。<br>  Bittau等人 [67]提议采用Prochlo系统作为实施ESA框架的一种方式。该系统采用整体隐私方法，考虑到安全计算方面（使用TEEs解决）、隐私披露方面（通过差异隐私解决）和验证方面（使用安全环境弱化认证功能）。<br>  更普遍地说，差异隐私的混编模型可以使用更广泛的局部随机者类，甚至可以自适应地选择这些局部随机者[157]。这可以使差异私有协议的错误远远小于本地模型中可能的错误，同时依赖于弱于中心模型的信任假设[105、157、43、179、178]。<br><b>混合差分隐私：</b> 另一个可行的方法是混合差分隐私[39]，它通过根据用户的信任模型偏好（例如对管理员信任与否）来划分用户，从而组合多个信任模型。在混合模型之前，有两种选择。第一种是使用最不可信的模型，它通常提供最低的效用，并且保守地将其统一应用于整个用户群。第二种方法是使用最信任的模型，它通常提供最高的实用程序，但只应用于最信任管理者的用户。通过允许多个模型共存，混合模型机制可以从给定的用户基础获得更多的效用，与纯本地或中心DP机制相比。例如，[39]描述了一个系统，其中大多数用户在本地隐私模型中贡献他们的数据，而一小部分用户选择在可信的管理员模型中贡献他们的数据。这使得能够设计一种机制，在某些情况下，该机制的性能优于应用于所有用户的保守的本地机制以及仅应用于小部分选择参加用户的可信管理员机制。这种结构可以直接应用于联邦学习环境中；然而，组合信任模型或计算模型的一般概念也可能激发类似新的联邦学习方法。<br><img src="/2022/09/06/advances-and-open-problems-in-federated-learning-yue-du-bi-ji/7.png" alt="图3 包含四个参与者的混编分析编码（ESA）框架"></p><h3 id="4-2-3-可验证性"><a class="header-anchor" href="#4-2-3-可验证性">¶</a>4.2.3 可验证性</h3><p>  与上述隐私技术正交的一个重要概念是验证性。一般来说，可验证的计算将使一方能够向另一方证明其已忠实地对其数据执行了所需的行为，而不会损害数据的潜在保密性。可验证计算的概念可追溯到Babai等人 [40]，并且已经在文献中的不同术语下进行了研究：检验[40]、认证计算[295]、委托计算[185]以及可验证性计算[173]。<br>  在FL的背景下，验证能力可用于两个目的。首先，它将使服务器能够向客户机证明它忠实地执行了预期的行为（例如，聚合输入、显示输入消息或添加用于差异隐私的噪声）。其次，它将使客户端能够向服务器证明其输入和行为遵循协议规范（例如，输入属于某个范围，或者数据是正确生成的密文）。<br>  多种技术可用于提供验证：零知识证明（ZKPs）、可信执行环境（TEEs）或远程认证。其中ZKPs提供了基于数学硬度的形式化密码安全保证，而其他技术则依赖于可信硬件的安全性假设。<br><b>零知识证明（ZKPs）：</b> 零知识证明是一种密码原语，它使一方（称为证明者）能够向另一方（称为验证者）声明证明，而验证者依赖于见证者已知的秘密信息，而不向验证者泄露这些信息。20世纪80年代末，Goldwasser等人引入了零知识（ZK）的概念[184]。它为隐私数据的可验证性问题提供了解决方案。虽然ZK构建方面存在大量的工作，但第一个将ZKP和通用功能的可验证计算引入实用领域的是Parnoetal的工作 [317]，它介绍了第一个针对简洁ZK的优化构建和实现。现在，ZKP协议可以实现100字节的证明大小和毫秒级的验证，无论被证明语句的大小。<br>  ZKP有三个显著的属性：完整性（如果陈述是真的，证明者和验证者遵循协议，验证者将接受证明）、可靠性（如果陈述是假的，验证者遵循协议，验证者将拒绝证明）和零知识（如果陈述是真的，证明者遵循协议协议中，验证者只会了解到声明是真实的，不会从交互中了解到任何隐私信息）。<br>  除了这些共性之外，在证明支持的语言、设置要求、证明和验证计算效率、交互性、简洁性和潜在的硬度假设方面，还有不同类型的零知识结构。有许多ZK结构支持特定的语句类，Schnorr证明[349]和Sigma协议[128]就是这样广泛使用的协议的例子。虽然此类协议在特定设置中有许多用途，但是能够支持任何功能的通用ZK系统提供了一个更广泛适用的工具（包括在FL的上下文中），因此我们将在接下来的讨论中重点讨论此类构造。<br>  不同构造之间的一个主要区别是需要可信设置。一些ZKP依赖于一个公共引用字符串（common reference string，CRS），该字符串使用应该保持y隐私性来计算，以保证证明的可靠性。这种CRS的计算被称为可信设置。虽然这种要求对于这样的系统是不利的，但是现有的能够实现最简洁的证明和验证者有效性的ZKP，都需要可信的设置。<br>  影响不同情况下适用性的另一个重要特性是，生成证明是否需要证明者和验证者之间的交互，这里我们区分非交互零知识证明（NIZK），该证明使验证者能够向验证者发送一条消息，且无需进一步通信。通常，我们可以将交互式的证明转换为非交互式的证明，从而对理想哈希函数的功能做出更有力的假设（即哈希函数生成结果完全随机）。<br>  此外，对ZKP系统的有效性有不同的测量，如证明的长度和证明者、验证者的计算复杂度。从评估执行时间来说，理想的证明者的复杂性应该是线性的，但许多现有的ZKPS引入额外的（有时产生重大影响）证明者，产生更多开销。最有效的验证者复杂度要求在估计的功能的输入的大小上至少是线性的，并且在FL服务器的工作场景下的设置中，这个输入规模将是巨大的。<br>  简洁的非交互式零知识证明（SNARKs）[65]是一种ZKP类型，它提供恒定的证明和验证大小，与输入大小成线性关系。这些有吸引力的性能是以更强的假设为代价的，而这一假设在大多数现有的方案中都是固有而可信的。大多数现有SNARK结构利用二次算术程序〔174, 317, 118〕，现在可在开源库中使用，如LiSnAgAg[[ 5 ] ]，并部署在加密货币场景中，如Zcash（57）。值得注意的是，SNARK系统通常需要证明者部分的开销；特别是，证明者的计算需要与被证明语句的大小上满足超线性关系。最近，谢等人〔418〕提出了一种ZKP系统，该系统能够实现线性证明者的复杂度，但增加了证明空间开销和验证时间。<br>  如果我们放松对构造的简洁性或非交互性的要求，就会有大量广泛的效率权衡的构造实现，避免可信设置要求，并使用更标准的加密假设[84、397、23、58]。<br>  近年来，越来越多的实际应用使用非交互式零知识证明，主要是由区块链驱动的。在FL中有效地使用交互式ZKP系统和NIZKs仍然是一个具有挑战性的开放性问题。在这种设置中，NIZKs可以向服务器证明客户机的输入。在验证者是客户端的情况下，创建一个可信的语句来验证将是一个挑战，因为它涉及到来自其他客户的输入。在这种情况下，最近的工作[76]使我们能够处理多个验证者共享陈述语句的情况。<br><b>可信执行环境和远程认证：</b> 我们在第4.2.1节中讨论了TEEs，但这部分的重点是TEEs提供可验证计算的可能。实际上，TEEs能够证明和验证在其环境中运行的代码（二进制）。特别是，当验证者知道（或可以复制）哪个二进制文件应该在安全环境中运行时，TEEs将能够提供完整性（除了输入之外，代码执行不会受到影响）和可证明性（TEE可以证明特定二进制文件正在执行，并且什么是启动状态）[373,385]。一般来说，远程认证允许验证者安全地测量远程硬件平台的内部状态，并可用于建立静态或动态信任源。虽然TEEs支持基于硬件的远程认证，但文献中提出了基于软件的远程认证[351]和混合远程认证设计[152,238]，并能够权衡硬件要求与可验证性。<br>  在联邦学习环境中，TEEs和远程认证对于客户端能够有效地验证服务器上运行的关键功能可能特别有用。例如，安全聚合或混编可以在TEEs中运行，并为它们的输出提供不同的隐私保证。因此，服务器随后对差异隐私数据应用的后处理逻辑可以在服务器上运行，并且对客户端保持不敏感。注意，这样的系统设计要求客户端知道并信任要在安全环境中应用的关键函数的确切代码（二进制）。此外，远程证明可以使服务器证明FL计算中涉及的客户端的特定要求，例如无泄漏、不变性和不可中断性（关于远程证明的最低要求的详细列表，请参阅[166]。</p><h2 id="4-3-防范外部恶意参与者"><a class="header-anchor" href="#4-3-防范外部恶意参与者">¶</a>4.3 防范外部恶意参与者</h2><p>  在本节中，我们假设可信服务器的存在，并讨论实现对外部恶意参与者（例如敌对客户、敌对分析者、消耗学习模型的敌对设备或其任意组合）的严格隐私保证的各种挑战和公开问题。<br>  如表7所述，恶意客户端可以在其参与的轮次中检查从服务器接收的所有消息（包括模型迭代），恶意分析师可以使用不同的超参数检查来自多个训练运行的模型迭代序列，在跨设备FL中，恶意设备可以通过白盒或黑盒访问最终模型。因此，要提供严格的保护以防范外部对手，首先必须考虑从中间迭代和最终模型中可以学到什么。</p><h3 id="4-3-1-评估迭代轮次和最终模型"><a class="header-anchor" href="#4-3-1-评估迭代轮次和最终模型">¶</a>4.3.1 评估迭代轮次和最终模型</h3><p>  为了更好地理解从中间迭代或最终模型中可以学到什么，我们建议量化联邦学习模型对特定攻击的易感性。在联邦学习环境中，这是一个特别有趣的问题。一方面，敌方从服务器接收到对模型的直接访问，拓宽了攻击面。另一方面，服务器确定对手将在训练过程的哪个特定阶段获得对模型的访问，并在每个阶段控制对手对模型的影响。<br>  对于经典的（非联邦的）计算模型，了解模型对攻击的敏感性是一个活跃而富有挑战性的研究领域[167,357,91,293]。最常用的易于攻击量化模型方法是使用代理（评估）数据集模拟对模型的攻击，该数据集与实际中预期的数据集类似。如果代理数据集确实与最终用户数据相似，那么这就说明了模型的预期攻击敏感性。更安全的方法是确定模型攻击敏感性的最坏情况上限。 [425]在理论上给出了可接近的界限，尽管这一上界在实际模型中通常松散、空洞而无法达到。经验方法也许能够提供更严格的界限，但对于许多类型的攻击和模型，这一努力可能是棘手的。这一领域中一个有趣的新兴研究领域考察了理论条件（关于评估模型和攻击），即在何种条件下，通过模拟失败隐私侵犯的失败攻击，就意味着没有更强的攻击能够成功完成这样的任务[134]。然而，这一领域仍处于初级阶段，需要做更多的工作来更好地理解评估（通过模拟攻击）的基本要求。<br>  联邦学习框架不仅为攻击提供了独特的设置，而且也为攻击数量和防御提供了独特的设置。具体来说，由于服务器可以控制每个用户在培训过程中何时可以访问和影响模型，因此可以设计新的可处理方法来量化模型的平均情况或最坏情况下的攻击敏感性。这样的方法将使得能够开发新的自适应防御，它可以应用于即时运行过程中，从而在最大化效用的前提下，面对不可抗拒的敌方时取得优先权。</p><h3 id="4-3-2-考虑中心差异隐私的模型训练"><a class="header-anchor" href="#4-3-2-考虑中心差异隐私的模型训练">¶</a>4.3.2 考虑中心差异隐私的模型训练</h3><p>  为了限制或消除从迭代（和/或最终模型）中可以了解到的关于用户的信息，可以在FL的迭代训练过程中使用用户级差异隐私[72,90,288,62]。使用这种技术，服务器通过剪裁单次更新的l2范数结果，聚合被剪裁的更新，然后将高斯噪声添加到聚合结果中。这样可以确保迭代不会过拟合任何单个用户的更新。为了跨回合跟踪总体隐私预测，可以使用更高级的组合理论[148,221]或在[72, 97,299,405]中提出的分析动量统计方法。动量统计方法特别适用于均匀亚采样高斯机制。<br>  在跨设备FL中，训练示例的数量在不同设备之间可能有很大的差异。因此，与中心模型[24]中关于用户级DP的最新工作类似，研究如何自适应地限制用户的贡献并剪裁模型参数仍然是一个有趣的研究方向[381,324]。更广泛地说，与记录级DP不同，在记录级DP中，各种规范学习和估计任务的准确性和隐私性之间的基本权衡关系很好理解，用户级DP从根本上来说就不那么容易理解（尤其是当贡献的数量在用户之间变化很大，并且没有先验的严格限制时）。因此，需要做更多的工作，以更好地理解在这种新出现的DP环境中的基本权衡。<br>  除上述内容外，还必须区分在训练期间可能看到（某些）中间迭代的恶意客户端和只能看到最终模型的恶意分析师（或部署设备）。尽管中心DP提供了对两种威胁模型的保护，但仔细的理论分析可以发现，对于上述高斯机制（或任何其他差异隐私机制）的具体实现，我们可能会得到这两种威胁模型的不同隐私参数。当然，对于恶意分析师，我们应该获得比恶意客户更强的差异隐私保证（因为恶意客户可能比恶意分析师获得更多的信息）。费尔德曼等人最近对这种“通过迭代进行隐私放大”的设置进行了凸优化问题研究[163]。然而，还不清楚[163]中的结果是否可以被迁移到非凸设置。<br>  非均匀设备采样隐私放大程序 跨设备FL的基本更新步骤在可用客户端的子集上进行。当所选择的用户子集从用户的基本群体中均匀抽样时，可以使用在[405]中开发的动量统计方法来分析该抽样过程的隐私增益。然而，这样的抽样程序在实践中几乎是不可能的。这是因为（a）服务提供商可能不知道设备的总体情况，并且（b）可用设备的特定子集可能会随时间而显著变化。因此，量化跨设备FL中的隐私放大是一个有趣的开放问题。<br>  随机源（改编自[288]） 大多数计算设备只能访问很少的熵源，而且它们的速率往往很低（硬件中断、车载传感器）。使用熵为加密安全的伪随机数生成器（PRNG）种子并根据需要使用PRNG的输出是标准要求，这在理论上也是合理的。基于标准密码原语的鲁棒高效PRNG存在，在现代CPU上具有每秒千兆字节的输出速率，并且要求种子短于128比特[343 ]。<br>  只要识别器在计算上有界，访问PRNG的随机算法A的输出分布与访问真实熵源的随机算法A的输出分布是不可区分的。相比之下，无论对手有多强大，差异隐私的保障对任何对手都是有效的。因此，几乎所有差异隐私的实现都只满足[298]引入的计算差异隐私的（变体）。从积极的方面来说，一个计算能力有限的对手无法分辨出两者的区别，这使得我们避免在这一点上过于局限。<br>  一个训练过程可能有多个非确定性来源（例如，退出层或生成模型的输入），但只有那些反映在隐私目次中的来源必须来自加密安全的PRNG。特别是，设备采样过程和加高斯噪声必须从加密安全的PRNG中提取，以满足计算差异隐私。<br>  评估差异隐私实现 众所周知，隐私和安全协议很难正确实现（例如，[296, 192]用于区分隐私）。什么技术可以用来测试FL实现的正确性？由于这些技术通常由那些可能选择不使用开源代码的组织部署，黑盒测试的可能性有多大？一些著作[137，275]开始在差异隐私的背景下探索这一领域，但仍有许多悬而未决的问题。</p><h3 id="4-3-3-迭代隐蔽"><a class="header-anchor" href="#4-3-3-迭代隐蔽">¶</a>4.3.3 迭代隐蔽</h3><p>  在典型的联邦学习系统中，模型迭代（即每轮训练后模型的更新版本）被假定为对系统中的多个参与者可见，包括选择参与该轮的服务器和客户端。但是，可以使用第4.2节中的工具来对这些参与者隐藏迭代模型。<br>  为了向客户端隐藏迭代，每个客户端都可以在提供保密特性的TEE中运行其联邦学习的本地部分（参见第4.2.1节）。服务器将验证预期的联邦学习代码是否在TEE中运行（依赖于TEE的认证和完整性功能），然后将加密的模型迭代结果传输到设备，以便它只能在TEE中解密。最后，模型更新将在返回到服务器之前在TEE内部加密，使用仅在安全环境内部和服务器上已知的密钥。不幸的是，TEE通常不适用于客户端，尤其是当这些客户端是智能手机等终端用户设备时。此外，即使存在TEE，它们也可能不够强大，无法支持训练计算，为了保护模型迭代，必须在TEE内部进行训练计算，并且可能需要高计算成本和/或大量的RAM。尽管TEE的能力可能会随着时间的推移而提高，以及诸如在[382]中所述的技术可以通过将计算的部分转移到TEE之外而减少对TEE的要求，同时保持计算的整体的证明、完整性和机密性需求。<br>  在MPC模型下也可以实现类似的保护[302，14]。例如，服务器可以在将迭代器的模型参数发送到客户端之前，使用只有服务器知道的密钥在同态加密方案下对其进行加密。然后，客户端可以使用密码系统的同态属性计算加密的模型更新，而无需解密模型参数。然后，可以将加密的模型更新返回到服务器进行聚合。这里的一个关键挑战是在解密之前在服务器上强制聚合，否则服务器可能会学习到客户端的模型更新。另一个具有挑战性的开放性问题是提高性能，因为即使是最先进的系统也需要相当可观的计算资源才能在深层神经网络中完成一轮训练。这方面的进展既可以通过算法的进步，也可以通过为MPC开发更高效的硬件加速器来实现[337]。<br>  向服务器隐藏模型迭代也会产生额外的挑战。在TEE模型下，联邦学习的服务器部分可以在TEE中运行，所有各方（即客户端和分析师）都验证服务器TEE仅在满足适当的培训标准后才发布最终模型。在MPC模型下，加密密钥可以保护模型迭代，密钥由分析师持有，在客户端之间共享，或由可信的第三方持有；在这种设置中，密钥持有者将被要求参与模型参数的解密，从而可以确保此过程只发生一次。</p><h3 id="4-3-4-对不断变化数据的重复分析"><a class="header-anchor" href="#4-3-4-对不断变化数据的重复分析">¶</a>4.3.4 对不断变化数据的重复分析</h3><p>  对于联邦学习的许多应用，分析师希望分析流式数据，并且还必须提供动态更新的学习模型，这些模型（1）对目前接受到的数据结果正确，以及（2）准确预测未来将要到达的数据。在没有隐私问题的情况下，分析师可以在新数据到达后简单地重新训练所学模型，以确保在任何时候都能达到最大的精度。然而，隐私保证等级随着关于相同数据的附加信息的发布而降低[147, 148]，这些附加信息引起更新的频率必须降低以保持整体分析的隐私性和准确性。<br>  动态数据库和时间序列数据的差异隐私[125, 124, 89]研究的最新进展都假设存在可信的管理员，他们可以在上线时看到原始数据，并发布动态更新的统计数据。一个悬而未决的问题是，如何将这些算法技术扩展到联邦设置，以实现对时间序列数据或其他动态演变数据库的私有联邦学习。<br>  具体开放性问题包括：</p><ol><li>分析师应如何在有新数据的情况下更新私有FL模型？或者，在数据集 $D$ 上使用FL私下学习的模型扩展到数据集 $D'$ （在给定的相似性度量中保证与D相似）的程度如何？由于FL已经出现在在线到达的样本上，并且没有过拟合它所用来训练的数据，因此这种模型很可能仍然会在新的数据集 $D'$ 上表现良好。这也与第5节探讨的鲁棒性问题有关。</li><li>解决隐私构成问题的一种方法是生成可以独立使用，而不会造成额外隐私损失的合成数据[145，9]。这来自于差异隐私的后处理保证[147]。奥根斯坦等人在[32]中探索证明了以联邦方式生成合成数据。在动态数据设置中，合成数据可以重复使用，直到它相对于新数据变得“过时”，并且必须更新。即使在以联邦方式生成数据之后，它也必须以私有和联邦方式进行更新。</li><li>之前关于动态数据库差异隐私的工作[124]或不公开地检测时间序列数据变化[125, 89]中的具体方法是否可以扩展到联邦设置？</li><li>如何在联邦模型中首先查询时间序列数据？在设计上，同一个用户不会被多次定期查询更新的数据点，因此很难在评估过程中内收集到真实的个人数据随时间变化的估计值。在这里，可以使用时间序列数据统计抽样的常用工具，但必须与隐私工具和联合工具一起使用。其他方法包括重新格式化查询，以便评估查询中的每个子查询都可以在设备上完全应答。</li></ol><h3 id="4-3-5-防止模型被盗或误用"><a class="header-anchor" href="#4-3-5-防止模型被盗或误用">¶</a>4.3.5 防止模型被盗或误用</h3><p>  在某些情况下，开发ML模型的参与者或组织可能有动机来限制检查、误用或窃取模型的能力。例如，限制对模型参数的访问可能会使对手更难搜索漏洞，例如产生意外模型输出的输入。<br>  如第4.3.3节所述，在推断下保护已部署的模型与在训练期间向客户端隐藏模型迭代的挑战密切相关。同样，可以使用TEEs和MPC方法。在TEE模型下，模型参数只能由设备上的TEE访问，如第4.3.3节所述；主要区别在于，所需的计算现在是推断而不是训练。<br>  如果不放弃设备推理所提供的优点，就很难使MPC策略适应这个用例：如果用户数据、模型参数和推理结果原来都是在设备上进行的，则不清楚还有哪些其他方参与了多方计算。例如，朴素地试图使用同态加密将要求解密密钥位于要使用推断功能的设备上，从而首先破坏加密的价值。要求分析师参与的解决方案（例如，持有加密密钥或模型参数本身）意味着对最终用户的额外推理延迟、带宽成本和连接要求（例如，对于处于飞行模式的设备，推理将不再可用）。<br>  必须注意的是，即使模型参数本身被成功隐藏；研究表明，在许多情况下，它们可以由对手重建，而对手只需访问基于这些参数的推理/预测API[384]。对于驻留在数百万或数十亿最终用户设备上的模型，需要采取哪些附加保护措施来防止此类问题，这是一个悬而未决的问题。</p><h2 id="4-4-针对敌意服务器的保护"><a class="header-anchor" href="#4-4-针对敌意服务器的保护">¶</a>4.4 针对敌意服务器的保护</h2><p>  在前面的章节中，我们假设存在一个可以编排整个训练过程的受信服务器。本节，我们讨论一种针对一个有敌意服务器的更合适的场景。特别是，我们首先调查这种环境和现有工作的挑战，然后继续描述未解决的问题以及如何使用第4.2节中讨论的技术来应对这些挑战。</p><h3 id="4-4-1-信道，女巫攻击，选取"><a class="header-anchor" href="#4-4-1-信道，女巫攻击，选取">¶</a>4.4.1 信道，女巫攻击，选取</h3><p>  在跨设备FL设置中，我们拥有一台具有大量计算资源的服务器和大量客户端，这些客户端（i）仅能与该服务器通信（如在星形网络拓扑中），并且（ii）连通性和带宽可能受到限制。 在执行给定的信任模型时，这提出了非常具体的要求。 特别是，客户端没有独立于服务器的清晰方法来在客户端之间建立安全通道。 如Reyzin等人所示。 [336]对于实际设置，在需要客户端之间专用通道的情况下，需要假设服务器在密钥分发阶段（如[73]中所做的）是诚实的（或至少是半诚实的）行为。 这包括基于MPC技术的密码解决方案。 对此假设的替代方法是将额外的参与方或公共公告板（例如，参见[341]）合并到客户端已知且可以信任的不与服务器串通的模型中。<br>  除了信任服务器以促进专用通信渠道之外，跨设备FL的参与者还必须信任服务器以公平，诚实的方式形成客户群。控制服务器的主动恶意攻击者可能会模拟大量伪造的客户端设备（“ Sybil攻击” [140]），或者可能会从可用设备池中优先选择以前受到破坏的设备。无论采用哪种方式，对手都可以在联邦学习的一轮中控制更多的参与者，这要比简单地从总体中的对手设备的基本速率所预期的要多。这将使打破MPC中至少有一部分设备是诚实的普遍假设变得容易得多，从而破坏了协议的安全性。即使协议本身的安全性保持不变（例如，如果其安全性植根于不同的信任源，例如安全的隔离区），也可能存在以下风险：如果已知大量敌对客户端的模型更新，或由对手控制该更新，则可能会破坏其余客户的更新的隐私。注意，这些顾虑也可以适用于TEE。例如，基于TEE的混洗器也可能遭受Sybil攻击；如果将单个诚实用户的输入与来自假用户的已知输入进行混洗，那么对手将很容易在混洗输出中识别出诚实用户的价值。<br>  请注意，在某些情况下，有可能在一轮客户端之间建立证明它们都在执行正确的协议，例如，如果客户端设备上有安全的隔离区，并且客户端之间可以进行远程证明。 在这些情况下，有可能为该回合中所有诚实的参与者建立隐私（例如，通过证明已正确遵循安全的多方计算协议，秘密且正确地添加了分布式差异隐私贡献等），即使 模型更新本身是对手已知的或由对手控制的。</p><h3 id="4-4-2-现有方案的缺陷"><a class="header-anchor" href="#4-4-2-现有方案的缺陷">¶</a>4.4.2 现有方案的缺陷</h3><p>  鉴于FL的目标是服务器在客户数据中构建人口级别模式的模型，自然而然的隐私目标是量化并可证明地限制服务器重建单个客户输入数据的能力。这涉及形式上的定义（a）作为FL执行结果显示给服务器的客户数据的视图是什么，以及（b）这种视图的隐私泄漏是什么。在FL中，我们特别希望保证服务器可以汇总来自客户端的报告，同时以某种方式掩盖每个单独客户端的贡献。如第4.2.2节所述，这可以通过多种方式完成，通常使用一些差异性隐私的概念。这种方法有很多种，每种方法都有其自身的弱点，尤其是在FL中。例如，如已经讨论的，中央DP遭受对可信任中央服务器的访问的需求。这引出了第4.2.2节中讨论的其他有前途的私人披露方法。在这里，我们概述了这些方法的一些缺点。<br><b>本地差分隐私（LDP）</b><br>  如前所述，LDP通过让每个客户端在将报告发送到中央服务器之前对其报告执行不同的私有转换，从而消除了对受信任的中央服务器的需求。 LDP假定用户的隐私完全来自该用户自己的随机性；因此，用户的隐私保证独立于所有其他用户添加的额外随机性。尽管LDP协议有效地加强了隐私并具有理论上的依据[156，135，136]，但许多结果表明，在保持实用性的同时实现局部差分隐私尤其具有挑战性，特别是在高维数据设置中[229，388， 219，51，220，424，142，111]。造成这种困难的部分原因是，引入的随机噪声的幅度必须与数据中信号的幅度相当，这可能需要合并客户端之间的报告。因此，要获得与中央设置相当的LDP效用，就需要相对较大的用户群或较大的参数选择。<br><b>混合差分隐私（HDP）</b><br>  差异隐私的混合模型可以通过根据用户的信任首选项对用户进行划分来帮助减少所需用户群的大小，但是它不能为用户本地添加的噪声提供隐私放大功能。 而且，尚不清楚哪个应用领域和算法可以最佳地利用混合信任模型数据[39]。 混合模型的当前工作通常假设，无论用户信任偏好如何，其数据都来自相同的分布[39、141、53]。 放宽此假设对于FL尤其重要，因为信任首选项和实际用户数据之间的关系可能并不重要。<br><b>随机混合模型</b><br>  随机混合模型可从用户的本地噪声中放大隐私，尽管它有两个缺点。首先是可信中介的要求；如果用户已经不信任管理员，那么他们不太可能会信任由管理员批准或创建的中介人（尽管TEE可能有助于弥合这一差距）。 Prochlo框架[67]（据我们所知）是唯一现有的实例。第二个缺点是随机混合模型的差异性隐私保证与参与计算的对手用户数量成比例地降低[43]。由于用户或管理员不知道该数字，因此将不确定性引入了用户所接收的真实隐私级别。在联合学习的情况下，这种风险尤其重要，因为用户（可能是对抗性的）是计算管道中的关键组成部分。当用户在本地添加自己的噪声时，安全的多方计算除了会给每个用户增加大量的计算和通信开销外，也无法解决此风险。<br><b>安全集聚协议</b><br>  [73]中的安全聚合协议在聚合客户报告时具有强大的隐私保证。 此外，该协议是针对联合学习的设置而量身定制的。 例如，它对于客户端在执行过程中退出（跨设备FL的一个共同特征）是健壮的，并且可以扩展到大量参与者和更长的的向量。 但是，这种方法有几个局限性：（a）假定服务器为半诚实的服务器（仅在私钥基础结构阶段），（b）允许服务器查看全方位的聚合（可能仍会泄漏信息）， （c）稀疏向量聚合效率不高；（d）缺乏强制客户输入格式正确的能力。 如何构建一个有效，强大的安全聚合协议来解决所有这些挑战是一个悬而未决的问题。</p><h3 id="4-4-3-分布式差分隐私训练"><a class="header-anchor" href="#4-4-3-分布式差分隐私训练">¶</a>4.4.3 分布式差分隐私训练</h3><p>  如果没有受信任的服务器，则可以使用分布式差异隐私（在第4.2.2节中介绍）来保护参与者的隐私。<br><b>分布式差分隐私下的通信，隐私和准确性权衡</b><br>  我们指出，在分布式差异隐私中，三个性能指标是普遍关注的：准确性，隐私和通信，并且一个重要的目标是确定这些参数之间可能的折衷。 我们注意到，在没有隐私要求的情况下，关于分布估计（例如[376]）和通信复杂性的文献已经很好地研究了通信和准确性之间的权衡（有关教科书的参考，请参见[248]） 。 另一方面，在假设所有用户数据都由一个实体保存并且因此不需要进行通信的集中式设置中，从[147，146]的基础性工作开始，就已经在中央DP中广泛研究了准确性和隐私之间的权衡取舍。<br><b>安全混合的折衷选择</b><br>  最近在混合模型中研究了这些折衷，以解决聚合的两个基本任务（目标是计算用户输入的总和）和频率估算（输入属于离散集，目标是 估算拥有给定元素的用户数）。 有关这两个问题的最新发展情况，请参见表9和表10。 两个值得注意的开放问题是（i）在改组模型中研究纯差异隐私，以及（ii）在多消息设置中确定变量选择的最佳的私密性，准确性和通信折衷情况（在最近[178]提到的单消息情况下紧紧贴合的下限）<br><b>安全集聚协议的折衷选择</b><br>  研究以下类似问题以进行安全聚合将非常有意思。假设一轮有 $n$ 个用户参与的联邦学习，同时假设每个用户 $i$ 持有数据 $x_i$。用户 $i$ 对 $x_i$ 采用算法 $A(.)$ 来获得 $y_i = A(x_i)$；在这里，$A(.)$ 可以被视为压缩方案和私有化方案。通过使用安全集聚协议作为黑盒，服务提供商可以观察 $\overline y = \Sigma_i A(x_i)$ 同时可以通过计算 $\hat{\overline x} = g(\overline y)$ 来使用 $\overline{y}$ 估计 $x_i$ 的真实总值 $\overline{x}$。理想情况下，我们希望以最小化x估计误差的方式设计 $A(.),g(.)$；形式上，我们想解决最优化问题 $min{g,A} ||g(\Sigma_i A(x_i)) - \Sigma_i x_i||$，此处 $||.||$ 可以是 $l_1$ 范数或者 $l_2$ 范数。当然，在不对 $g(.)$ 和 $A(.)$ 施加任何约束的情况下，我们始终可以选择它们作为恒等函数，并获得0错误。然而，$A(.)$ 需要满足两个约束：（1） $A(.)$ 应该输出B位（可以认为是每个用户的通信成本）,同时（2） $\overline y = \Sigma_i A(x_i)$ 应当是 $\overline x = \Sigma_i x_i$ 的 $(\varepsilon, \delta)-DP$ 版本。因此，关注的基本问题是确定在聚合时也能实现DP且同时满足固定通信预算的最优算法 $A$。换个角度看问题，对于固定的 $n,B,\varepsilon,\delta$，我们希望实现的最小的 $l_1$ 或 $l_2 $ 误差是多少？我们注意到Agarwal等人的最新工作[13]提供了一种基于均匀量化和二项式噪声相加的候选算法A。但是，尚不清楚所提出的方法是否是在这种情况下的最佳方法。因此，在上述约束下得出 $l_1$ 或 $l_2$ 误差的下限是非常重要的。<br><b>隐私账户？隐私登录？</b><br>  在DP的中心模型中，经常使用欠采样的高斯机制来实现DP，并且使用==矩值会计==方法在FL的各轮之间紧密跟踪隐私预算（请参见第4.3节中的讨论）。 但是，在DP的分布式设置中，由于与安全混合和安全聚合的实际实现相关的有限精度问题，因此无法使用高斯机制。 因此，该空间中的现有工作已恢复为具有离散性质的噪声分布（例如，添加伯努利噪声或二项式噪声）。 尽管这样的分布有助于解决由安全混合/聚合的基础实现所施加的有限精度约束，但它们自然不会从==矩数会计==方法中受益。 因此，一个重要的开放问题是推导针对这些分布式DP考虑的离散（和有限支持）噪声分布量身定制的==隐私权计费==技术。<br><b>处理用户掉线问题</b><br>  上面的分布式DP模型假设参与的客户端在一轮中保持与服务器的连接。 但是，当大规模运行时，某些客户端会由于网络连接断开或暂时不可用而退出。 这要求分布式噪声生成机制要针对此类遗漏具有鲁棒性，并且还要影响扩张联邦学习的规模和对大量参与客户端的分析。<br>就健壮的分布式噪声而言，客户端退出可能会导致添加的噪声太少，无法满足差分隐私epsilon目标。 保守的方法是增加每个客户端的噪音，以便即使使用最少数量的客户端才能满足差分隐私 $\varepsilon$ 目标，以使服务器完成安全聚合并计算总和。 但是，当更多的客户报告时，这会导致过多的噪音，这引发了一个问题，即是否有可能提供更有效的解决方案。<br>  在扩展方面，增加参与安全聚合回合的客户端数量时，丢失的客户端数量将成为瓶颈。 同时收集足够的客户也可能是一个挑战。 为此，可以对协议进行结构化，以便客户端可以在长时间运行的聚合回合过程中多次连接，以完成其任务。 更普遍的，还有在文献中尚未系统地解决当客户可能间歇性可用时的大规模操作的问题。<br><b>新的可信赖的模型</b><br>  联合学习框架利用联合学习的独特计算模型，并可能在对抗性用户的能力上做出现实的假设，从而推动了比以前使用的模型更加精细的新信任模型的开发。例如，假设多少比例的客户可能会受到对手的损害？攻击者是否有可能同时攻击服务器和大量设备，或者通常假设攻击者只能攻击一个或另一个设备就足够了吗？在联合学习中，服务器通常由众所周知的实体（例如长期存在的组织）操作。可否利用它来建立一个信任模型，在该模型中，服务器的行为是可信的但经过验证的，即，其中不会阻止服务器偏离所需协议，但是如果服务器确实偏离了协议，则很有可能被检测到（从而破坏信任，声誉，以及托管组织的潜在财务或法律地位）？</p><h3 id="4-4-3-在训练子模型时保护隐私"><a class="header-anchor" href="#4-4-3-在训练子模型时保护隐私">¶</a>4.4.3 在训练子模型时保护隐私</h3><p>  有许多这种情况出现，就是其中每个客户端可能具有仅与正在训练的完整模型的相对较小部分有关的本地数据。 例如，对大型清单进行操作的模型，包括自然语言模型（对单词的清单进行操作）或内容排名模型（对内容的清单进行操作）），经常使用嵌入查找表作为神经网络的第一层。 通常，客户仅与极少量的清单项进行交互，并且在许多训练策略下，客户数据支持更新的唯一嵌入向量是与客户交互的物料相对应的嵌入向量。<br>  再举一个例子，多任务学习策略可能是实现个性化的有效方法，但可能会产生复合模型，其中任何特定的客户端仅使用与该客户端的用户群相关联的子模型，如3.3.2节所述。<br>  如果不关心沟通效率，那么子模型训练就像标准的联合学习：客户将在参与时下载完整模型，使用与他们相关的子模型，然后提交涵盖整个模型参数的集合的模型更新（即，除了与相关子模型相对应的条目中，其余所有地方都为零）。 但是，在部署联合学习时，通信效率通常是一个重要问题，这引发了我们是否可以实现通信效率高的子模型训练的问题。<br>  如果没有隐私敏感信息进入客户将更新哪个特定子模型的选择，那么可能会有直接的方法来适应联合学习以实现有效的通信子模型训练。 例如，一个人可以运行多个联合学习过程的副本，每个子模型一个,或者并行（例如，客户端基于他们希望更新的子模型，选择合适的联合学习实例参与），或者按顺序（例如， 对于每轮FL，服务器都会发布要更新哪个子模型的信息），或者两者混合。 但是，尽管此方法具有较高的通信效率，但服务器可以观察到客户端选择的子模型。<br>  能否在保持客户的子模型选择私密性的同时，实现沟通效率高的子模型联合学习？ 一种有前景的方法是将PIR用于私有子模型下载，同时使用针对稀疏向量优化的安全聚合变体来聚合模型更新[94，216，310]。<br>  该领域中的开放问题包括表征与实际感兴趣的子模型训练问题相关联的稀疏机制，以及开发在这些稀疏机制中通信效率高的稀疏安全聚合技术。 与仅简单地使每种技术独立运行（例如，通过在两个功能的实现之间分担一些消耗）相比，是否可以共同优化私有信息检索（PIR）和安全聚合以实现更好的通信效率，这也是一个悬而未决的问题。<br>  某些形式的局部和分布式差分隐私在这里也带来了挑战，因为通常会将噪声添加到向量的所有元素中，甚至是零。 结果，在每个客户端上添加此噪声将把原本稀疏的模型更新（即仅子模型上的非零）转换为密集的私有化模型更新（几乎在任何地方几乎都为非零）。 是否可以解决这种紧张关系是一个悬而未决的问题，即是否存在有意义的分布式差分隐私实例化，该实例化也保持了模型更新的稀疏性。<br><img src="/2022/09/06/advances-and-open-problems-in-federated-learning-yue-du-bi-ji/8.png" alt="图九：具有 $(\varepsilon,\delta)$-差分隐私的的多消息混合模型中的差分私有聚合协议的比较。参与方数目为n，$l$ 为整数参数。消息大小以位为单位。为便于阅读，我们假设 $\varepsilon \le O(1)$，并且渐近符号被抑制。"><br><img src="/2022/09/06/advances-and-open-problems-in-federated-learning-yue-du-bi-ji/9.png" alt="图十：在不同模型的DP中，对大小为B的域以及n个以上的用户进行频率估计时，预期最大误差的上限和下限。 边界固定的，正的隐私参数 $\varepsilon$ 和 $\delta$ 表示界限，并且渐近符号抑制因子 $\overline\Theta / \overline{O}/\overline\Omega$ 在B和n中为多对数。==每个用户的通信以发送的位数为单位。 在所有的上限中，该协议相对于用户是对称的，并且不需要公共随机性，引用的是我们所知道的第一个结果，它暗示了规定的界限。"></p><h2 id="4-5-用户感知（users’-perception）"><a class="header-anchor" href="#4-5-用户感知（users’-perception）">¶</a>4.5 用户感知（users’ perception）</h2><p>  联邦学习蕴含了数据收集和最小化的原则（focused data collection and minimization），并且可以减小许多由系统本身带来的隐私风险。然而，正如上文所说，搞清楚联邦学习本身提供或者不提供哪些保护措施、哪些技术可以用来抵御4.1节中的威胁模型是十分重要的。前几节重点关注在抵御精度威胁模型时，隐私的严格量化（rigorous quantification of privacy against precise threat models），这节重点关注用户体验（users’ perception）和需求的相关挑战。<br>  以下是几个具有重要实用价值的待解决的问题。是否有办法可以让普通用户直观地了解联邦学习的优缺点。联邦学习的基础结构的哪些参数和功能可能足以（或不足）满足隐私和数据最小化要求？联邦学习可能会让用户误以为他的隐私没有问题了吗（Might federated learning give users a false sense of privacy?）？当用户更多地了解联邦学习在他的数据上的操作时，如何能使用户对他的数据隐私感到安全且保障隐私数据确实是安全的？不同的用户对隐私的评估是否一致呢？人们想要保护的非隐私的内容（facts）呢？了解这些可以让我们设计出更好的机制吗？有什么方法可以很好地模拟人们的隐私偏好，从而决定如何设置这些参数？如果不同技术的实用程序/隐私/安全属性不同，谁来决定使用哪种技术？只是服务提供商？还是用户？还是他们的操作系统？他们的政治管辖权？（ Their political jurisdiction?）是否有像“受保护的隐私（仅）” [230]这样的机制可以为大多数用户提供隐私保障，同时允许对社会优先事项（例如反恐）进行有针对性的监视？有没有一种方法可以让用户选择所需的隐私级别？<br>  对于解决这些问题，似乎有两个重要的方向特别重要。</p><h3 id="4-5-1-了解特定分析任务的隐私需求"><a class="header-anchor" href="#4-5-1-了解特定分析任务的隐私需求">¶</a>4.5.1 了解特定分析任务的隐私需求</h3><p>  FL的许多潜在应用涉及复杂的学习任务和来自用户的高维数据，这两者都可能导致需要大量噪声来保护差分隐私。但是，如果用户不太介意受保护的数据不受各种干扰，则可以放宽隐私约束，允许添加少量的噪声。例如，考虑由智能家居恒温器生成的数据，使得设备在房屋空置时关闭，而在居民返回家园时打开。根据此数据，观察者将推断出居民晚上什么时候回家，这可能是高度敏感的。但是，较粗略的信息结构只能显示居民是否在凌晨2-4点之间处于睡眠状态，这可以说不那么敏感。<br>  这种方法在Pufferfish隐私框架中规范化地提出[235]，该框架允许分析人员指定一类受保护的推断（predicates ），必须在保证差异性隐私的前提下进行学习，而所有其他推断（predicates ）也可以在没有差异性隐私的情况下进行学习。为了使这种方法在实践中提供令人满意的隐私保证，分析人员必须了解用户对其特定分析任务和数据收集程序的隐私需求。可以修改联邦学习框架，以允许各个用户指定他们允许和不允许的推断（inferences） 。这些数据限制可以在设备上进行处理，在联邦学习模型更新步骤中仅与服务器共享“允许”信息，也可以在收集数据后将其作为聚合步骤的一部分。应该做进一步的工作来开发将这种用户偏好纳入联邦学习模型的技术工具，并开发对于用户有意义地偏好的技术（meaningful preference elicitation from users.）。</p><h3 id="4-5-2-行为研究以激发隐私首选项"><a class="header-anchor" href="#4-5-2-行为研究以激发隐私首选项">¶</a>4.5.2 行为研究以激发隐私首选项</h3><p>  任何要求个人用户指定自己的隐私标准的隐私保护方法也应包括行为或现场研究，以确保用户可以表达知情的偏好。任何可以获得隐私的方法都需要用户自己来指定隐私保护标准，而且这些方法需要包括行为和领域内的研究，这样就可以保证用户充分地表达自己的偏好（ informed preferences）。这应同时包括教育成分（educational component）和偏好测量（preference measurement）。教育部分应衡量并提高用户对所使用的隐私技术（例如第4.2节）和数据使用细节的理解（译者注：这里的说的“教育”估计是指用户使用引导）。对于涉及联邦学习的应用程序，还应包括联邦学习的说明以及将要发送到服务器的数据是什么。一旦研究的过程说明（educational component）证实了典型用户可以很好地理解学习过程所提供的隐私保护，那么研究者就可以开始偏好激发了（preference elicitation）。这可以在行为实验室，大规模现场实验或专题研究小组中发生。这里应当谨慎，以确保提供有关其偏好数据的用户足够了解情况，以提供高质量的数据，并能够代表目标人群。尽管行为经济学和实验经济学的丰富领域早已表明，人们在公共和私人条件下的行为有所不同（也就是说，其他人是否观察到他们的选择），但在引起人们对差异性隐私的偏好方面所做的行为工作却很少。[ 126，10]。扩展这一工作范围将是迈向未来广泛实施隐私联邦学习的关键一步。在这里，教育部分（educational component)的结果将对确保研究参与者充分了解情况并理解他们面临的决定很有用。这是这些实验的重要参与者，这些实验应遵循道德原则，并且不涉及任何欺骗行为。</p>]]></content>
      
      
      <categories>
          
          <category> 科研学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 联邦学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>差分隐私实现机制</title>
      <link href="/2022/03/17/chai-fen-yin-si-shi-xian-ji-zhi/"/>
      <url>/2022/03/17/chai-fen-yin-si-shi-xian-ji-zhi/</url>
      
        <content type="html"><![CDATA[<p>  在实践中为了使一个算法满足差分隐私保护的要求，对不同的问题有不同的实现方法，这些实现方法称为“机制”。$ Laplace $ 机制 $ Laplace \; Mechanism $ 与指数机制 $ Exponential \; Mechanism $ 是两种最基础的差分隐私保护实现机制。其中，$ Laplace $ 机制适用于对数值型结果的保护，指数机制则适用于非数值型结果。</p><h2 id="Laplace-机制"><a class="header-anchor" href="#Laplace-机制">¶</a>$ Laplace $ 机制</h2><p>  $ Laplace $ 机制通过向确切的查询结果中加入服从 $ Laplace $ 分布的随机噪声来实现 $ \varepsilon -$ 差分隐私保护。记位置参数为 $ 0 $、尺度参数为 $ b $ 的 $ Laplace $ 分布为 $ Lap(b) $，那么其概率密度函数为<br>$$<br>p(x) = \frac{1}{ {2b} }\exp ( - \frac{ {|x|} }{b})<br>$$<br>  <b>定义1. $ Laplace $ 机制.</b>给定数据集 $ D $，设有函数 $ f:D \to {R^d} $，其敏感度为 $ \Delta f $，那么随机算法 $ M(D) = f(D) + Y $ 提供 $ \varepsilon -$ 差分隐私保护，其中 $ Y \sim Lap(\Delta f/\varepsilon ) $ 为随机噪声，服从尺度参数为 $ \Delta f/\varepsilon $ 的 $ Laplace $ 分布。<br><img src="/2022/03/17/chai-fen-yin-si-shi-xian-ji-zhi/1.png" alt="Laplace概率密度函数"></p><h2 id="指数机制"><a class="header-anchor" href="#指数机制">¶</a>指数机制</h2><p>  由于 $ Laplace $ 机制仅适用于数值型查询结果，而在许多实际应用中，查询结果为实体对象（例如一种方案或一种选择）。对此，$ McSherry $ 等人提出了指数机制。<br>  设查询函数的输出域为 $ Range $，域中的每个值 $ r \in Range $ 为一实体对象。在指数机制下，函数 $ q(D,r) \to R $ 称为输出值 $ r $ 的可用性函数，用来评估输出值 $ r $ 的优劣程度。<br>  <b>定义2. 指数机制.</b>设随机算法 $ M $ 输入为数据集 $ D $，输出为一实体对象 $ r \in Range $ ， $ q(D,r) $ 为可用性函数，$ \Delta q $ 为函数 $ q(D,r) $ 的敏感度。若算法 $ M $ 以正比于 $ \exp (\frac{ {\varepsilon q(D,r)} }{ {2\Delta q} }) $ 的概率从 $ Range $ 中选择并输出 $ r $ ，那么算法 $ M $ 提供 $ \varepsilon -$ 差分隐私保护。<br>  以下是一个指数机制的应用实例。假如拟举办一场体育比赛，可供选择的项目来自集合｛足球，排球，篮球，网球｝，参与者们为此进行了投票，现要从中确定一个项目，并保证整个决策过程满足 $ \varepsilon -$ 差分隐私保护要求。以得票数量为可用性函数，显然 $ \Delta q = 1 $ 。那么按照指数机制，在给定的隐私保护预算  $ \varepsilon $ 下，可以计算出各种项目的输出概率，如下图所示：<br><img src="/2022/03/17/chai-fen-yin-si-shi-xian-ji-zhi/2.png" alt="指数机制应用示例"><br>  可以看出，在ε较大时（如 $ \varepsilon ＝１ $），可用性最好的选项被输出的概率被放大。当 $ \varepsilon $ 较小时，各选项在可用性上的差异则被平抑，其被输出的概率也随着ε的减小而趋于相等。</p>]]></content>
      
      
      <categories>
          
          <category> 科研学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 差分隐私 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>差分隐私性质</title>
      <link href="/2022/03/15/chai-fen-yin-si-xing-zhi/"/>
      <url>/2022/03/15/chai-fen-yin-si-xing-zhi/</url>
      
        <content type="html"><![CDATA[<h1>差分隐私保护算法的组合性质</h1><p>  一个复杂的隐私保护问题通常需要多次应用差分隐私保护算法才能得以解决．在这种情况下，为了保证整个过程的隐私保护水平控制在给定的预算 $\varepsilon$ 之内，需要合理地将全部预算分配到整个算法的各个步骤中．这时可以利用隐私保护算法的两个组合性质，如下图所示：<br><img src="/2022/03/15/chai-fen-yin-si-xing-zhi/1.png" alt="差分隐私保护算法的组合性质"><br>  <b>性质1.序列组合性.</b>设有算法 $ {M_1},{M_2}, \cdots ,{M_n} $，其隐私保护预算分别为 $ {\varepsilon _1},{\varepsilon _2}, \cdots ,{\varepsilon _n} $， 那么对于同一数据集 $ D $，由这些算法构成的组合算法 $ M({M_1}(D)),{M_2}(D), \cdots ,{M_n}(D)) $ 提供 $ (\sum\limits_{i = 1}^n { {\varepsilon _i} } )- $ 差分隐私保护。<br>  该性质表明，一个差分隐私保护算法序列构成的组合算法，其提供的隐私保护水平为全部预算的总和。该性质也称为“序列组合性”。<br>  <b>性质2.序列组合性.</b>设有算法 $ {M_1},{M_2}, \cdots ,{M_n} $，其隐私保护预算分别为 $ {\varepsilon _1},{\varepsilon _2}, \cdots ,{\varepsilon _n} $，那对于不相交的数据集 $ {D_1},{D_2}, \cdots ,{D_n} $， 这些算法构成的组合算法 $ M({M_1}({D_1})),{M_2}({D_2}), \cdots ,{M_n}({D_n})) $ 提供 $ (\max {\varepsilon _i}) - $ 差分隐私保护。<br>  该性质表明，如果一个差分隐私保护算法序列中所有算法处理的数据集彼此不相交，那么该算法序列构成的组合算法提供的隐私保护水平取决于算法序列中的保护水平最差者，即预算最大者。该性质也称为“并行组合性”。</p><h1>差分隐私保护算法的变换不变性</h1><p>  给定任意一个算法 $ A_1 $ 满足 $ \varepsilon - $ 差分隐私。对于任意算法 $ A_2 $ ​（$ A_2 $ 不一定满足差分隐私的算法），则有 $ A(\cdot)=A_2(A_1(\cdot)) $ 满足 $ \varepsilon - $ 差分隐私。<br>  说明了差分隐私对于后处理算法具有免疫性, 如果一个算法的结果满足 $ \varepsilon - $ 差分隐私, 那么在这个结果上进行的任何处理都不会对隐私保护有所影响。</p><h1>差分隐私保护算法的中凸性</h1><p>  给定2个算法 $ A_1 $ 和 $ A_2​ $ 满足 $ \varepsilon - $ 差分隐私。对于任意的概率 $ p \in [0,1] $ ，用符号 $ A_p $ 表示为一种机制，它以 $ p $ 的概率使用 $ A_1 $ 算法，以 $ 1 − p $ 的概率使用 $ A_2 $​ 算法则 $ A_p $ 机制满足 $ \varepsilon - $ 差分隐私。<br>  说明了如果有 $ 2 $ 个不同的差分隐私算法, 都提供了足够的不确定性来保护隐私, 那么可以通过选择任意的算法来应用到数据上实现对数据的隐私保护, 只要选择的算法和数据是独立的。</p>]]></content>
      
      
      <categories>
          
          <category> 科研学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 差分隐私 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>玻尔兹曼机</title>
      <link href="/2022/03/07/bo-er-zi-man-ji/"/>
      <url>/2022/03/07/bo-er-zi-man-ji/</url>
      
        <content type="html"><![CDATA[<p>  在本文开始前，为大家强推一位大佬，因为玻尔兹曼机（BM）、受限玻尔兹曼机（RBM）、深度信念网络（DBN）相关内容是我毕设的主题，恰好在网上看到了这位大佬的相关研究，特别赞，以下附上该为大佬的<a href="https://www.zhihu.com/people/qing-kuang-shu-sheng-18-44">知乎主页</a>和大佬的<a href="https://github.com/2019ChenGong/Machine-Learning-Notes">gthub主页</a>。里面有大佬对于这些模型的公式推导。下面写一些我学习后的个人总结，里面大部分是一些大佬的推导过程，大家想深入研究，直接看大佬的专栏就好，这里仅用来自我总结。</p><h1>介绍</h1><p>  玻尔兹曼机（Boltzmann Machine，BM）可以看作是一种随机动力系统，每个变量的状态都以一定的概率受到其他变量的影响。下面是任何讲到BM都会涉及到的经典模型图：<br><img src="/2022/03/07/bo-er-zi-man-ji/1.png" alt="玻尔兹曼模型图"><br>  其中BM模型有如下的三个特性：<br>  （1）每个随机变量都可以用一个二值的随机变量表示，即0和1。<br>  （2）所有节点之间都是全连接的。<br>  （3）每两个变量之间的相互影响是对称的，即权重对称。<br>  我们假设 $ {v_{D \times 1} } \in { \{0,1\}^D } $，$ {h_{P \times 1} } \in { \{0,1\}^P} $。根据“受限玻尔兹曼”那节的知识，可以得出，概率图的联合概率分布为：<br>$$<br>\begin{array}{l}<br>P(v,h) = \frac{1}{2}\exp {  - E(v,h)} \\<br>E(v,h) =  - ({v^T} \cdot W \cdot h + \frac{1}{2}{v^T} \cdot Lv + \frac{1}{2}{h^T} \cdot J \cdot h)<br>\end{array}<br>$$<br>其中，$ L = { [ { {L_{ij} } } ]_{D \times D} } $ 表示可见层神经元内部的权重，$ J = { [ { {J_{ij} } } ]_{P \times P} } $ 表示隐藏层神经元内部的权重，$ W = { [ { {w_{ij} } } ]_{D \times P} } $ 表示可见层和隐藏层神经元之间的权重。在此问题中，要学习的参数集合为 $ \theta  = \{ W,L,J\} $。</p><h1>基于极大似然的梯度上升</h1><p>  基于极大似然的梯度上升包括极大似然函数和梯度两个部分。<b>极大似然估计的主要思路是，使极大似然函数最大时的参数</b>。首先明确一下，要求的参数为 $ \theta  = \{ W,L,J\} $。假设样本集合为 $v$，$ |v| = D $，那么似然函数为：<br>$$ \sum\limits_v {P(v) = } \sum\limits_v {\sum\limits_h {P(v,h)} } $$<br>那么对数似然函数为：<br>$$ \frac{1}{D}\sum\limits_v {\log P(v)} $$</p><h2 id="似然导数求解"><a class="header-anchor" href="#似然导数求解">¶</a>似然导数求解</h2><p>  是对对数似然函数求导，即为：<br>$$<br>\frac{\partial }{ {\partial \theta } }\frac{1}{D}\sum \limits_v {\log p(v) } = \frac{1}{D}\sum\limits_v {\frac{ {\partial \log p(v)} }{ {\partial \theta } } }<br>$$<br>而Boltzmann Distribution 的 log 似然梯度为：<br>$$<br>\frac{1}{D}\frac{ \partial }{ { \partial \theta } }\log P(v) = \frac{1}{D}\left( {\sum \limits_h {\sum \limits_v {P(v,h)} } \frac{ \partial }{ { \partial \theta } }E(h,v) - \sum \limits_h {P(h|v)\frac{ \partial }{ { \partial \theta } }E(h,v)} } \right)<br>$$<br>我们对 $ w $ 求导，可以得到：<br>$$<br>\frac{1}{D}\sum \limits_v {\frac{ {\partial \log P(v)} }{ {\partial W} } } =  \frac{1}{D}\sum \limits_v {\sum \limits_h {P(v|h)} }  \cdot v{h^T} - \sum \limits_v {\sum \limits_h {P(v,h)} }  \cdot v{h^T}<br>$$<br>可以简写为：<br>$$<br>\frac{1}{D}\sum \limits_v {\frac{ {\partial \log P(v)} }{ {\partial W} } } = \mathbb{E}_{P_{data} }[v{h^T}] - \mathbb{E}_{P_{model} }[v{h^T}]<br>$$<br>其中：<br>$$<br>\begin{array}{l}<br>{P_{data}} = {P_{data}}(v) \cdot {P_{model}}(h|v) \\<br>{P_{model}} = {P_{model}}(v,h)<br>\end{array}<br>$$<br>在 $ \sum\limits_v {\sum\limits_h {P(v,h)} } $ 中，$ P(v,h) $ 是生成模型，本身就是我们建立的模型，所以被称为 $P_{model}$。而在 $ \sum\limits_v {\sum\limits_h {P(v|h)} } $ 首先从经验分布 $P(v)$ 从采样得到 $v$，然后利用模型分布来求解 $P(h|v)$，所以 $P_{data} = P_{data}(v) \cdot P_{model}(h|v)$。采样出 $P_{model}(h|v)$ 和 $P_{model}(v)$ 就可以求解出 $P_{model}(h,v)$ 了。按照同样的方法可以求得对 ${L, J}$ 的导数。<br>  同理通过计算可以得到每个参数矩阵的似然梯度为：<br>$$<br>\begin{array}{l}<br>\Delta W = \alpha ({\mathbb{E}_{ {P_{data} } } }[v{h^T}] - {\mathbb{E}_{ {P_{model} } } }[v{h^T}]) \\<br>\Delta L = \alpha ({\mathbb{E}_{ {P_{data} } } }[v{v^T}] - {\mathbb{E}_{ {P_{model} } } }[v{v^T}]) \\<br>\Delta J = \alpha ({\mathbb{E}_{ {P_{data} } } }[h{h^T}] - {\mathbb{E}_{ {P_{model} } } }[h{h^T}])<br>\end{array}<br>$$</p><h1>基于MCMC的似然梯度下降</h1><h2 id="MCMC似然梯度求解总述"><a class="header-anchor" href="#MCMC似然梯度求解总述">¶</a>MCMC似然梯度求解总述</h2><p>  在上面我们使用梯度上升法来使 log 似然函数达到最大，从而求解对应的最优参数。参数更新公式为：<br>$$<br>{\theta ^{(t + 1)} } = {\theta ^{(t)} } + \Delta \theta<br>$$<br>其中，$ \Delta \theta  = \{ {\Delta W,\Delta L,\Delta J} \} $ 。以 $ \Delta W $ 为例，$ \Delta W $ 是一个矩阵 $ \Delta W = \left[ {\Delta {w_{ij} } } \right] $。其中：<br>$$<br>\Delta {w_{ij} } = \alpha \left[ {\underbrace { {\mathbb{E}_{ {P_{data} } } }\left[ { {v_i}{h_j} } \right]}_{Postive \; phase} - \underbrace { {\mathbb{E}_{ {P_{\bmod el} } } }\left[ { {v_i}{h_j} } \right]}_{Negative \; phase} } \right]<br>$$<br>  在 Boltzmann Machines 中，Postive phase 和 Negative phase 都是 Intractable。所以，Hinton 提出了用 MCMC 来对 $ P(h|v) $ 进行采样。<b>在求解 $ \Delta W $ 中，主要是解决三个部分，$ P_{data}(v) $, $ P_{model}(h|v) $, $ P_{model}(v, h) $，其中 $ P_{model}(v, h) = P_{model}(h|v) · P_{model}(v) $。所以，而 $ P_{data}(v) $ 和 $ P_{model}(v) $ 相对比较简单，所以难点在于 $ P_{model}(h|v) $ 的求解。</b>但在 Boltzmann Machines 中，由于关系过于复杂，没有办法分解，甚至最大团分解都没有用，因为最大团就是自己，那么连 $ P_{model}(h|v) $ 都求不出来，那么 Postive phase 和 Negative phase 都是 Intractable。<br>  但是通过推导可以得出：<br>$$<br>\begin{array}{l}<br>P({v_i} = 1|h,{v_{ - i} }) = \sigma (\sum\nolimits_{j = 1}^P { {w_{ij} }{h_j} }  + \sum\nolimits_{k = 1\backslash i}^D { {L_{ik} }{v_k} } )\<br>P({h_j} = 1|v,{h_{ - j} }) = \sigma (\sum\nolimits_{j = 1}^P { {w_{ij} }{v_i} }  + \sum\nolimits_{m = 1\backslash j}^D { {J_{im} }{h_n} } )<br>\end{array}<br>$$<br>  公式表达的是，在已知一个节点以外的所有的点的条件下，这个节点的条件概率是可求的。其中 $ 1 \backslash i $ 表达的意思是 $ 1 ∼ D $ 但不包括 $ i $ 的所有节点。<br>  具体的推导过程在大佬的博客和github中有，这里不再搬运。<br>  <b>其他具体的介绍以及推导过程，点击该链接可以下载查看。</b><a href="White_2020_04_08_Boltzmann_Machine.pdf" title="相关资料下载">点击下载</a></p>]]></content>
      
      
      <categories>
          
          <category> 科研学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工智能 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度信念网络</title>
      <link href="/2022/03/04/shen-du-xin-nian-wang-luo/"/>
      <url>/2022/03/04/shen-du-xin-nian-wang-luo/</url>
      
        <content type="html"><![CDATA[<h1>简介</h1><p>  $ Deep \; Belief \; Network $ 是 $ Hinton $ 在 2006 年提出的方法，应用在分类问题上的效果明显好过 $ SVM $。首先，来看看 $ Deep \; Belief \; Network $ 这个名字的含义，$ Belief \; Network $ 实际上就是 $ Bayes \; Network $（有向图模型），而 $ Deep $ 的含义就很简单了，代表有很多层。所以，从字面上理解，$ Deep \; Belief \; Network $ 可以认为是有很多层的有向图模型。$ Deep \; Belief \; Network $ 的概率图模型如下所示：<br><img src="/2022/03/04/shen-du-xin-nian-wang-luo/1.png" alt="Deep Belief Network 的概率图模型"><br>  从上述图中，可以看出 $ DBN $ 是一个混合模型，上面是 $ Restricted \; Boltzmann \; Model $ $ (RBM) $ 模型，下面是一个 $ Sigmoid \; Belief \; Network \; (SBN) $。而每个节点都服从 $ 0/1 $的伯努利分布，实际上就是一个分层模型。深层的含义，我们将会在下文中描述。注意，这里的 $ w $ 是用来描述节点直接连接权重的矩阵。</p><h2 id="SBN-简介"><a class="header-anchor" href="#SBN-简介">¶</a>$ SBN $ 简介</h2><p>  $ SBN $ 的概率图模型如下图所示：<br><img src="/2022/03/04/shen-du-xin-nian-wang-luo/2.png" alt="Sigmoid Belief Network 的概率图模型"><br>其中：<br>$$<br>P({S_i} = 1) = \frac{1}{ {1 + \exp \left\{ { {b_i} + {w_1}{s_1} + {w_2}{s_2} + {w_3}{s_3} } \right\} } }<br>$$</p><h2 id="DBN-的概率联合分布"><a class="header-anchor" href="#DBN-的概率联合分布">¶</a>$ DBN $ 的概率联合分布</h2><p>  在使用极大似然估计法中基本离不开求联合概率分布。所以，这里需要求 $ DBN $ 的联合概率分布，而求联合概率分布最终的就是因子分解。那么，我们首先要理顺一下各层之间的依赖关系。显然，$ v $ 层只和 $ h^{(1)} $ 有关， $ h^{(1)} $ 层只和  $ h^{(2)} $ ，那么有：<br>$$<br>\begin{array}{ccccc}<br>P(v,{h^{(1)} },{h^{(2)} },{h^{(3)} }) = &amp; P(v|{h^{(1)} },{h^{(2)} },{h^{(3)} })P({h^{(1)} },{h^{(2)} },{h^{(3)} }) &amp; \\<br>= &amp; P(v|{h^{(1)} })P({h^{(1)} },{h^{(2)} },{h^{(3)} }) &amp; \\<br>= &amp; P(v|{h^{(1)} })P({h^{(1)} }|{h^{(2)} },{h^{(3)} })P({h^{(2)} },{h^{(3)} }) &amp; \\<br>= &amp; \prod\limits_i { P({v_{i} }|{h^{(1)} }) } \prod\limits_j { P(h_{j}^{(1)} | {h^{(2)} },{h^{(3)} }) P({h^{(2)} },{h^{(3)} }) }<br>\end{array}<br>$$<br>将这三个部分分布表示。根据 $ SBN $ 公式的结论，可以类比的得出：<br>$$<br>\begin{array}{cc}<br>P({v_i}|{h^{(1)} }) = sigmoid({(W_{: , i}^{(1)})}^{T} \cdot {h^{(1)} } + b_{i}^{(0)}) \\<br>P(h_{j}^{(1)}|{h^{(2)} }) = sigmoid({(W_{:,j}^{(2)})^T} \cdot {h^{(2)} } + b_j^{(1)})<br>\end{array}<br>$$<br>而 $ h^{(2)} $ 和 $ h^{(3)}  $ 之间是 $ RBM $ 模型，沿用在 $ RBM $ 那一章讲的 $ Boltzmann \; Distribution $ 可以得到：<br>$$<br>P({h^{(2)} },{h^{(3)} }) = \frac{1}{Z} \exp \{ {({h^{(3)} })^T} {w^{(3)} }{h^{(2)} } + {({h^{(2)} })^T}{b^{(2)} } + {({h^{(3)} })^T}{b^{(3)} } \}<br>$$<br>其中参数为：<br>$$<br>\theta  = \{ {W^{(1)} },{W^{(2)} },{W^{(3)} },{b^{(0)} },{b^{(1)} },{b^{(2)} },{b^{(2)} }\}<br>$$</p><h1>$ DBN $ 的叠加</h1><p>  原始的 $ RBM $ 模型的表达方式是使用对比散度的方法求解。概率图模型如下图所示：<br><img src="/2022/03/04/shen-du-xin-nian-wang-luo/3.png" alt="Restricted Boltzmann Distribution 的概率图模型"><br>$ RBM $ 的对数似然梯度的表达形式如下所示：<br>$$<br>\frac{\partial }{ {\partial {\omega _{ij} } } }\log P(v) = \sum\limits_h {P(h|v){h_i}{v_j} }  - \sum\limits_h {\sum\limits_v {P(h,v){h_i}{v_j} } }<br>$$<br>但是该公式的计算过于复杂，基本是 $ intractable $。所以提出了用结合梯度上升法的 $ Gibbs $ 采样来求梯度，从而使得 $ P(v) $ 的 $ Log ; Likelihood $ 达到最大，公式如下所示：<br>$$<br>\begin{array}{ccccc}<br>\Delta {\omega _{ij}} \leftarrow  - \Delta {\omega _{ij} } + \frac{\partial }{ {\partial {\omega _{ij} } } }\log P(v) \<br>\frac{\partial }{ {\partial {\omega _{ij} } } }\log P(v) = P({h_i} = 1|{v^{(0)} })v_j^{(0)} - P({h_i} = 1|{v^{(k)} })v_j^{(k)}<br>\end{array}<br>$$<br>引入 $ RBM $ 是为了探究观测变量的数据结构的关系，其中未观察变量被看作观察变量发生的原因。所以，模型的关注重点实际上是 $ v $，而 $ h $ 不过是我们为了探究 $ v $ 的数据结构和发生的原因，所做的模型假设而已。$ v $ 是没有标签的数据，可以认为是 $ RBM $ 方法生成的，我们可以通过改进 $ RBM $，让生成的数据更接近真实分布。</p><h2 id="RBM-的改进"><a class="header-anchor" href="#RBM-的改进">¶</a>$ RBM $ 的改进</h2><p>  根据 $ Restricted ; Boltzmann ; Distribution $ 的概率图模型可知<br>$$<br>P(v) = \sum\limits_{ {h^{(1)} } } {P(v,{h^{(1)} })}  = \sum\limits_{ {h^{(1)}}} {P({h^{(1)} })P(v|{h^{(1)} })}<br>$$<br>通常 $ P(h^{(1)}) $ 看成是 $ prior $ 先验，$ P(v|h^{(1)}) $ 则看成是一个生成过程。$ RBM $ 在无向图中并没有箭头，无向图可以看成是一个双向的有向图，如下所示：<br><img src="/2022/03/04/shen-du-xin-nian-wang-luo/4.png" alt="Restricted Boltzmann Distribution 的有向图概率图模型"><br>  这样，将无向图改写成有向图，可以把概率图分成 $ h \rightarrow v $ 和 $ v \rightarrow h $ 两个过程。两个过程的权重都是一样的。那么假定在 $ RBM $ 已经学习出来的情况下（ $ w^{(1)} $ 的参数是确定的），$ P(v|h^{(1)}) $ 可以看成是 $ h \rightarrow v $ ，显示是不变得，而$ P(h^{(1)}) $ 显然也是确定的，和 $ v \rightarrow h $ 过程相关，$ w^{(1)} $ 和 $ v $ 都是确定的。那么，我们可以猜想，可不可以不用 $ w $ 来表示 $ P(h^{(1)}) $，给 $ P(h^{(1)}) $ 重新赋一组参数，构建一个新的模型来对 $ P(h^{(1)}) $ 重新建模，用另一个 $ RBM $ 来表示 $ P(h^{(1)}) $，从而通过提高 $ P(h^{(1)}) $ 的办法来提高 $ P(v) $<br>  我们可以添加一层 $ RBM $，这样就可以给 $ P(h^{(1)}) $ 重新赋一组参数，然后通过新的 $ RBM $ 参数进行优化的方式来提高 $ P(h^{(1)}) $ 。如下图所示：<br><img src="/2022/03/04/shen-du-xin-nian-wang-luo/5.png" alt="Restricted Boltzmann Machine 的改进示意图"><br>  由此，可以认为这样一个模型，比原来的 $ RBM $ 更好，因为假设 $ P(v|b^{(1)}) $ 是固定的，实际可以优化 $ P(h^{(1)}) $ 来进一步提高模型的性能。同样的思路，可以用同样的办法来优化 $ P(h^{(2)}) $ ，所以就达到了不停的往上加的效果。<br>  我们希望在训练 $ h(1) $ 层的时候，希望 $ v $ 不会对 $ h(1) $ 造成影响，否则计算复杂度就太高了。所以，就假设在训练 $ h(1) $ 的过程中和 $ v $ 无关，所以概率图模型就变为：<br><img src="/2022/03/04/shen-du-xin-nian-wang-luo/6.png" alt="服从假设后的 RBM 改进示意图"><br>  <b>其他具体的介绍以及推导过程、训练过程，点击该链接可以下载查看。</b><a href="White_2020_04_01_Deep_Belief_Network.pdf" title="相关资料下载">点击下载</a></p>]]></content>
      
      
      <categories>
          
          <category> 科研学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工智能 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>受限玻尔兹曼机</title>
      <link href="/2022/03/03/shou-xian-bo-er-zi-man-ji/"/>
      <url>/2022/03/03/shou-xian-bo-er-zi-man-ji/</url>
      
        <content type="html"><![CDATA[<h1>简介</h1><p>  受限玻尔兹曼机（英语：$ restricted \; Boltzmann \; machine \; , RBM $ ）是一种可通过输入数据集学习概率分布的随机生成神经网络。受限玻兹曼机在降维、分类、协同过滤、特征学习和主题建模中得到了应用。根据任务的不同，受限玻兹曼机可以使用监督学习或无监督学习的方法进行训练。受限玻尔兹曼机是玻尔兹曼机（$ Boltzman \; machine，BM $）的一种特殊拓扑结构。$ BM $ 的原理起源于统计物理学，是一种基于能量函数的建模方法，能够描述变量之间的高阶相互作用，$ BM $ 的学习算法较复杂，但所建模型和学习算法有比较完备的物理解释和严格的数理统计理论作基础。$ BM $ 是一种对称耦合的随机反馈型二值单元神经网络，由可见层和多个隐层组成，网络节点为可见单元（$ visible \;unit $）和隐单元（$ hidden \; unit $），用可见单元和隐单元来表达随机网络与随机环境的学习模型，通过权值表达单元之间的相关性。正如名字所提示的那样，受限玻兹曼机是一种玻兹曼机的变体，但限定模型必须为二分图。模型中包含对应输入参数的输入（可见）单元和对应训练结果的隐单元，每条边必须连接一个可见单元和一个隐单元。（与此相对，“无限制”玻兹曼机包含隐单元间的边，使之成为递归神经网络。）这一限定使得相比一般玻兹曼机更高效的训练算法成为可能，特别是基于梯度的对比分歧（$ contrastive \; divergence $）算法。受限玻兹曼机也可被用于深度学习网络。具体地，深度信念网络可使用多个RBM堆叠而成，并可使用梯度下降法和反向传播算法进行调优。</p><h1>结构</h1><p>  $ RBM $ 可以说是所有神经网络中最简单的架构之一，属于两层神经网络，并且这些浅层神经网络是 $ DBN $（深度信念网络）的构建块。如下图，一个受限玻尔兹曼机包含一个输入层 $ visible \; layer $（第一层）和隐藏层 $ hidden \; layer $（第二层）。<br><img src="/2022/03/03/shou-xian-bo-er-zi-man-ji/1.png" alt="在这幅图片中"><br>  因为所有可见（或输入）节点的输入都被传递到所有的隐藏节点了，所以 $ RBM $ 可以被定义为对称二分图。对称意味着每个可见节点都与一个隐藏节点相连。二分则意味着它具有两部分，或者两层。图是一个数学术语，指的是由节点和边组成的网络。<br>  在每一个隐藏节点，每个输入 $ x $ 都与对应的权重 $ w $ 相乘。也就是说，一个输入 $ x $ 会拥有 12 个权重（4 个输入节点 × 3 个输出节点）。两层之间的权重总会形成一个矩阵，矩阵的行数等于输入节点的个数，列数等于输出节点的个数。每个隐藏节点会接收 4 个与对应权重相乘的输入。这些乘积的和再一次与偏置相加，并将结果馈送到激活函数中以作为隐藏单元的输出。<br>  如果这两层是更深网络的一部分，那么第一个隐藏层的输出会被传递到第二个隐藏层作为输入，从这里开始就可以有很多隐藏层，直到它们增加到最终的分类层。对于简单的前馈网络，$ RBM $ 节点起着自编码器的作用，除此之外，别无其它。<br><img src="/2022/03/03/shou-xian-bo-er-zi-man-ji/2.png" alt="在这幅图片中"></p><h1>能量模型</h1><p>  有些深度学习架构使用能量来衡量模型的质量，深度学习模型的目的之一是编码变量间的依赖关系。给变量的每种配置分配一个标量作为能量，可以描述这一依赖关系。能量较高意味着变量配置的兼容性不好。能量模型总是尝试最小化一个预先定义的能量函数。<br>  $ RBM $ 的能量函数定义为：<br>$$ E(v,h) =  - \sum\limits_i { {a_i}{v_i} - \sum\limits_j { {b_j}{h_j} - \sum\limits_{i,j} { {v_i}{h_j}{w_{ij} } } } } $$<br>  由定义可知，能量函数的值取决于变量/输入状态、隐藏状态、权重和偏置的配置。RBM的训练包括为给定的输入值寻找使能量达到最小值的参数。</p><h1>能量模型</h1><p>  受限玻尔兹曼机是一个概率模型。这一模型并不分配离散值，而是分配概率。在每一时刻 $ RBM $ 位于一个特定的状态。该状态指输入层 $v$ 和隐藏层 $h$ 的神经元值。观察到 $v$ 和 $h$ 的特定状态的概率由以下联合分布给出：<br>$$<br>\begin{array}{l}<br>p(v,h) = \frac{1}{Z}{e^{ - E(v,h)} } \\<br>Z = \sum\limits_{v,h} { {e^{ - E(v,h)} } }<br>\end{array}<br>$$<br>  这里 $Z$ 被称为<b>配分函数</b>（partition function），该函数累加所有输入向量和隐藏向量的可能组合。<br>  在物理学中，这一联合分布称为玻尔兹曼分布，它给出一个微粒能够在能量 $E$ 的状态下被观测到的概率。就像在物理中一样，我们分配一个观测到状态 $v$ 和 $h$ 的概率，这一概率取决于整个模型的能量。不幸的是，由于配分函数 $Z$ 中 $v$ 和 $h$ 所有可能的组合数目十分巨大，计算这一联合分布十分困难。而给定状态 $v$ 计算状态 $h$ 的条件概率，以及给定状态 $h$ 计算状态 $v$ 的条件概率则要容易得多：<br>$$<br>\begin{array}{l}<br>p(h|v) = \prod\limits_i {p({h_i}|v)} \\<br>p(h|v) = \prod\limits_i {p({v_i}|h)}<br>\end{array}<br>$$<br>  $ RBM $ 中的每个神经元只可能是二元状态0或1中的一种。我们最关心的因子是隐藏层或输入层地神经元位于状态1（激活）的概率。给定一个输入向量 $v$ ，单个隐藏神经元 $j$ 激活的概率为：<br>$$ p({h_j} = 1|v) = \frac{1}{ {1 + {e^{( - ({b_j} + {W_j}{v_i}))} } } } = \sigma ({b_j} + \sum\limits_i { {v_i}{w_{ij} } } ) $$<br>  其中，$\sigma$ 为 $ sigmoid $ 函数。以上等式可由对之前的条件概率等式应用贝叶斯定理推导得出，类似地，单个输入神经元 $i$ 为 1 的概率为：<br>$$<br>p({v_j} = 1|h) = \frac{1}{ {1 + {e^{( - ({a_i} + {W_i}{h_j}))} } } } = \sigma ({a_i} + \sum\limits_j { {h_j}{w_{ij} } } )<br>$$<br>  <b>其他具体的介绍以及推导过程，点击该链接可以下载查看。</b><a href="White_2020_02_28_Restricted_Boltzmann_Machine.pdf" title="相关资料下载">点击下载</a></p>]]></content>
      
      
      <categories>
          
          <category> 科研学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工智能 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>差分隐私个人理解（一）</title>
      <link href="/2022/01/24/chai-fen-yin-si-ge-ren-li-jie-yi/"/>
      <url>/2022/01/24/chai-fen-yin-si-ge-ren-li-jie-yi/</url>
      
        <content type="html"><![CDATA[<h1>差分隐私简介</h1><p>  差分隐私（differential privacy）是密码学中的一种手段，旨在提供一种当从统计数据库查询时，通过混淆数据库查询结果，来实现数据在个人层面的隐私性，最大化数据查询的准确性。<br>  举一个简单的例子：例如，下表显示了一个医疗数据集 $ D $ 。<br><img src="/2022/01/24/chai-fen-yin-si-ge-ren-li-jie-yi/1.png" alt="医疗数据集示例"><br>其中的每个记录表示某个人是否患有癌症（１表示是，０表示否）。数据集为用户提供统计查询服务（例如计数查询），但不能泄露具体记录的值．设用户输入参数 $ S_{i} $，调用查询函数 $ f(i)=count(i) $ 来得到数据集前 $ i $ 行中满足“诊断结果” $ = 1 $ 的记录数量，并将函数值反馈给用户。假设攻击者欲推测 $ Alice $ 是否患有癌症，并且知道 $ Alice $在数据集的第5行，那么可以用 $ count(5) - count(4) $ 来推出正确的结果。<br>  但是，如果 $ f $ 是一个提供 $\varepsilon$-差分隐私保护的查询函数，例如$ f(i)=count(i) + noise $，其中 $ noise $ 是服从某种随机分布的噪声．假设 $ f(5) $ 可能的输出来自集合 $ \{2,2.5,3 \} $，那么 $ f(4) $ 也将以几乎完全相同的概率输出 $ \{2,2.5,3 \} $ 中的任一可能的值，因此攻击者无法通过 $f(5) - f(4) $ 来得到想要的结果．这种针对统计输出的随机化方式使得攻击者无法得到查询结果间的差异，从而能保证数据集中每个个体的安全。</p><h1>差分隐私的定义</h1><p>  对于一个有限域 $ Z $，$ z \in Z $ 为 $ Z $ 中的元素，从 $ Z $ 中抽样所得 $ z $ 的集合组成数据集 $ D $，其样本量为 $ n $，属性的个数为维度 $ d $ 。<br>  对数据集 $ D $ 的各种映射函数被定义为查询 $ (Query) $ ，用 $ F = \{ f_{1},f_{2},\cdots \} $ 来表示一组查询，算法 $ M $ 对查询 $ F $ 的结果进行处理，使之满足隐私保护的条件，此过程称为隐私保护机制。<br>  设数据集 $ D $ 和 $ {D’} $，具有相同的属性结构，两者的对称差记作 $ D \Delta D’ $，$ |D \Delta D’| $ 表示 $ D \Delta D’ $ 中记录的数量．若$ |D \Delta D’| = 1 $，则称 $ D $ 和 $ {D’} $ 为邻近数据集 $(Adjacent Dataset )$。<br>  <b>定义1.差分隐私.</b>设有随机算法 $ M $，$ P_{M} $ 为 $ M $ 所有可能的输出构成的集合．对于任意两个邻近数据集（什么是临近数据集在下面相关概念中会介绍） $ D $ 和 $ {D’} $ 以及 $ P_{M} $ 的任何子集 $ S_{M} $，若算法 $ M $ 满足：<br>$$<br>\Pr [M(D) \in {S_M}] \le \exp (\varepsilon ) \times \Pr [M(D’) \in {S_M}]<br>$$<br>则称算法 $ M $ 提供 $\varepsilon$-差分隐私保护，其中参数 $ \varepsilon $ 称为隐私保护预算。<br>  我们来看一个极端的设定，当 $ \varepsilon = 0 $ ，得到<br>$$<br>\Pr [M(D) \in {S_M}] = \Pr [M(D’) \in {S_M}]<br>$$<br>也就是说，通过算法 $ M $ 对两个相邻的数据集进行查询后，得到的查询集合是相同的，即不会揭露数据集中的信息，因此随机算法 $ M $ 具有极好的隐私性，但是这样会导致查询不会提供关于这个数据库的任何信息。所以，一般让 $ \varepsilon $ 取一个比较小的数，比如 $ 0.05 $，让 $ M $ 满足这种条件就可以了，又能通过 $ M $ 获得一些有用的信息。直白地说，就是只有一行不同的两个数据库，进行随机算法 $ M $ 后，这两个数据库分别得到的结果有相似的概率分布。</p><h1>相关概念介绍</h1><h2 id="数据集（数据库）的距离"><a class="header-anchor" href="#数据集（数据库）的距离">¶</a>数据集（数据库）的距离</h2><p>  对于数据库 $ x $，它的第一范数为：<br>$$<br>{\left| { {x} } \right|_{1} } = \sum\limits_{i = 1}^{|\chi |} { {x_i} }<br>$$<br>两个数据库 $ x $ 和 $ y $ 的 $ l_{1} $ 距离是 $ {\left| {x - y} \right|_1} $。对于 $ {\left| { {x} } \right|_1} $ 表示数据库x的大小。 $ {\left| {x - y} \right|_1} $ 表示数据 $ x $ 和 $ y $ 不同元素的个数。</p><h2 id="隐私保护预算-varepsilon"><a class="header-anchor" href="#隐私保护预算-varepsilon">¶</a>隐私保护预算 $ \varepsilon $</h2><p>  从定义１可以看出，隐私保护预算 $ \varepsilon $ 用来控制算法 $ M $ 在两个邻近数据集上获得相同输出的概率比值，它事实上体现了 $ M $ 所能够提供的隐私保护水平。在实际应用中， $ \varepsilon $ 通常取很小的值，例 如 $ 0.01 $，$ 0.1 $，或者，$ \ln 2 $，$ \ln 3 $ 等。 $ \varepsilon $ 越小，表示隐私保护水平越高．当 $ \varepsilon $ 等于 $ 0 $ 时，保护水平达到最高，此时对于任意邻近数据集，算法都将输出两个概率分布完全相同的结果，这些结果也不能反映任何关于数据集的有用的信息．因此， $ \varepsilon $ 的取值要结合具体需求来达到输出结果的安全性与可用性的平衡。</p><h2 id="敏感度"><a class="header-anchor" href="#敏感度">¶</a>敏感度</h2><p>  差分隐私保护可以通过在查询函数的返回值中加入适量的干扰噪声来实现。加入噪声过多会影响结果的可用性，过少则无法提供足够的安全保障。敏感度是决定加入噪声量大小的关键参数，它指删除数据集中任一记录对查询结果造成的最大改变。在差分隐私保护方法中定义了两种敏感度，即全局敏感度（Global Sensitivity）和局部敏感度（Local Sensitivity）。</p><h3 id="全局敏感度"><a class="header-anchor" href="#全局敏感度">¶</a>全局敏感度</h3><p>  <b>定义2.全局敏感度.</b>设有函数 $ f:D \to {R^d} $ ，输入为一数据集，输出为一 $ d $ 维实数向量。对于任意的邻近数据集 $ D $ 和 $ {D’} $ ，其实<br>$$ G{S_f} = \max\limits_{D, D’} { \left| {f(D) - f({D’})} \right|_1} $$<br>称为函数 $ f $ 的全局敏感度。<br>  其中，$ { \left| {f(D) - f({D’})} \right|_1} $ 是 $ f(D) $ 和 $ f(D’) $ 之间的 $1$-阶范数距离。<br>  函数的全局敏感度由函数本身决定，不同的函数会有不同的全局敏感度．一些函数具有较小的全局敏感度（例如计数函数，其全局敏感度为 $１$），因此只需加入少量噪声即可掩盖因一个记录被删除对查询结果所产生的影响，实现差分隐私保护．但对于某<br>些函数而言，例如求平均值、求中位数等函数，则往往具有较大的全局敏感度．以求中位数函数为例，设函数为 $ f(D) = median({x_1},{x_2}, \cdots ,{x_n}) $，其中 $ x_{i} (i=１, \cdots ,n) $ 是区间 $ [a, b] $ 中的一个实数，不妨设 $ n $ 为奇数，且数据已被排序，那么函数的返回值即为第 $ m=(n + 1) / 2 $ 个数．在某种极端的情况下，设 $ x_1 = x_2 = \cdots = $ $ x_m = a $ 且 $ x_{m + 1} = x_{m + 2} = \cdots = x_{n} = b $，那么从中删除一个数就可能使函数的返回值由 $ a $ 变为 $ b $，因此函数的全局敏感度为 $ b - a $，这可能是一个很大的值。<br>  当全局敏感度较大时，必须在函数输出中添加足够大的噪声才能保证隐私安全，导致数据可用性较差。</p><h3 id="局部敏感度"><a class="header-anchor" href="#局部敏感度">¶</a>局部敏感度</h3><p>  <b>定义2.局部敏感度.</b>设有函数 $ f:D \to {R^d} $ ，输入为数据集 $ D $ ，输出为一 $ d $ 维实数向量。对于给定数据集 $ D $ 和它的任意邻近数据集 $ {D’} $ ，则<br>$$<br>LS_{f}(D) = \max\limits_{D} { \left| {f(D) - f({D’})} \right|_1}<br>$$<br>称为函数 $ f $ 在 $ D $ 上的局部敏感度。<br>  局部敏感度由函数 $ f $ 及给定数据集 $ D $ 中的具体数据共同决定。由于利用了数据集的数据分布特征，局部敏感度通常要比全局敏感度小得多。以前文的求<br>中位数函数为例，其局部敏感度为 $ \max ( x_{m} - x_{m - 1}, x_{m + 1} - x_{m}) $。另外，局部敏感度与全局敏感度之间的关系可以表示为<br>$$<br>GS_{f} = \max\limits_{D}(LS_{f}(D))<br>$$<br>  但是，由于局部敏感度在一定程度上体现了数据集的数据分布特征，如果直接应用局部敏感度来计算噪声量则会泄露数据集中的敏感信息。因此，局部敏感度的平滑上界（Smooth Upper Bound）被用来与局部敏感度一起确定噪声量的大小。</p>]]></content>
      
      
      <categories>
          
          <category> 科研学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据安全 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>内存管理（一）</title>
      <link href="/2021/04/12/nei-cun-guan-li-yi/"/>
      <url>/2021/04/12/nei-cun-guan-li-yi/</url>
      
        <content type="html"><![CDATA[<h1>内存管理概念</h1><h2 id="内存管理的基本原理和要求"><a class="header-anchor" href="#内存管理的基本原理和要求">¶</a>内存管理的基本原理和要求</h2><p>  内存管理就是操作系统对内存的管理和分配。<br>  内存管理的功能有：<br>  （1）<strong>内存空间的分配与回收</strong>：由操作系统完成主存储器空间的分配和管理。尤其是在编程时，可以提高编程效率。<br>  （2）<strong>地址转换</strong>：在多道程序环境下，程序中的逻辑地址与内存中的物理地址不可能一致，因此存储管理必须提供地址变换功能，把逻辑地址转换成相应的物理地址。<br>  （3）<strong>内存空间的扩充</strong>：利用虚拟存储技术或自动覆盖技术，从逻辑上扩充内存。<br>  （4）<strong>存储保护</strong>：保证各道作业在各自的存储空间内运行，互不干扰。</p><h3 id="程序装入和链接"><a class="header-anchor" href="#程序装入和链接">¶</a>程序装入和链接</h3><p>  创建进程首先要将程序和数据装入内存。将用户源程序变为可在内存中执行的程序，其主要经过以下几个步骤：<br>  （1）<strong>编译</strong>：由编译程序将用户源代码编译成若干目标模块<br>  （2）<strong>链接</strong>：由链接程序将编译后形成的一组目标模块及所需的库函数链接在一起，形成一个完整的装入模块。<br>  （3）<strong>装入</strong>：由装入程序将装入模块装入内存运行。<br><img src="/2021/04/12/nei-cun-guan-li-yi/1.png" alt="将用户程序变为可在内存中执行的程序的步骤"><br>  程序的链接有以下三种方式：<br>  （1）<strong>静态链接</strong>：在程序运行之前，先将各目标模块及它们所需的库函数链接成一个完整的可执行程序，以后不再拆开。<br>  （2）<strong>装入时动态链接</strong>：将用户源程序编译后所得到的一组目标模块，在装入内存时，采用边装入边链接的方式。<br>  （3）<strong>运行时动态链接</strong>：对某些目标模块的链接，是在程序执行中需要该目标模块时才进行的，其优点是便于修改和更新，便于实现对目标模块的共享。<br>  内存的装入模块在装入内存时，同样有以下三种方式：<br>  （1）<strong>绝对装入</strong>：在编译时，如果知道程序将驻留在内存的某个位置，编译程序将产生绝对地址的目标代码。绝对装入程序按照装入模块中的地址，将程序和数据装入内存。由于程序中的逻辑地址与实际内存地址完全相同，故不需对程序和数据的地址进行修改。<strong>绝对装入方式只适用于单道程序环境</strong>。另外，程序中所使用的绝对地址,可在编译或汇编时给出，也可由程序员直接赋予。而通常情况下在程序中采用的是符号地址，编译或汇编时再转换为绝对地址。<br><img src="/2021/04/12/nei-cun-guan-li-yi/2.png" alt="绝对装入方式"><br>  （2）<strong>可重定位装入</strong>：在多道程序环境下，多个目标模块的起始地址通常都是从$0$开始，程序中的其他地址都是相对于起始地址的,此时应采用可重定位装入方式。根据内存的当前情况，将装入模块装入到内存的适当位置。<strong>装入时对目标程序中指令和数据的修改过程称为重定位</strong>，地址变换通常是在装入时一次完成的，所以又称为静态重定位。静态重定位的特点是在一个作业装入内存时，必须分配其要求的全部内存空间，如果没有足够的内存，就不能装入该作业。此外，作业一旦进入内存后，在整个运行期间不能在内存中移动，也不能再申请内存空间。<br><img src="/2021/04/12/nei-cun-guan-li-yi/3.png" alt="可重装定位装入方式"><br>  （3）<strong>动态运行时装入</strong>：又称<strong>动态重定位</strong>，装入程序把装入模块装入内存后，并不立即把装入模块中的相对地址转化为绝对地址，而是推迟到程序要真正执行时才进行。因此，装入内存后的所有地址均为相对地址。这种方法需要一个<strong>重定位寄存器</strong>的支持。动态重定位有如下特点：**可以将程序分配到不连续的存储区中；在程序运行之前可以只装入它的部分代码即可投入运行，然后在程序运行期间，根据需要动态申请分配内；便于程序段的共享，可以向用户提供一个比存储空间大得多的地址空间。**如下图所示：<br><img src="/2021/04/12/nei-cun-guan-li-yi/4.png" alt="动态重定位"></p><h3 id="逻辑地址空间与物理地址空间"><a class="header-anchor" href="#逻辑地址空间与物理地址空间">¶</a>逻辑地址空间与物理地址空间</h3><p>  程序经过编译后，每个目标模块都是从$0$号单元开始编址，称为该目标模块的<strong>相对地址（或逻辑地址）</strong>。当链接程序将各个模块链接成一个完整的<font color="red"><strong>可执行目标程序</strong></font>时，链接程序顺序依次按各个模块的相对地址构成统一的从$0$号单元开始编址的<strong>逻辑地址空间</strong>。并且用户程序和程序员<font color="red"><strong>只需要知道逻辑地址而内存管理的具体机制则是完全透明的</strong></font>。<strong>不同进程可以有相同的逻辑地址，因为这些相同的逻辑地址可以映射到主存的不同位置</strong>。<br>  <font color="blue"><strong>物理地址空间</strong></font>是指内存中<strong>物理单元</strong>的集合，它是地址转换的最终地址，进程在运行时<strong>执行指令和访问数据</strong>都要通过物理地址从主存中存取。当装入程序将可执行代码装入内存时，<strong>必须通过地址转换将逻辑地址转换成物理地址</strong>，这个过程称为<font color="blue"><strong>地址重定位</strong></font>。</p><h3 id="内存保护"><a class="header-anchor" href="#内存保护">¶</a>内存保护</h3><p>  内存分配前，需要保护操作系统不受用户进程的影响，同时保护用户进程不受其他用户进程的影响。可以采取的两种内存保护的方法：<br>  （1）在$CPU$中设置一对<font color="blue"><strong>上、下限寄存器</strong></font>，存放用户在主存中的下限和上限地址，每当CPU要访问一个地址时，分别和两个寄存器的值相比，判断有无越界。<br>  （2）通过采用<font color="blue"><strong>重定位寄存器（基址寄存器）</strong></font>和<font color="blue"><strong>界地址寄存器（限长寄存器）</strong></font>来实现这种保护。重定位寄存器含最小的物理地址值，界地址寄存器含逻辑地址的最大值。每个逻辑地址值必须小于界地址寄存器，内存管理机构动态的将逻辑地址与界地址寄存器进行比较，如果未发生地址越界，则加上重定位寄存器的值后映射成物理地址，再送交内存单元。其中，<strong>界地址寄存器是用来与逻辑地址进行比较的，如果界地址寄存器中存储的逻辑地址的最大值小于逻辑地址，则未发生越界。而重定位寄存器是用来“加”的，逻辑地址加上重定位寄存器中的值就能得到物理地址</strong>。<br><img src="/2021/04/12/nei-cun-guan-li-yi/5.png" alt="重定位和界地址寄存器的硬件支持"></p><h2 id="覆盖与交换"><a class="header-anchor" href="#覆盖与交换">¶</a>覆盖与交换</h2><p>  在多道程序环境下，计算机的内存往往并不充裕，操作系统需要对内存进行管理，进行合理的划分和有效的动态分配，并且需要<strong>进行内存的扩充</strong>，而<font color="red"><strong>覆盖</strong></font>与<font color="red"><strong>交换</strong></font>这两种技术就是用来扩充内存的方法。</p><h3 id="覆盖"><a class="header-anchor" href="#覆盖">¶</a>覆盖</h3><p>  早期的计算机内存很小，经常会出现内存大小不够的情况。后来人们引入了覆盖技术，用来解决<strong>程序大小超过物理内存总和</strong>的问题。<br>  <strong>覆盖的基本思想</strong>：因为程序并非任何时候都要访问程序及数据各个部分，因此，将程序分为多个段（多个模块）。常用的段常驻内存，不常用的段在需要时调入内存。内存中分为一个<strong>固定区</strong>和若干个<strong>覆盖区</strong>。需要常驻内存的段放在<strong>固定区</strong>中，其余部分按调用关系分段，首先将那些即将要访问的段放入覆盖区，其他段放在外存中，在需要调用前，系统再将其调入覆盖区，替换覆盖区中原有的段。例如在下图中：该程序正文段所需要的内存空间是：<br>$$A(20KB) + B(50KB) + F(30KB) + C(30KB) + D(20KB) + E(40KB) = 190KB$$<br>但在采用了覆盖技术后，只需要空间：<br>$$A(20KB) + B(50KB) + E(40KB) = 110KB$$<br><img src="/2021/04/12/nei-cun-guan-li-yi/6.png" alt="覆盖技术示例图"><br>  覆盖技术的特点：（1）打破了必须将一个程序的全部信息装入主存后才能运行的限制。（2）当同时运行程序的代码量大于主存时仍不能运行。（3）内存中能够更新的地方只有覆盖区的段，不在覆盖区中的段会常驻内存。</p><h3 id="交换"><a class="header-anchor" href="#交换">¶</a>交换</h3><p>  <strong>交换（对换）技术的设计思想</strong>：内存空间紧张时，系统将内存中某些处于等待状态（或在$CPU$调度原则下被剥夺运行权利）的程序从内存移到外存，这一过程称为<strong>换出</strong>。把外存中某些已具备运行条件的进程换入内存（进程在内存与磁盘间动态调度）这一过程称为<strong>换入</strong>。那些被换出至外存的进程称为挂起状态，再根据换出之前进程所处的状态又可以分为<font color="blue"><strong>就绪挂起</strong></font>以及<font color="blue"><strong>阻塞挂起</strong></font>两种状态。在之前中提到的<strong>中级调度采用的就是交换技术</strong>。下面是进程各种状态之间的转化关系：<br><img src="/2021/04/12/nei-cun-guan-li-yi/7.png" alt="进程各状态之间的转换关系"><br>  交换需要注意的问题：（1）交换需要备份存储。（2）为了有效使用$CPU$，需要使每个进程的执行时间比交换时间长。（3）若换出进程，则必须保证该进程完全处于空闲状态。（4）交换空间通常作为磁盘的一整块，且独立于系统文件。（5）交换通常在有许多进程运行且内存空间吃紧时开始启动，而在系统负荷降低时暂停。<br>  <font color="red"><strong>交换技术主要在不同进程（或作业）中进行，而覆盖则用于同一个程序或进程中</strong></font>。现代操作系统是通过虚拟内存技术来解决覆盖技术对于用户和程序员不透明，主存无法存放用户程序的矛盾的。</p><h2 id="连续分配管理方式"><a class="header-anchor" href="#连续分配管理方式">¶</a>连续分配管理方式</h2><p>  连续分配方式是指为一个用户程序分配一个连续的内存空间，连续分配的方式主要包括<strong>单一连续分配</strong>、<strong>固定分区分配</strong>和<strong>动态分区分配</strong>。</p><h3 id="单一连续分配"><a class="header-anchor" href="#单一连续分配">¶</a>单一连续分配</h3><p>  </p>]]></content>
      
      
      <categories>
          
          <category> 课本学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 操作系统 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>进程管理（四）</title>
      <link href="/2021/04/07/jin-cheng-guan-li-si/"/>
      <url>/2021/04/07/jin-cheng-guan-li-si/</url>
      
        <content type="html"><![CDATA[<h1>死锁</h1><h2 id="死锁的概念"><a class="header-anchor" href="#死锁的概念">¶</a>死锁的概念</h2><h3 id="死锁的定义"><a class="header-anchor" href="#死锁的定义">¶</a>死锁的定义</h3><p>  在多道程序系统中，多个进程会并发执行，提高了处理器的吞吐量和资源的利用率。但是在多个进程的并发执行过程中，经常会出现死锁的问题，死锁就是：<strong>多个进程因竞争资源而造成的一种僵局（互相等待），若无外力作用，这些进程都将无法向前进行</strong>。</p><h3 id="死锁产生的原因"><a class="header-anchor" href="#死锁产生的原因">¶</a>死锁产生的原因</h3><h4 id="系统资源的竞争"><a class="header-anchor" href="#系统资源的竞争">¶</a>系统资源的竞争</h4><p>  通常系统拥有的不可剥夺的资源在数量上不足以满足多个进程运行的需求，使得进程在运行过程中，会因资源争夺而陷入僵局，例如：磁带、打印机等。只有对不可剥夺资源的竞争才可能产生死锁。</p><h4 id="进程推进顺序非法"><a class="header-anchor" href="#进程推进顺序非法">¶</a>进程推进顺序非法</h4><p>  （1）进程在运行过程中，请求和释放资源顺序不当，也会导致死锁。例如：并发进程$P_{1}, P_{2}$分别保持了资源$R_{1}, R_{2}$，而进程$P_{1}$申请资源$R_{2}$、进程$P_{2}$申请资源$R_{1}$时，两者都会因为所需资源被占用而阻塞。<br>  （2）信号量使用不当。<br>  （3）进程间彼此相互等待对方发来的消息，也会使得这些进程间无法继续向前推进。例如：进程$A$等待进程$B$发的消息，进程$B$也在等待进程$A$发送的消息，所以可以看出进程$A$和进程$B$不是因为竞争同一资源，而是等待对方的资源导致死锁。</p><h4 id="死锁产生的必要条件"><a class="header-anchor" href="#死锁产生的必要条件">¶</a>死锁产生的必要条件</h4><p>  死锁必须同时满足以下的$4$个条件，只要其中任意一个条件不成立，死锁就不会发生。<br>  （1）<strong>互斥条件</strong>：指进程对所分配到的资源进行排它性使用，即在一段时间内某资源只由一个进程占用。如果此时还有其它进程请求资源，则请求者只能等待，直至占有资源的进程用毕释放。<br>  （2）<strong>不剥夺条件</strong>：指进程已获得的资源，在未使用完之前，不能被剥夺，只能在使用完时由自己释放（主动释放）。<br>  （3）<strong>请求并保持条件</strong>：指进程已经保持至少一个资源，但又提出了新的资源请求，而该资源已被其它进程占有，此时请求进程阻塞，但又对自己已获得的其它资源保持不放。<br>  （4）<strong>循环等待条件</strong>：即存在一个等待队列：$P_{1}$占有$P_{2}$的资源，$P_{2}$占有$P_{3}$的资源，$P_{3}$占有$P_{1}$的资源。这样就形成了一个等待环路。若干进程之间形成一种头尾相接的循环等待资源关系。如下图所示：<br><img src="/2021/04/07/jin-cheng-guan-li-si/1.png" alt="循环等待示例图"></p><h2 id="死锁的处理策略"><a class="header-anchor" href="#死锁的处理策略">¶</a>死锁的处理策略</h2><p>  设法破坏产生死锁的$4$个必要条件之一，或者允许产生死锁，但是当死锁发生时有能力检测出死锁，并有能力实现恢复。</p><h3 id="死锁预防"><a class="header-anchor" href="#死锁预防">¶</a>死锁预防</h3><p>  <strong>通过设置一些限制条件，破坏死锁的四个必要条件中的一个或几个，让死锁无法发生</strong>。例如，将资源分层，得到上一层资源后才能够申请下一层资源，这样就破坏了循环等待条件。用户申请资源时，要求一次性申请所需要的全部资源，这就破坏了占有并等待条件。当一个已经占有某些不可剥夺资源的进程，请求新的资源而得不到满足时，它必须释放已经占有的所有资源，待以后需要时再重新申请，这就破坏了不剥夺条件。<br>  这些预防死锁的方法破坏了系统的并行性和并发性，通常会降低系统的效率。</p><h3 id="避免死锁"><a class="header-anchor" href="#避免死锁">¶</a>避免死锁</h3><p>  在资源的动态分配过程中，用某种方法防止系统进入不安全状态，从而避免死锁。<br>  预防死锁的几种策略，会严重地损害系统性能。因此在避免死锁时，要施加较弱的限制，从而获得较满意的系统性能。由于在避免死锁的策略中，允许进程动态地申请资源。因而，系统在进行资源分配之前预先计算资源分配的安全性。若此次分配不会导致系统进入不安全状态，则将资源分配给进程；否则，进程等待。其中最具有代表性的避免死锁算法是银行家算法。</p><h3 id="死锁的检测及解除"><a class="header-anchor" href="#死锁的检测及解除">¶</a>死锁的检测及解除</h3><p>  无须采取任何限制性措施，允许进程在运行过程中发生死锁。通过系统的检测机构及时地检测出死锁的发生，然后采取某种措施解除死锁。<br>  三种思索策略的比较如下表所示：<br><img src="/2021/04/07/jin-cheng-guan-li-si/2.png" alt="死锁处理策略的比较"></p><h2 id="死锁预防-v2"><a class="header-anchor" href="#死锁预防-v2">¶</a>死锁预防</h2><p>  只需要破坏死锁产生的$4$个必要条件之一即可。<br>  （1）<strong>破坏互斥条件</strong>：例如允许系统资源都能共享使用。但是这种方法不可行。<br>  （2）<strong>破坏不剥夺条件</strong>：当一个已保持了某些不可剥夺的进程请求新的资源而得不到满足时，它必须释放已经保持的所有资源，待以后需要时重新申请，但是这样会导致已获得的资源可能造成前一阶段工作的失效，同时反复的申请和释放资源会增加系统开销，降低系统吞吐量。<br>  （3）<strong>破坏请求并保持条件</strong>：采取预先静态分配方法，即进程在运行前一次申请完它所需要的全部资源，在他的资源未满足前，不投入运行，一旦投入运行，不再提出其他资源请求。但这样会导致系统资源被严重浪费。<br>  （4）<strong>破坏循环等待条件</strong>：将系统中的所有资源统一编号，进程可在任何时刻提出资源申请，但所有申请必须按照资源的编号顺序（升序）提出。这样做就能保证系统不出现死锁。这种方法存在的问题是，编号必须稳定，这就限制了新类型设备的增加，而且经常会发生作业使用资源的顺序与系统规定顺序不同，造成资源浪费。</p><h2 id="死锁避免"><a class="header-anchor" href="#死锁避免">¶</a>死锁避免</h2><p>  死锁避免同样属于事先预防策略，但是并不是事先采取某种限制措施破坏死锁的必要条件，而是在资源动态分配的过程中，防止系统进入不安全状态，以避免发生死锁。并且这种方法施加的限制条件比较弱，可以获得较好的系统性能。</p><h3 id="系统安全状态"><a class="header-anchor" href="#系统安全状态">¶</a>系统安全状态</h3><p>  安全状态是指系统按某种进程推进顺序$(P_{1},P_{2},P_{3},…,P_{n})$为每个进程$P_{i}$分配其所需的资源，直至满足每个进程对资源的最大需求，使每个进程都可顺序完成。此时称$(P_{1},P_{2},P_{3},…,P_{n})$为安全序列。若系统无法找到一个安全序列，则称系统处于不安全状态。<br>  例如系统中有三个进程$P_{1}$、$P_{2}$和$P_{3}$,共有$12$台磁带机。进程$P_{1}$总共需要$10$台磁带机，$P_{2}$和$P_{3}$ 分别需要$4$台和$9$台。假设在$T_{0}$时刻，进程$P_{1}$、$P_{2}$和$P_{3}$已分别获得$5$合、$2$台和$2$台，尚有$3$台未分配，见下表：<br><img src="/2021/04/07/jin-cheng-guan-li-si/3.png" alt="资源分配表"><br>  如果要满足各进程的最大进程，$p_{1}$差$5$台，$p_{2}$差$2$台，$p_{3}$差$7$台，当前可用资源只有$3$台。<br>  将所有进程推进顺序的情况一一列举：<br>  （1）$(P_{1}, P_{2}, P_{3})$，因为$3 &lt; 5$，$P_{1}$没法完成，不符合。<br>  （2）$(P_{1}, P_{3}, P_{2})$，因为$3 &lt; 5$，$P_{1}$没法完成，不符合。<br>  （3）$(P_{2}, P_{1}, P_{3})$，因为$3 &gt; 2$，$P_{2}$可以完成，$P_{2}$完成后释放资源，可用资源为$5$台，$5 = 5$，$P_{1}$可以完成；$P_{1}$完成后释放资源，可用资源为$10$台，$10 &gt; 7$，$P_{3}$可以完成；所有进程都可以完成，符合。<br>  （4）$(P_{2}, P_{3}, P_{1})$，因为$3 &gt; 2$，$P_{2}$可以完成；$P_{2}$完成后释放资源，可用资源为$5$台，$5 &lt; 7$，$P_{3}$没法完成，不符合。<br>  （5）$(P_{3}, P_{1}, P_{2})$，$3 &lt; 7$，$P_{3}$没法完成，不符合。<br>  （5）$(P_{3}, P_{2}, P_{1})$，$3 &lt; 7$，$P_{3}$没法完成，不符合。<br>  综上，存在一个安全序列$(P_{2}, P_{1}, P_{3})$，即只要系统按此进程序列分配资源，则每个进程都能顺利完成,此时系统便进入安全状态,否则进入不安全状态。</p><h3 id="银行家算法"><a class="header-anchor" href="#银行家算法">¶</a>银行家算法</h3><p>  在银行中，客户申请贷款的数量是有限的，每个客户在第一次申请贷款时要声明完成该项目所需的最大资金量，在满足所有贷款要求时，客户应及时归还。银行家在客户申请的贷款数量不超过自己拥有的最大值时，都应尽量满足客户的需要。<br>  因此银行家算法的思想是：把操作系统视为银行家，操作系统管理的资源相当于银行家管理的资金，进程向操作系统请求分配资源相当于用户向银行家贷款。操作系统按照银行家制定的规则为进程分配资源。进程运行之前先声明对各种资源的最大需求量，当进程在执行中继续申请资源时，先测试该进程已占用的资源数与本次申请的资源数之和是否超过该进程声明的最大需求量。若超过则拒绝分配资源，若未超过则再测试系统现存的资源能否满足该进程尚需的最大资源量，若能满足则按当前的申请量分配资源，否则也要推迟分配。</p><h4 id="银行家算法数据结构"><a class="header-anchor" href="#银行家算法数据结构">¶</a>银行家算法数据结构</h4><p>  为了实现银行家算法，在系统中必须设置这样四个数据结构，分别用来描述系统中可利用的资源、所有进程对资源的最大需求、系统中的资源分配，以及所有进程还需要多少资源的情况。<br>  （1）可利用资源向量$Available$。这是一个含有$m$个元素的数组，其中的每一个元素代表一类可利用的资源数目，其初始值是系统中所配置的该类全部可用资源的数目，其数值随该类资源的分配和回收而动态地改变。如果$Available[j] = K$，则表示系统中现$R_{j}$类资源$K$个。<br>  （2）最大需求矩阵$Max$。这是一个$n * m$的矩阵，它定义了系统中$n$个进程中的每个进程对$m$类资源的最大需求。如果$Max[i, j] = K$，则表示进程$i$需要$R_{j}$ 类资源的最大数目为$K$。<br>  （3）分配矩阵$Allocation$。这也是一个$n * m$的矩阵，它定义了系统中每一类资源当前已分配给每一进程的资源数。如果$Allocation[i, j] = K$，则表示进程$i$当前己分得$R_{j}$类资源的数目为$K$。<br>  （4）需求矩阵$Need$.这也是一个$n * m$的矩阵，用以表示每一个进程尚需的各类资源数。如果$Need[i, j] = K$，则表示进程$i$还需要$R_{j}$类资源$K$个方能完成其任务。<br>  上述三个矩阵间存在下述关系:<br>$$Need[i, j] = Max[i, j] - allocation[i, j]$$</p><h4 id="银行家算法描述"><a class="header-anchor" href="#银行家算法描述">¶</a>银行家算法描述</h4><p>  设$Request_{i}$是进程$P_{i}$的请求向量，如果$Requesti_{i}[j] = K$，表示进程$P_{i}$需要$K$个$R_{j}$类型的资源。当$P_{i}$发出资源请求后，系统按下述步骤进行检査:<br>  （1）如果$Request_{i}[j] ≤ Need[i, j]$便转向步骤$(2)$；否则认为出错，因为它所需要的资源数已超过它所宣布的最大值。<br>  （2）如果$Request_{i}[j] ≤ Available[j]$，便转向步骤$(3)$；否则，表示尚无足够资源，$P_{i}$须等待。<br>  （3）系统试探着把资源分配给进程$P_{i}$，并修改下面数据结构中的数值：<br>$$<br>　　Available[j] = Available[j] - Request_{i}[j];\\<br>　　Allocation[i, j] = Allocation[i, j] + Request_{i}[j]; \\<br>　　Need[i, j] = Need[i,j] - Request_{i}[j];<br>$$<br>  （4）系统执行安全性算法，检查此次资源分配后系统是否处于安全状态。若安全，才正式将资源分配给进程$P_{i}$，以完成本次分配；否则，将本次的试探分配作废，恢复原来的资源分配状态，让进程$P_{i}$等待。</p><h4 id="安全性算法"><a class="header-anchor" href="#安全性算法">¶</a>安全性算法</h4><p>  设置工作向量$Work$，它表示系统中的剩余可用资源数目，它含有$m$个元素，在执行安全算法开始时，$Work = Available$；<br>  （1）初始时安全序列为空。<br>  （2）从$Need$矩阵中找出符合下面条件的行：改行对应的进程不再安全序列中，而且该行小于等于$Work$向量，找到后，把对应的进程加入安全序列；若找不到，执行步骤$(4)$<br>  （3）进程$P_{i}$进入安全序列后，可顺利执行，直至完成，并释放分配给它的资源，因此应该执行$Work = Work + Allocation[i]$，其中$Allocatin[i]$表示进程$P_{i}$代表的在$Allocation$矩阵中对应的行，返回步骤$(2)$。<br>  （4）若此时安全序列中已有所有进程，则系统处于安全状态，否则系统处于不安全状态。</p><h2 id="死锁检测和解除"><a class="header-anchor" href="#死锁检测和解除">¶</a>死锁检测和解除</h2><p>  当系统为进程分配资源时，不采取任何措施，则应该提供死锁检测和解除手段。</p><h3 id="资源分配图"><a class="header-anchor" href="#资源分配图">¶</a>资源分配图</h3><p>  系统思索可利用资源分配图来描述，用圆圈代表一个进程，用框代表一类资源，因为一种类型的资源可能有多个，因此用框中的一个圆代表一类资源中的一个资源。从进程到资源的有向边称为<strong>请求边</strong>，表示该进程申请一个单位的该类资源；从资源到进程的边称为<strong>分配边</strong>，表示该类资源已有一个资源分配给了该进程。资源分配图如下图所示：<br><img src="/2021/04/07/jin-cheng-guan-li-si/4.png" alt="资源分配图实例"></p><h3 id="死锁定理"><a class="header-anchor" href="#死锁定理">¶</a>死锁定理</h3><p>  $S$为死锁的条件是当且仅当$S$状态的资源分配图是不可完全简化的，该条件为<strong>死锁定理</strong>。</p><h3 id="死锁解除"><a class="header-anchor" href="#死锁解除">¶</a>死锁解除</h3><p>  （1）<strong>资源剥夺法</strong>：挂起某些死锁进程，并抢占它的资源，将这些资源分配给其他的死锁进程。<br>  （2）<strong>撤销进程法</strong>：强制撤销部分甚至全部死锁进程，并剥夺这些进程的资源。撤销的原则可以按进程优先级和撤销进程代价的高低进行。<br>  （3）<strong>进程回退法</strong>：让一个或（多个）进程回退到足以回避死锁的地步，进程回退时自愿释放资源而非被剥夺。要求系统保持进程的历史信息，设置还原点。</p>]]></content>
      
      
      <categories>
          
          <category> 课本学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 操作系统 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>经典同步问题</title>
      <link href="/2021/04/06/jing-dian-tong-bu-wen-ti/"/>
      <url>/2021/04/06/jing-dian-tong-bu-wen-ti/</url>
      
        <content type="html"><![CDATA[<h1>经典同步动作</h1><h2 id="生产者-消费者问题"><a class="header-anchor" href="#生产者-消费者问题">¶</a>生产者-消费者问题</h2><p>  <strong>问题描述</strong>：一组生产者进程和一组消费者进程共享一个初始为空、大小为$n$的缓冲区，只有缓冲区没满时，生产者才能把消息放入缓冲区，否则必须等待；只有缓冲区不空时，消费者才能从中取出消息，否则必须等待。由于缓冲区是临界资源，它只允许一个生产者放入消息，或一个消费者从中取出消息。<br>  <strong>问题分析</strong>：<br>  （1）<strong>关系分析</strong>：生产者和消费者对缓冲区互斥访问是互斥关系，因为只有生产者生产之后，消费者才能消费，所以也存在同步关系。<br>  （2）<strong>整理思路</strong>：只有生产者和消费者，<strong>同时存在互斥关系和同步关系</strong>，所以相当于解决互斥和同步的$P/V$操作。<br>  （3）<strong>信号量设置</strong>：信号量$mutex$作为互斥信号量，用于控制互斥访问缓冲池，互斥信号量初值为$1$；信号量$full$用于记录当前缓冲池中的“满”缓冲区数，初值为$0$。信号量$empty$用于记录当前缓冲池中的“空”缓冲区数，初值为$n$。</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">semaphore mutex &#x3D; 1;                  &#x2F;&#x2F; 临界区互斥信号量semaphore empty &#x3D; n;                  &#x2F;&#x2F; 空闲缓冲区semaphore full &#x3D; 0;                   &#x2F;&#x2F; 缓冲区初始化为空producer() &#123;                          &#x2F;&#x2F; 生产者进程    while(1) &#123;        produce an item in nextp;     &#x2F;&#x2F; 生产数据        P(empty);                     &#x2F;&#x2F; 获取空缓冲区单元        P(mutex);                     &#x2F;&#x2F; 进入临界区        add nextp to buffer;          &#x2F;&#x2F; 将数据放入缓冲区        V(mutex);                     &#x2F;&#x2F; 离开临界区，释放互斥信号量        V(full);                      &#x2F;&#x2F; 满缓冲区数加1    &#125;&#125;consumer() &#123;                          &#x2F;&#x2F; 消费者进程    while(1) &#123;        P(full);                      &#x2F;&#x2F; 获取满缓冲区单元        P(mutex);                     &#x2F;&#x2F; 进入临界区        remove an item from buffer;   &#x2F;&#x2F; 从缓冲区中取出数据        V(mutex);                     &#x2F;&#x2F; 离开临界区，释放互斥信号量        V(empty);                     &#x2F;&#x2F; 空缓冲区数加1        consume the item;             &#x2F;&#x2F; 消费数据    &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>  需要注意的是：（1）<strong>当缓冲区中有空时，便可对empty变量执行$P$操作，一旦取走一个产品便要执行$V$操作以释放空闲区</strong>。（2）<strong>对$empty$和$full$变量的操作必须放在对$mutex$的$P$操作之前</strong>。</p><h2 id="生产者-消费者问题（扩展）"><a class="header-anchor" href="#生产者-消费者问题（扩展）">¶</a>生产者-消费者问题（扩展）</h2><p>  <strong>问题描述</strong>：桌子上有一个盘子，每次只能向其中放一个水果。爸爸专门向盘子中放入苹果，妈妈专向盘子中放橘子，儿子专等吃盘子中的橘子，女儿专等吃盘子中的苹果。只有当盘子为空时，爸爸或妈妈才可向盘子中放一个水果；仅当盘子中有自己需要的水果时，儿子或女儿可以从盘子中取出。<br>  <strong>问题分析</strong>：<br>  <strong>关系分析</strong>：因为每次只能向其中放入一只水果可知，爸爸和妈妈是互斥关系。爸爸和女儿、妈妈和儿子是同步关系，而且这两对进程必须连起来，儿子和女儿之间没有互斥和同步关系，因此他们是选择条件执行，不可能并发。<br><img src="/2021/04/06/jing-dian-tong-bu-wen-ti/1.png" alt="进程之间的关系"><br>  <strong>整理思路</strong>：在这个问题中有四个进程，但是可抽象为两个生产者和消费者被连接到大小为$1$的缓冲区上。<br>  <strong>信号量设置</strong>：首先设置互斥信号量$plate$，表示是否允许向盘子中放入水果，初值为$1$表示允许放入且只允许放入一个。信号量$apple$表示盘子中是否有苹果，初值为$0$表示盘子为空且不允许取，为$1$则相反。信号量$orange$表示盘子中是否有橘子，初值为$0$表示盘子为空，不允许取，为$1$则相反。</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">semaphore plate&#x3D;l, apple&#x3D;0， orange&#x3D;0;dad() &#123;                               &#x2F;&#x2F; 父亲进程    while (1) &#123;        prepare an apple;        P(plate);                     &#x2F;&#x2F; 互斥向盘中取、放水果        put the apple on the plate;   &#x2F;&#x2F; 向盘中放苹果        V(apple);                     &#x2F;&#x2F; 允许取苹果    &#125;mom() &#123;                                &#x2F;&#x2F; 母亲进程    while (1) &#123;        prepare an orange;            &#x2F;&#x2F; 互斥向盘中取、放水果        P(plate);                     &#x2F;&#x2F; 向盘中放橘子        put the orange on the plate;  &#x2F;&#x2F; 允许取橘子        V(orange);    &#125;&#125;son() &#123;                               &#x2F;&#x2F; 儿子进程    while(1) &#123;        P(orange);                    &#x2F;&#x2F; 互斥向盘中取橘子        take an orange from the plate;&#x2F;&#x2F; 允许向盘中取、放水果        V(plate);        eat the orange;    &#125;&#125;daughter() &#123;                          &#x2F;&#x2F; 女儿进程    while(1) &#123;        P(apple);                     &#x2F;&#x2F; 互斥向盘中取苹果        take an apple from the plate;        V(plate);                     &#x2F;&#x2F; 运行向盘中取、放水果        eat the apple;    &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>  在上面的进程中，$dad()$和$daughter()$、$mon()$和$son()$必须连续执行，儿子和女儿也必须在拿走水果后执行$V(plate)$操作。</p><h2 id="读者-写作问题"><a class="header-anchor" href="#读者-写作问题">¶</a>读者-写作问题</h2><p>  <strong>问题描述</strong>：有读者和写者两组并发进程，共享一个文件，当两个或两个以上的进程同时访问共享数据时不会产生副作用，但若某个写进程和其他进程（读进程和写进程）同时访问共享数据时，则可能导致数据不一致的错误。因此要求：（1）<strong>允许多个读者可以同时对文件执行读操作</strong>；（2）<strong>只允许一个写者往文件中写信息</strong>；（3）<strong>任一写者在写完成操作之前不允许其他读者或写者工作</strong>；（4）<strong>写者执行写操作，应让已有的读者和写者全部退出</strong>。<br><img src="/2021/04/06/jing-dian-tong-bu-wen-ti/2.png" alt="读写进程对文件的操作"><br>  <strong>问题分析</strong>：<br>  <strong>关系分析</strong>：在题目分析读者和写者是互斥的，写者和写者也是互斥的，读者和读者不存在互斥问题。<br>  <strong>整理思路</strong>：在这个问题中，有两个进程，读者和写者，写者比较简单，它和任何进程互斥，用互斥信号量的$P$操作、$V$操作即可解决。读者的问题比较复杂，它必须在实现与写者互斥的同时，实现与其他读者的同步，因此一对简单的$P$、$V$操作无法解决问题。这里就需要用到计数器，用来判断当前是否有读者读文件，当有读者时，写者是无法写文件的，此时读者会一只占用文件，当没有读者时，写者才可以写文件，同时，<strong>不同读者对计数器的访问也应该是互斥的</strong>。<br>  <strong>信号量设置</strong>：首先设置信号量$count$为计数器，用于记录当前读者的数量，初值为$0$，设置$mutex$为互斥信号量，用于保护更新$count$变量时的互斥，设置互斥信号量$rw$，用于保证读者和写者的互斥访问。</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">int count&#x3D;0;                         &#x2F;&#x2F; 用于记录当前的读者数量semaphore mutex&#x3D;1;                   &#x2F;&#x2F; 用于保护更新count变量时的互斥semaphore rw&#x3D;1;                      &#x2F;&#x2F; 用于保证读者和写者互斥地访问文件writer() &#123;                           &#x2F;&#x2F; 写者进程    while(1) &#123;        P(rw);                       &#x2F;&#x2F; 互斥访问共享文件        writing;                     &#x2F;&#x2F; 写入        V(rw);                       &#x2F;&#x2F; 释放共享文件    &#125;&#125;reader() &#123;    while(1) &#123;                       &#x2F;&#x2F; 读者进程        P(mutex);                    &#x2F;&#x2F; 互斥访问count变量        if(count &#x3D;&#x3D; 0) &#123;             &#x2F;&#x2F; 当第一一个读进程读共享文件时            P(rW);                   &#x2F;&#x2F; 阻止写进程写        &#125;        count++;                     &#x2F;&#x2F; 读者计数器加1        V(mutex);                    &#x2F;&#x2F; 释放互斥变量count        reading;                     &#x2F;&#x2F; 读取        P(mutex);                    &#x2F;&#x2F; 互斥访问count变量        count--;                     &#x2F;&#x2F; 读者计数器减1        if(count &#x3D;&#x3D; 0) &#123;             &#x2F;&#x2F; 当最后一个读进程读完共享文件            V(rw);                   &#x2F;&#x2F; 允许写进程写        &#125;        V(mutex);                    &#x2F;&#x2F; 释放互斥变量count    &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>  上面的方法时读进程优先，只要存在一个读进程，写进程都需要等待，这种方式会导致写进程长时间等待，出现饿死现象。为了避免这种现象的产生，当有读进程正在读取共享文件时，有些进程请求访问，这时应禁止后续读进程的请求，等到已在共享文件的读进程执行完毕，立即让写进程执行，只有在无写进程执行的情况下，才允许读进程再次运行。</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">int count&#x3D;0;                              &#x2F;&#x2F; 用于记录当前的读者数量semaphore mutex&#x3D;1;                        &#x2F;&#x2F; 用于保护更新count变量时的互斥semaphore rw&#x3D;1 ;                          &#x2F;&#x2F; 用于保证读者和写者互斥地访问文件semaphore w&#x3D;1;                            &#x2F;&#x2F; 用于实现“写优先”writer() &#123;                                &#x2F;&#x2F; 写者进程    while(l) &#123;        P(w);                             &#x2F;&#x2F; 在无写进程请求时进入        P(rw);                            &#x2F;&#x2F; 互斥访问共享文件        writing;                          &#x2F;&#x2F; 写入        V(rw);                            &#x2F;&#x2F; 释放共享文件        V(w);                             &#x2F;&#x2F; 恢复对共享文件的访问    &#125;&#125;reader() &#123;                                &#x2F;&#x2F; 读者进程    while(1) &#123;        P(w);                             &#x2F;&#x2F; 在无写进程请求时进入        P(mutex);                         &#x2F;&#x2F; 互斥访问count变量        if(count &#x3D;&#x3D; 0) &#123;                  &#x2F;&#x2F; 当第一个读进程读共享文件时            P(rw);                        &#x2F;&#x2F; 阻止写进程写        &#125;        count++;                          &#x2F;&#x2F; 读者计数器加1        V(mutex);                         &#x2F;&#x2F; 释放互斥变量count        V(w);                             &#x2F;&#x2F; 恢复对共享文件的访问，如果在这里不即使回复对共享文件的访问，则会出现后续读进程无法同时与该进程读取文件，形成互斥，与实际情况不符        reading;                          &#x2F;&#x2F; 读取        P(mutex);                         &#x2F;&#x2F; 互斥访问count变量        count--;                          &#x2F;&#x2F; 读者计数器减1        if(count &#x3D;&#x3D; 0) &#123;                  &#x2F;&#x2F; 当最后一个读进程读完共享文件            V(rw);                        &#x2F;&#x2F; 允许写进程写        &#125;        V(mutex);                         &#x2F;&#x2F; 释放互斥变量count    &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>  这个并不是一个完全的写操作优先的方式，因为当一个写进程访问文件时，若有一些读进程要求访问文件，后又另一个写进程请求访问文件，则当前访问文件的写进程结束对文件的写操作时，会是一个读进程而不是写进程占用文件。</p><h2 id="哲学家进餐问题"><a class="header-anchor" href="#哲学家进餐问题">¶</a>哲学家进餐问题</h2><p>  <strong>问题描述</strong>：一张圆桌边上坐着5名哲学家，每两秒哲学家之间的桌子上摆一根筷子，两根筷子中间是一碗米饭，哲学家们倾注毕生经历用于思考和进餐，哲学家在思考时，并不影响他人，只有当哲学家饥饿时，才试图拿起左、右两根筷子（一根一根的拿起）。若筷子已在他人手上，则需要等待。饥饿的哲学家只有同时拿到了两根筷子才可以开始进餐，进餐完毕后，放下筷子继续思考。具体样例如下图所示。<br><img src="/2021/04/06/jing-dian-tong-bu-wen-ti/3.png" alt="5名哲学家进餐"><br>  <strong>问题分析</strong>：<br>  <strong>关系分析</strong>：$5$名哲学家与左右邻居对其中间筷子的访问是互斥关系。<br>  <strong>整理思路</strong>：在这个问题中，存在$5$个进程，最关键的地方在于如何让一名哲学家拿到左右两根筷子而不造成死锁或饥饿现象。解决方法有两个：（1）让他们同时拿两根筷子。（2）对每个哲学家的动作制定规则，避免饥饿或死锁现象发生。<br>  <strong>信号量设置</strong>：定义互斥信号量数组$chopstick[5] = {1, 1, 1, 1, 1}$，用于对5个筷子的互斥访问，哲学家按照顺序编号为$0\sim4$，哲学家$i$左边筷子的编号为$i$，哲学家右边筷子的编号为$(i+1)$ % $5$。</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">semaphore chopstick[5] &#x3D; &#123;1, 1, 1, 1, 1&#125;; &#x2F;&#x2F; 定义信号量数组chopstick[5]，并初始化Pi() &#123;                                    &#x2F;&#x2F; i号哲学家的进程    do&#123;        P(chopstick[i]);                  &#x2F;&#x2F; 取左边筷子        P(chopstick[(i + 1) % 5]);        &#x2F;&#x2F; 取右边筷子        eat;                              &#x2F;&#x2F; 进餐        V(chopstick[i]);                  &#x2F;&#x2F; 放回左边筷子        V(chopstick[(i + 1) % 5]);        &#x2F;&#x2F; 放回右边筷子        think;                            &#x2F;&#x2F; 思考    &#125; while(1);&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>  该算法存在一下问题：当$5$名哲学家都想要进餐并分别拿起左边的筷子时（都恰好执行完$chopstick[i]$；），筷子已经被拿光，等到他们再想拿右边的筷子时（执行$wait(chopstick[i + 1] \% 5)$；）就全被阻塞，出现死锁。为了防止死锁发生，可对哲学家进行一些限制条件。比如：（1）至多允许4名哲学家同时进餐；（2）仅当一名哲学家左右两边的筷子都可用时，才允许他抓取筷子；（3）对哲学家顺序编号，要求奇数号哲学家先拿左边的筷子，然后拿右边的筷子，而偶数号哲学家刚好相反。</p><h3 id="采用第一种改进方法"><a class="header-anchor" href="#采用第一种改进方法">¶</a>采用第一种改进方法</h3><p>  设置一个初值为$4$的信号量$r$，只允许$4$个哲学家同时去拿左筷子，这样就能保证至少有一个哲学家可以就餐，不会出现饿死和死锁的现象。具体代码如下所示：</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">semaphore chopstick[5]&#x3D;&#123;1, 1, 1, 1, 1&#125;;semaphore r&#x3D;4;void pi(int i) &#123;    while(true) &#123;        P(r);                        &#x2F;&#x2F; 请求进餐        P(chopstick[i]);             &#x2F;&#x2F; 请求左手边的筷子        P(chopstick[(i+ 1) % 5]);    &#x2F;&#x2F; 请求右手边的筷子        eat();        V(chopstick[(i+1) mod 5]);   &#x2F;&#x2F; 释放右手边的筷子        V(chopstick[i);              &#x2F;&#x2F; 释放左手边的筷子        V(r);                        &#x2F;&#x2F; 释放信号量r        think();    &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="采用第二种改进方法"><a class="header-anchor" href="#采用第二种改进方法">¶</a>采用第二种改进方法</h3><p>  利用信号量的保护机制实现。通过互斥信号量$mutex$对$eat()$之前取左侧和右侧筷子的操作进行保护，可以防止死锁的出现。</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">semaphore mutex &#x3D; 1;semaphore chopstick[5] &#x3D; &#123;1, 1, 1, 1, 1&#125;;void Pi(int i) &#123;    while(true) &#123;        P(mutex);        P(chopstick[i]);        P(chopstick[(i+ 1) % 5]);        V(mutex);        eat();        V(chopstick[(i+ 1) % 5]);        V(chopstick[i]);        think();    &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="采用第三种改进方法"><a class="header-anchor" href="#采用第三种改进方法">¶</a>采用第三种改进方法</h3><p>  规定奇数号哲学家先拿左筷子再拿右筷子，而偶数号哲学家相反。按照下图，将是$2$，$3$号哲学家竞争$3$号筷子，$4$，$5$号哲学家竞争$5$号筷子。$1$号哲学家不需要竞争。最后总会有一个哲学家能获得两支筷子而进餐。<br><img src="/2021/04/06/jing-dian-tong-bu-wen-ti/4.png" alt="哲学家进餐编号示意图"></p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">semaphore chopstick[5] &#x3D; &#123;1, 1, 1, 1, 1&#125;;void philosopher(int i) &#123;    while(true) &#123;        if(i % 2 &#x3D;&#x3D; 0) &#123;       &#x2F;&#x2F; 偶数哲学家,先右后左            P(chopstick[(i + 1) % 5]);            P(chopstick[i]) ;            eat();            V(chopstick[i]);            V(chopstick[(i+ 1) % 5]);        &#125;        else &#123;                   &#x2F;&#x2F; 奇数哲学家，先左后右            P(chopstick[i]);            P(chopstick[(i + 1) % 5]);            eat();            V(chopstick[(i + 1) % 5]);            V(chopstick[i]);        &#125;    &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="吸烟者问题"><a class="header-anchor" href="#吸烟者问题">¶</a>吸烟者问题</h2><p>  <strong>问题描述</strong>：假设一个系统有三个抽烟者进程和一个供应者进程，每个抽烟者不停地卷烟并抽调它，但是卷起来并抽调一支烟，抽烟者需要有：烟草、纸和胶水。三个抽烟者中，第一个拥有烟草，第二个拥有纸，第三个拥有胶水。供应者进程无限地提供三种材料。供应者每次将两种材料放在桌子上，拥有剩下那种材料的抽烟者卷一根烟并抽掉它，并给供应者一个信号告诉已完成，此时供应者就会将另外两种材料放到桌上，如此重复（让三个抽烟者轮流地抽烟）。<br>  <strong>问题分析</strong>：<br>  <strong>关系分析</strong>：在上述问题中，供应者与三个抽烟者是同步关系，抽烟者与抽烟者之间是互斥关系。<br>  <strong>整理思路</strong>：整体有$4$个进程。<br>  <strong>信号量设置</strong>：信号量$offer1$、$offer2$、$offer3$分别表示烟草和纸的组合，烟草和胶水的组合、纸和胶水组合。信号量$finish$用于互斥进行抽烟的动作。</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">int num&#x3D;0;               &#x2F;&#x2F; 存储随机数semaphore offer1 &#x3D; 0;      &#x2F;&#x2F; 定义信号量对应烟草和纸组合的资源semaphore offer2 &#x3D; 0;      &#x2F;&#x2F; 定义信号量对应烟草和胶水组合的资源semaphore offer3 &#x3D; 0;      &#x2F;&#x2F; 定义信号量对应纸和胶水组合的资源semaphore finish &#x3D; 0;      &#x2F;&#x2F; 定义信号量表示抽烟是否完成process P1() &#123;           &#x2F;&#x2F; 供应者    while(1) &#123;        num++;        num &#x3D; num % 3;        if(num &#x3D;&#x3D; 0) &#123;            V(offer1);   &#x2F;&#x2F; 提供烟草和纸        &#125;        else if (num &#x3D;&#x3D; 1) &#123;            V(offer2) ;  &#x2F;&#x2F; 提供烟草和胶水        &#125;        else &#123;            V(offer3);   &#x2F;&#x2F; 提供纸和胶水        &#125;        任意两种材料放在桌子上;        P(finish);    &#125;&#125;process P2 () &#123;          &#x2F;&#x2F; 拥有烟草者    while(1) &#123;        P(offer3);        拿纸和胶水，卷成烟，抽掉;        V(finish);    &#125;&#125;process P3() &#123;           &#x2F;&#x2F; 拥有纸者    while(1) &#123;        P(offer2);        拿烟草和胶水，卷成烟，抽掉;        V(finish);    &#125;&#125;process P4() &#123;           &#x2F;&#x2F; 拥有胶水者    while(1) &#123;        P(offerl);        拿烟草和纸，卷成烟，抽掉;        V(finish);    &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="个人总结"><a class="header-anchor" href="#个人总结">¶</a>个人总结</h2><p>  （1）在解决进程同步问题中，首先是要理解所有进程之间的同步、互斥等关系，了解哪些进程之间是可以同时执行的，这样有利于在整体上把控解决方案。<br>  （2）信号量个数的设置往往与互斥进程数和进程执行条件有关。<br>  （3）了解$P/V$操作的具体含义，对解题会更加有益。<br>  （4）所写的代码并不是串行执行的，而是要具有并行的思想，多考虑如果这些进程同时发生会有什么样的影响。</p>]]></content>
      
      
      <categories>
          
          <category> 课本学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 操作系统 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>进程管理（三）</title>
      <link href="/2021/03/31/jin-cheng-guan-li-san/"/>
      <url>/2021/03/31/jin-cheng-guan-li-san/</url>
      
        <content type="html"><![CDATA[<h1>进程同步</h1><h2 id="进程同步的基本概念"><a class="header-anchor" href="#进程同步的基本概念">¶</a>进程同步的基本概念</h2><p>  并发执行的进程之间存在相互制约的关系，为了协调进程之间的相互制约关系，进入了进程同步的概念。例如在系统计算算式：$1+2*3$，其中乘法进程一定要在加法进程之前进行运算（在这里把加、乘运算比作进程），但是因为操作系统具有异步性，如果不加以制约，会出现运行顺序相反的情况。同时熟悉以下概念</p><h3 id="临界资源"><a class="header-anchor" href="#临界资源">¶</a>临界资源</h3><p>  在操作系统中，虽然多个进程可以共享系统中的各种资源，但其中许多资源一次只能为一个进程所用，将这些一次仅允许一个进程使用的资源称为<strong>临界资源</strong>，许多物理资源如打印机等都属于临界资源，一些变量、数据等都可以被若干进程共享，也属于临界资源。在每个进程中，访问临界资源的那段代码称为<strong>临界区</strong>。为了保证临界资源的正确使用，把其访问过程分为4个部分：<br>  （1）进入区：在进入区要检查可否进入临界区，若能进入临界区，则应设置正在访问临界区的标志，以阻止其他进程同时进入临界区。<br>  （2）临界区：访问临界资源的那段代码称。<br>  （3）退出区：将正在访问临界区的标志清除。<br>  （4）剩余区：代码中的剩余部分。</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">do &#123;    entry section     &#x2F;&#x2F; 进入区    critical section  &#x2F;&#x2F; 临界区    exit section      &#x2F;&#x2F; 退出区    remainder section &#x2F;&#x2F; 剩余区 &#125; while(true)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="同步"><a class="header-anchor" href="#同步">¶</a>同步</h3><p>  同步即直接制约关系，是指为完成某种任务而建立的两个或多个进程，这些进程因为需要在某些位置上协调它们的工作次序而等待、传递信息所产生的制约关系。</p><h3 id="互斥"><a class="header-anchor" href="#互斥">¶</a>互斥</h3><p>  互斥即间接制约关系，当一个进程进入临界区使用临界资源时，另一个进程必须等待，当占用临界资源的进程退出临界区后，另一进程才允许去访问此临界资源。为了禁止两个进程同时进入临界区，同步机制应遵循以下准则：<br>  （1）<strong>空闲让进</strong><br>  （2）<strong>忙则等待</strong><br>  （3）<strong>有限等待</strong>：对请求访问的进程，应保证能在有限时间内进入临界区。<br>  （4）<strong>让权等待</strong>：当进程不能进入临界区时，应立即释放处理器，防止进程忙等待。</p><h2 id="实现临界区互斥的基本方法"><a class="header-anchor" href="#实现临界区互斥的基本方法">¶</a>实现临界区互斥的基本方法</h2><h3 id="软件实现法"><a class="header-anchor" href="#软件实现法">¶</a>软件实现法</h3><p>  在进入区设置并检查一些标志来标明是否有进程在临界区中，若已有进程在临界区，则在进入临界区通过循环检查进行等待，进程离开临界区后则在退出区修改标志。</p><h4 id="单标志法"><a class="header-anchor" href="#单标志法">¶</a>单标志法</h4><p>  设置一个公用型整数变量turn，用于指示被允许进入临界区的进程编号，即若$turn = 0$，则允许$P_{0}$进程进入临界区。该算法可确保每次只允许一个进程进入临界区。但两个进程必须交替进入临界区。若某个进程不再进入临界区，则另一个进程也无法进入临界区，这样很容易造成资源利用不充分。该情况如下所示：<br>  $P_{0}$进程：</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">while(turn !&#x3D; 0);  &#x2F;&#x2F; 进入区critical section;  &#x2F;&#x2F; 临界区turn &#x3D; 1;          &#x2F;&#x2F; 退出区remainder section; &#x2F;&#x2F; 剩余区 <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>  $P_{1}$进程：</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">while(turn !&#x3D; 1);  &#x2F;&#x2F; 进入区critical section;  &#x2F;&#x2F; 临界区turn &#x3D; 0;          &#x2F;&#x2F; 退出区remainder section; &#x2F;&#x2F; 剩余区 <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>  若$P_{0}$顺利进入临界区并从临界区离开，则此时临界区是空闲的，但$P_{1}$并没有进入临界区的打算，$turn = 1$一直成立，$P_{0}$就无法再次进入临界区（一直被while循环困住）。</p><h4 id="双标志法先检查"><a class="header-anchor" href="#双标志法先检查">¶</a>双标志法先检查</h4><p>  基本思想是在每个进程访问临界区资源之前，先查看临界资源是否正被访问，若正在被访问，该进程需等待；否则，进程才进入自己的临界区。因此，设置一个数据$flag[i]$,如果第$i$个元素值为$FALSE$，表示$P_{i}$进程未进入临界区，值为$TRUE$，表示$P_{i}$进程进入临界区。例如下面的代码表示：<br>  $P_{i}$进程：</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">while(flag[j]);      &#x2F;&#x2F; 进入区  （1）flag[i] &#x3D; TRUE;      &#x2F;&#x2F; 进入区  （3）critical section;    &#x2F;&#x2F; 临界区flag[i] &#x3D; FALSE;     &#x2F;&#x2F; 退出区remainder section;   &#x2F;&#x2F; 剩余区 <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>  $P_{j}$进程：</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">while(flag[i]);      &#x2F;&#x2F; 进入区  （2）flag[j] &#x3D; TRUE;      &#x2F;&#x2F; 进入区  （3）critical section;    &#x2F;&#x2F; 临界区flag[j] &#x3D; FALSE;     &#x2F;&#x2F; 退出区remainder section;   &#x2F;&#x2F; 剩余区 <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>  这样做的优点是：<strong>不用交替进入，可连续使用</strong>。<br>  缺点是：$P_{i}$和$P_{j}$可能同时进入临界区。当两个进程都没有进入时，$flag[i]$和$flag[j]$全部为$FALSE$，上述算法按照（1）（2）（3）（4）执行时，会同时进入临界区（违背了“忙则等待”）。即在检查对方的$flag$后和切换自己的$flag$前有一段时间，结果都检查通过。这里的问题出在检查和修改操作不能一次进行。</p><h4 id="双标志法后检查"><a class="header-anchor" href="#双标志法后检查">¶</a>双标志法后检查</h4><p>  与<strong>双标志法先检查</strong>不同的是，该算法先将自己的标志设置为$TRUE$，再检测对方的状态标志，若对方标志为$TRUE$，则进程等待；否则进入临界区。<br>  $P_{i}$进程：</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">flag[i] &#x3D; TRUE;      &#x2F;&#x2F; 进入区  while(flag[j]);      &#x2F;&#x2F; 进入区  critical section;    &#x2F;&#x2F; 临界区flag[i] &#x3D; FALSE;     &#x2F;&#x2F; 退出区remainder section;   &#x2F;&#x2F; 剩余区 <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>  $P_{j}$进程：</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">flag[j] &#x3D; TRUE;      &#x2F;&#x2F; 进入区  while(flag[i]);      &#x2F;&#x2F; 进入区  critical section;    &#x2F;&#x2F; 临界区flag[j] &#x3D; FALSE;     &#x2F;&#x2F; 退出区remainder section;   &#x2F;&#x2F; 剩余区 <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>  在这种情况下，可能会发生两个进程同时都想进入临界区时，它们分别将自己的标志值$flag$设置为$TRUE$，并且同时检测对方的状态（执行$while$语句），发现对方也要进入临界区时，双方互相谦让，结果两个进程都无法进入临界区，从而导致“饥饿”现象。</p><h4 id="Peterson’s-Algorithm"><a class="header-anchor" href="#Peterson’s-Algorithm">¶</a>Peterson’s Algorithm</h4><p>  为了防止两个进程为进入临界区而无限期等待，又设置了变量$turn$，每个进程在先设置自己的标志后再设置$turn$标志。这时，再同时检测另一个进程状态标志和不允许进入标志，以便保证两个进程同时要求进入临界区时，只允许一个进程进入临界区。<br>  $P_{i}$进程：</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">flag[i] &#x3D; TRUE; turn &#x3D; j;      &#x2F;&#x2F; 进入区  while(flag[j] &amp;&amp; turn &#x3D;&#x3D; j);   &#x2F;&#x2F; 进入区  critical section;              &#x2F;&#x2F; 临界区flag[i] &#x3D; FALSE;               &#x2F;&#x2F; 退出区remainder section;             &#x2F;&#x2F; 剩余区 <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>  $P_{j}$进程：</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">flag[j] &#x3D; TRUE; turn &#x3D; i;      &#x2F;&#x2F; 进入区  while(flag[i] &amp;&amp; turn &#x3D;&#x3D; i);   &#x2F;&#x2F; 进入区  critical section;              &#x2F;&#x2F; 临界区flag[j] &#x3D; FALSE;               &#x2F;&#x2F; 退出区remainder section;             &#x2F;&#x2F; 剩余区 <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>  当进程$P_{i}$设置$flag[i] = true$时，就表示该进程想要进入临界区，同时$turn = j$，此时进程$P_{j}$已在临界区中，符合进程$P_{i}$中的循环条件，所以$P_{i}$不能进入临界区。若$P_{j}$不想要进入临界区，即$flag[i] = false$,循环条件不符合，则$P_{i}$可以顺利进入，反之亦然。</p><h3 id="硬件实现方法"><a class="header-anchor" href="#硬件实现方法">¶</a>硬件实现方法</h3><p>  计算机提供了特殊的硬件指令，允许对一个字中的内容进行检测和修正，或对两个字的内容进行交换。通过硬件支持实现临界段问题的方法称为<strong>低级方法</strong>，或者<strong>元方法</strong>。</p><h4 id="中断屏蔽方法"><a class="header-anchor" href="#中断屏蔽方法">¶</a>中断屏蔽方法</h4><p>  当一个进程正在使用处理机执行它的临界区代码时，防止其他进程进入其临界区进行访问的最简方法是，禁止一切中断发生，或称之为屏蔽中断、关中断。因为CPU只发生在中断时引起进程切换，因此屏蔽中断能够保证当前允许的进程让临界区代码顺利地执行完，进而保证互斥的正确实现，然后执行开中断。其典型模式为：</p><pre class="line-numbers language-none"><code class="language-none">...关中断临界区开中断...<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>  但是<strong>这种方法限制了处理机交替执行程序的能力，因此执行的效率会明显降低</strong>。对内核来说，在它执行更新变量或列表的几条指令期间，关中断是很方便的，但关中断的权力交给用户则很不明智，若一个进程关中断后不再开中断，则系统可能会因此终止。</p><h4 id="硬件指令方法"><a class="header-anchor" href="#硬件指令方法">¶</a>硬件指令方法</h4><p>  $TestAndSet$指令：这条指令为原子操作，即执行代码时不允许被中断，其功能是读出指定标志后把该标志设置为真。具体指令功能如下：</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">boolean TestAndSet(boolean *lock) &#123;    boolean old;    old &#x3D; *lock;    *lock &#x3D; true;    return old;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>  为每个临界资源设置一个共享变量$lock$，表示资源的两种状态：$ture$表示正在被占用，其初值为$false$。在进程访问临界资源之前，利用$TsetAndSet$检查和修改标志$lock$；若有进程在临界区，则重复检查，直到退出进程，利用该指令实现进程互斥算法描述如下：</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">while TestAndSet(&amp;lock);&#x2F;&#x2F; 进程的临界区代码段lock &#x3D; false;&#x2F;&#x2F; 进程的其他代码<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>  $Swap$指令：功能是交换两个字（字节）的内容，具体功能描述如下:</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">Swap(boolean *a, boolean *b) &#123;    boolean temp;    temp &#x3D; *a;    a* &#x3D; *b;    *b &#x3D; temp;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>  <strong>上述两个指令的描述仅是功能实现，而并非软件实现的定义，真正是由硬件逻辑直接实现的，不会被中断</strong>。利用$Swap$指令实现进程互斥的算法描述如下：</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">key &#x3D; true;while(key !&#x3D; false) &#123;    Swap(&amp;lock, &amp;key);&#125;&#x2F;&#x2F; 进程的临界区代码段lock &#x3D; false;&#x2F;&#x2F; 进程的其他代码<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>  为每个临界资源设置一个变量$lock$，初值为$false$；在每个进程中再设置一个局部布尔变量$key$，用于与$lock$交换信息。在进入临界区前，先利用$Swap$指令交换$lock$与$key$的内容，然后检查$key$的状态；有进程在临界区时，重复交换和检查过程，直到进程退出。</p><p>  硬件方法的优点是：（1）适用于任意数目的进程，不管是单处理机还是多处理机.（2）简单，容易验证其正确性。（3）支持进程内有多个临界区，只需要为每个临界区设立一个布尔值变量。<br>  但是其任然存在缺点：（1）进程等待进入临界区时要耗费处理机时间，不能实现让权等待。（2）从等待进程中随机选择一个进入临界区，有的进程可能一直选不上，从而导致“饥饿”现象。</p><h2 id="信号量"><a class="header-anchor" href="#信号量">¶</a>信号量</h2><p>  信号量机制是一种功能较强的机制，可以用来解决互斥与同步问题，只能被两个原语“$P$操作”和“$V$操作”访问。<strong>原语是指完成某种功能且不被分割，不被中断执行的操作序列</strong>，通常由硬件实现。该特性在单处理机上可由软件通过屏蔽中断方法实现。<strong>原语之所以不能被中断执行，是因为原语对变量的操作过程若被打断，可能会去允许另一个对同一变量的操作过程。从而出现临界段问题</strong>。</p><h3 id="整型信号量"><a class="header-anchor" href="#整型信号量">¶</a>整型信号量</h3><p>  整型信号量被定义为一个用于表示资源数目的整型量$S$，$wait$和$signal$操作可描述为</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">wait(S) &#123;    while(S &lt;&#x3D; 0);    S &#x3D; S - 1;&#125;signal(S) &#123;    S &#x3D; S + 1;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>  $wait$操作中，只要信号量$S &lt;= 0$，就会不断测试，因此一直使进程处于“忙等状态”。</p><h3 id="记录型信号量"><a class="header-anchor" href="#记录型信号量">¶</a>记录型信号量</h3><p>  记录型信号量是不存在“忙等”现象的进程同步机制。除需要一个用于代表资源数目的整型变量$value$外，再增加一个进程链表$L$，用于链接所有等待该资源的进程。其结构体如下：</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">typedef struct &#123;    int value;    struct process *L;&#125; semaphore;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>  相应的$wait$和$signal$操作如下：</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">void wait(semaphore S) &#123;     &#x2F;&#x2F; 相当于申请资源    S.value--;    if(S.value &lt; 0) &#123;        add this process to S.L;        block(S.L);    &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>  在$wait$操作中，$S.value–$表示进程请求一个该类资源，当$S.value &lt; 0$时，表示该资源已分配完毕，因此进程应调用$block$原语，进行自我阻塞，放弃处理机，并插入该类资源的等待队列$S.L$。</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">void signal(semaphore S) &#123;   &#x2F;&#x2F;相当于释放资源    S.value++;    if(S.value &lt;&#x3D; 0) &#123;        remove a process P from S.L;        wakeup(P);    &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>  $signal$操作，表示释放一个资源，使系统中可供分配的该类资源数增$1$，因此有$S.value++$。若加$1$后仍是$S.value &lt;= 0$，则表示在$S.L$中仍有等待该资源的进程被阻塞，因此还应调用$wakeup$原语，将$S.L$中的第一个等待进程唤醒。</p><h3 id="利用信号量实现同步"><a class="header-anchor" href="#利用信号量实现同步">¶</a>利用信号量实现同步</h3><p>  信号量机制能用于解决进程间的各种同步问题。设$S$为实现进程$P_{1}$和$P_{2}$同步的公共信号量，初值为$0$。<strong>进程$P_{2}$中的语句$y$要使用进程$P_{1}$中语句$x$的允许结果，所以只有当语句$x$执行完成之后，语句$y$才可以执行</strong>。</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">semaphore S &#x3D; 0;     &#x2F;&#x2F; 初始化信号量P1() &#123;    sqrt(10000);    x;               &#x2F;&#x2F; 语句x    V(s)             &#x2F;&#x2F; 告诉进程P2，语句 x 已经完成    ...&#125;P2() &#123;    ...    P(S);            &#x2F;&#x2F; 检查语句 x 是否运行完成    y;               &#x2F;&#x2F; 检查无误， 运行 y 语句    ...&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>  若$P_{2}$先执行到$P(S)$时，$S$为$0$，执行$P$操作会把进程$P_{2}$阻塞，并放入阻塞队列，当进程$P_{1}$中的$x$执行完后，执行$V$操作，把$P_{2}$从阻塞队列中放回就绪队列，当$P_{2}$得到处理机时，就继续执行。</p><h3 id="利用信号量实现进程互斥"><a class="header-anchor" href="#利用信号量实现进程互斥">¶</a>利用信号量实现进程互斥</h3><p>  设$S$为实现进程$P_{1}$，$P_{2}$互斥的信号量，由于每次只允许一个进程进入临界区，所以$S$的初值应为$1$（即资源可用数为$1$）。只需要把临界区置于$P(S)$和$V(S)$之间，即可实现两个进程对临界区的互斥访问。</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">semaphore S &#x3D; 1;          &#x2F;&#x2F; 初始化信号量P1() &#123;    ...    P(S);                 &#x2F;&#x2F; 准备开始访问临界资源，加锁    进程 P1 的临界区       V(S);                 &#x2F;&#x2F; 访问结束，解锁    ...&#125;P2() &#123;    ...    P(S);                 &#x2F;&#x2F; 准备开始访问临界资源，加锁    进程 P2 的临界区    V(S);                 &#x2F;&#x2F; 访问结束，解锁&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>  当没有进程在临界区时，任意一个进程要进入临界区，就要执行$P$操作，把$S$的值减为$0$，然后进入临界区；当有进程存在于临界区时，$S$的值为$0$，在有进程要进入临界区，执行$P$操作时将会被阻塞，直至在临界区中的进程退出，这样便实现了临界区的互斥。</p><h3 id="信号量实现前驱关系"><a class="header-anchor" href="#信号量实现前驱关系">¶</a>信号量实现前驱关系</h3><h2 id="管程"><a class="header-anchor" href="#管程">¶</a>管程</h2><p>  在信号量机制中，每个要访问临界资源的进程都必须自备同步的$PV$操作，大量分散的同步操作给系统管理带来了麻烦，且容易因同步操作不当而导致系统死锁。于是产生了一种新的进程同步工具——管程，降低了思索发生的可能性。</p><h3 id="管程的定义"><a class="header-anchor" href="#管程的定义">¶</a>管程的定义</h3><p>  系统中的各种硬件资源和软件资源，均可用数据接结构抽象地描述其资源特性。利用共享数据结构抽象地表示系统中的共享资源，而把对数据结构实施的操作定义为一组过程。进程对共享资源的申请、释放等操作，都通过这组过程来实现，这组过程还可以根据资源情况，或接受或阻塞进程的访问，实现进程互斥。<strong>这个代表共享数据结构，以及由对该共享数据结构实施操作的一组过程所组成的资源管理程序，称为管程</strong>。因此管程由以下$4$部分组成：<br>  （1）管程的名称<br>  （2）局部于管程内部的共享结构数据说明。<br>  （3）对该数据结构进行操作的一组过程（或函数）<br>  （4）对局部于管程内部的共享数据设置初始值的语句。</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">monitor Demo &#123; &#x2F;&#x2F; 定义一个名称为“Demo”的管程    &#x2F;&#x2F; 定义共享数据结构，对应系统中的某种共享资源    &#x2F;&#x2F; 共享数据结构 S；    &#x2F;&#x2F; 对共享数据结构初始化的语句    init_code() &#123;        S &#x3D; 5;     &#x2F;&#x2F; 初始资源数等于 5    &#125;    &#x2F;&#x2F; 过程 1：申请一个资源    take_away() &#123;        &#x2F;&#x2F; 对共享数据结构的 x 的一系列处理        S--;       &#x2F;&#x2F; 可用资源数 -1        ...    &#125;    &#x2F;&#x2F; 过程 2：归还一个资源    give_back() &#123;        &#x2F;&#x2F; 对共享数据结构 x 的一系列处理；        S++;       &#x2F;&#x2F; 可用资源数 +1        ...    &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>  管程很像面向对象程序设计中的一个类，<strong>会把所有的共享资源的操作封装起来</strong>，<strong>并且每次仅允许一个进程进入管程</strong>。</p><h3 id="条件变量"><a class="header-anchor" href="#条件变量">¶</a>条件变量</h3><p>  当一个进程进入管程后被阻塞，直到阻塞的原因被解除时，在此期间，如果该进程不释放管程，则其他进程无法进入管程。因此将阻塞原因定义为<strong>条件变量$condition$</strong>,因为一个进程被阻塞的原因有多个，因此在管程中设置了多个条件变量，每个条件变量保存了一个等待队列，用于记录因该条件变量而阻塞的所有进程，对条件变量只能进行两种操作，$wait$和$signal$。<br>  <strong>$x.wait$</strong>：当$x$对应的条件不满足时，正在调用管程的进程调用$x.wait$将自己插入$x$条件的等待队列，并释放管程。此时其他进程可以使用该管程。<br>  <strong>$x.signal$</strong>：$x$对应的条件发生了变化，则调用$x.signal$，唤醒一个因$x$条件而阻塞的进程。</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">monitor Demo &#123;    共享数据结构 S;    condition x;                         &#x2F;&#x2F; 定义一个条件变量    init_code() &#123;...&#125;    take_away() &#123;        if(S &lt;&#x3D; 0) x.wait();            &#x2F;&#x2F; 资源不够，在条件变量x上阻塞等待        &#x2F;&#x2F; 资源足够，分配资源，做一系列相应处理    &#125;    give_back() &#123;        &#x2F;&#x2F; 归还资源，做一系列相应处理；        if(&#x2F;*有进程在等待*&#x2F;) x.xignal;    &#x2F;&#x2F; 唤醒一个阻塞进程    &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>  条件变量和信号量的比较：<br>  相似点：条件变量的$wait/signal$操作类似于信号量的$P/V$操作，可以实现进程的阻塞/唤醒。<br>  不同点：条件变量是“没有值的”，仅实现了“排队等待”功能；而信号量是“有值”的，信号量的值反映了剩余资源数，而在管程中，剩余资源数用共享数据结构记录。</p><h2 id="经典的同步问题"><a class="header-anchor" href="#经典的同步问题">¶</a>经典的同步问题</h2><p>  这部分内容单独用一章博客进行讲述。</p>]]></content>
      
      
      <categories>
          
          <category> 课本学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 操作系统 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2021华为软件精英挑战赛初赛代码及思路</title>
      <link href="/2021/03/30/2021-hua-wei-ruan-jian-jing-ying-tiao-zhan-sai-chu-sai-dai-ma-ji-si-lu/"/>
      <url>/2021/03/30/2021-hua-wei-ruan-jian-jing-ying-tiao-zhan-sai-chu-sai-dai-ma-ji-si-lu/</url>
      
        <content type="html"><![CDATA[<h2 id="2021华为软件精英挑战赛训练赛、正式赛思路分享"><a class="header-anchor" href="#2021华为软件精英挑战赛训练赛、正式赛思路分享">¶</a>2021华为软件精英挑战赛训练赛、正式赛思路分享</h2><p>    有幸再次参加了华为软件精英挑战赛（去年由于不知道数据集有坑，导致没能进入复赛，今年决定再来一次弥补去年的遗憾）<br>    今年的赛题相比去年个人感觉还是好了一些的，从任务指导书所给的评分规则来看，确实要比去年单一的按程序运行时间来评判要好一些。并且题目属于开放性赛题，没有唯一的标准答案，所以在逻辑思维上可以很好的区分参赛选手（至少我是这么认为的）。具体的赛题可以到比赛官网中进行下载，或者点击这个链接<a href="https://developer.huaweicloud.com/hero/forum.php?mod=viewthread&amp;tid=112802">初赛赛题下载</a><br>    说说我们队伍的情况吧，我们是武长赛区正式赛排行榜中的第七名。<br><img src="/2021/03/30/2021-hua-wei-ruan-jian-jing-ying-tiao-zhan-sai-chu-sai-dai-ma-ji-si-lu/2.png" alt="队伍最终最好成绩"></p><p>但是因为队友题目出来当晚，官方没有禁止开源代码的情况下，开源了一份baseline（只开源了一次），那份baseline没有加任何迁移算法，只是一个很基础的供选手参赛的代码，在训练赛中也只是50e的分数，（参赛的同学应该知道50e的分数是一个什么水平，就是垫底的分数）在大赛继续进行到12号的时候，因为开源代码的选手零零星星出现了不少，官方禁止了开源，我们也没有对外透漏过任何代码，在正式赛结束之后，很不幸，被查重了，我很呵呵，我加了迁移的算法怎么与其他人去冲突？？？？查重不应该是在逻辑上查重吗？难道有人用了我的代码，自动的在我们的思路带领下，和我们写了一样的调度算法？？？？很迷惑，申诉之后，官方的回信也的内容只有大赛的四条比赛规则（感觉只是走了一个流程），而且我们知道的其他一些开源的队伍，竟然没有被查到，更加迷惑了。我大三下了，为了这个比赛放弃了一段时间的考研复习，就是希望在这个赛事中找回去年的不爽，结果再次寒心……。吃一堑长一智吧。<br>    说点开心的吧，在正式赛中团队整体奋斗的是我和另外一位老哥，老哥实力很强，在开始冲的还是很猛的，后期因为一些其他的事情，没有把太多精力放在这个上面，因为进复赛很稳了，但是没想到，呵呵呵。当然在这次比赛中，还认识了其他不少的大佬，比赛嘛，最开心的是认识很多朋友了。</p><h3 id="赛题分析"><a class="header-anchor" href="#赛题分析">¶</a>赛题分析</h3><p>    在分析赛提前提醒大家，想要尽快写baseline、出结果、上分，一定要先读题，认认真真读题，好好分析赛题。<br>    题目大致是给你一些服务器和虚拟机的类型，服务器分为两个节点A、B，<strong>服务器拥有的资源（CPU 和内存）均匀分布在这两个节点上</strong>。这句话是重点！！！这句话是重点！！！这句话是重点！！！也就意味着，如果服务器的类型为NV603 ，其CPU为92C，内存为324G，那么其 A、B 两个节点分别包含的资源为：CPU核数：46C 和，内存大小162G 的资源。并且保证服务器的CPU 核数和内存大小均为偶数。并且在题目中还给出了服务器的硬件成本以及每日耗能成本。<br>    题目中所给的虚拟机有两种部署方式，分别为单双节点部署，单节点部署指的是一台虚拟机所需的资源（CPU和内存）完全由主机上的一个节点提供；<strong>双节点部署指的是一台虚拟机所需的资源（CPU 和内存）必须由一台服务器的两个节点同时提供，并且每个节点提供总需求资源的一半</strong>。<br>    赛题要求根据所给的请求序列，创建服务器，部署虚拟机，或者按照用户请求在对应的服务器上删除相应的虚拟机。但要注意，服务器上的任意一个节点(A和 B)上的资源负载(CPU 和内存)均不能超过其容量上限。在完成每一天的服务器的扩容之后，在处理每一天的新请求之前，你还可以对当前存量虚拟机进行一次迁移，即把虚拟机从一台服务器迁移至另一台服务器。对于单节点部署的虚拟机，将其从一台服务器的 A 节点迁移至 B 节点(或反之)也是允许的。但迁移的虚拟机总量不超过当前存量虚拟机数量的千分之五。<br>    通读赛题所有要求之后，其实我们可以发现这是一个类似于装箱的问题，把服务器比作箱子，把虚拟机比作需要放进去的货物。在一般的装箱问题中，我们首先要考虑的就是如何选择箱子，即选择用什么样的箱子来装什么样的货物是最合理的。这就需要涉及到对所有的箱子进行特征上的分析。接下来就是我们队伍对于整个赛题的思路（谨代表团队思路，如果讲解有误，勿喷！）</p><h3 id="购买服务器思路"><a class="header-anchor" href="#购买服务器思路">¶</a>购买服务器思路</h3><p>    首先最开始我们想到的当然就是如何购买服务器，买容量最大的？买价格最便宜的？买CPU和内存大小最接近的？……上面的这几种思路并不没有一个标准的正确，首先在这种调度问题中，是没有一个正确的答案的，通过这些思路最终得到的结果是好是坏，很大一部分是取决于数据集的，但是我们也不能因为没有正确的答案，而且五花八门的胡乱猜测。首先，我们继续回到题目背景中去，背景中有如下一句话：<br><img src="/2021/03/30/2021-hua-wei-ruan-jian-jing-ying-tiao-zhan-sai-chu-sai-dai-ma-ji-si-lu/1.png" alt="赛题中的背景信息"><br>众所周知，这种比赛的目的一方面是为了选拔优秀选手，另一方面也是为了给自己公司当前所存在的问题，在民间寻找解决方法，可能你的思路在这个比赛中节约了一点点成本，放在真正的市场中，可以让华为的成本节约<strong>亿点点</strong>。<br>    我们队伍在题目中所给数据中的服务器的规格进行了一个粗略的分析，服务器的种类比较丰富，每种服务器上CPU和内存的大小也不同，我们按照以下的几种方式对服务器进行了排序，并在每种排序后面写了我们的思考依据：</p><ol><li>CPU+内存：这样为了保证可以选择资源足够大的服务器</li><li>每日的耗能：如果请求天数足够多，服务器开启时间足够长，那么每日耗能的费用要远远大于其硬件成本</li><li>CPU * 0.75 + 内存 * 0.25：魔法数字，后面还有部分也会用到，对于我们版本的代码来说，很重要！！！。</li><li>服务器的硬件成本：在请求天数较小，每天请求数量较小的情况下， 硬件成本所占的费用比重较大</li><li>CPU：仅作为参考</li><li>内存：仅作为参考</li><li>abs(CPU - 内存)：大规模虚拟机的部署情况下，所有虚拟机所需的内存和CPU总和是接近的或者会趋近于某一个比值，为了让服务器可以部署更多的虚拟机，我们就尽量使服务器的CPU和内存大小接近，</li><li>服务器的硬件成本 / (CPU + 内存)：考虑服务器的性价比。</li></ol><h3 id="处理请求思路"><a class="header-anchor" href="#处理请求思路">¶</a>处理请求思路</h3><p>    本题中的数据输入是一次性输入的，我们可以先把所有的数据保存下来，然后去对这些信息集中处理，这样我们就可以用一个上帝视角来解决这个问题，也可以按照每一条请求进行中规中距的进行处理（因为我们队伍依次对请求进行处理的分数还是比较客观的，又因为是初赛，也没有去花太多功夫去更改已经写好的baseline，所以没有采用上面看似比较良好的上帝视角去处理）在得到每一次的请求之后，判断其为“add”，还是“del”，依次进行请求处理。</p><h3 id="选择服务器"><a class="header-anchor" href="#选择服务器">¶</a>选择服务器</h3><p>    在刚开始遇到“add”请求，或者在当前已有服务器无法满足请求中虚拟机所需要的资源时，我们需要重新开启新的服务器，在这时我们就会遇到服务器的选择问题。我们队伍根据上面的选择思路以及实验中得到结果进行对比后，采用的是第二种排序方式，然后对排好序的服务器从前到后依次遍历，选择刚好符合该虚拟机的服务器，具体代码如下：</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">int ii;for (ii &#x3D; 0; ii &lt; model_pair_size; ii++)&#123; &#x2F;&#x2F;找到一个最合适虚拟机的服务器，vv代表的是虚拟机信息的结构体    if (vv.cpu &lt; model_pair[ii].first.cpu &#x2F; 2 &amp;&amp; vv.Memory &lt; model_pair[ii].first.Memory &#x2F; 2 &amp;&amp; !vv.Is_Double_node)    &#123;        break;    &#125;    if (vv.cpu &lt; model_pair[ii].first.cpu &amp;&amp; vv.Memory &lt; model_pair[ii].first.Memory &amp;&amp; vv.Is_Double_node)    &#123;        break;    &#125;&#125;server ss &#x3D; model_pair[ii].first;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>    在我们初赛的代码中，其他排序得到的结果没有这种排序的方式费用低，并且在线下数据集和正式赛中，（3）、（4）两种排序方式最终得到的分数是相同的，并且在线下数据集中，根据这两种方式排序，得到的服务器的顺序大致是相同的。（很可能是我们猜测的系数刚好符合简化后数据集的系数）。</p><h3 id="添加虚拟机"><a class="header-anchor" href="#添加虚拟机">¶</a>添加虚拟机</h3><p>    如果在当前服务器有满足需要部署的虚拟机所需要资源的情况下，我们部署的策略是，首先遍历所有服务器，找到当前虚拟机部署在第一次出现的合适的服务器，对于双节点虚拟机，计算其部署后剩余资源的大小，即<strong>剩余CPU * 0.75 + 剩余内存 * 0.25</strong>，如果为单节点虚拟机，<br>只计算其一个节点的剩余资源，计算公式还是如上所示。<strong>在单节点中，选取A、B节点也要考虑两个节点之间的负载均衡，因为后面可能会出现双节点虚拟机不会因为负载不均衡而无法部署</strong>。最终找到剩余资源最小的那个服务器进行部署，具体代码如下所示：</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">for (int i &#x3D; 0; i &lt; sizes; i++)&#123;    if (vv.Is_Double_node)    &#123;        if (server_myselfs[i].A_cpu &gt;&#x3D; vv.cpu &#x2F; 2 &amp;&amp; server_myselfs[i].A_Memory &gt;&#x3D; vv.Memory &#x2F; 2 &amp;&amp; server_myselfs[i].B_cpu &gt;&#x3D; vv.cpu &#x2F; 2 &amp;&amp; server_myselfs[i].B_Memory &gt;&#x3D; vv.Memory &#x2F; 2)        &#123;            if (((server_myselfs[i].A_cpu + server_myselfs[i].B_cpu - vv.cpu) * Q3 + (server_myselfs[i].A_Memory + server_myselfs[i].B_Memory - vv.Memory) * Q4) &lt; myself_max)            &#123;            myself_max &#x3D; (server_myselfs[i].A_cpu + server_myselfs[i].B_cpu - vv.cpu) * Q3 + (server_myselfs[i].A_Memory + server_myselfs[i].B_Memory - vv.Memory) * Q4;                myself_idnex &#x3D; i;                flag &#x3D; 1;            &#125;        &#125;    &#125;    else    &#123;    int flag1 &#x3D; 0, flag2 &#x3D; 0;        if (server_myselfs[i].A_cpu &gt;&#x3D; vv.cpu &amp;&amp; server_myselfs[i].A_Memory &gt;&#x3D; vv.Memory)        &#123;        if (((server_myselfs[i].A_cpu - vv.cpu) * Q3 + (server_myselfs[i].A_Memory - vv.Memory) * Q4) &lt; myself_max)            &#123;            myself_max &#x3D; (server_myselfs[i].A_cpu - vv.cpu) * Q3 + (server_myselfs[i].A_Memory - vv.Memory) * Q4;                myself_idnex &#x3D; i;                A_or_B &#x3D; 1;                flag &#x3D; 1;                flag1 &#x3D; 1;            &#125;        &#125;        int myself_max_tmp &#x3D; myself_max;        if (server_myselfs[i].B_cpu &gt;&#x3D; vv.cpu &amp;&amp; server_myselfs[i].B_Memory &gt;&#x3D; vv.Memory)        &#123;        if (((server_myselfs[i].B_cpu - vv.cpu) * Q3 + (server_myselfs[i].B_Memory - vv.Memory) * Q4) &lt; myself_max)            &#123;            myself_max &#x3D; (server_myselfs[i].B_cpu - vv.cpu) * Q3 + (server_myselfs[i].B_Memory - vv.Memory) * Q4;                myself_idnex &#x3D; i;                A_or_B &#x3D; 0;                flag &#x3D; 1;                flag2 &#x3D; 1;            &#125;        &#125;        if (myself_max_tmp &lt;&#x3D; myself_max &amp;&amp; flag1 &#x3D;&#x3D; 1 &amp;&amp; flag2 &#x3D;&#x3D; 1)        &#123;        myself_max &#x3D; myself_max_tmp;            A_or_B &#x3D; 1;            flag &#x3D; 1;        &#125;        if (myself_max_tmp &gt;&#x3D; myself_max &amp;&amp; flag1 &#x3D;&#x3D; 1 &amp;&amp; flag2 &#x3D;&#x3D; 1)        &#123;        A_or_B &#x3D; 0;            flag &#x3D; 1;        &#125;    &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="迁移调度"><a class="header-anchor" href="#迁移调度">¶</a>迁移调度</h3><p>    在迁移过程中，我们使用的是一层虚拟机循环，一层服务器循环，我们对上一天中的所有虚拟机进行遍历，在循环中将这个虚拟机部署到其他合适的服务器上，首先去计算当前虚拟机在当前服务器CPU和内存的使用率，**当前CPU和内存的剩余资源所占总资源的百分比小于0.07，那么我们直接跳过，**这样做的理由如下：</p><ol><li>首先可以加速，在正式赛中，数据量比线下的数据量要大很多，我们可以在牺牲一部分迁移的情况下来避免超时，正式赛的超时现象很严重。</li><li>其次，在服务器所剩余资源较小的情况下，我们可以认为当前服务器已经达到了负载均衡，满足理想中的条件，如果再加迁移，可能会影响均衡。</li></ol><p>    在正式赛的提交中，我们的代码在取该值为0.07时，得到的分数是最好的。由于迁移代码较长，就不在此处进行放置，大家可以在我们公布的源码中进行理解。</p><h3 id="可以优化的思路"><a class="header-anchor" href="#可以优化的思路">¶</a>可以优化的思路</h3><p>    因为我们的当前版本的分数可以进入复赛，加上队友后期有事，就没有花太多精力去进行其他方面的优化（也是本人比较菜，写了几个bug，懒得改了）现在分享一些其我们想到的他方面的一些没有实现的优化吧：</p><ol><li>可以将每天的请求进行提前保存，对需要部署的虚拟机进行排序选择，或者可以根据当天的虚拟机的CPU和内存进行拟合，对服务器重新排序，选择合适的服务器。</li><li>可以在每次迁移前，对所有的服务器进行排序，将利用率较小的服务器迁移到利用率较高的服务器上去。</li><li>可以在整个添加服务器上进行单双节点分治，即单节点部署的虚拟机可以迁移到有双节点部署的服务器上去，但是双节点部署的虚拟机不可以迁移到只有单节点部署的服务器上面，这点在我们中间版本的代码中是有一定提升的，但是因为后面的版本有了一些改动，所以就没有继续采用，大佬们可以自己实验以下。</li><li>服务器的选择，线上和线下的服务器中了以及特征是差不多的，因为上面有两种排序得到结果是相同的，线下的数据也是相同的。我们的服务器选择不算差，但是与前排大佬比起来还是逊色一些的，所以服务器的选择可以使用一些更高级的拟合来进行。</li></ol><h3 id="队伍正式赛分数最优代码"><a class="header-anchor" href="#队伍正式赛分数最优代码">¶</a>队伍正式赛分数最优代码</h3><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">#include &lt;iostream&gt;#include &lt;string&gt;#include &lt;vector&gt;#include &lt;unordered_map&gt;#include &lt;fstream&gt;#include &lt;algorithm&gt;#include &lt;ctime&gt;using namespace std;&#x2F;&#x2F;clock_t startTime, endTime;&#x2F;&#x2F; 服务器typedef struct A&#123;    &#x2F;&#x2F;型号, CPU核数, 内存大小, 硬件成本, 能耗成本    string model;              &#x2F;&#x2F;型号    double cpu;                &#x2F;&#x2F;CPU核数    double Memory;             &#x2F;&#x2F;内存大小    long long Hardware_cost;   &#x2F;&#x2F;硬件成本    long long Energy_cost_day; &#x2F;&#x2F;每日能耗成本&#125; server;&#x2F;&#x2F; 虚拟机typedef struct B&#123;    &#x2F;&#x2F;型号, CPU核数, 内存大小, 是否双节点部署    string model;       &#x2F;&#x2F;型号    double cpu;         &#x2F;&#x2F;CPU核数    double Memory;      &#x2F;&#x2F;内存大小    int Is_Double_node; &#x2F;&#x2F;是否双节点部署&#125; VM;&#x2F;&#x2F;用来存放当前所有服务器的信息的typedef struct C&#123;    string model; &#x2F;&#x2F;型号    double A_cpu; &#x2F;&#x2F;已经当前还未使用数量    double A_Memory;    double B_cpu;    double B_Memory;    long long Energy_cost_day;    int flag;    vector&lt;pair&lt;int, int&gt;&gt; VM_ids;&#125; server_myself;&#x2F;&#x2F; 每一天中增加的虚拟机信息typedef struct D&#123;    string model;    int index;    int server_id;      &#x2F;&#x2F;存放在哪一个server中    int Is_Double_node; &#x2F;&#x2F;是否使用双节点部署    int A_or_B;         &#x2F;&#x2F;如果使用的是单节点部署，那么部署在哪个节点,1代表A&#125; add_VM;typedef struct E&#123;    string op;    string model;    int id;&#125; operators;int cost;                                 &#x2F;&#x2F;这个用来记录消费，用来评估算法水平unordered_map&lt;string, server&gt; server_map; &#x2F;&#x2F;存储所有服务器的信息server server_buf;vector&lt;pair&lt;server, double&gt;&gt; model_pair;unordered_map&lt;string, VM&gt; VM_map;VM VM_buf;vector&lt;pair&lt;int, int&gt;&gt; ids_pair;unordered_map&lt;int, add_VM&gt; adds;vector&lt;server_myself&gt; server_myselfs;int day &#x3D; 0;int migrations_index &#x3D; 0;int sum_vm &#x3D; 0;vector&lt;int&gt; cin_size;vector&lt;operators&gt; cin_buf;string buf;double Q1 &#x3D; 0.75, Q2 &#x3D; 0.25;double Q3 &#x3D; 0.75, Q4 &#x3D; 0.25;void InitServer(string buf)&#123;    int j &#x3D; 1;    server_buf.model.clear();    &#x2F;&#x2F; 服务器型号    while (buf[j] !&#x3D; &#39;,&#39;)    &#123;        server_buf.model.push_back(buf[j]);        j++;    &#125;    j++;    while (buf[j] &#x3D;&#x3D; &#39; &#39;)    &#123;        j++;    &#125;    &#x2F;&#x2F; 服务CPU大小    server_buf.cpu &#x3D; 0;    while (buf[j] !&#x3D; &#39;,&#39;)    &#123;        server_buf.cpu &#x3D; server_buf.cpu * 10 + buf[j] - &#39;0&#39;;        j++;    &#125;    j++;    while (buf[j] &#x3D;&#x3D; &#39; &#39;)    &#123;        j++;    &#125;    &#x2F;&#x2F; 服务内存大小    server_buf.Memory &#x3D; 0;    while (buf[j] !&#x3D; &#39;,&#39;)    &#123;        server_buf.Memory &#x3D; server_buf.Memory * 10 + buf[j] - &#39;0&#39;;        j++;    &#125;    j++;    while (buf[j] &#x3D;&#x3D; &#39; &#39;)    &#123;        j++;    &#125;    &#x2F;&#x2F; 服务硬件成本    server_buf.Hardware_cost &#x3D; 0;    while (buf[j] !&#x3D; &#39;,&#39;)    &#123;        server_buf.Hardware_cost &#x3D; server_buf.Hardware_cost * 10 + buf[j] - &#39;0&#39;;        j++;    &#125;    j++;    while (buf[j] &#x3D;&#x3D; &#39; &#39;)    &#123;        j++;    &#125;    &#x2F;&#x2F; 服务耗能成本    server_buf.Energy_cost_day &#x3D; 0;    while (buf[j] !&#x3D; &#39;)&#39;)    &#123;        server_buf.Energy_cost_day &#x3D; server_buf.Energy_cost_day * 10 + buf[j] - &#39;0&#39;;        j++;    &#125;    server_map[server_buf.model] &#x3D; server_buf;    &#x2F;&#x2F;double weight &#x3D; server_buf.cpu * 0.75 + server_buf.Memory * 0.25;    double weight &#x3D; server_buf.Energy_cost_day;    model_pair.push_back(make_pair(server_buf, weight));&#125;void InitVM(string buf)&#123;    int j &#x3D; 1;    VM_buf.model.clear();    while (buf[j] !&#x3D; &#39;,&#39;)    &#123;        VM_buf.model.push_back(buf[j]);        j++;    &#125;    j++;    while (buf[j] &#x3D;&#x3D; &#39; &#39;)    &#123;        j++;    &#125;    VM_buf.cpu &#x3D; 0;    while (buf[j] !&#x3D; &#39;,&#39;)    &#123;        VM_buf.cpu &#x3D; VM_buf.cpu * 10 + buf[j] - &#39;0&#39;;        j++;    &#125;    j++;    while (buf[j] &#x3D;&#x3D; &#39; &#39;)    &#123;        j++;    &#125;    VM_buf.Memory &#x3D; 0;    while (buf[j] !&#x3D; &#39;,&#39;)    &#123;        VM_buf.Memory &#x3D; VM_buf.Memory * 10 + buf[j] - &#39;0&#39;;        j++;    &#125;    j++;    while (buf[j] &#x3D;&#x3D; &#39; &#39;)    &#123;        j++;    &#125;    VM_buf.Is_Double_node &#x3D; buf[j] - &#39;0&#39;;    VM_map[VM_buf.model] &#x3D; VM_buf;&#125;string Operation(string buf, int &amp;index)&#123;    string op;    while (buf[index] !&#x3D; &#39;,&#39;)    &#123;        op.push_back(buf[index]);        index++;    &#125;    index++;    while (buf[index] &#x3D;&#x3D; &#39; &#39;)    &#123;        index++;    &#125;    return op;&#125;bool static cmp(pair&lt;server, double&gt; &amp;A, pair&lt;server, double&gt; &amp;B)&#123;    return A.second &lt; B.second;&#125;bool static cmp2(server_myself &amp;A, server_myself &amp;B)&#123;    return A.A_cpu + A.B_cpu &lt; B.A_cpu + B.B_cpu;&#125;&#x2F;*bool static cmp3(server_myself &amp;A, server_myself &amp;B)&#123;    return A.id &lt; B.id;&#125;*&#x2F;&#x2F;&#x2F;ofstream outfile(&quot;out2.txt&quot;, ios::trunc);&#x2F;&#x2F;ofstream outfile2(&quot;out.txt&quot;, ios::trunc);void Select(int &amp;sum, vector&lt;string&gt; &amp;migrations)&#123;    int j &#x3D; migrations_index;    int server_myselfs_len &#x3D; (int)server_myselfs.size();    int ids_pair_len &#x3D; (int)ids_pair.size();    int find_sum &#x3D; 0;    &#x2F;&#x2F; sort(server_myselfs.begin(), server_myselfs.end(), cmp2);    while (1)    &#123;        j++;        find_sum++;        j &#x3D; j % ids_pair_len;                if (j &#x3D;&#x3D; migrations_index)        &#123;            break;        &#125;        &#x2F;&#x2F; if(find_sum&gt;ids_pair_len&#x2F;2)&#123;        &#x2F;&#x2F;     break;        &#x2F;&#x2F; &#125;        &#x2F;&#x2F;调度不能超过总数的5&#x2F;1000;        if (sum &gt;&#x3D; (sum_vm &#x2F; 1000 * 5 - 1))        &#123;            break;        &#125;        if (ids_pair[j].second)        &#123; &#x2F;&#x2F;表示当前虚拟机还存在的            add_VM temp &#x3D; adds[ids_pair[j].first];            int max_index &#x3D; temp.server_id;            int max_index_or &#x3D; max_index;            &#x2F;*            if (max_index !&#x3D; server_myselfs[max_index].id)            &#123;                cout &lt;&lt; &quot;Error&quot; &lt;&lt; endl;            &#125;            *&#x2F;            double max, max1, max2;            if (temp.Is_Double_node)            &#123;                max &#x3D; (server_myselfs[max_index].A_cpu + server_myselfs[max_index].B_cpu) * Q1 + (server_myselfs[max_index].A_Memory + server_myselfs[max_index].B_Memory) * Q2;                max1 &#x3D; (server_myselfs[max_index].A_cpu + server_myselfs[max_index].B_cpu) * 1.0 &#x2F; (server_map[server_myselfs[max_index].model].cpu);                max2 &#x3D; (server_myselfs[max_index].A_Memory + server_myselfs[max_index].B_Memory) * 1.0 &#x2F; (server_map[server_myselfs[max_index].model].Memory);            &#125;            else            &#123;                if (temp.A_or_B)                &#123;                    max &#x3D; server_myselfs[max_index].A_cpu * Q1 + server_myselfs[max_index].A_Memory * Q2;                    max1 &#x3D; server_myselfs[max_index].A_cpu * 1.0 &#x2F; (server_map[server_myselfs[max_index].model].cpu &#x2F; 2);                    max2 &#x3D; server_myselfs[max_index].A_Memory * 1.0 &#x2F; (server_map[server_myselfs[max_index].model].Memory &#x2F; 2);                &#125;                else                &#123;                    max &#x3D; server_myselfs[max_index].B_cpu * Q1 + server_myselfs[max_index].B_Memory * Q2;                    max1 &#x3D; server_myselfs[max_index].B_cpu * 1.0 &#x2F; (server_map[server_myselfs[max_index].model].cpu &#x2F; 2);                    max2 &#x3D; server_myselfs[max_index].B_Memory * 1.0 &#x2F; (server_map[server_myselfs[max_index].model].Memory &#x2F; 2);                &#125;            &#125;                        if (max1 &lt; 0.07 &amp;&amp; max2 &lt; 0.07)            &#123;                continue;            &#125;                        int flag &#x3D; 0;            int A_or_B &#x3D; 0;            VM vv &#x3D; VM_map[temp.model];            for (int i &#x3D; 0; i &lt; server_myselfs_len; i++)            &#123;                if (i &#x3D;&#x3D; max_index_or)                &#123;                    continue;                &#125;                if (vv.Is_Double_node)                &#123;                    if (server_myselfs[i].A_cpu &gt;&#x3D; vv.cpu &#x2F; 2 &amp;&amp; server_myselfs[i].A_Memory &gt;&#x3D; vv.Memory &#x2F; 2 &amp;&amp; server_myselfs[i].B_cpu &gt;&#x3D; vv.cpu &#x2F; 2 &amp;&amp; server_myselfs[i].B_Memory &gt;&#x3D; vv.Memory &#x2F; 2)                    &#123;                        if (((server_myselfs[i].A_cpu + server_myselfs[i].B_cpu - vv.cpu) * Q1 + (server_myselfs[i].A_Memory + server_myselfs[i].B_Memory - vv.Memory) * Q2) &lt; max)                        &#123;                            max &#x3D; (server_myselfs[i].A_cpu + server_myselfs[i].B_cpu - vv.cpu) * Q1 + (server_myselfs[i].A_Memory + server_myselfs[i].B_Memory - vv.Memory) * Q2;                            max_index &#x3D; i;                            flag &#x3D; 1;                        &#125;                    &#125;                &#125;                else                &#123;                    if (server_myselfs[i].A_cpu &gt;&#x3D; vv.cpu &amp;&amp; server_myselfs[i].A_Memory &gt;&#x3D; vv.Memory)                    &#123;                        if (((server_myselfs[i].A_cpu - vv.cpu) * Q1 + (server_myselfs[i].A_Memory - vv.Memory) * Q2) &lt; max)                        &#123;                            max &#x3D; (server_myselfs[i].A_cpu - vv.cpu) * Q1 + (server_myselfs[i].A_Memory - vv.Memory) * Q2;                            max_index &#x3D; i;                            A_or_B &#x3D; 1;                            flag &#x3D; 1;                        &#125;                    &#125;                    if (server_myselfs[i].B_cpu &gt;&#x3D; vv.cpu &amp;&amp; server_myselfs[i].B_Memory &gt;&#x3D; vv.Memory)                    &#123;                        if (((server_myselfs[i].B_cpu - vv.cpu) * Q1 + (server_myselfs[i].B_Memory - vv.Memory) * Q2) &lt; max)                        &#123;                            max &#x3D; (server_myselfs[i].B_cpu - vv.cpu) * Q1 + (server_myselfs[i].B_Memory - vv.Memory) * Q2;                            max_index &#x3D; i;                            A_or_B &#x3D; 0;                            flag &#x3D; 1;                        &#125;                    &#125;                &#125;            &#125;            if (flag)            &#123;                &#x2F;&#x2F;outfile2 &lt;&lt; &quot;day: &quot; &lt;&lt; day &lt;&lt; &quot;j: &quot; &lt;&lt; j &lt;&lt; &quot; ,&quot; &lt;&lt; ids_pair_len &lt;&lt; endl;                add_VM tt &#x3D; adds[ids_pair[j].first];                sum++;                if (vv.Is_Double_node)                &#123;                    &#x2F;&#x2F;还原                    server_myselfs[max_index_or].A_cpu +&#x3D; vv.cpu &#x2F; 2;                    server_myselfs[max_index_or].A_Memory +&#x3D; vv.Memory &#x2F; 2;                    server_myselfs[max_index_or].B_cpu +&#x3D; vv.cpu &#x2F; 2;                    server_myselfs[max_index_or].B_Memory +&#x3D; vv.Memory &#x2F; 2;                    server_myselfs[max_index].A_cpu -&#x3D; vv.cpu &#x2F; 2;                    server_myselfs[max_index].A_Memory -&#x3D; vv.Memory &#x2F; 2;                    server_myselfs[max_index].B_cpu -&#x3D; vv.cpu &#x2F; 2;                    server_myselfs[max_index].B_Memory -&#x3D; vv.Memory &#x2F; 2;                    tt.server_id &#x3D; max_index;                    tt.Is_Double_node &#x3D; 1;                    migrations.push_back(&quot;(&quot; + to_string(ids_pair[j].first) + &quot;,&quot; + to_string(max_index) + &quot;)&quot;);                &#125;                else                &#123;                    if (tt.A_or_B)                    &#123;                        server_myselfs[max_index_or].A_cpu +&#x3D; vv.cpu;                        server_myselfs[max_index_or].A_Memory +&#x3D; vv.Memory;                    &#125;                    else                    &#123;                        server_myselfs[max_index_or].B_cpu +&#x3D; vv.cpu;                        server_myselfs[max_index_or].B_Memory +&#x3D; vv.Memory;                    &#125;                    if (A_or_B)                    &#123;                        tt.A_or_B &#x3D; 1;                        tt.server_id &#x3D; max_index;                        server_myselfs[max_index].A_cpu -&#x3D; vv.cpu;                        server_myselfs[max_index].A_Memory -&#x3D; vv.Memory;                        migrations.push_back(&quot;(&quot; + to_string(ids_pair[j].first) + &quot;,&quot; + to_string(max_index) + &quot;,A&quot; + &quot;)&quot;);                    &#125;                    else                    &#123;                        tt.A_or_B &#x3D; 0;                        tt.server_id &#x3D; max_index;                        server_myselfs[max_index].B_cpu -&#x3D; vv.cpu;                        server_myselfs[max_index].B_Memory -&#x3D; vv.Memory;                        migrations.push_back(&quot;(&quot; + to_string(ids_pair[j].first) + &quot;,&quot; + to_string(max_index) + &quot;,B&quot; + &quot;)&quot;);                    &#125;                &#125;                if ((server_myselfs[max_index_or].A_cpu + server_myselfs[max_index_or].B_cpu) &#x3D;&#x3D; server_map[server_myselfs[max_index_or].model].cpu)                &#123;                    if ((server_myselfs[max_index_or].A_Memory + server_myselfs[max_index_or].B_Memory) &#x3D;&#x3D; server_map[server_myselfs[max_index_or].model].Memory)                    &#123;                        server_myselfs[max_index_or].flag &#x3D; 0;                    &#125;                &#125;                adds[ids_pair[j].first] &#x3D; tt;            &#125;        &#125;    &#125;    migrations_index &#x3D; j;&#125;void ReadallData(int T)&#123;    for (int k &#x3D; 0; k &lt; T; k++)    &#123;        int R;        cin &gt;&gt; R;        getchar();        cin_size.push_back(R);        for (int j &#x3D; 0; j &lt; R; j++)        &#123;            getline(cin, buf);            int index &#x3D; 1;            operators ops;            ops.op &#x3D; Operation(buf, index);            if (ops.op &#x3D;&#x3D; &quot;add&quot;)            &#123;                while (buf[index] !&#x3D; &#39;,&#39;)                &#123;                    ops.model.push_back(buf[index]);                    index++;                &#125;                index++;                while (buf[index] &#x3D;&#x3D; &#39; &#39;)                &#123;                    index++;                &#125;                ops.id &#x3D; 0; &#x2F;&#x2F; 虚拟机ID                while (buf[index] !&#x3D; &#39;)&#39;)                &#123;                    ops.id &#x3D; ops.id * 10 + buf[index] - &#39;0&#39;;                    index++;                &#125;            &#125;            else            &#123;                ops.id &#x3D; 0;                while (buf[index] !&#x3D; &#39;)&#39;)                &#123;                    ops.id &#x3D; ops.id * 10 + buf[index] - &#39;0&#39;;                    index++;                &#125;            &#125;            cin_buf.push_back(ops);        &#125;    &#125;&#125;int main()&#123;    &#x2F;&#x2F;startTime &#x3D; clock();    int N; &#x2F;&#x2F;服务器种类    cin &gt;&gt; N;    getchar();    for (int i &#x3D; 0; i &lt; N; i++)    &#123;        getline(cin, buf);        InitServer(buf);    &#125;    server server_tem;    int max &#x3D; 10000000, server_tem_index &#x3D; 0;    server_tem_index &#x3D; 11;    sort(model_pair.begin(), model_pair.end(), cmp);    &#x2F;&#x2F;M    int M;    cin &gt;&gt; M;    getchar();    for (int i &#x3D; 0; i &lt; M; i++)    &#123;        getline(cin, buf);        InitVM(buf);    &#125;    &#x2F;&#x2F;T    int T;    cin &gt;&gt; T;    ReadallData(T);    int model_pair_size &#x3D; model_pair.size();    long long cost_min &#x3D; 5000000000;    long long cost &#x3D; 0;    vector&lt;string&gt; res_min;    int i1_index &#x3D; 0;    cost &#x3D; 0;    vector&lt;string&gt; res;    unordered_map&lt;string, int&gt; purchase;    &#x2F;&#x2F;用来存放服务器当天使用种类    vector&lt;pair&lt;string, int&gt;&gt; purchase_num; &#x2F;&#x2F;用来存放服务器当天使用种类个数    for (int k &#x3D; 0; k &lt; T; k++)    &#123;        day++;        vector&lt;string&gt; migrations;        int sum &#x3D; 0;        if (day % 1 &#x3D;&#x3D; 0 &amp;&amp; day !&#x3D; 1)        &#123;            Select(sum, migrations);        &#125;        purchase.clear();        purchase_num.clear();        purchase_num.push_back(make_pair(&quot;000&quot;, 0)); &#x2F;&#x2F;purchase_num第0个是没有用的        vector&lt;string&gt; dis;        int len &#x3D; server_myselfs.size();        vector&lt;vector&lt;int&gt;&gt; ids;        vector&lt;int&gt; dis_id;        vector&lt;operators&gt; day_ops;                for (int j &#x3D; 0; j &lt; cin_size[k]; j++)        &#123;            operators ops &#x3D; cin_buf[i1_index++];            if (ops.op &#x3D;&#x3D; &quot;add&quot;)            &#123;                sum_vm++;                int flag &#x3D; 0;                VM vv &#x3D; VM_map[ops.model];                add_VM add_tt;                add_tt.model &#x3D; ops.model;                add_tt.Is_Double_node &#x3D; 0;                dis_id.push_back(ops.id);                int myself_idnex &#x3D; 0, A_or_B &#x3D; 0;                double myself_max &#x3D; 10000000;                int sizes &#x3D; server_myselfs.size();                for (int i &#x3D; 0; i &lt; sizes; i++)                &#123;                    if (vv.Is_Double_node)                    &#123;                        if (server_myselfs[i].A_cpu &gt;&#x3D; vv.cpu &#x2F; 2 &amp;&amp; server_myselfs[i].A_Memory &gt;&#x3D; vv.Memory &#x2F; 2 &amp;&amp; server_myselfs[i].B_cpu &gt;&#x3D; vv.cpu &#x2F; 2 &amp;&amp; server_myselfs[i].B_Memory &gt;&#x3D; vv.Memory &#x2F; 2)                        &#123;                            if (((server_myselfs[i].A_cpu + server_myselfs[i].B_cpu - vv.cpu) * Q3 + (server_myselfs[i].A_Memory + server_myselfs[i].B_Memory - vv.Memory) * Q4) &lt; myself_max)                            &#123;                                myself_max &#x3D; (server_myselfs[i].A_cpu + server_myselfs[i].B_cpu - vv.cpu) * Q3 + (server_myselfs[i].A_Memory + server_myselfs[i].B_Memory - vv.Memory) * Q4;                                myself_idnex &#x3D; i;                                flag &#x3D; 1;                            &#125;                        &#125;                    &#125;                    else                    &#123;                        int flag1 &#x3D; 0, flag2 &#x3D; 0;                        if (server_myselfs[i].A_cpu &gt;&#x3D; vv.cpu &amp;&amp; server_myselfs[i].A_Memory &gt;&#x3D; vv.Memory)                        &#123;                            if (((server_myselfs[i].A_cpu - vv.cpu) * Q3 + (server_myselfs[i].A_Memory - vv.Memory) * Q4) &lt; myself_max)                            &#123;                                myself_max &#x3D; (server_myselfs[i].A_cpu - vv.cpu) * Q3 + (server_myselfs[i].A_Memory - vv.Memory) * Q4;                                myself_idnex &#x3D; i;                                A_or_B &#x3D; 1;                                flag &#x3D; 1;                                flag1 &#x3D; 1;                            &#125;                        &#125;                        int myself_max_tmp &#x3D; myself_max;                        if (server_myselfs[i].B_cpu &gt;&#x3D; vv.cpu &amp;&amp; server_myselfs[i].B_Memory &gt;&#x3D; vv.Memory)                        &#123;                            if (((server_myselfs[i].B_cpu - vv.cpu) * Q3 + (server_myselfs[i].B_Memory - vv.Memory) * Q4) &lt; myself_max)                            &#123;                                myself_max &#x3D; (server_myselfs[i].B_cpu - vv.cpu) * Q3 + (server_myselfs[i].B_Memory - vv.Memory) * Q4;                                myself_idnex &#x3D; i;                                A_or_B &#x3D; 0;                                flag &#x3D; 1;                                flag2 &#x3D; 1;                            &#125;                        &#125;                        if (myself_max_tmp &lt;&#x3D; myself_max &amp;&amp; flag1 &#x3D;&#x3D; 1 &amp;&amp; flag2 &#x3D;&#x3D; 1)                        &#123;                            myself_max &#x3D; myself_max_tmp;                            A_or_B &#x3D; 1;                            flag &#x3D; 1;                        &#125;                        if (myself_max_tmp &gt;&#x3D; myself_max &amp;&amp; flag1 &#x3D;&#x3D; 1 &amp;&amp; flag2 &#x3D;&#x3D; 1)                        &#123;                            A_or_B &#x3D; 0;                            flag &#x3D; 1;                        &#125;                    &#125;                &#125;                if (flag)                &#123;                    server_myselfs[myself_idnex].VM_ids.push_back(make_pair(ops.id, 1));                    server_myselfs[myself_idnex].flag &#x3D; 1;                    if (vv.Is_Double_node)                    &#123;                        add_tt.Is_Double_node &#x3D; 1;                        server_myselfs[myself_idnex].A_cpu -&#x3D; vv.cpu &#x2F; 2;                        server_myselfs[myself_idnex].A_Memory -&#x3D; vv.Memory &#x2F; 2;                        server_myselfs[myself_idnex].B_cpu -&#x3D; vv.cpu &#x2F; 2;                        server_myselfs[myself_idnex].B_Memory -&#x3D; vv.Memory &#x2F; 2;                        add_tt.server_id &#x3D; myself_idnex;                        if (myself_idnex &gt;&#x3D; len)                        &#123;                            ids[myself_idnex - len].push_back(ops.id);                        &#125;                    &#125;                    else                    &#123;                        if (A_or_B)                        &#123;                            add_tt.A_or_B &#x3D; 1;                            server_myselfs[myself_idnex].A_cpu -&#x3D; vv.cpu;                            server_myselfs[myself_idnex].A_Memory -&#x3D; vv.Memory;                            add_tt.server_id &#x3D; myself_idnex;                            if (myself_idnex &gt;&#x3D; len)                            &#123;                                ids[myself_idnex - len].push_back(ops.id);                            &#125;                        &#125;                        else                        &#123;                            add_tt.A_or_B &#x3D; 0;                            server_myselfs[myself_idnex].B_cpu -&#x3D; vv.cpu;                            server_myselfs[myself_idnex].B_Memory -&#x3D; vv.Memory;                            add_tt.server_id &#x3D; myself_idnex;                            if (myself_idnex &gt;&#x3D; len)                            &#123;                                ids[myself_idnex - len].push_back(ops.id);                            &#125;                        &#125;                    &#125;                &#125;                else                &#123;                    int ii;                                        for (ii &#x3D; 0; ii &lt; model_pair_size; ii++)                    &#123; &#x2F;&#x2F;找到一个最合适服务器的                        if (vv.cpu &lt; model_pair[ii].first.cpu &#x2F; 2 &amp;&amp; vv.Memory &lt; model_pair[ii].first.Memory &#x2F; 2 &amp;&amp; !vv.Is_Double_node)                        &#123;                            break;                        &#125;                        if (vv.cpu &lt; model_pair[ii].first.cpu &amp;&amp; vv.Memory &lt; model_pair[ii].first.Memory &amp;&amp; vv.Is_Double_node)                        &#123;                            break;                        &#125;                    &#125;                                        &#x2F;&#x2F; if((vv.cpu&lt;model_pair[server_tem_index].first.cpu&#x2F;2&amp;&amp;vv.Memory&lt;model_pair[server_tem_index].first.Memory&#x2F;2&amp;&amp;!vv.Is_Double_node)||(vv.cpu&lt;model_pair[server_tem_index].first.cpu&amp;&amp;vv.Memory&lt;model_pair[server_tem_index].first.Memory&amp;&amp;vv.Is_Double_node))&#123;                    &#x2F;&#x2F;     ii &#x3D; server_tem_index;                    &#x2F;&#x2F; &#125;                    &#x2F;&#x2F; else&#123;                    &#x2F;*                    double tmp1 &#x3D; vv.cpu * 1.0 &#x2F; vv.Memory * 1000;                    double maxn &#x3D; 999999;                    for (int ii &#x3D; 0; ii &lt; model_pair_size; ii++)                    &#123; &#x2F;&#x2F;找到一个最合适服务器的                                                double tmp2 &#x3D; model_pair[ii].first.cpu * 1.0 &#x2F; model_pair[ii].first.Memory * 1000;                                                                        if (vv.cpu &lt; model_pair[ii].first.cpu &#x2F; 2 &amp;&amp; vv.Memory &lt; model_pair[ii].first.Memory &#x2F; 2 &amp;&amp; !vv.Is_Double_node)                        &#123;                            if (abs(tmp1 - tmp2) &lt; maxn)                             &#123;                                iii &#x3D; ii;                                maxn &#x3D; abs(tmp1 - tmp2);                            &#125;                        &#125;                        if (vv.cpu &lt; model_pair[ii].first.cpu &amp;&amp; vv.Memory &lt; model_pair[ii].first.Memory &amp;&amp; vv.Is_Double_node)                        &#123;                            if (abs(tmp1 - tmp2) &lt; maxn)                             &#123;                                iii &#x3D; ii;                                maxn &#x3D; abs(tmp1 - tmp2);                            &#125;                        &#125;                    &#125;                    *&#x2F;                    &#x2F;&#x2F; &#125;                    server ss &#x3D; model_pair[ii].first;                    cost +&#x3D; ss.Hardware_cost;                    server_myself tt;                    tt.A_cpu &#x3D; ss.cpu &#x2F; 2;                    tt.A_Memory &#x3D; ss.Memory &#x2F; 2;                    tt.B_cpu &#x3D; ss.cpu &#x2F; 2;                    tt.B_Memory &#x3D; ss.Memory &#x2F; 2;                    tt.model &#x3D; ss.model;                    tt.Energy_cost_day &#x3D; ss.Energy_cost_day;                    tt.flag &#x3D; 1;                    int tt_id &#x3D; 0;                    int tt_index &#x3D; 0;                    if (purchase[ss.model] &#x3D;&#x3D; 0)                    &#123;                        purchase[ss.model] &#x3D; purchase_num.size();                        purchase_num.push_back(make_pair(ss.model, 1));                        tt_id &#x3D; server_myselfs.size();                        tt_index &#x3D; ids.size();                        server_myselfs.push_back(tt);                        ids.push_back(&#123;ops.id&#125;);                    &#125;                    else                    &#123;                        if (purchase[ss.model] &#x3D;&#x3D; (int)purchase_num.size())                        &#123;                            tt_id &#x3D; server_myselfs.size();                            tt_index &#x3D; ids.size();                            server_myselfs.push_back(tt);                            ids.push_back(&#123;ops.id&#125;);                        &#125;                        else                        &#123;                            int sum &#x3D; 0;                            for (int i &#x3D; 1; i &lt;&#x3D; purchase[ss.model]; i++)                            &#123;                                sum +&#x3D; purchase_num[i].second;                            &#125;                            tt_id &#x3D; len + sum;                            tt_index &#x3D; sum;                            server_myselfs.insert(server_myselfs.begin() + tt_id, tt);                            vector&lt;int&gt; temp &#x3D; &#123;ops.id&#125;;                            ids.insert(ids.begin() + sum, temp);                            for (int i &#x3D; sum + 1; i &lt; (int)ids.size(); i++)                            &#123;                                for (int j &#x3D; 0; j &lt; (int)ids[i].size(); j++)                                &#123;                                    if (adds.find(ids[i][j]) !&#x3D; adds.end())                                    &#123;                                        adds[ids[i][j]].server_id++;                                    &#125;                                &#125;                            &#125;                        &#125;                        purchase_num[purchase[ss.model]].second++;                    &#125;                    server_myselfs[tt_id].flag &#x3D; 1;                    server_myselfs[tt_id].VM_ids.push_back(make_pair(ops.id, 1));                    if (vv.Is_Double_node)                    &#123;                        add_tt.Is_Double_node &#x3D; 1;                        if (vv.cpu &#x2F; 2 &lt;&#x3D; server_myselfs[tt_id].A_cpu &amp;&amp; vv.Memory &#x2F; 2 &lt;&#x3D; server_myselfs[tt_id].A_Memory)                        &#123;                            server_myselfs[tt_id].A_cpu -&#x3D; vv.cpu &#x2F; 2;                            server_myselfs[tt_id].A_Memory -&#x3D; vv.Memory &#x2F; 2;                            server_myselfs[tt_id].B_cpu -&#x3D; vv.cpu &#x2F; 2;                            server_myselfs[tt_id].B_Memory -&#x3D; vv.Memory &#x2F; 2;                            add_tt.server_id &#x3D; tt_id;                        &#125;                        else                        &#123;                            cout &lt;&lt; &quot;----------CPU OR Memory 有问题的&quot; &lt;&lt; endl;                            return 0;                        &#125;                    &#125;                    else                    &#123;                        add_tt.A_or_B &#x3D; 1;                        if (vv.cpu &lt;&#x3D; server_myselfs[tt_id].A_cpu &amp;&amp; vv.Memory &lt;&#x3D; server_myselfs[tt_id].A_Memory)                        &#123;                            server_myselfs[tt_id].A_cpu -&#x3D; vv.cpu;                            server_myselfs[tt_id].A_Memory -&#x3D; vv.Memory;                            add_tt.server_id &#x3D; tt_id;                        &#125;                        else                        &#123;                            cout &lt;&lt; &quot;----------CPU OR Memory 有问题的&quot; &lt;&lt; endl;                            return 0;                        &#125;                    &#125;                &#125;                add_tt.index &#x3D; (int)ids_pair.size();                ids_pair.push_back(make_pair(ops.id, 1));                adds[ops.id] &#x3D; add_tt;            &#125;            else            &#123;                &#x2F;&#x2F;回收部分                add_VM del &#x3D; adds[ops.id];                ids_pair[del.index].second &#x3D; 0;                sum_vm--;                if (del.Is_Double_node)                &#123;                    server_myselfs[del.server_id].A_cpu +&#x3D; VM_map[del.model].cpu &#x2F; 2;                    server_myselfs[del.server_id].A_Memory +&#x3D; VM_map[del.model].Memory &#x2F; 2;                    server_myselfs[del.server_id].B_cpu +&#x3D; VM_map[del.model].cpu &#x2F; 2;                    server_myselfs[del.server_id].B_Memory +&#x3D; VM_map[del.model].Memory &#x2F; 2;                &#125;                else                &#123;                    if (del.A_or_B)                    &#123;                        server_myselfs[del.server_id].A_cpu +&#x3D; VM_map[del.model].cpu;                        server_myselfs[del.server_id].A_Memory +&#x3D; VM_map[del.model].Memory;                    &#125;                    else                    &#123;                        server_myselfs[del.server_id].B_cpu +&#x3D; VM_map[del.model].cpu;                        server_myselfs[del.server_id].B_Memory +&#x3D; VM_map[del.model].Memory;                    &#125;                &#125;                for (int i &#x3D; 0; i &lt; (int)server_myselfs[del.server_id].VM_ids.size(); i++)                &#123;                    if (server_myselfs[del.server_id].VM_ids[i].first &#x3D;&#x3D; ops.id)                    &#123;                        server_myselfs[del.server_id].VM_ids[i].second &#x3D; 0;                    &#125;                &#125;                if ((server_myselfs[del.server_id].A_cpu + server_myselfs[del.server_id].B_cpu) &#x3D;&#x3D; server_map[server_myselfs[del.server_id].model].cpu)                &#123;                    if ((server_myselfs[del.server_id].A_Memory + server_myselfs[del.server_id].B_Memory) &#x3D;&#x3D; server_map[server_myselfs[del.server_id].model].Memory)                    &#123;                        server_myselfs[del.server_id].flag &#x3D; 0;                    &#125;                &#125;                &#x2F;&#x2F;adds.erase(id);            &#125;        &#125;        &#x2F;*        for (int i &#x3D; 0; i &lt; (int)server_myselfs.size(); i++)        &#123;            if (server_myselfs[i].flag)            &#123;                cost +&#x3D; server_myselfs[i].Energy_cost_day;            &#125;        &#125;        *&#x2F;        for (int i &#x3D; 0; i &lt; (int)dis_id.size(); i++)        &#123;            if (adds[dis_id[i]].Is_Double_node)            &#123;                dis.push_back(&quot;(&quot; + to_string(adds[dis_id[i]].server_id) + &quot;)&quot;);            &#125;            else            &#123;                if (adds[dis_id[i]].A_or_B)                &#123;                    dis.push_back(&quot;(&quot; + to_string(adds[dis_id[i]].server_id) + &quot;,&quot; + &quot;A)&quot;);                &#125;                else                &#123;                    dis.push_back(&quot;(&quot; + to_string(adds[dis_id[i]].server_id) + &quot;,&quot; + &quot;B)&quot;);                &#125;            &#125;        &#125;        res.push_back(&quot;(purchase,&quot; + to_string(purchase_num.size() - 1) + &quot;)&quot;);        for (int i &#x3D; 1; i &lt; (int)purchase_num.size(); i++)        &#123;            res.push_back(&quot;(&quot; + purchase_num[i].first + &quot;,&quot; + to_string(purchase_num[i].second) + &quot;)&quot;);        &#125;        res.push_back(&quot;(migration,&quot; + to_string(migrations.size()) + &quot;)&quot;); &#x2F;&#x2F;一个简单的调度算法的        for (int i &#x3D; 0; i &lt; (int)migrations.size(); i++)        &#123;            res.push_back(migrations[i]);        &#125;        for (int i &#x3D; 0; i &lt; (int)dis.size(); i++)        &#123;            res.push_back(dis[i]);        &#125;    &#125;    for (int i &#x3D; 0; i &lt; (int)res.size(); i++)    &#123;        cout &lt;&lt; res[i] &lt;&lt; endl;        &#x2F;&#x2F;outfile &lt;&lt; res[i] &lt;&lt; endl;    &#125;    &#x2F;&#x2F; cout &lt;&lt; cost_min &lt;&lt; endl;    &#x2F;&#x2F;cout &lt;&lt; cost &lt;&lt; endl;    &#x2F;&#x2F;endTime &#x3D; clock();    &#x2F;&#x2F; cout &lt;&lt; &quot;The run time is:&quot; &lt;&lt;(double)(endTime - startTime) &#x2F; CLOCKS_PER_SEC &lt;&lt; &quot;s&quot; &lt;&lt; endl;    return 0;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>    复赛的题目应该是在初赛上加一些约束或者其他的数据，这里面的优化思路还是有一定的指引性的，仅供参考。如果在我的博客中有什么写的不妥的，或者错误的地方，欢迎大家留言批评指正。<br><strong>凡不能摧毁我者，必将使我更强大！！！</strong></p>]]></content>
      
      
      <categories>
          
          <category> 比赛总结 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 比赛算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>进程管理（二）</title>
      <link href="/2021/03/27/jin-cheng-guan-li-er/"/>
      <url>/2021/03/27/jin-cheng-guan-li-er/</url>
      
        <content type="html"><![CDATA[<h1>处理机调度</h1><h2 id="调度概念"><a class="header-anchor" href="#调度概念">¶</a>调度概念</h2><h3 id="调度的基本概念"><a class="header-anchor" href="#调度的基本概念">¶</a>调度的基本概念</h3><p>  因为在我们平时使用的多道程序系统中，进程的数量会多于处理机的个数，因此会经常出现进程争用处理机的情况。而处理机的调度就是<strong>对处理机进行分配，从就绪队列中按照一定的算法（公平、高效）选择一个进程并将处理机分配给它允许，以实现进程并发地执行。</strong></p><h3 id="调度层次"><a class="header-anchor" href="#调度层次">¶</a>调度层次</h3><p>  一个作业从提交开始直到完成，往往要经历以下的三级调度：<br>  （1）<strong>作业调度</strong>：主要任务是按一定的原则，从外存上处于后备状态的作业中挑选一个（或多个）作业，给它（们）分配内存、输入/输出设备等必要的资源，并建立相应的进程，以使它（们）获得竞争处理机的权利。<strong>多道批处理系统中大多数配有作业调度，而其他系统中通常不需要配置作业调度，并且作业调度的执行频率较低，通常为几分钟一次</strong>。<br>  （2）<strong>中级调度</strong>：为了提高内存利用率和系统吞吐量，应将那些暂时不能运行的进程调至外存等待，把此时的进程状态称为<strong>挂起态</strong>。当这些进程已经具备运行条件时且内存有空闲时，就由中级调度来决定把外存上的那些已经具备运行条件的就绪进程重新调入内存，并且修改其状态为就绪态，挂在就绪队列上等待。<br>  （3）<strong>进程调度</strong>：也被称为低级调度，按照某种方法和策略从就绪队列中选取一个进程，将处理机分配给它。<strong>进程调度是操作系统中最基本的一种调度</strong>，在一般的操作系统中都必须配置进程调度。<br><img src="/2021/03/27/jin-cheng-guan-li-er/1.png" alt="处理机的三级调度"><br>  因为在这里涉及到了作业和进程的调度，所以我在这里按照我的理解以及我参考其他的人博客中的理解来进行一定的解释（有误请大家私聊告诉我）。<br>  <strong>作业与进程的区别</strong>：一个进程是一个程序对某个数据集的执行过程，是资源的基本单位。作业是用户需要计算机完成的某项任务，是要求计算机所做工作的集合。一个作业的完成要经过作业提交、作业收容、作业执行和作业完成4个阶段。而进程是对已提交完毕的程序所执行过程的描述，是资源分配的基本单位。主要区别如下：<br>  （1）作业是用户向计算机提交任务的任务实体，在用户向计算机提交作业后，系统将它放入外存中的作业等待队列中等待执行。而进程则是完成用户任务的执行实体，是向系统申请分配资源的基本单位，任一进程，只要它被创建，总有相应的部分存在于内存中。<br>  （2）一个作业可由多个进程组成，且必须至少由一个进程组成，反过来不成立。<br>  （3）<strong>作业的概念主要用在批处理系统中</strong>，像分时系统中就没有作业的概念，而<strong>进程的概念则用在几乎所有的多道程序系统中</strong>。</p><h3 id="三级调度的联系"><a class="header-anchor" href="#三级调度的联系">¶</a>三级调度的联系</h3><p>  （1）作业调度为进程活动做准备，进程调度使进程正常活动起来，中级调度将暂时不能运行的进程挂起，中级调度处于作业调度和进程调度之间。<br>  （2）作业调度次数较少，中级调度次数略多，进程调度频率最高。<br>  （3）进程调度是最基本的，不可或缺的。</p><h2 id="调度的时机、切换与过程"><a class="header-anchor" href="#调度的时机、切换与过程">¶</a>调度的时机、切换与过程</h2><p>  进程调度和切换程序是操作系统内核程序。一般情况下，在请求调度的事件发生后，才可能运行进程调度程序，调度了新的就绪进程后，才会进行进程间的切换。但是在实际中可能会发送以下一些情况导致不能进行进程的调度与切换。<br>  （1）<strong>在处理中断的过程中</strong><br>  （2）<strong>进程在操作系统内核程序临界区中</strong>：进入临界区后，需要独占式的访问共享数据，理论上要加锁来防止其他程序进入，在解锁前不应切换到其他进程，以加快共享数据的释放。<br>  （3）<strong>其他需要完全屏蔽中断的原子操作过程中</strong>：如加锁、解锁、中断现场保护，即使中断也要进行相应的屏蔽。<br>  进行进程调度切换的情况如下:<br>  （1）发送引起调度条件且当前进程无法继续马上运行下去时，进行调度切换（非剥夺式调度）<br>  （2）中断处理结束或自陷处理结束后，返回被中断进程的用户态程序执行现场前，若置上请求调度标志，马上进行进程调度与切换（剥夺式调度）</p><h2 id="进程调度方式"><a class="header-anchor" href="#进程调度方式">¶</a>进程调度方式</h2><p>  （1）非剥夺调度方式：当一个进程在处理机上执行时，即使有更重要的进程进入就绪队列，处理机上的进程依然可以执行直到变为阻塞状态后，让出处理机。优点是<strong>实现简单，系统开销小适用于大多数的批处理系统，但是不适用于分时系统和大多数的实时系统</strong>。<br>  （2）剥夺调度方式：当一个在处理机上运行的进程在遇到另一个更为重要的进程进入就绪队列后，立即暂停，将处理机让出给更为重要的进程。<strong>对提高系统吞吐率和相应效率都有明显的好处</strong>。</p><h2 id="调度的基本准则"><a class="header-anchor" href="#调度的基本准则">¶</a>调度的基本准则</h2><p>  （1）<strong>CPU利用率</strong><br>  （2）<strong>系统吞吐量</strong>：表示单位时间内CPU完成作业的数量。<br>  （3）<strong>周转时间</strong>：从作业提交到作业完成所经历的时间，是作业等待、在就绪队列中排队、在处理机上运行以及进行输入/输出操作所花费时间的总和。<br>  作业的周转时间可用公式表示如下：<br>$$周转时间 = 作业完成时间 - 作业提交时间$$<br>  平均周转时间是指多个作业周转时间的平均值：<br>$$平均周转时间 = (作业1的周转时间 + … + 作业 n 的周转时间) / n$$<br>  带权周转时间是指作业周转时间与作业实际运行时间的比值。<br>  平均带权周转时间是指多个作业带权周转时间的平均值：<br>$$平均带权周转时间 = (作业1的带权周转时间 + … + 作业 n 的带权周转时间) / n$$<br>  （4）<strong>等待时间</strong>：指进程处于等处理机状态时间之和。因为处理机调度算法在实际上并不影响作业执行或输入/输出操作，只影响作业在就绪队列中等待所花的时间，所以衡量一个调度算法优劣常常只需简单地考察等待时间。<br>  （5）<strong>响应时间</strong>：是指从用户提交请求到系统首次产生响应所用的时间。</p><h2 id="典型的调度算法"><a class="header-anchor" href="#典型的调度算法">¶</a>典型的调度算法</h2><h3 id="先来先服务（FCFS）调度算法"><a class="header-anchor" href="#先来先服务（FCFS）调度算法">¶</a>先来先服务（FCFS）调度算法</h3><p>  在作业调度中，每次从后备队列中选择最先进入该队列的一个或几个作业，将它们调入内存，分配必要的资源，创建进程并放入就绪队列。进程调度类似。算法实例如下图所示：<br><img src="/2021/03/27/jin-cheng-guan-li-er/2.png" alt="FCFS调度算法性能"><br>  从上图中可以看出，该算法的特点是：算法简单，但是效率低；对长作业比较有利，对短作业不利；有利于CPU繁忙型作业，而不利于I/O繁忙型作业。</p><h3 id="短作业优先（SJF）调度算法"><a class="header-anchor" href="#短作业优先（SJF）调度算法">¶</a>短作业优先（SJF）调度算法</h3><p>  从队列中选出一个估计运行时间最短的作业优先调度，既可用于作业调度，也可用于进程调度。算法实例如下图所示：<br><img src="/2021/03/27/jin-cheng-guan-li-er/3.png" alt="SJF调度算法性能"><br>  SJF调度算法也存在不容忽视的缺点<br>  （1）<strong>对长作业不利</strong>。严重的是，若一长作业（进程）进入系统的后备队列（就绪队列），由于调度程序总是优先调度那些（即使是后进来的）短作业（进程），将导致长作业（进程）长期不被调度——饥饿<br>  （2）<strong>完全未考虑作业（进程）的紧迫程度，因而不能保证紧迫性作业（进程）会被及时处理</strong>。<br>  （3）由于作业（进程）的长短只是根据用户所提供的估计执行时间而定的，而用户又可能会有意或无意地缩短其作业的估计运行时间，致使该算法不一定能真正做到短作业优先调度。<br>  <strong>SJF调度算法的平均等待时间、平均周转时间最少</strong>。</p><h3 id="优先级调度算法"><a class="header-anchor" href="#优先级调度算法">¶</a>优先级调度算法</h3><p>  既可用于作业调度，又可用于进程调度。每次从后备作业队列中选择优先级最高的一个或几个作业（进程）进行执行。根据新的更高优先级进程能否抢占正在执行的进程，将调度算法分为两种（1）<strong>非剥夺式优先级调度算法</strong>、（2）<strong>剥夺式优先级调度算法</strong>。（在本文上面以及有讲解，此处省略。）<br>  根据进程创建后其优先级是否可以改变，可以将进程优先级分为以下两种。<br>  （1）<strong>静态优先权</strong>：静态优先权在创建进程时确定，且在进程的整个运行期间保持不变。确定进程优先权的依据有进程类型、进程对资源的需求、用户要求。<br>  （2）<strong>动态优先权</strong>：在创建进程时赋予的优先权是随进程的推进或随其等待时间的增加而改变，以获得更好的调度性能。<br>  在一般情况下，进程优先级的设置可以参照以下的原则：<br>  （1）<strong>系统进程 &gt; 用户进程</strong><br>  （2）交互型进程 &gt; 非交互型进程<br>  （3）I/O型进程 &gt; 计算型进程，因为I/O设备的处理速度要比CPU慢很多，所以需要让I/O设备尽早开始工作。</p><h3 id="高响应比优先调度算法"><a class="header-anchor" href="#高响应比优先调度算法">¶</a>高响应比优先调度算法</h3><p>  主要用于作业调度，既考虑作业估计的运行时间也考虑作业的等待时间，综合了先来先服务和最短作业优先两种算法的特点。该算法中的响应比是指作业等待时间与运行比值，响应比公式定义如下：<br>$$响应比R_{p} =（等待时间+要求服务时间）/ 要求服务时间$$<br>  优点有：（1）等待时间相同的作业，则要求服务的时间愈短，其优先权愈高，——对短作业有利；（2）要求服务的时间相同的作业，则等待时间愈长，其优先权愈高，——是先来先服务；（3）对于长作业，优先权随等待时间的增加而提高，其等待时间足够长时，其优先权便可升到很高， 从而也可获得处理机——对长作业有利。<br>  缺点：要进行响应比计算，增加了系统开销。</p><h3 id="时间片轮转调度算法"><a class="header-anchor" href="#时间片轮转调度算法">¶</a>时间片轮转调度算法</h3><p>  系统将所有的就绪进程按先来先服务的原则排成一个队列，每次调度时，把CPU分配给队首进程，并令其执行一个时间片；当执行的时间片用完时，由一个计时器发出时钟中断请求，调度程序便停止该进程的执行，并将其放就绪队列尾；然后，再把处理机分配给就绪队列中新的队首。<br>  <strong>时间片大小的选取很重要</strong>，若时间片足够大，以至于所有进程都能在一个时间片内执行完毕，则算法退化为先来先服务调度算法。若时间片很小，则处理机将在进程间过于频繁地切换，使得处理机的开销增大，而真正用于进程处理的时间很少。<br>  时间片的长短通常由：系统的响应时间、就绪队列中的进程数目和系统的处理能力来决定。</p><h3 id="多级反馈队列调度算法"><a class="header-anchor" href="#多级反馈队列调度算法">¶</a>多级反馈队列调度算法</h3><p>  是时间片轮转调度算法和优先级调度算法的综合。其具体思路如下：<br>  （1）先将它放入第一个队列的末尾，按FCFS原则排队等待调度。<br>  （2）如果时间片内完成，便可准备撤离系统。<br>  （3）如果时间片内未能完成，调度程序便将该进程转入第二队列的末尾等待再次被调度执行。<br>  （4）当第一队列中的进程都执行完，系统再按FCFS原则调度第二队列。在第二队列的稍放长些的时间片内仍未完成，再依次将它放入第三队列。<br>  （5）依次降到第n队列后，在第n队列中便采取按时间片轮转的方式运行。<br><img src="/2021/03/27/jin-cheng-guan-li-er/4.png" alt="多级反馈队列调度算法"><br>  需要注意的是以下几个方面：<br>  （1）设置多个就绪队列，各队列有不同的优先级,优先级从第一个队列依次降低。<br>  （2）赋予各队列进程执行时间片大小不同, 优先权越高，时间片越短。<br>  （3）仅当优先权高的队列（如第一队列）空闲时，调度程序才调度第二队列中的进程运行。<br>  （4）高优先级抢占时，被抢占的进程放回原就绪队列末尾。<br>  优点：（1）终端型作业用户：短作业优先。（2）短批处理作业用户：周转时间较短。（3）长批处理作业用户：经过前面几个队列得到部分执行，不会长期得不到处理。</p>]]></content>
      
      
      <categories>
          
          <category> 课本学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 操作系统 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>进程管理（一）</title>
      <link href="/2021/03/23/jin-cheng-guan-li-yi/"/>
      <url>/2021/03/23/jin-cheng-guan-li-yi/</url>
      
        <content type="html"><![CDATA[<h1>进程与线程</h1><h2 id="进程的概念与特征"><a class="header-anchor" href="#进程的概念与特征">¶</a>进程的概念与特征</h2><h3 id="进程的概念"><a class="header-anchor" href="#进程的概念">¶</a>进程的概念</h3><p>  <strong>进程概念</strong>：<strong>进程是进程实体的运行过程</strong>，<strong>是系统进行资源分配和调度的一个独立单位</strong>。<br>  在上述定义中，我们应该如何去理解进程实体？在最初的单道程序环境下，当我们的程序被载入到内存之后，它会被划分为程序段和数据段。如下图所示：<br><img src="/2021/03/23/jin-cheng-guan-li-yi/1.png" alt="程序在内存存储图"><br>  但是在早期，由于计算机内存只支持一道应用程序，所以我们把该进程的程序段和数据段放在固定的位置。但是随之计算机的发展，计算机可以支持多道程序并发运行，操作系统为了去记录这些进程的程序段和数据段的位置，构建了一个叫<strong>程序控制块</strong>（<strong>PBC</strong>）的数据结构来存放这些信息。如下图所示：<br><img src="/2021/03/23/jin-cheng-guan-li-yi/2.png" alt="PBC存储信息"><br>  至此，可以使参与并发执行的程序（包括数据）能够独立地运行，因此<strong>进程实体</strong>=<strong>PCB</strong>+<strong>程序段</strong>+<strong>数据段</strong>。（进程实体也可以称为进程映像）所以<strong>进程创建的实质就是创建进程映像中的PCB</strong>，反之如此。在这里可以看出<strong>PCB是进程存在的唯一标志</strong>。在一般情况下，我们把进程实体就称为进程，但是严格来说，<strong>进程实体和进程并不一样，进程实体是静态的，而进程是动态的</strong>。<br>  同时在进程的定义中，要准确的理解其系统资源，在定义中的系统资源实际上是指处理机、存储器和其他设备服务于某个进程的“时间”。因为进程在多道程序中并发执行时，通常由处理机的时间片为其分配运行时间，这也决定了进程一定是一个动态的概念。</p><h3 id="进程的特征"><a class="header-anchor" href="#进程的特征">¶</a>进程的特征</h3><p>  因为在多道程序环境下，允许多个程序并发执行，在这个过程中，这些程序就失去了封闭性，因此引出进程，所以进程的基本特征也是对进程管理提出的基本要求。<br>（1）<strong>动态性</strong>：进程是程序的一次执行，从创建到活动、暂停、终止等过程，具有自己的生命周期，是动态地产生、变化和消亡的。该特性是进程最基本的特性。<br>（2）<strong>并发性</strong>：顾名思义，多个进程实体可以同时存在于内存中，进程能够在同一时间段内同时运行，该特性是操作系统的重要特征，使程序与其他进程的程序并发执行，提高了资源利用率。<br>（3）<strong>独立性</strong>：指进程实体是一个能独立运行、独立获得资源和独立接受调度的基本单位。<strong>注意</strong>：必须是进程实体，也就是说，必须要创建PCB！！！。<br>（4）<strong>异步性</strong>：由于进程的相互制约，使得进程具有执行的间断性，即进程按各自独立的、不可预知的速度向前推进，比如，当正在执行的进程提出某种资源请求时，如打印请求，而此时打印机正在为其他某进程打印，由于打印机属于临界资源，因此正在执行的进程必须等待，且放弃处理机，直到打印机空闲，并再次把处理机分配给该进程时，该进程方能继续执行。可见，由于资源等因素的限制，进程的执行通常都不是“一气呵成”，而是以“停停走走”的方式运行。异步性就是描述进程这种以不可预知的速度走走停停、何时开始何时暂停何时结束不可预知的性质。异步性会导致执行结果的不可再现性，为此操作系统中必须配置响应的进程同步机制。<br>（5）<strong>结构性</strong>：每个进程都配置一个PCB对其进行描述，其结构为<strong>PCB</strong>+<strong>程序段</strong>+<strong>数据段</strong>。</p><h2 id="进程的状态与转换"><a class="header-anchor" href="#进程的状态与转换">¶</a>进程的状态与转换</h2><p>  进程在其生命周期内，由于进程之间的相互制约以及系统的运行环境的变化，导致其状态也在不断地发生变化。在一般情况下存在以下五种状态：<br>  （1）<strong>运行态</strong>：进程在处理及上运行，并且在单机环境下，每个时刻最多只有一个进程在运行。<br>  （2）<strong>就绪态</strong>：进程获得了除处理机外的一切所需资源，一旦得到处理机，便可立即运行。系统中处于就绪状态的进程可能有多个，通常将它们排成一个队列，称为就绪队列。<br>  （3）<strong>阻塞态</strong>：又称<strong>等待态</strong>，进程正在等待某一事件而暂停运行，如等待某资源为可用（<strong>不包括处理机，这点与就绪态有明显的差别</strong>）或等待输入/输出完成，即使处理机空闲，该程序也不能运行。<br>  （4）<strong>创建态</strong>：进程正在被创建，尚未转到就绪态。创建进程通常需要多个步骤：首先申请一个空白的PCB，并向PCB中填写一些控制和管理进程的信息；然后由系统为该该进程分配运行时所必须的资源；最后把该进程转入就绪态。<br>  （5）<strong>结束态</strong>：进程正从系统中消失，可能是进程正常结束或其他原因中断推出运行。进程需要结束运行时，系统首先必须将该进程置为结束态，然后进一步处理资源释放和回收等工作。<br>  <strong>注意</strong>，就绪态和等待态是不同的，因为就绪态是指进程仅缺少处理机，只要获得处理机资源就立即运行；而等待态是指进程需要其他资源（除了处理机）或等待某一事件。五种进程状态的转换情况如下图所示：<br><img src="/2021/03/23/jin-cheng-guan-li-yi/3.png" alt="5种进程状态的转换"><br>  （1）<strong>就绪态</strong>$\rightarrow$<strong>运行态</strong>：处于就绪态的进程被调度后，获得处理机资源（分派处理机时间片），于是进程由就绪态转换为运行态。<br>  （2）<strong>运行态</strong>$\rightarrow$<strong>就绪态</strong>：当处于运行态的进程在时间片用完之后，会让出处理机，转化为就绪态，或者在可剥夺的操作系统种，当有更高级的进程就绪时，调度程序将正在执行的进程转化为就绪态，让更高级的进程执行。<br>  （2）<strong>运行态</strong>$\rightarrow$<strong>阻塞态</strong>：进程请求某一资源的使用和分配或等待某一事件的发生（例如I/O操作的完成）时，就会从运行态转换为阻塞态。<strong>进程以系统调用的形式请求操作系统提供服务，这是一种特殊的、由运行用户态程序调用操作系统内核过程的形式</strong>。<br>  需要注意的是，一个进程从运行态变成阻塞态是主动行为，而从阻塞态变成就绪态是被动的行为，需要其他相关进程协助。</p><h2 id="进程控制"><a class="header-anchor" href="#进程控制">¶</a>进程控制</h2><p>  进程控制主要是对系统中的所有进程实施有效的管理，其具有创建新进程、撤销已有进程、实现进程状态转换等功能。通常，我们把进程控制用的程序段称为<strong>原语</strong>。它是一个不可分割的基本单位。</p><h3 id="进程的创建"><a class="header-anchor" href="#进程的创建">¶</a>进程的创建</h3><p>  一个进程可以创建另一个进程，创建者为父进程，被创建的进程称为子进程。子进程可以继承父进程所拥有的资源。并且在其被撤销时，要归还所有从父进程那里得到的资源。当父进程被撤销时，必须撤销其所有的子进程。创建进程的过程如下：<br>  （1）为新进程分配一个唯一的标识号，并且申请一个空白的PCB，<strong>若PCB申请失败，则创建失败</strong>，这与在上文中（进程的概念）我解释的进程创建的实质相对应。<br>  （2）为进程分配资源，例如为程序和数据以及用户栈分配必要的内存空间。当资源不足时，进程会进入阻塞态。<br>  （3）初始化PCB，设置进程优先级等。<br>  （4）当进程就绪队列可以接纳新进程后，就将新进程插入队列中，等待被调度运行。</p><h3 id="进程的终止"><a class="header-anchor" href="#进程的终止">¶</a>进程的终止</h3><p>  在进程被执行的过程中，通常会因为一些事件而引起进程终止。主要由以下的几种情况：<br>  （1）正常结束。<br>  （2）异常结束，如存储区越界，非法指令、I/O故障等使程序无法继续运行。<br>  （3）外界干预：程应外界的请求而终止运行，例如：操作员和操作系统的干预，父进程请求和父进程终止。<br>  操作系统终止进程的过程如下：<br>  （1）根据被终止进程的标识符，检索PCB，从中读出该进程的状态。<br>  （2）若被终止进程处于执行状态，立即终止该进程的执行，将处理机资源分配给其他进程。<br>  （3）若该进程还有子孙进程，则应将其所有的子孙进程终止。<br>  （4）将该进程所有的资源全部归还与父进程或者操作系统。<br>  （5）将PCB从所在队列（链表中）删除。</p><h3 id="进程的阻塞和唤醒"><a class="header-anchor" href="#进程的阻塞和唤醒">¶</a>进程的阻塞和唤醒</h3><p>  正在执行的进程由于期待的某些事件未发生，由系统自动执行阻塞原语（Block，也就是执行阻塞程序），使自己由运行态变为阻塞态。对此可以看出，<strong>进程的阻塞是进程自身的一种主动行为，同时，也只有处于阻塞态的进程（获得了CPU）才可能将其转为阻塞态</strong>。阻塞原语的执行过程如下：<br>  （1）找到将要被阻塞进程的标识号对应的PCB。<br>  （2）若该进程为运行态，则保护其现场，将其状态转为阻塞态，停止运行。<br>  （3）把该PCB插入相应时间的等待队列，将处理机资源调度给其他就绪进程。<br>  当阻塞进程所期待的事件出现时，由有关进程调用唤醒原语，将该进程唤醒。唤醒原语（Wakeup）的执行过程如下：<br>  （1）把该事件的等待队列种找到相应进程的PCB。<br>  （2）将其从等待队列中移除，并置其状态为就绪态。<br>  （3）把该PCB插入就绪队列，等待调度程序调度。<br>  需要注意的是，这一对原语作用恰好相反，必须成对使用，<strong>其中阻塞原语是被阻塞进程自我调用实现的，而唤醒原语则是有一个被唤醒进程合作或被其他相关进程调用实现的。</strong></p><h3 id="进程切换"><a class="header-anchor" href="#进程切换">¶</a>进程切换</h3><p>  通常情况下，进程的创建、撤销等操作都是利用系统调用进入内核，在内核中由相应处理程序完成的。而进程的切换是指处理机从一个进程的运行转到另一个进程上的运行，同样需要内核的支持，所以<strong>任何进程都是在操作系统内核的支持下运行的</strong>，其切换过程如下：<br>  （1）保存处理机上下文，包括程序计数器和其他寄存器。<br>  （2）更新PCB信息<br>  （3）把进程的PCB移入到相应的队列，如就绪、在某事件阻塞等队列。<br>  （4）选择另一个进程执行，并更新其PCB。<br>  （5）更新内存管理的数据结构。<br>  （6）恢复处理机上下文。<br>  需要注意一点的是，<strong>调度和切换是两种不同的情况，调度是指决定资源分配给哪个进程的行为，是一种决策行为，而切换则是指实际分配的行为，是执行行为。</strong></p><h2 id="进程的组织"><a class="header-anchor" href="#进程的组织">¶</a>进程的组织</h2><p>  **进程是一个独立的运行单位，也是操作系统进程资源分配和调度的基本单位。**由PCB + 数据段 + 程序段组成。其中最为核心的部分是PCB。</p><h3 id="进程控制块（PCB）"><a class="header-anchor" href="#进程控制块（PCB）">¶</a>进程控制块（PCB）</h3><p>  操作系统通过PCB表来管理和控制进程。在进程创建时，操作系统为进程新建一个PCB，该结构之后可以存储在内存中，并且在任意时刻都可以存取，在进程的执行过程中，系统可以通过PCB来了解进程的先行状态信息，以便对其进行管理和控制，在进程结束时，系统收回其PCB，该进程也随之消亡。并且在PCB中还保存了进程状态及优先级，处理机状态信息，数据和程序的内存初始地址。发生断点的处理机环境。<br>  为了方便进程的调度和管理，需要将各进程的PCB用适当的方法组织起来，目前常用的方法有链式法和索引法。链式方式就是将同一状态的PCB链接成队列的。索引方式是将同一状态的进程组织在一个索引表中，索引表的表项指向相应的PCB。</p><h3 id="程序段"><a class="header-anchor" href="#程序段">¶</a>程序段</h3><p>  程序段就是能被进程调度程序调度到CPU执行的程序代码段。<strong>程序可被多个进程共享，即多个进程可有运行同一个程序</strong>。</p><h3 id="数据段"><a class="header-anchor" href="#数据段">¶</a>数据段</h3><p>  一个进程的数据段，可以是进程对应的程序加工处理的原始数据，也可以是程序执行时产生的中间或最终结果。</p><h2 id="进程的通信"><a class="header-anchor" href="#进程的通信">¶</a>进程的通信</h2><p>  进程通信是指进程之间的信息交换。其中PV操作属于低级通信方式，而高级通信是指以较高的效率传输大量数据的通信方式。高级通信主要有以下三种方式：</p><h3 id="共享存储"><a class="header-anchor" href="#共享存储">¶</a>共享存储</h3><p>  在通信的进程之间存在一块可直接访问的共享空间，通过对这片共享空间进行读/写操作，实现进程之间的信息交换。操作系统只负责为通信进程提供可共享使用的存储空间和同步互斥工具（P/V原语），而其中的数据交换则由用户自己安排读/写指令完成。共享存储又分为两种：（1）低级方式的基于数据结构的共享。（2）高级方式的基于存储区的共享。<br>  <strong>用户进程空间一般都是独立的，进程运行期间一般不能访问其他进程空间，必须通过特殊的系统调用才能实现。进程内的线程是自然共享进程空间的</strong>。<br><img src="/2021/03/23/jin-cheng-guan-li-yi/4.png" alt="共享存储"></p><h3 id="消息传递"><a class="header-anchor" href="#消息传递">¶</a>消息传递</h3><p>  在消息传递系统中，进程间的数据交换是以格式化的消息为单位的。<strong>如果通信的进程之间不存在可直接访问的共享空间，则必须利用操作系统提供的消息传递方法实现进程通信</strong>。操作系统为进程提供了发送消息和接收消息两个原语来进行数据交换。消息传递的通信方式分为两种。<br>  （1）直接通信方式：发送进程把消息发送给接收进程，把消息挂在接收进程的消息缓冲队列上，接收进程从消息缓冲队列中得到消息。该过程可以看作两个人之间写信进行交流的过程。<br><img src="/2021/03/23/jin-cheng-guan-li-yi/5.png" alt="直接通信方式"><br>  （2）间接通信方式：即发送在某个实体，接收进程从该实体中获取消息。该实体类似于一个邮差的功能。</p><h3 id="管道通信"><a class="header-anchor" href="#管道通信">¶</a>管道通信</h3><p>  管道通信时消息传递的一种特殊方式。这里的“管道”是指用于连接一个读进程和一个写进程，以实现它们之间的通信的一个共享文件。其中向管道输入和接收数据都是由进程实现的。为了协调读写进程的通信，管道机制必须提供<strong>互斥</strong>、<strong>同步和确定对方的存在</strong>的三方面能力。<br>  <strong>从管道读数据是一次性操作，数据一旦被读取，它就从管道中被抛弃，释放空间以便写入更多的数据。并且管道在某一时刻只能单向传输，要实现父子进程双方互动通信，需要定义两个管道</strong>。</p><h2 id="线程的概念和多线程模型"><a class="header-anchor" href="#线程的概念和多线程模型">¶</a>线程的概念和多线程模型</h2><h3 id="线程的基本概念"><a class="header-anchor" href="#线程的基本概念">¶</a>线程的基本概念</h3><p>  <strong>引入进程的目的是更好地使多道程序并发执行，提高资源利用率和系统吞吐量；引入线程的目的则是减小程序在并发执行时所付出的时空开销，提高操作系统的并发性能。</strong><br>  <strong>线程是一个基本的CPU执行单元，也是程序执行流的最小单元。线程是进程中的一个实体，是被系统独立调度和分配的基本单位（与进程要进行区分，进程是系统进行资源分配和调度的一个独立单位）</strong><br>  <strong>线程自己不拥有系统资源，只拥有一点在运行中必不可少的资源。但是它可与同属一个进程的其他线程共享进程所拥有的全部资源。</strong><br>  <strong>一个线程可以创建和撤销另一个线程，同一个进程中的多个线程之间可以并发执行。</strong><br>  由于线程之间的相互制约，致使线程在运行中呈现出间断性。并且线程也有就绪、阻塞和运行三种状态。</p><h3 id="进程与线程的比较（理解重点）"><a class="header-anchor" href="#进程与线程的比较（理解重点）">¶</a>进程与线程的比较（理解重点）</h3><p>  （1）调度：<strong>线程是独立调度的基本单位，进程是拥有资源的基本单位</strong>，在同一进程中，线程的切换不会引起进程的切换，。在不同进程中进行线程切换，会引起进程切换。<br>  （2）拥有资源：进程是拥有资源的基本单位，而线程不拥有系统资源。但线程可以访问其隶属进程的系统资源。<br>  （3）并发性：不仅进程之间可以并发执行，而且多个线程之间也可以并发执行，从而使操作系统具有更好的并发性，提高系统的吞吐量。<br>  （4）系统开销：由于创建或撤销进程时，系统都要为之分配或回收资源，因此操作系统所付出的开销远大于创建或撤销线程时的开销。因为同一进程内的多个线程共享进程的地址空间，所以这些线程之间的通信非常容易实现，甚至无需操作系统的干预。<br>  （5）地址空间和其他资源：进程的地址空间之间相互独立，同一进程的各线程间共享进程的资源，某进程内的线程对于其他进程不可见。<br>  （6）通信方面：进程间通信需要进程同步和互斥手段的辅助，以保证数据的一致性，而线程间可以直接读/写进程数据段（如全局变量）来进行通信。</p><h3 id="线程的属性"><a class="header-anchor" href="#线程的属性">¶</a>线程的属性</h3><p>  在多线程操作系统把线程作为独立运行（或调度）的基本单位之后，进程就已经不再是一个基本的可执行实体。因此所谓进程处于“执行状态”，实际上是指该进程中的某线程正在执行。线程的主要属性如下：<br>  （1）线程不拥有系统资源，但每个线程有一个唯一的标识符和一个线程控制块，其记录了线程执行的寄存器和栈等现场状态。<br>  （2）不同的线程可以执行相同的程序。<br>  （3）同一个进程中的各个线程共享该进程所拥有的资源。<br>  （4）线程是处理机的地理调度单位，多个线程是可以并发执行的。<br>  （5）一个线程被创建后，便开始了它的生命周期，直至终止。</p><h3 id="线程的实现方式"><a class="header-anchor" href="#线程的实现方式">¶</a>线程的实现方式</h3><p>  分为用户级线程和内核级线程。</p><h3 id="多线程模型"><a class="header-anchor" href="#多线程模型">¶</a>多线程模型</h3><p>  有些系统支持用户线程和内核线程，由此产生了不同的多线程模型，即实现用户级线程和内核级线程的连接方式。主要有（1）多对一模型、（2）一对一模型、（3）多对多模型。</p>]]></content>
      
      
      <categories>
          
          <category> 课本学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 操作系统 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>计算机系统概述（三）</title>
      <link href="/2021/03/22/ji-suan-ji-xi-tong-gai-shu-san/"/>
      <url>/2021/03/22/ji-suan-ji-xi-tong-gai-shu-san/</url>
      
        <content type="html"><![CDATA[<h1>操作系统的运行环境</h1><h2 id="操作系统的运行机制"><a class="header-anchor" href="#操作系统的运行机制">¶</a>操作系统的运行机制</h2><p>  在计算机系统中，CPU通常会执行两种不同性质的程序，（1）<strong>操作系统的内核程序即管理程序</strong>，（2）<strong>用户自编程序即应用程序</strong>。<br>  <strong>管理程序</strong>：执行一些特权指令，这些指令不允许用户直接使用，例如I/O指令、置中断指令、存取用于内存保护的寄存器等。<br>  <strong>用户自编程序</strong>：出于安全考虑不能执行这些指令。<br>  用户自编程序运行在用户态，操作系统内核程序运行在核心态。并且现在操作系统几乎都是层次式的结构。操作系统的各项功能分别被设置在不同的层次上。一些与硬件关联较紧密的模块，如时钟管理、中断处理、设备驱动等处于最底层；其次是一些运行频率较高的程序，如进程管理、存储器管理和设备管理等。并且上面的这两部分构成了操作系统的<strong>内核</strong>，内核的指令操作工作在<strong>核心态</strong>。内核主要包括以下四部分内容。</p><h3 id="时钟管理"><a class="header-anchor" href="#时钟管理">¶</a>时钟管理</h3><p>  时钟管理的主要作用如下几点：<br>  （1）计时，即向用户提供标准的系统时间。<br>  （2）<strong>通过时钟中断的管理，切换进程</strong>。在分时操作系统中就有很好的体现（采用时间片轮转调度）。<br>  （3）衡量作业的运行程度（在批处理系统中，无论是多道批处理系统还是单道批处理系统，在微观上都是属于串行，因此可以用作业的执行时间来衡量作业的运行程度）。</p><h3 id="中断机制"><a class="header-anchor" href="#中断机制">¶</a>中断机制</h3><p>  在中断机制中，只有一小部分功能属于内核，它们保护和恢复中断现场的信息，转移控制权到相关的处理程序。提高系统的并行处理能力。并且可以提高多道程序运行环境中CPU的利用率。例如：<strong>进程的管理和调度</strong>，<strong>系统功能的调用</strong>，<strong>设备驱动</strong>……</p><h3 id="原语"><a class="header-anchor" href="#原语">¶</a>原语</h3><p>  按照上述所说的，操作系统是层次式的结构，那么在其底层一定是一些可被调用的具有特定功能的公用程序，例如：CPU切换、设备驱动等，因此将这些称为<strong>原语</strong>。其具有以下的特征：<br>  （1）处于操作系统的最底层，最接近硬件的部分。<br>  （2）其运行具有原子性，其操作必须一次性完成。<br>  （3）被频繁调用，且运行时间较短。</p><h3 id="系统控制的数据结构及处理"><a class="header-anchor" href="#系统控制的数据结构及处理">¶</a>系统控制的数据结构及处理</h3><p>  用来等级状态信息的数据结构，如作业控制块、进程控制块、消息队列、链表、内存分配表等。其常见操作有以下3种：<br>  （1）<strong>进程管理</strong>：进程状态管理、进程调度和分派、创建与撤销进程控制块等。<br>  （2）<strong>存储器管理</strong>存储器的空间分配和回收、内存信息保护程序、代码对换程序等。<br>  （3）<strong>设备管理</strong>：缓冲区管理、设备分配和回收。</p><h2 id="中断和异常"><a class="header-anchor" href="#中断和异常">¶</a>中断和异常</h2><h3 id="定义"><a class="header-anchor" href="#定义">¶</a>定义</h3><p>  <strong>CPU运行上层程序时，唯一可以实现从用户态进入核心态的方式就是中断或异常</strong>。并且中断可以在程序并未使用某种资源时，把它对那种资源的占有权释放，提高资源利用率。<br>  <strong>中断</strong>：指计算机运行过程中，出现某些意外情况需主机干预时，机器能自动停止正在运行的程序并转入处理新情况的程序，处理完毕后又返回原被暂停的程序继续运行。<br>  <strong>异常</strong>：指的是在程序运行过程中发生的异常事件，通常是由外部问题（如硬件错误、输入错误）所导致的。</p><h3 id="中断处理过程"><a class="header-anchor" href="#中断处理过程">¶</a>中断处理过程</h3><p>  中断处理流程图如下图所示：<br><img src="/2021/03/22/ji-suan-ji-xi-tong-gai-shu-san/1.png" alt="中断处理的流程"><br>  （1）<strong>关中断</strong>：CPU响应中断后，首先要保护程序的现场状态，不应响应更高级中断源的中断请求。否则现场会保存不完整。<br>  （2）<strong>保存断点</strong>：将原来的程序断点保存起来，在中断服务程序执行完毕后能正确地返回到原来的程序。<br>  （3）<strong>中断服务程序寻址</strong>：取出中断服务程序的入口地址送入程序计数器PC。<br>  （4）<strong>保存现场和屏蔽字</strong>：现场信息一般是指程序状态字寄存器PSWR和某些通用寄存器的内容。<br>  （5）<strong>开中断</strong>：允许更高级中断请求得到响应。<br>  （6）<strong>关中断</strong>：保证在恢复现场和屏蔽字时不被中断。<br>  （7）<strong>开中断、中断返回</strong>：中断服务程序的最后一条指令通常是一条中断返回指令，使其返回到原程序的断点处，以便继续执行原程序。</p><h3 id="系统调用"><a class="header-anchor" href="#系统调用">¶</a>系统调用</h3><p>  系统中的各种共享资源都由操作系统统一掌管，因此在用户程序种，凡是与资源有关的操作（如存储分配、进行I/O传输及管理文件等），都必须通过系统调用方式向操作系统提出服务请求，并由操作系统代为完成。系统调用功能大致可分为如下几类：<br>  （1）<strong>设备管理</strong>：完成设备的请求或释放，以及设备启动等功能。<br>  （2）<strong>文件管理</strong>：完成文件的读、写、创建即删除等功能。<br>  （3）<strong>进程控制</strong>：完成进程的创建、撤销、阻塞及唤醒等功能。<br>  （4）<strong>进程通信</strong>：完成进程之间的消息传递或信号传递等功能。<br>  （5）<strong>内存管理</strong>：完成内存的分配、回收以及获取作业占用内存区大小及实址等功能。<br>  这样做的好处是<strong>保证系统的稳定性和安全性，防止用户程序随意更改或访问重要的资源系统，影响其他进程的运行</strong>。</p><p>  操作系统的运行环境可以理解为：用户通过操作系统运行上层程序（如用户自编程序），而上层程序的运行需要操作系统的底层管程序提供服务支持。当需要管理程序服务时，系统通过硬件中断机制进入核心态，运行管理程序，或者在出现异常时，被动提供管理程序服务。实现了从用户态转入核心态，在管理服务程序结束后，保存程序现场退出中断处理程序或异常处理程序，返回断点处继续执行用户自编程序。具体流程图如下所示：<br><img src="/2021/03/22/ji-suan-ji-xi-tong-gai-shu-san/2.png" alt="系统调用执行过程"></p>]]></content>
      
      
      <categories>
          
          <category> 课本学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 操作系统 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>计算机系统概述（二）</title>
      <link href="/2021/03/22/ji-suan-ji-xi-tong-gai-shu-er/"/>
      <url>/2021/03/22/ji-suan-ji-xi-tong-gai-shu-er/</url>
      
        <content type="html"><![CDATA[<h1>操作系统的发展与分类</h1><h2 id="手工操作阶段"><a class="header-anchor" href="#手工操作阶段">¶</a>手工操作阶段</h2><p>  所有的工作都需要人工干预，缺点如下：<br>  （1）用户独占全机，虽然不会出现因资源已被其他用户占用而等待的现象，但资源利用低。<br>  （2）CPU等待手工操作，CPU的利用不充分。</p><h2 id="批处理阶段（操作系统开始出现）"><a class="header-anchor" href="#批处理阶段（操作系统开始出现）">¶</a>批处理阶段（操作系统开始出现）</h2><p>  解决人机矛盾及CPU和I/O设备之间速度不匹配的矛盾，出现了单道批处理系统和多道批处理系统。</p><h3 id="单道批处理系统"><a class="header-anchor" href="#单道批处理系统">¶</a>单道批处理系统</h3><p>  主要特征：<br>  （1）<strong>自动性</strong>：在顺利的情况下，磁带上的一批作业能自动地逐个运行，而且无须人工干预。<br>  （2）<strong>顺序性</strong>：磁带上的各道作业顺序地进入内存，先调入内存的作业先完成。<br>  （3）<strong>单道性</strong>：内存中仅有一道程序运行，当该程序完成或发生异常情况时，才换入其后继程序进入内存运行。<br>  缺点：内存中作业运行期间发出输入/输出请求后，高速的CPU要等待低速的I/O完成，资源利用率和系统的吞吐量较低。</p><h3 id="多道批处理系统"><a class="header-anchor" href="#多道批处理系统">¶</a>多道批处理系统</h3><p>  多道批处理系统允许多个程序同时进入内存并允许它们在CPU中交替地运行，这些程序共享系统中的各种硬/软件资源，当一道程序因I/O请求而暂停运行时，CPU便立即转去运行另一道程序。<br>  主要特征如下：<br>  （1）<strong>多道</strong>：计算机内存中同时存放多道相互独立的程序。<br>  （2）<strong>宏观上并行</strong>：同时进入系统的多道程序都处于运行过程中，即各自运行，但都未运行完毕。<br>  （3）<strong>微观上串行</strong>：内存中的多道程序轮流占有CPU，交替执行。<br>  多道程序设计技术的实现需要解决以下问题：<br>  （1）如何分配处理器。<br>  （2）多道程序的内存分配问题。<br>  （3）I/O设备如何分配。<br>  （4）如何组织和存放大量的程序和数据，以方便用户使用并保证其安全性与一致性。<br>  <strong>优点</strong>：（1）资源利用率高。（2）系统吞吐量大，CPU和其他资源保持“忙碌”状态。<br>  <strong>缺点</strong>：（1）用户响应的时间较长。（2）不提供人机交互能力，用户既不了解自己程序的运行情况，又不能控制计算机。</p><h2 id="分时操作系统"><a class="header-anchor" href="#分时操作系统">¶</a>分时操作系统</h2><p>  <strong>分时技术</strong>：把处理及的运行时间分成很短的时间片，按时间片把处理器分配给各联机作业使用。若某个作业在分配给它的时间片内不能完成其计算，则该作业暂时停止运行，把处理器让给其他的作业使用，等待下一次继续使用。<br>  <strong>分是操作系统</strong>：用户通过终端同时共享一台主机，这些终端连接在主机上，用户可以同时与主机进行交互操作而互不干扰。分时系统特征如下：<br>  （1）<strong>同时性</strong>：允许多个终端用户同时使用一台计算机。<br>  （2）<strong>交互性</strong>：用户可以方便地与系统进行人机对话。<br>  （3）<strong>独立性</strong>：多个用户可以彼此独立操作，互不干扰。<br>  （4）<strong>及时性</strong>：用户在请求计算机时可以很快得到响应。<br><strong>分时操作系统也是支持多道程序设计的，但是与多道批处理系统还是存在差别的</strong>，具体差别如下：<br>  多道程序系统是在计算机内存中同时存放几道相互独立的程序，使它们在管理程序控制之下，相互穿插的运行。 两个或两个以上程序在计算机系统中同处于开始和结束之间的状态。这就称为多道程序技术运行的特征：多道、宏观上并行、微观上串行。<br>  分时操作系统是使一台计算机同时为几个、几十个甚至几百个用户服务的一种操作系统。把计算机与许多终端用户连接起来，分时操作系统将系统处理机时间与内存空间按一定的时间间隔，轮流地切换给各终端用户的程序使用。由于时间间隔很短，每个用户的感觉就像他独占计算机一样。<br>  总之，分时操作系统主要是针对于多用户来说的，而多道程序系统主要是针对于多程序来说的，注意用户和程序之间的区别。</p><h2 id="实时操作系统"><a class="header-anchor" href="#实时操作系统">¶</a>实时操作系统</h2><p>  可以在某个时间限制内完成某些紧急任务而不需要时间片排队。其分为两种情况：<br>  （1）<strong>硬实时系统</strong>：某个动作必须要在规定的时刻或时间段内完成，如飞行自动控制系统。<br>  （2）<strong>软实时系统</strong>：偶尔违反时间规定且不会引起任何永久性的损害。如飞机订票系统。</p><h2 id="网络操作系统"><a class="header-anchor" href="#网络操作系统">¶</a>网络操作系统</h2><p>  通过计算机网络把各台计算机联合在一起，实现各台计算机之间数据的互相传送。<br>  <strong>特点</strong>：网络中各种资源的共享及计算机之间的通信。</p><h2 id="分布式计算机系统"><a class="header-anchor" href="#分布式计算机系统">¶</a>分布式计算机系统</h2><p>  属于分布式计算机系统需要满足以下条件：<br>  （1）系统中任意两台计算机通过通信方式交换信息。<br>  （2）系统中的每台计算机具有相同的地位。<br>  （3）每台计算机的资源为所有用户共享。<br>  （4）任意台计算机都可以构成子系统，并且可以重构。<br>  （5）任何工作都可以分布在几台计算机上，由他们协同完成。<br>  主要特点如下：<strong>分布性</strong>和<strong>并行性</strong>。</p><h2 id="个人操作系统"><a class="header-anchor" href="#个人操作系统">¶</a>个人操作系统</h2><p>  有常见的Windows、Linux、Macintosh等，应用最为广泛。</p><p>  还有嵌入式操作系统、服务器操作系统等。</p>]]></content>
      
      
      <categories>
          
          <category> 课本学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 操作系统 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>计算机系统概述（一）</title>
      <link href="/2021/03/20/ji-suan-ji-xi-tong-gai-shu-yi/"/>
      <url>/2021/03/20/ji-suan-ji-xi-tong-gai-shu-yi/</url>
      
        <content type="html"><![CDATA[<h1>操作系统的基本概念</h1><h2 id="什么是操作系统？"><a class="header-anchor" href="#什么是操作系统？">¶</a>什么是操作系统？</h2><p>  操作系统（Operating System， OS）是指：控制和管理整个计算机系统的硬件与软件资源，合理地组织、调度计算机的工作与资源的分配，进而为用户和其他软件提供方便的接口和环境的程序集合。<strong>操作系统是计算机系统中最基本的系统软件</strong>。<br>  其作用简单来说：<br>（1）控制和管理整个计算机系统的硬件与软件资源。<br>（2）组织和调度计算机工作和资源的分配。<br>（3）提供给用户和其他软件方便的接口和环境。</p><h2 id="操作系统的特征"><a class="header-anchor" href="#操作系统的特征">¶</a>操作系统的特征</h2><p>  操作系统是一种系统软件，其基本特征包括<strong>并发、共享、虚拟和异步</strong>。</p><h3 id="（1）并发（Concurrence）"><a class="header-anchor" href="#（1）并发（Concurrence）">¶</a>（1）并发（Concurrence）</h3><p>  并发是指两个或多个事件在同一时间间隔内发生。操作系统的并发性是指计算机系统中同时存在多个运行的程序。因此我们可以认为它有上述的作用（2）.同时在整个操作系统中，我们引入进程的目的，也是为了使程序能并发执行。<br>  在上面我们需要注意的是，并发和并行的区别，通俗来说，并发是指事情同一时间段内发生，例如在我写这篇博客的整个时间段内，我顺便去偷看了旁边几个漂亮的妹纸，在宏观上来说，这些事情都是在这个时间段上是同时发生的，但是在微观上的其中几分钟，几秒钟来说，这些事情其实是交替发生的，整个过程大致就是“写博客-看妹纸-写博客-看妹纸-看妹纸-写博客”（不好意思，其实我只看了一次😇 😇 😇）而并行是指在同一时刻发生的，例如在我偷看妹纸的时候，发现我对面的哥们（经典无中生友😂 😂 😂）其实也在和我一样，不过他在偷看旁边的男生😳 😳 😳，那么我们两个人在相同的时刻做的这件事情，这就叫做并行。<br>  在我们进行提到的多线程中，如果你的电脑为单核CPU，那么即使你加再多的线程，也是并发执行，因为一个核心，只能处理一个任务。一个核心处理多任务的方法，（1）排队，一个一个执行。（2）一个执行一小段时间，在多个任务间切换，没有被服务的任务只能等待，表现就是你的电脑会有些卡顿。如果你的CPU有两个核心，那么它的每个核心，在此时此刻，可以分别服务一个任务，这样就可以实现并行。</p><h3 id="（2）共享（Sharing）"><a class="header-anchor" href="#（2）共享（Sharing）">¶</a>（2）共享（Sharing）</h3><p>  即资源共享，是指操作系统中的资源可供内存中多个并发执行的进程共同使用。共享主要分为<strong>互斥共享方式</strong>和<strong>同时访问方式</strong>。</p><h4 id="互斥共享方式"><a class="header-anchor" href="#互斥共享方式">¶</a>互斥共享方式</h4><p>  操作系统中的资源虽然可以同时提供给多个进程使用，但一个时间段内只允许一个进程访问该资源。<br>  举一个通俗的例子来解释，如果有一个妹纸问我有没有时间陪她出去玩，但是同时我的平时一个很讨厌的人也在问我能否陪他出去，这两个进程同时请求使用我嘴的资源，这时我不可能同时回答他们两个人的问题，是或者不是都会让其中一个人误会我的意思（当然是想和妹纸出去，拒绝另外一个人）。这就是互斥共享。<br>  我们把上述的资源共享方式称为互斥共享，把一段时间内只允许一个进程访问的资源称为<strong>临界资源</strong>或者<strong>独占资源</strong>，例如：打印机、磁带机等。</p><h4 id="同时访问方式"><a class="header-anchor" href="#同时访问方式">¶</a>同时访问方式</h4><p>  允许一个时间段内由多个进程“同时”对他们进行访问<br>  “同时”通常上是宏观的，而在微观上，这些进程可能是交替地对该资源进行访问即“分时共享”的。比如说：今天在图书馆的这一个下午，看妹纸和写博客这两件事情都可以向我的眼睛和大脑来发送请求，使用这两个资源，在这段时间内，我可以看一眼妹子，然后写几行博客，交替进行这两个进程。在计算机中可供多个进程“同时”访问的典型资源是磁盘设备。<br>  互斥共享要求一种资源在一段时间内（哪怕是一段很小的时间）只能，满足一个请求，否则就会出现严重的问题（例如打字机一行打印A文档内容，一行打印B文档内容）而同时访问共享通常要求一个请求分几个时间片段间隔地完成。<br>  并发和共享之间互为存在的条件：<strong>（1）资源共享是以程序的并发为条件的，若系统不允许程序并发执行则不存在资源共享的问题。（2）若系统不能对资源共享实施有效的管理，则影响到程序的并发执行，甚至根本无法存在并发执行</strong>。</p><h3 id="（3）虚拟（Virtual）"><a class="header-anchor" href="#（3）虚拟（Virtual）">¶</a>（3）虚拟（Virtual）</h3><p>  虚拟是指把一个物理上的实体转变为若干逻辑上的对应物。操作系统的虚拟技术可归纳为：<strong>（1）时分复用技术：处理器的分时共享</strong>。<strong>（2）空分复用技术：虚拟存储器</strong>。</p><h3 id="（4）异步（Asynchronism）"><a class="header-anchor" href="#（4）异步（Asynchronism）">¶</a>（4）异步（Asynchronism）</h3><p>  在多道程序环境中，允许多个程序并发执行，但由于资源优先，进程的执行不是一贯到底，而是走走停停，以不可预知的速度向前推进。</p><h2 id="操作系统的目标和功能"><a class="header-anchor" href="#操作系统的目标和功能">¶</a>操作系统的目标和功能</h2><p>  （1）为了给多道程序提供良好的运行环境，操作系统应具有以下几方面的功能：<strong>处理机管理、存储器管理、设备管理和文件管理</strong>。<br>  （2）为了方便用户使用操作系统，还必须向用户提供接口。<br>  （3）操作系统可用来扩充机器，以提供更方便的服务、更高的资源利用率。</p><h3 id="操作系统作为计算机系统资源的管理者"><a class="header-anchor" href="#操作系统作为计算机系统资源的管理者">¶</a>操作系统作为计算机系统资源的管理者</h3><h4 id="处理机管理"><a class="header-anchor" href="#处理机管理">¶</a>处理机管理</h4><p>  在多道程序环境下，<strong>处理机的分配和运行都以进程、线程为基本单位</strong>，因而对处理机的管理可归结为对进程的管理。因此进程何时创建、何时撒销、如何管理、如何避免冲突、合理共享就是进程管理的最主要的任务。进程管理的主要功能包括<strong>进程控制、进程同步、进程通信、死锁处理、处理机调度等。</strong></p><h4 id="存储器管理"><a class="header-anchor" href="#存储器管理">¶</a>存储器管理</h4><p>  存储器管理是为了给多道程序的运行提供良好的环境，方便用户用及提高内存的利用率，主要包括<strong>内存分配与回收、地址映射、内存保护与共享和内存扩充等功能</strong>。</p><h4 id="文件管理"><a class="header-anchor" href="#文件管理">¶</a>文件管理</h4><p>  计算机中的信息都是以文件的形式存在的，操作系统中负责文件管理的部分称为文件系统，文件管理包括<strong>文件存储空间的管理、目录管理及文件读写管理和保护等</strong>。</p><h4 id="设备管理"><a class="header-anchor" href="#设备管理">¶</a>设备管理</h4><p>  设备管理的主要任务是完成用户的I/O请求，方便用户使用各种设备，并提高设备的利用率，主要包括<strong>缓冲管理、设备分配、设备处理和虚拟设备等功能</strong>。</p><h3 id="操作系统作为用户与计算机硬件系统之间的接口"><a class="header-anchor" href="#操作系统作为用户与计算机硬件系统之间的接口">¶</a>操作系统作为用户与计算机硬件系统之间的接口</h3><p>  为了方便用户对于计算机硬件的操作以及运行自己计算机上的程序，操作系统为用户提供了接口，接口分为两类，<strong>（1）命令接口，（2）程序接口</strong>。</p><h4 id="命令接口"><a class="header-anchor" href="#命令接口">¶</a>命令接口</h4><p>  命令接口的目的是，用户利用这些命令来组织和控制作业的执行。其分为<strong>联机命令接口</strong>和<strong>脱机命令接口</strong>。<br>  <strong>联机命令接口</strong>又称交互式命令接口，适用于分时或实时系统的接口，通常是用户在键盘通过控制台或者终端输入操作命令，操作系统的命令解释程序解释执行输入的命令后，完成功能后，将指挥权返回终端。<br>  <strong>脱机命令接口</strong>又称批处理命令接口，适用于批处理系统。用户不能直接干预作业的运行，而是事先用相应的作业控制命令以作业操作说明书的形式提交给操作系统，等待系统中命令解释器对作业进行逐条处理。</p><h4 id="程序接口（系统调用）"><a class="header-anchor" href="#程序接口（系统调用）">¶</a>程序接口（系统调用）</h4><p>  程序接口由一组系统调用命令(也称广义指令)组成。用户通过在程序中使用这些系统调用来请求操作系统为其提供服务。如使用各种外部设备、申请分配和回收内内存及其他用程序接口实现的，当前最为流行的是图形用户界面(GUT),即图形接口。</p><h3 id="操作系统用作扩充机器"><a class="header-anchor" href="#操作系统用作扩充机器">¶</a>操作系统用作扩充机器</h3><p>  裸机没有任何软件支持，而我们实际中的计算机系统是若干层软件改造之后的，在裸机的外层，就是操作系统，这也不难理解为什么我们称其为硬件与用户之间的中介，因为其提供了大量的资源管理功能和方便用户的各种服务功能。</p>]]></content>
      
      
      <categories>
          
          <category> 课本学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 操作系统 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>xbwcj的失败史</title>
      <link href="/2021/03/19/xbwcj-de-shi-bai-shi/"/>
      <url>/2021/03/19/xbwcj-de-shi-bai-shi/</url>
      
        <content type="html"><![CDATA[<h2 id="分享一句话"><a class="header-anchor" href="#分享一句话">¶</a>分享一句话</h2><p><font face="微软雅黑" size="4">你我都是内卷背景下不起眼的尘埃，被时代的洪流裹挟向前。</font></p><h2 id="个人失败历史"><a class="header-anchor" href="#个人失败历史">¶</a>个人失败历史</h2><p>入学以来大一参加计算机设计大赛失败。😟<br>入学以后第一场考试以及后面无数考试失利。😩<br>大一参加校内数学建模新生杯惨败。😞<br>大一参加华中赛数学建模无功而返。😑<br>大一参加互联网+因特殊原因退出。😔<br>……<br>大二开始接触DataFountain竞赛，一直徘徊在复赛圈，与决赛无缘。<br>开始学习acm算法，在各种比赛中被吊打。<br>开始学习一些简单的开发，但是没有完整做出一款属于自己的开发的应用。<br>大二参加华为软件精英挑战赛，被数据漏洞坑害（参赛经验不足，获取比赛信息时效性低），赛区64强止步。<br>大二参加全国k-code程序设计大赛，止步21名，倒在决赛圈（前二十名）门口。<br>参加MathorCup成功参赛。<br>大二暑期建模培训最终选拔全校第七名，特殊原因被淘汰。<br>……<br>大三参加天池Redis数据库中间件挑战赛止步决赛。<br>……<br>还有很多自己以及以往的失败历史。</p><h2 id="未来准备参加的比赛"><a class="header-anchor" href="#未来准备参加的比赛">¶</a>未来准备参加的比赛</h2><p>美赛成绩未出<br>继续准备参加计算机设计大赛、华为软件精英挑战赛、等之类的其他比赛。</p><h2 id="xbwcj的数学建模比赛的准备"><a class="header-anchor" href="#xbwcj的数学建模比赛的准备">¶</a>xbwcj的数学建模比赛的准备</h2><p>1、提高自己的编程能力，编程能力的提升是在平时就要锻炼的，平时写代码的时候不要马虎应付，多看博客等其他网站上的一些常用算法与伪代码，最重要的是了解算法的思维，严谨的思维+超强的编程能力才能轻松应对数模中的编程。<br>2、强烈推荐去看司守奎的《数学建模算法与应用》这本书，学习上面的一些比较常见的算法，自己一定要手动实现一次，自己不手动实现，就很难理解里面的编程思维。<br>3、多去准备一些比较新的智能优化算法，推荐公众号：数学算法实验室、智能优化算法。尝试自己改进一些算法，将这些算法做成接口的形式，在使用时只需要传参就可以。准备一些可以做出精美图片的数据分析的软件，例如Oracle提供的Data Visualization Desktop的软件，也可以使用Office的PPT功能。熟练使用可以在比赛过程中省时省力。或者准备一些Matlab或者Python的作图代码，例如简单的条形图、折线图、雷达图等，或者高级点的词云之类的，也以接口的形式封装。<br>4、如何上手？一定一定要自己独自去完成一次数模的全部流程，了解每个步骤应该做什么如何去做。只有全部接触才能知道自己的优势以及劣势，了解自己应该找什么样的队友来取长补短。<br>5、暑期培训前的准备：2020年由于疫情的原因，选拔赛是以组队的形式直接做题来选拔。在暑期培训前无论是校赛还是校外的数模比赛中，都可以自己去找队友，因此最好在暑期选拔培训前就确定好自己的队友，尽早开始磨合，利用其他的数模比赛找到队伍的不足，这样可以在选拔赛中发挥的更好一些。校外的数模比赛建议参加：华中赛、泰迪杯、数维杯、深圳杯、亚太赛。（挑选其中几个即可）</p><h2 id="xbwcj的校外比赛经验"><a class="header-anchor" href="#xbwcj的校外比赛经验">¶</a>xbwcj的校外比赛经验</h2><p>推荐几个我经常浪荡的比赛网站：<br><a href="https://tianchi.aliyun.com/competition/gameList/activeList">天池</a>、<a href="https://competition.huaweicloud.com/competitions">华为云</a>、<br><a href="https://www.datafountain.cn/competitions">DataFountain</a>、<a href="https://www.kaggle.com/competitions">kaggle</a>、<br>这些比赛有奖金，并且你会学习到很多平时在课堂里面学不到的知识技能。刚开始得奖会比较难，因为这些比赛中参加的不只是本科生，一般是面向所有在校学生，而且硕士生所占比例不低，所以开始大家可以去涨经验，后面能到何种地步就看大家自己的发挥了。🐶🐶🐶<br>如果想要提高自己的编程能力：<br><a href="https://ac.nowcoder.com/acm/problem/list">牛客</a>、<a href="https://www.luogu.com.cn/problem/list">洛谷</a>、<a href="http://acm.hdu.edu.cn/">hduOJ</a>、<a href="https://codeforces.com/problemset">CodeForces</a>、<a href="https://nanti.jisuanke.com/acm">计蒜客</a>。<br>去刷里面的题目，如果不是专门去训练acm，不需要去做太难的，达到中等就可以应付绝大多数公司的绝大多数笔试题了。也可以帮助你了解比较重要的基础算法。</p><h2 id="xbwcj个人大学总结经验（其实是自己的不足与后悔之处）"><a class="header-anchor" href="#xbwcj个人大学总结经验（其实是自己的不足与后悔之处）">¶</a>xbwcj个人大学总结经验（其实是自己的不足与后悔之处）</h2><p>1、一定要尽早规划，自己以后是找工作还是要读研，读研的话，是准备考研，还是A、B保，如果保研自己还欠缺什么，尽早做打算。<br>2、精心，静心去做一件事，贪多嚼不烂。<br>3、无论做什么，基础一定要扎实。</p>]]></content>
      
      
      <categories>
          
          <category> 个人分享 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 个人分享 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>xbwcj的失败史</title>
      <link href="/2021/03/19/xbwcj-de-shi-bai-shi-he-ccr-de-guang-hui-sui-yue/"/>
      <url>/2021/03/19/xbwcj-de-shi-bai-shi-he-ccr-de-guang-hui-sui-yue/</url>
      
        <content type="html"><![CDATA[<h1>XBW</h1><h2 id="分享一句话"><a class="header-anchor" href="#分享一句话">¶</a>分享一句话</h2><p><font face="微软雅黑" size="4">你我都是内卷背景下不起眼的尘埃，被时代的洪流裹挟向前。</font></p><h2 id="个人失败历史"><a class="header-anchor" href="#个人失败历史">¶</a>个人失败历史</h2><p>入学以来大一参加计算机设计大赛失败。😟<br>入学以后第一场考试以及后面无数考试失利。😩<br>大一参加校内数学建模新生杯惨败。😞<br>大一参加华中赛数学建模无功而返。😑<br>大一参加互联网+因特殊原因退出。😔<br>……<br>大二开始接触DataFountain竞赛，一直徘徊在复赛圈，与决赛无缘。<br>开始学习acm算法，在各种比赛中被吊打。<br>开始学习一些简单的开发，但是没有完整做出一款属于自己的开发的应用。<br>大二参加华为软件精英挑战赛，被数据漏洞坑害（参赛经验不足，获取比赛信息时效性低），赛区64强止步。<br>大二参加全国k-code程序设计大赛，止步21名，倒在决赛圈（前二十名）门口。<br>参加MathorCup成功参赛。<br>大二暑期建模培训最终选拔全校第七名，特殊原因被淘汰。<br>……<br>大三参加天池Redis数据库中间件挑战赛止步决赛。<br>……<br>还有很多自己以及以往的失败历史。</p><h2 id="未来准备参加的比赛"><a class="header-anchor" href="#未来准备参加的比赛">¶</a>未来准备参加的比赛</h2><p>美赛成绩未出<br>继续准备参加计算机设计大赛、华为软件精英挑战赛、等之类的其他比赛。</p><h2 id="xbwcj的数学建模比赛的准备"><a class="header-anchor" href="#xbwcj的数学建模比赛的准备">¶</a>xbwcj的数学建模比赛的准备</h2><p>1、提高自己的编程能力，编程能力的提升是在平时就要锻炼的，平时写代码的时候不要马虎应付，多看博客等其他网站上的一些常用算法与伪代码，最重要的是了解算法的思维，严谨的思维+超强的编程能力才能轻松应对数模中的编程。<br>2、强烈推荐去看司守奎的《数学建模算法与应用》这本书，学习上面的一些比较常见的算法，自己一定要手动实现一次，自己不手动实现，就很难理解里面的编程思维。<br>3、多去准备一些比较新的智能优化算法，推荐公众号：数学算法实验室、智能优化算法。尝试自己改进一些算法，将这些算法做成接口的形式，在使用时只需要传参就可以。准备一些可以做出精美图片的数据分析的软件，例如Oracle提供的Data Visualization Desktop的软件，也可以使用Office的PPT功能。熟练使用可以在比赛过程中省时省力。或者准备一些Matlab或者Python的作图代码，例如简单的条形图、折线图、雷达图等，或者高级点的词云之类的，也以接口的形式封装。<br>4、如何上手？一定一定要自己独自去完成一次数模的全部流程，了解每个步骤应该做什么如何去做。只有全部接触才能知道自己的优势以及劣势，了解自己应该找什么样的队友来取长补短。<br>5、暑期培训前的准备：2020年由于疫情的原因，选拔赛是以组队的形式直接做题来选拔。在暑期培训前无论是校赛还是校外的数模比赛中，都可以自己去找队友，因此最好在暑期选拔培训前就确定好自己的队友，尽早开始磨合，利用其他的数模比赛找到队伍的不足，这样可以在选拔赛中发挥的更好一些。校外的数模比赛建议参加：华中赛、泰迪杯、数维杯、深圳杯、亚太赛。（挑选其中几个即可）</p><h2 id="xbwcj的校外比赛经验"><a class="header-anchor" href="#xbwcj的校外比赛经验">¶</a>xbwcj的校外比赛经验</h2><p>推荐几个我经常浪荡的比赛网站：<br><a href="https://tianchi.aliyun.com/competition/gameList/activeList">天池</a>、<a href="https://competition.huaweicloud.com/competitions">华为云</a>、<br><a href="https://www.datafountain.cn/competitions">DataFountain</a>、<a href="https://www.kaggle.com/competitions">kaggle</a>、<br>这些比赛有奖金，并且你会学习到很多平时在课堂里面学不到的知识技能。刚开始得奖会比较难，因为这些比赛中参加的不只是本科生，一般是面向所有在校学生，而且硕士生所占比例不低，所以开始大家可以去涨经验，后面能到何种地步就看大家自己的发挥了。🐶🐶🐶<br>如果想要提高自己的编程能力：<br><a href="https://ac.nowcoder.com/acm/problem/list">牛客</a>、<a href="https://www.luogu.com.cn/problem/list">洛谷</a>、<a href="http://acm.hdu.edu.cn/">hduOJ</a>、<a href="https://codeforces.com/problemset">CodeForces</a>、<a href="https://nanti.jisuanke.com/acm">计蒜客</a>。<br>去刷里面的题目，如果不是专门去训练acm，不需要去做太难的，达到中等就可以应付绝大多数公司的绝大多数笔试题了。也可以帮助你了解比较重要的基础算法。</p><h2 id="xbwcj个人大学总结经验（其实是自己的不足与后悔之处）"><a class="header-anchor" href="#xbwcj个人大学总结经验（其实是自己的不足与后悔之处）">¶</a>xbwcj个人大学总结经验（其实是自己的不足与后悔之处）</h2><p>1、一定要尽早规划，自己以后是找工作还是要读研，读研的话，是准备考研，还是A、B保，如果保研自己还欠缺什么，尽早做打算。<br>2、精心，静心去做一件事，贪多嚼不烂。<br>3、无论做什么，基础一定要扎实。</p><h1>Terence</h1><h2 id="最近做的事"><a class="header-anchor" href="#最近做的事">¶</a>最近做的事</h2><ul><li><p>和好朋友一起参加  <code>DataWhale</code> 三月份的组队学习（推荐系统和区块链方面）</p></li><li><p>学习英语和编程</p></li><li><p>准备了解一下多模态这个领域，也顺便考虑一下自己以后的研究方向，准备读研也不得不面对这个问题，以后要做什么？</p></li></ul><p>就以这个 <code>以后要做什么？</code> 题简单跟大家分享下</p><h2 id="以后要做什么？"><a class="header-anchor" href="#以后要做什么？">¶</a>以后要做什么？</h2><p>留给我们的选择其实不多，读研 or 就业</p><p>读研</p><ul><li>绩点要稳住（大数据专业绝大部分专业必修都在前两年）走 A 保</li><li>比赛拿奖项走 B 保</li></ul><p>科研</p><ul><li>理论研究</li><li>数据挖掘</li><li>机器学习</li><li>NLP（自然语言处理）</li><li>CV（计算机视觉）</li><li>多模态</li></ul><p>职业</p><ul><li><p>数据科学家</p></li><li><p>数据分析师</p></li><li><p>算法工程师</p></li></ul><h2 id="参加的比赛"><a class="header-anchor" href="#参加的比赛">¶</a>参加的比赛</h2><ul><li>华中地区大学生数学建模邀请赛省赛</li><li>华为大学生ICT大赛湖北区域省赛</li><li>全国大学生数学建模省赛</li><li>美国大学生数学建模大赛国赛</li><li>全国大学生节能减排国赛</li><li>中国大学生计算机设计大赛国赛</li><li>中国大学生服务外包创新创业大赛国赛</li><li>Kaggle、天池、腾讯广告…</li></ul><h2 id="数学建模"><a class="header-anchor" href="#数学建模">¶</a>数学建模</h2><p>讲下数学建模，其实刚才雯雯也说了很多，也非常详细，跟我观点也蛮吻合的。有一个观点，就是在咱们学校成功进入校队，参加国赛和美赛，不是一件容易的事情。确实它收益很大，每一年都有很多同学通过数学建模成功保研。并不是很推荐大家去很功利地做一件事情，数模带给我们的不仅仅是一个保研的资格或者说门票，更多的是带来一种能力。</p><ul><li>分析问题，将实际问题建立数学模型的能力</li><li>解决问题，运用自己所学去求解模型的能力</li><li>撰写科技论文的能力</li><li>团队配合</li></ul><p>2019年数学建模选拔形式（具体细节可以在教务处官网查询）</p><ul><li><p>编程：开卷、不限语言（主要是 Matlab ）对 8 个题目进行求解，求解问题大多是线性方程、0-1背包、最优化、智能算法以及开放题。最终按成绩排名。</p></li><li><p>建模：这个我就不太清楚了</p></li></ul><h2 id="其他比赛"><a class="header-anchor" href="#其他比赛">¶</a>其他比赛</h2><ul><li><p>ACM</p></li><li><p><a href="http://i.whut.edu.cn/xxtg/znbm/jwc/202103/t20210317_484335.shtml">【教务处】关于组织参加 “2021 年（第14届）中国大学生计算机设计大赛”的通知</a></p></li><li><p><a href="http://i.whut.edu.cn/xxtg/znbm/jwc/202103/t20210304_483136.shtml">【教务处】关于组织参加第十二届中国大学生服务外包创新创业大赛的通知</a></p></li><li><p><a href="http://i.whut.edu.cn/xxtg/znbm/jwc/202103/t20210304_483133.shtml">【教务处】关于举办“2021年中国高校计算机大赛-团体程序设计天梯赛”校内选拔赛的通知</a></p></li></ul><p>怎么说，不断去接触去尝试才会有更多可能。你说比赛它难不难，要想拿到好名次确实很难，没有一帆风顺的路，当你觉得自己真的真的真的快撑不下去的时候，那就别撑了，我们又不是把伞，老撑着干嘛:happy:</p><h2 id="小结"><a class="header-anchor" href="#小结">¶</a>小结</h2><p>找到自己喜欢的事情，然后坚持下去就好了。如果有一件事（限学业方面），能够让你忘记时间地投入进去，那就是足够热爱的事情啦！</p>]]></content>
      
      
      <categories>
          
          <category> 个人分享 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 个人分享 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据结构--线性表</title>
      <link href="/2021/03/08/shu-ju-jie-gou-xian-xing-biao/"/>
      <url>/2021/03/08/shu-ju-jie-gou-xian-xing-biao/</url>
      
        <content type="html"><![CDATA[<h2 id="什么是线性表？"><a class="header-anchor" href="#什么是线性表？">¶</a>什么是线性表？</h2><p>线性表（linear_list）是最常用且最简单的一种数据结构，一个线性表相当于是n个数据元素的有限序列，其中每个数据元素表示的内容在不同的情况下并不相同，有可能是一个数字、一个符号之类的。若将线性表用集合表示为:<br>$$<br>(a_{1}, …, a_{i-1}, a_{i}, a_{i+1}, …, a_{n})<br>$$</p><p>由集合中可以看出，除第一个外，集合中的每个数据元素均只有一个前驱，除最后一个外，集合中的每个数据元素均只有一个后继。</p><h2 id="线性表的两种表示形式"><a class="header-anchor" href="#线性表的两种表示形式">¶</a>线性表的两种表示形式</h2><p>（1）顺序表示：用一组地址连续的存储单元依次存储线性表的数据元素，称为线性表的顺序存储结构，可随机存取表中任一元素（其实就是数组）<br>（2）链式表示：用一组任意的存储单元存储线性表中的数据元素，称为线性表的链式存储结构。它的存储单元可以是连续的，也可以是不连续的。在表示数据元素之间的逻辑关系时，除了存储其本身的信息之外，还需存储一个指示其直接后继的信息（即直接后继的存储位置），这两部分信息组成数据元素的存储映像，称为结点（node）。它包括两个域；存储数据元素信息的域称为数据域；存储直接后继存储位置的域称为指针域。指针域中存储的信息称为指针或链。<br>可以用下图来更加直观的表示顺序结构和链式结构</p><h2 id="线性表的一般定义"><a class="header-anchor" href="#线性表的一般定义">¶</a>线性表的一般定义</h2><p>线性表的抽象数据类型定义如下：<br>数据对象：$$D , = , {a_{i} , | , a_{i} , \in , ElemSet, ,i = 1,2,\cdots,n, , n \geq 0}$$<br>数据关系：$$R1 , = , {&lt; a_{i-1},a_{i} &gt; |a_{i-1},a{i} \in D,,i=2,\cdots,n}$$</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">ADT List &#123;    &#x2F;&#x2F;基本操作    InitList(&amp;L) &#x2F;&#x2F;构造空线性表L    DestoryList(&amp;L) &#x2F;&#x2F;销毁线性表L    ClearList(&amp;L) &#x2F;&#x2F;将线性表L变为空    ListEmpty(L) &#x2F;&#x2F;若线性表L为空表，返回TRUE，否则返回FASLSE    ListLength(L) &#x2F;&#x2F;返回线性表中的数据元素个数    GetElem(L, i, &amp;e) &#x2F;&#x2F;用e返回L中第i个数据元素的值    PriorElem(L, cur_e, &amp;pre_e) &#x2F;&#x2F; 如果cur_e是线性表中的元素，而且不是第一个，那么我们就可以返回该元素前一个元素的值    NextElem(L, cur_e, &amp;next_e) &#x2F;&#x2F; &#x2F;&#x2F;如果cur_e是线性表中的元素，而且不是最后一个，就返回它下一个元素的值    Listinsert(&amp;L, i, e)&#x2F;&#x2F;如果线性表存在了，而且i符合条件，则在i位置插入一个元素    ListDelete(&amp;L, i)&#x2F;&#x2F;删除i位置上的元素    ListDelete_data(&amp;L, e, order)&#x2F;&#x2F;删除指定的元素e，order决定是删除一个，还是全部。    Connect_two_List(L_a,L_b,&amp; L_c)&#x2F;&#x2F;连接两个线性表，除去重复的内容    print(L)&#x2F;&#x2F;打印线性表    &#x2F;*        此处省略部分其他常见的操作    *&#x2F;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在上述的函数中需要注意，如果要改变原本线性表的内容，则需要传入线性表的地址，如果只是对线性表进行查询等不改变原本的线性表内容的操作，则不需要以地址的形式传入。（这部分其实就是C/C++的对传入函数参数值改变的原理，在此做一个小的提醒）。</p><h2 id="顺序存储结构常见的结构体实现"><a class="header-anchor" href="#顺序存储结构常见的结构体实现">¶</a>顺序存储结构常见的结构体实现</h2><p>我们通过线性表顺序结构来实现线性表的一些基础操作。</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">#include&lt;bits&#x2F;stdc++.h&gt;#define SIZE 100#define SIZE_T 150using namespace std;typedef int ElemType;struct List &#123;    ElemType *data; &#x2F;&#x2F;数据    int length; &#x2F;&#x2F;长度    int size; &#x2F;&#x2F;线性表初始容量&#125;;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="顺序存储结构InitList-函数的实现"><a class="header-anchor" href="#顺序存储结构InitList-函数的实现">¶</a>顺序存储结构InitList()函数的实现</h2><p>线性表的顺序表示可以看作对数组进行一些操作，初始化函数就是将表的长度变为0，对data进行堆内存的分配和初始容量的初始化。</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">&#x2F;&#x2F;创建一个空的线性表void InitList(List &amp;newList) &#123;    &#x2F;&#x2F;初始容量为startsize    newList.size &#x3D; SIZE_T;    &#x2F;&#x2F;首先开辟空间    newList.data &#x3D; (int *)malloc(SIZE * sizeof(ElemType));    if(!newList.data)    &#123;        exit(OVERFLOW); &#x2F;&#x2F; 存储分配失败    &#125;    &#x2F;&#x2F;空表，长度是0    newList.length &#x3D; 0;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="顺序存储结构DestoryList-函数的实现"><a class="header-anchor" href="#顺序存储结构DestoryList-函数的实现">¶</a>顺序存储结构DestoryList()函数的实现</h2><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">&#x2F;&#x2F;前提是线性表已经存在void Destory (List &amp;newList)&#123;if (newList &#x3D;&#x3D; NULL)    &#123;        exit(OVERFLOW);&#x2F;&#x2F; 线性表不存在    &#125;    &#x2F;&#x2F;首先释放堆内存free(newList.data);    &#x2F;&#x2F;每次释放堆内存后，应将对应的指针指向NULL，这是一个比较好的编程习惯    newList.data &#x3D; NULL;    newList.length &#x3D; 0;    newList.size &#x3D; 0;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="顺序存储结构ClearList-函数的实现"><a class="header-anchor" href="#顺序存储结构ClearList-函数的实现">¶</a>顺序存储结构ClearList()函数的实现</h2><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">&#x2F;&#x2F;前提是线性表已经存在void ClearList(List &amp;newList) &#123;    if (newList &#x3D;&#x3D; NULL)    &#123;        exit(OVERFLOW);&#x2F;&#x2F; 线性表不存在    &#125;    newList.length &#x3D; 0;free(newList.data);    newList.data &#x3D; NULL;    &#x2F;&#x2F;重新开辟空间    newList.data &#x3D; (int *)malloc(SIZE * sizeof(ElemType));&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="顺序存储结构ListEmpty-函数的实现"><a class="header-anchor" href="#顺序存储结构ListEmpty-函数的实现">¶</a>顺序存储结构ListEmpty()函数的实现</h2><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">&#x2F;&#x2F; 判断线性表是否为空bool ListEmpty(List newList) &#123;    if(newList.length &#x3D;&#x3D; 0)    &#123;        return 1;    &#125;    else    &#123;        return 0;    &#125;    return 0;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="顺序存储结构ListLength-函数的实现"><a class="header-anchor" href="#顺序存储结构ListLength-函数的实现">¶</a>顺序存储结构ListLength()函数的实现</h2><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">&#x2F;&#x2F; 返回线性表的长度int ListEmpty(List newList) &#123;       return newList.length;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> 课本学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据结构 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2021/02/13/hello-world/"/>
      <url>/2021/02/13/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a class="header-anchor" href="#Quick-Start">¶</a>Quick Start</h2><h3 id="Create-a-new-post"><a class="header-anchor" href="#Create-a-new-post">¶</a>Create a new post</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo new &quot;My New Post&quot;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a class="header-anchor" href="#Run-server">¶</a>Run server</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo server<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a class="header-anchor" href="#Generate-static-files">¶</a>Generate static files</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo generate<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a class="header-anchor" href="#Deploy-to-remote-sites">¶</a>Deploy to remote sites</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo deploy<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      <categories>
          
          <category> hexo </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo blog </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>BAS算法</title>
      <link href="/2021/01/29/bas-suan-fa/"/>
      <url>/2021/01/29/bas-suan-fa/</url>
      
        <content type="html"><![CDATA[<h2 id="天牛须搜索-Beetle-Antennae-Search-BAS"><a class="header-anchor" href="#天牛须搜索-Beetle-Antennae-Search-BAS">¶</a>天牛须搜索(Beetle Antennae Search-BAS)</h2><p>又称甲壳虫须搜索，类似于遗传算法、粒子群算法、模拟退火等智能优化算法，天牛须搜索不需要知道函数的具体形式，不需要梯度信息，就可以实现高效寻优。<br><strong>优点</strong>：相比于粒子群算法，天牛须搜索只需要一个个体，即一只天牛，运算量大大降低。</p><h2 id="仿生学原理"><a class="header-anchor" href="#仿生学原理">¶</a>仿生学原理</h2><p>天牛须搜索算法模仿自然界中天牛觅食行为。在天牛觅食过程中，其并不知道食物在哪里，但食物会产生特殊气味，吸引天牛向着食物前进。天牛通过其两只触角对空气中的食物气味进行感知，且根据食物距离两只触角的距离远近不同，两只触角所感知的气味浓度也有所差异。当食物处于天牛左侧时，左侧触角感知的气味浓度强于右侧触角感知的气味浓度，天牛根据两只触角所感知的浓度差，向着浓度强的一侧随机前进。通过一次次迭代，最终找到食物的位置。</p><h2 id="行为启发"><a class="header-anchor" href="#行为启发">¶</a>行为启发</h2><p>食物的气味就相当于一个函数,这个函数在三维空间每个点值都不同,天牛两个须可以采集自身附近两点的气味值,天牛的目的是找到仝局气味值最大的点仿照天牛的行为,我们就可以高效的进行函数寻优。</p><h2 id="算法模型"><a class="header-anchor" href="#算法模型">¶</a>算法模型</h2><p>BAS算法主要是通过在不停的左右触角气味浓度比对中前进，同其他算法相比，原理十分简单。<br>在进行两只触角气味浓度计算之前，需要对其进行一系列准备工作，在$D$维空间中天牛的位置为$X = (x_{1}, x_{2}, … , x_{n})$,天牛左右两只触角的位置被定义为如下公式所示模型：</p><p>$$<br>\left{ \begin{array}{l}<br>{X_r} = X + l * \mathop d\limits^ \to  \<br>{X_l} = X - l * \mathop d\limits^ \to<br>\end{array} \right.<br>$$</p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 智能优化算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>前缀和、二维前缀和与差分个人理解</title>
      <link href="/2020/05/28/qian-zhui-he-er-wei-qian-zhui-he-yu-chai-fen-ge-ren-li-jie/"/>
      <url>/2020/05/28/qian-zhui-he-er-wei-qian-zhui-he-yu-chai-fen-ge-ren-li-jie/</url>
      
        <content type="html"><![CDATA[<h1>前缀和</h1><h2 id="什么是前缀和"><a class="header-anchor" href="#什么是前缀和">¶</a>什么是前缀和</h2><p>  前缀和顾名思义就是指一个数组的某一个下标的（包括该下标）之前的所有数组元素的和。现在我们假设有某一数组a = [1, 2, 3, 4, 5, 6, 7, 8, 9]。其前缀和数组为sum，那么sum数组与a数组对应的关系如下图所示。<br><img src="/2020/05/28/qian-zhui-he-er-wei-qian-zhui-he-yu-chai-fen-ge-ren-li-jie/1.png" alt="在这里插入图片描述"><br>  由上面的对应关系我们可以得到他们满足如下的公式。</p><p><img src="/2020/05/28/qian-zhui-he-er-wei-qian-zhui-he-yu-chai-fen-ge-ren-li-jie/2.png" alt="在这里插入图片描述"><br>  以上的公式即为<strong>一维前缀和</strong>一维前缀和的代码模板如下所示。</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">&#x2F;**     * 一维前缀和     *     * @a 表示原数组     * @sum 表示a数组的一维前缀和     *&#x2F;     const int maxn &#x3D; 1e5 + 10;     int a[9] &#x3D; &#123;1, 2, 3, 4, 5, 6, 7, 8, 9&#125;;      int sum[maxn];    void oneDimen(int num) &#123;&#x2F;&#x2F;num表示数组a的长度        sum[0] &#x3D; a[0];        for (int i &#x3D; 1; i &lt; num; i++) &#123;            sum[i] &#x3D; sum[i - 1] + a[i];        &#125;    &#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="前缀和使用情况"><a class="header-anchor" href="#前缀和使用情况">¶</a>前缀和使用情况</h2><p>  我们在做题的时候经常会遇到查询问题，例如给出一个数组a，再给出m次查询，每次查询都会给出两个数L，R，分表表示查询区间的左右范围。如果我们只是使用最简单的朴素查询的方法，每次遍历区间，进行m次的查询，这样在题目所给数据范围较小的情况下可以进行，但是当查询次数很大时，其时间复杂度为<strong>O（n*m）<strong>会使运行TLE，所以我们使用上述的前缀和可以使时间复杂度降低为</strong>O(m+n)</strong></p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">int query(int L, int R)&#123;retrun sum[R] - sum[L - 1];&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h1>差分</h1><h2 id="什么是差分"><a class="header-anchor" href="#什么是差分">¶</a>什么是差分</h2><p>  差分就是指相邻两个数的差，我们假设存在一个数组，如下图所示<br><img src="/2020/05/28/qian-zhui-he-er-wei-qian-zhui-he-yu-chai-fen-ge-ren-li-jie/3.png" alt="在这里插入图片描述"><br>  具体代码模板如下：</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">const int maxn &#x3D; 1e5 + 10;int a[9] &#x3D; &#123;1, 2, 3, 4, 5, 6, 7, 8, 9&#125;; int diff[maxn];&#x2F;&#x2F;求出差分数组void chafen(int num)&#123;&#x2F;&#x2F;num表示原数组的长度diff[0] &#x3D; a[0];for(int i &#x3D; 1; i &lt; num; i++)&#123;diff[i] &#x3D; a[i] - a[i - 1];&#125;&#125;&#x2F;&#x2F;对区间进行加操作void addarray(int L, int R, int k)&#123;&#x2F;&#x2F;L和R分别代表对加区间的左右范围，k表示在区间里每个元素加的数字diff[L] +&#x3D; k;diff[R + 1] -&#x3D; k;&#x2F;&#x2F;这里特别要注意，因为在前面进行区间加后，后面一个数与前面这个数的差变小了，所以要在后面这个数的差分数组减去前面区间所增加的数字。&#125;&#x2F;&#x2F;通过差分数组和原数组a推理得到进行区间加后数组中某一个元素的值void get_a()&#123;for(int i &#x3D; 1; i &lt;&#x3D; n; i++)&#123;a[i] &#x3D; doff[i] + a[i - 1];&#125; &#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="差分使用情况"><a class="header-anchor" href="#差分使用情况">¶</a>差分使用情况</h2><p>  区间加：把数组a[l]到a[r]都加上k，这种操作称为区间加。在进行区间加的操作后得到的数组b，我们对数组b进行查询，但可以发现如果是L——-R非常大的情况下，通过朴素的区间范围内主次累加求和这个操作执行的次数又很多，那时间复杂度会很高。所以可以使用差分的思想来降低复杂度。</p><h1>二维前缀和</h1><h2 id="什么是二维前缀和"><a class="header-anchor" href="#什么是二维前缀和">¶</a>什么是二维前缀和</h2><p><img src="/2020/05/28/qian-zhui-he-er-wei-qian-zhui-he-yu-chai-fen-ge-ren-li-jie/4.png" alt="在这里插入图片描述"><br>  在上图中深蓝色的部分代表的是二维数组的索引，浅蓝色的部分代表的是二维数组的每个元素的值。其二维前缀和如下图所示<br><img src="/2020/05/28/qian-zhui-he-er-wei-qian-zhui-he-yu-chai-fen-ge-ren-li-jie/5.png" alt="在这里插入图片描述"><br>  前缀和数组里每一个位置都表示原数组当前索引左上方的数字的和。<br>如上表中的而为前缀和数组：prefixSum[3, 3] = src[0~2, 0~2]的和;<br>二维前缀和数组的计算步骤如下所示。<br>可以分为四种情况</p><ol><li>i == 0 &amp;&amp; j ==0，只有一个直接赋值即可：prefixSum[0, 0] = a[0, 0]。</li><li>i == 0，最左边的一列，二维前缀和为元素上一行相同列的元素加该数字，公式为prefixSum[0, j] = prefixSum[0, j-1] + a[0, j]；</li><li>j == 0，最上面一排，与i == 0类似prefixSum[i, o] = prefixSum[i-1, 0] + a[i, 0];</li><li>i!=0 || j!=0，其公式为 prefixSum[i][j] = prefixSum[i - 1][j] + prefixSum[i][j - 1] + a[i][j] - prefixSum[i - 1][j - 1];<br>其代码模板如下所示</li></ol><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">&#x2F;** * 二维前缀和 * * @param src 原数组 * @return 二维前缀和 *&#x2F; const int maxn &#x3D; 100; int prefixSum[maxn][maxn];void twoDimen(int a[][], int n, int m) &#123;&#x2F;&#x2F;n和m分别代表二维原始数组的行列长度    for (int i &#x3D; 0; i &lt; n; i++) &#123;        for (int j &#x3D; 0; j &lt; m; j++) &#123;            if (i &#x3D;&#x3D; 0 &amp;&amp; j &#x3D;&#x3D; 0) &#123;&#x2F;&#x2F;第0个，最左上角                prefixSum[i][j] &#x3D; a[i][j];            &#125; else if (i &#x3D;&#x3D; 0) &#123;&#x2F;&#x2F;第一行，最顶部一行                prefixSum[i][j] &#x3D; prefixSum[i][j - 1] + a[i][j];            &#125; else if (j &#x3D;&#x3D; 0) &#123;&#x2F;&#x2F;第一列，最左边一列                prefixSum[i][j] &#x3D; prefixSum[i - 1][j] + a[i][j];            &#125; else &#123;&#x2F;&#x2F;其他                prefixSum[i][j] &#x3D; prefixSum[i - 1][j] + prefixSum[i][j - 1] + a[i][j] - prefixSum[i - 1][j - 1];            &#125;        &#125;    &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="二维前缀和的使用情况"><a class="header-anchor" href="#二维前缀和的使用情况">¶</a>二维前缀和的使用情况</h2><p>  一般使用二维前缀和可以求子矩阵的最大值。通过求解出整个矩阵的二维前缀和数组，然后对二位前缀和数组中的元素进行查询，找到其和最大的子矩阵。</p><h2 id="二维前缀和的差分"><a class="header-anchor" href="#二维前缀和的差分">¶</a>二维前缀和的差分</h2><p>  二维前缀和也可以使用差分的形式。方法是和一维类似的，我们也是需要另开一个数组记录修改操作，最后求前缀和时统计修改操作，只是二维每一次操作需要记录4个位置，一维只需要记录2个位置。具体模板代码如下所示。</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">void chafen()&#123;for(int i&#x3D;0;i&lt;m;i++)&#123;&#x2F;&#x2F;m是修改操作次数 int x1,y1,x2,y2,p;cin&gt;&gt;x1&gt;&gt;y1&gt;&gt;x2&gt;&gt;y2&gt;&gt;p;b[x1][y1]+&#x3D;p;b[x2+1][y2+1]+&#x3D;p;b[x2+1][y1]-&#x3D;p;b[x1][y2+1]-&#x3D;p;&#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>  以上部分来自个人理解以及从其他大佬的博客中领悟到的，有些内容可能与其他大佬相似，如有侵权，请及时指出，立马进行修正。有写的不好地方也请及时指出，本人菜鸡，勿喷。</p><p><font face="微软雅黑" size="4">你我都是内卷背景下不起眼的尘埃，被时代的洪流裹挟向前。</font></p>]]></content>
      
      
      <categories>
          
          <category> ACM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ACM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>线段树个人理解</title>
      <link href="/2020/05/27/xian-duan-shu-ge-ren-li-jie/"/>
      <url>/2020/05/27/xian-duan-shu-ge-ren-li-jie/</url>
      
        <content type="html"><![CDATA[<h1>线段树</h1><p><img src="/2020/05/27/xian-duan-shu-ge-ren-li-jie/1.png" alt="在这里插入图片描述"></p><h2 id="定义："><a class="header-anchor" href="#定义：">¶</a>定义：</h2><p>  线段树是一种二叉搜索树，与区间树相似，它将一个区间划分成一些单元区间，每个单元区间对应线段树中的一个叶结点。使用线段树可以快速的查找某一个节点在若干条线段中出现的次数，时间复杂度为O(logN)。而未优化的空间复杂度为2N，实际应用时一般还要开4N的数组以免越界，因此有时需要离散化让空间压缩。对于线段树中的每一个非叶子节点[a,b]，它的左儿子表示的区间为[a,(a+b)/2]，右儿子表示的区间为[(a+b)/2+1,b]。因此线段树是平衡二叉树，最后的子节点数目为N，即整个线段区间的长度。</p><h2 id="作用范围："><a class="header-anchor" href="#作用范围：">¶</a>作用范围：</h2><p>  线段树的适用范围很广，可以在线维护修改以及查询区间上的最值，求和。更可以扩充到二维线段树（矩阵树）和三维线段树（空间树）。对于一维线段树来说，每次更新以及查询的时间复杂度为O(logN)。还支持区间修改，单点修改等操作。</p><h2 id="实现原理："><a class="header-anchor" href="#实现原理：">¶</a>实现原理：</h2><p>  线段树主要是把一段大区间平均地划分成两段小区间进行维护，再用小区间的值来更新大区间。这样既能保证正确性，又能使时间保持在log级别（因为这棵线段树是平衡的）。也就是说，一个[L…R]的区间会被划分成[L…(L+R)/2]和[(L+R)/2+1…R]这两个小区间进行维护，直到L=R。但是在上述的过程中我们会遇到以下几个问题，就是我们该如何建树，建树的过程中每一个下标我们该如何去分配，分派到的每一个空间我们应该用来存放哪些数据。</p><p><img src="/2020/05/27/xian-duan-shu-ge-ren-li-jie/2.png" alt="在这幅图片中"><br>  在这里我们仅对线段树中常见的区间最大值问题进行解释讨论。假设所给的区间为F[1:6] = {1, 9, 7, 8, 2, 3}。那么其对应的线段树的结构就如上图所示。其中红色的圆圈就代表线段树对应的每一个结点的下标。蓝色方框中的Max就是我们每一个结点所存放的内容，即每一个区间存放的最大值。Max下面的内容是对这个区间范围的一个说明，并不需要存放在数组中。<br>  仔细看这幅图我们会发现，其中结点的下标并不连续（在图中结点的标号并没有10，11）。这是因为我们在用数组对线段树进行模拟的时候，必须要提前对整个树的空间进行提前的开辟，所开辟的空间虽然并没有使用到，但是其仍然真是存在，这也是为什么我们在对数组进行开辟空间时一般会选择4<em>n的大小以避免出现RE。<br>  通过观察上面的线段树结点标号我们可以发现，对于一个区间[L,R]来说，最重要的数据当然就是区间的左右端点L和R，但是大部分的情况我们并不会去存储这两个数值，而是通过递归的传参方式进行传递。这种方式用指针好实现，定义两个左右子树递归即可，按时指针表示过于繁琐，而且不方便各种操作，大部分的线段树都是使用数组进行表示，那这里怎么快速使用下标找到左右子树呢。这就会涉及到每个结点下表数字的规律。我们发现在线段树中每个非叶子结点的度都为2，且父亲节点的左右两个孩子分别存储父亲一半的区间，而每个父亲结点存放的欧式孩子的最大值，而且左孩子的下标都为偶数，右孩子的下标都是奇数且左孩子下标数+1，即：<br><strong><em><em>L = Father</em>2 （左子树下标为父亲下标的两倍）<br>R = Father</em>2+1（右子树下标为父亲下标的两倍+1）</strong></em>*<br>或<br><strong>k&lt;&lt;1（结点k的左子树下标）<br>k&lt;&lt;1|1（结点k的右子树下标）</strong><br>所以建树的操作可用如下代码实现</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">const int maxn &#x3D; 1e5+5;const int INF &#x3D; 0x3f3f3f3f;int tree[maxn&lt;&lt;2],temp[maxn];&#x2F;&#x2F;tree[]数组表示线段树数组，temp[]表示存放原始数据的数组void Build(int l,int r,int rt)&#123; &#x2F;&#x2F;l,r表示当前节点区间，rt表示当前节点编号      if(l&#x3D;&#x3D;r) &#123;&#x2F;&#x2F;若到达叶节点，即区间的左右值相等           tree[rt]&#x3D;temp[l];&#x2F;&#x2F;储存数组值           return;      &#125;      int mid &#x3D; (l+r)&gt;&gt;1;  &#x2F;&#x2F;mid表示中间点    &#x2F;&#x2F;左右递归       Build(l,mid,rt&lt;&lt;1);      Build(mid+1,r,rt&lt;&lt;1|1);      tree[rt] &#x3D; max(tree[rt&lt;&lt;1],tree[rt&lt;&lt;1|1];&#x2F;&#x2F;更新信息&#125;  <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="线段树的基本操作："><a class="header-anchor" href="#线段树的基本操作：">¶</a>线段树的基本操作：</h2><h3 id="一、点更新"><a class="header-anchor" href="#一、点更新">¶</a>一、点更新</h3><p><img src="/2020/05/27/xian-duan-shu-ge-ren-li-jie/3.png" alt="在这里插入图片描述"><br>  假设我们将上述的区间F[1:6] = {1, 9, 7, 8, 2, 3}中的F[3] = 7通过对其+3更改为10。那么我们应当对线段树进行如下的几个操作。</p><ol><li>我们通过线段树的根结点向下遍历，通过叶子结点所在的区间进行查询，在每一处根结点与我们改变的值相比较，如果F[3] = 10大于当前根结点中存储的Max值，那么将Max = 10，否则不变且继续向下遍历。</li><li>直至到L=R时，即为我们改变的叶子结点，将其中存储的值变为我们上述的F[3] = 10，退出。<br>  具体代码实现如下：</li></ol><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">&#x2F;&#x2F;递归方式更新 updata(p,v,1,n,1);void updata(int p,int v,int l,int r,int rt)&#123;    &#x2F;&#x2F;p为下标，v为要加上的值，l，r为结点区间，rt为结点下标    if(l &#x3D;&#x3D; r)&#123;    &#x2F;&#x2F;左端点等于右端点，即为叶子结点，直接加上v即可        temp[rt] +&#x3D; v;        tree[rt] +&#x3D; v;    &#x2F;&#x2F;原数组和线段树数组都得到更新        return ;    &#125;    int m &#x3D; l + ((r-l)&gt;&gt;1);    &#x2F;&#x2F;m则为中间点，左儿子的结点区间为[l,m],右儿子的结点区间为[m+1,r]    if(p &lt;&#x3D; m)    &#x2F;&#x2F;如果需要更新的结点在左子树区间        updata(p,v,l,m,rt&lt;&lt;1);    else    &#x2F;&#x2F;如果需要更新的结点在右子树区间        updata(p,v,m+1,r,rt&lt;&lt;1|1);    tree[rt] &#x3D; max(tree[rt&lt;&lt;1],tree[rt&lt;&lt;1|1];    &#x2F;&#x2F;更新父节点的值&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="二、区间查询"><a class="header-anchor" href="#二、区间查询">¶</a>二、区间查询</h3><p>  线段树的每个结点存储的都是一段区间的信息 ，这就意味着如果我们刚好要查询这个区间，那么则直接返回这个结点的信息即可，比如对于上面线段树，如果我直接查询[1,6]这个区间的最值，那么直接返回根节点信息10即可，查询[1,2]直接返回9。但是有时题目中为了设置难度并不会轻易让我们查询每个结点所表示的区间。比如现在我要查询[2,5]区间的最值，这时候我们会发现并不存在某个节点的区间是[2,5]，那么这时我们应该采取一些什么方法来进行区间信息的查询呢？<br><img src="/2020/05/27/xian-duan-shu-ge-ren-li-jie/4.png" alt="在这里插入图片描述"></p><ol><li>首先我们发现区间[2,5]在线段树中包括的节点有[2,2]，[3,3]，[4,4]，[5,5]，[4,5]。但是[4,4]，[5,5]这两个信息的区间已经被[4,5]区间所包含，所以我们真正需要查询的结点为[2,2]，[3,3]，[4,5]这三个区间所在的结点。</li><li>其次从根节点开始往下递归，如果当前结点是要查询的区间的真子集，则返回这个结点的信息且不需要再往下递归。<br>  具体代码如下</li></ol><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">&#x2F;&#x2F;递归方式区间查询 query(Ld,Rd,1,n,1);int query(int Ld,int Rd,int l,int r,int rt)&#123;    &#x2F;&#x2F;[Ld,Rd]即为要查询的区间，l，r为结点区间，rt为结点下标    if(Ld &lt;&#x3D; l &amp;&amp; r &lt;&#x3D; Rd)    &#x2F;&#x2F;如果当前结点的区间真包含于要查询的区间内，则返回结点信息且不需要往下递归        return tree[rt];    int ans &#x3D; -INF;    &#x2F;&#x2F;返回值变量，根据具体线段树查询的什么而自定义    int mid &#x3D; (l+r)&gt;&gt;1;    &#x2F;&#x2F;m则为中间点，左儿子的结点区间为[l,m],右儿子的结点区间为[m+1,r]    if(Ld &lt;&#x3D; m)    &#x2F;&#x2F;如果左子树和需要查询的区间交集非空        ans &#x3D; max(ans, query(L,R,l,m,k&lt;&lt;1));    if(Rd &gt; m)    &#x2F;&#x2F;如果右子树和需要查询的区间交集非空，注意这里不是else if，因为查询区间可能同时和左右区间都有交集        ans &#x3D; max(ans, query(L,R,m+1,r,k&lt;&lt;1|1));    return ans;    &#x2F;&#x2F;返回当前结点得到的信息    &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="三、区间更新"><a class="header-anchor" href="#三、区间更新">¶</a>三、区间更新</h3><p>  在线段树的区间更新中我们引进了一个新的思想，Lazy_tag，字面意思就是懒惰标记的意思，实际上它的功能也就是偷懒= =，因为对于一个区间[L,R]来说，我们每次都更新区间中的每一个值，那样的话更新的复杂度将会是O(NlogN)，这太高了，所以引进了Lazy_tag，这个标记一般用于处理线段树的区间更新。<br>　　线段树在进行区间更新的时候，为了提高更新的效率，所以每次更新只更新到更新区间完全覆盖线段树结点区间为止，这样就会导致被更新结点的子孙结点的区间得不到需要更新的信息，所以在被更新结点上打上一个标记，称为lazy-tag，等到下次访问这个结点的子结点时再将这个标记传递给子结点，所以也可以叫延迟标记。<br>　　也就是说递归更新的过程，更新到结点区间为需要更新的区间的真子集不再往下更新，下次若是遇到需要用这下面的结点的信息，再去更新这些结点，所以这样的话使得区间更新的操作和区间查询类似，复杂度为O(logN)。</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">void Pushdown(int rt)&#123;    &#x2F;&#x2F;更新子树的lazy值，这里是RMQ的函数，要实现区间和等则需要修改函数内容    if(lazy[rt])&#123;    &#x2F;&#x2F;如果有lazy标记        lazy[rt&lt;&lt;1] +&#x3D; lazy[rt];    &#x2F;&#x2F;更新左子树的lazy值        lazy[rt&lt;&lt;1|1] +&#x3D; lazy[rt];    &#x2F;&#x2F;更新右子树的lazy值        t[rt&lt;&lt;1] +&#x3D; lazy[rt];        &#x2F;&#x2F;左子树的最值加上lazy值        t[rt&lt;&lt;1|1] +&#x3D; lazy[rt];    &#x2F;&#x2F;右子树的最值加上lazy值        lazy[rt] &#x3D; 0;    &#x2F;&#x2F;lazy值归0    &#125;&#125;&#x2F;&#x2F;递归更新区间 updata(L,R,v,1,n,1);void updata(int Ld,int Rd,int v,int l,int r,int rt)&#123;    &#x2F;&#x2F;[Ld,Rd]即为要更新的区间，l，r为结点区间，k为结点下标    if(Ld &lt;&#x3D; l &amp;&amp; r &lt;&#x3D; Rd)&#123;    &#x2F;&#x2F;如果当前结点的区间真包含于要更新的区间内        lazy[rt] +&#x3D; v;    &#x2F;&#x2F;懒惰标记        t[rt] +&#x3D; v;    &#x2F;&#x2F;最大值加上v之后，此区间的最大值也肯定是加v    &#125;    else&#123;        Pushdown(k);    &#x2F;&#x2F;重难点，查询lazy标记，更新子树        int m &#x3D; l + ((r-l)&gt;&gt;1);        if(Ld &lt;&#x3D; m)    &#x2F;&#x2F;如果左子树和需要更新的区间交集非空            update(Ld,Rd,v,l,m,rt&lt;&lt;1);        if(m &lt; Rd)    &#x2F;&#x2F;如果右子树和需要更新的区间交集非空            update(Ld,Rd,v,m+1,r,rt&lt;&lt;1|1);        Pushup(rt);    &#x2F;&#x2F;更新父节点    &#125;&#125;&#x2F;&#x2F;递归方式区间查询 query(Ld,Rd,1,n,1);int query(int Ld,int Rd,int l,int r,int rt)&#123;    &#x2F;&#x2F;[L,R]即为要查询的区间，l，r为结点区间，k为结点下标    if(Ld &lt;&#x3D; l &amp;&amp; r &lt;&#x3D; Rd)    &#x2F;&#x2F;如果当前结点的区间真包含于要查询的区间内，则返回结点信息且不需要往下递归        return t[rt];    else&#123;        Pushdown(rt);    &#x2F;**每次都需要更新子树的Lazy标记*&#x2F;        int res &#x3D; -INF;    &#x2F;&#x2F;返回值变量，根据具体线段树查询的什么而自定义        int mid &#x3D; l + ((r-l)&gt;&gt;1);    &#x2F;&#x2F;m则为中间点，左儿子的结点区间为[l,m],右儿子的结点区间为[m+1,r]        if(Ld &lt;&#x3D; m)    &#x2F;&#x2F;如果左子树和需要查询的区间交集非空            res &#x3D; max(res, query(Ld,Rd,l,m,rt&lt;&lt;1));        if(Rd &gt; m)    &#x2F;&#x2F;如果右子树和需要查询的区间交集非空，注意这里不是else if，因为查询区间可能同时和左右区间都有交集            res &#x3D; max(res, query(Ld,Rd,m+1,r,rt&lt;&lt;1|1));        return res;    &#x2F;&#x2F;返回当前结点得到的信息    &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><font face="微软雅黑" size="4">你我都是内卷背景下不起眼的尘埃，被时代的洪流裹挟向前。</font></p>]]></content>
      
      
      <categories>
          
          <category> ACM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ACM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>武汉理工大学《软件工程》复习总括三</title>
      <link href="/2019/11/05/wu-han-li-gong-da-xue-ruan-jian-gong-cheng-fu-xi-zong-gua-san/"/>
      <url>/2019/11/05/wu-han-li-gong-da-xue-ruan-jian-gong-cheng-fu-xi-zong-gua-san/</url>
      
        <content type="html"><![CDATA[<h1>第六章软件结构体系</h1><h2 id="软件结构体系的内容："><a class="header-anchor" href="#软件结构体系的内容：">¶</a><strong>软件结构体系的内容：</strong></h2><p><strong>1、构件</strong>：代表着一组基本的构成要素<br><strong>2、连接件</strong>：也就是构件之间的连接关系<br><strong>3、约束</strong>：是作用于构件或者连接关系上的一些限制条件<br><strong>4、质量</strong>：是系统的质量属性，如性能、可扩展性、可修改性、可重用性、安全性等。<br><strong>5、物理分布</strong>：代表着构件连接之后形成的拓扑结构，描述了软件到硬件之间的影射。</p><h2 id="软件结构体系发展的阶段："><a class="header-anchor" href="#软件结构体系发展的阶段：">¶</a>软件结构体系发展的阶段：</h2><p><img src="/2019/11/05/wu-han-li-gong-da-xue-ruan-jian-gong-cheng-fu-xi-zong-gua-san/1.png" alt="在这里插入图片描述"></p><h2 id="体系结构、软件框架、设计模式三者的联系和区别："><a class="header-anchor" href="#体系结构、软件框架、设计模式三者的联系和区别：">¶</a>体系结构、软件框架、设计模式三者的联系和区别：</h2><p><strong>体系结构</strong>：描述某一特定应用领域中系统组织的惯用模式，反映领域中众多系统所共有的结构和语义特性，例如：MVC<br><strong>软件框架</strong>：由开发人员定制的应用系统骨架，整个或部分系统的可重用设计，由一组抽象构件和构件实例之间的交互方式组成。例如Django就是一个开放源代码的应运框架，由Python写成。<br><strong>设计模式</strong>：描述软件系统设计过程中常见问题的一些解决方案，从大量的成功实践中总结出来的，且被广泛公认的实践和知识。<br><strong>软件框架和体系结构的区别及关系：</strong><br><img src="/2019/11/05/wu-han-li-gong-da-xue-ruan-jian-gong-cheng-fu-xi-zong-gua-san/2.png" alt="在这里插入图片描述"><br><strong>软件框架和实际模式的区别及关系：</strong><br><img src="/2019/11/05/wu-han-li-gong-da-xue-ruan-jian-gong-cheng-fu-xi-zong-gua-san/3.png" alt="在这里插入图片描述"></p><h2 id="软件工程问题中的关键的角色："><a class="header-anchor" href="#软件工程问题中的关键的角色：">¶</a>软件工程问题中的关键的角色：</h2><p><strong>用户</strong>：使用系统实现某种目标<br><strong>软件系统</strong>：待开发的系统<br><strong>环境</strong>：软件系统以外的任何事物</p><h2 id="软件设计的原则（高内聚低耦合）："><a class="header-anchor" href="#软件设计的原则（高内聚低耦合）：">¶</a>软件设计的原则（高内聚低耦合）：</h2><p><strong>内聚性</strong>：是一个模块或子系统内部的依赖程度。分为七种：功能内聚、信息内聚、通信内聚、过程内聚、时间内聚、逻辑内聚、巧合内聚。<br><img src="/2019/11/05/wu-han-li-gong-da-xue-ruan-jian-gong-cheng-fu-xi-zong-gua-san/4.png" alt="在这里插入图片描述"><br><strong>耦合性</strong>：是两个模块或者子系统之间依赖关系的强度，程序结构各个模块之间相互关联的度量。模块之间的联系方式一般有7中：非直接耦合、数据耦合、标记耦合、控制耦合、外部耦合、公共耦合、内容耦合。<br><img src="/2019/11/05/wu-han-li-gong-da-xue-ruan-jian-gong-cheng-fu-xi-zong-gua-san/5.png" alt="在这里插入图片描述"></p><h2 id="软件体系结构常见的风格："><a class="header-anchor" href="#软件体系结构常见的风格：">¶</a>软件体系结构常见的风格：</h2><p>可以根据文字描述判断出风格的种类即可。<br>**<img src="/2019/11/05/wu-han-li-gong-da-xue-ruan-jian-gong-cheng-fu-xi-zong-gua-san/6.png" alt="在这里插入图片描述"><br><strong>管道/过滤器风格</strong>：把系统任务分成若干连续的处理步骤，这些步骤由通过系统的数据流连接，一个步骤的输出是下一个步骤的输入。<br><strong><img src="/2019/11/05/wu-han-li-gong-da-xue-ruan-jian-gong-cheng-fu-xi-zong-gua-san/7.png" alt="在这里插入图片描述"><br>主程序—子程序风格</strong>：结构化程序设计的一种典型风格，从功能的观点设计系统，逐步分解和细化，形成整个系统的体系结构。<br><strong><img src="/2019/11/05/wu-han-li-gong-da-xue-ruan-jian-gong-cheng-fu-xi-zong-gua-san/8.png" alt="在这里插入图片描述"><br>面向对象风格</strong>：系统被看作是对象的集合，每个对象都有一个它自己的功能集合，数据及作用在数据上的操作被封装成抽象数据类型，只通过接口与外界交互，内部的设计决策则被封装起来。<br>**<img src="/2019/11/05/wu-han-li-gong-da-xue-ruan-jian-gong-cheng-fu-xi-zong-gua-san/9.png" alt="在这里插入图片描述"><br><strong>层次结构风格----客户机／服务器体系结构</strong>：一种分布式系统模型<br><strong>服务器</strong>：为客户机提供服务<br><strong>客户机</strong>：负责与用户的交互       类似于网络编程交互的情景。<br><strong><img src="/2019/11/05/wu-han-li-gong-da-xue-ruan-jian-gong-cheng-fu-xi-zong-gua-san/10.png" alt="在这里插入图片描述"><br>层次结构----浏览器/服务器结构</strong>：<br><img src="/2019/11/05/wu-han-li-gong-da-xue-ruan-jian-gong-cheng-fu-xi-zong-gua-san/11.png" alt="在这里插入图片描述"><br><strong>层次结构----模型/视图/控制器：（MVC）</strong><br>**<img src="/2019/11/05/wu-han-li-gong-da-xue-ruan-jian-gong-cheng-fu-xi-zong-gua-san/12.png" alt="在这里插入图片描述"><br><strong>基于事件的隐式调用风格</strong>：将应用看成是一个构件集合，每个构件直至发生对它有影响的事件时才有所动作<br>**<img src="/2019/11/05/wu-han-li-gong-da-xue-ruan-jian-gong-cheng-fu-xi-zong-gua-san/13.png" alt="在这里插入图片描述"><br><strong>仓库风格</strong>：以数据为中心，适合于数据由一个模块产生而由其他模块使用的情形<br><img src="/2019/11/05/wu-han-li-gong-da-xue-ruan-jian-gong-cheng-fu-xi-zong-gua-san/14.png" alt="在这里插入图片描述"></p><h1>第七章面向对象设计</h1><h2 id="面向对象的设计和分析中的三种类："><a class="header-anchor" href="#面向对象的设计和分析中的三种类：">¶</a>面向对象的设计和分析中的三种类：</h2><p><strong>1、实体类</strong>：对应系统需求中的每个实体，它们通常需要保存在永久的存储体中，一般使用数据库表和文件来记录，实体类包括存储和传递数据的类，也包括操作数据的类<br><strong>2、控制类</strong>：用于体现应用程序的执行逻辑，提供相应的业务操作，将控制类抽象出来可以降低界面和数据库之间的耦合度；<br><strong>3、边界类</strong>：用于对外部用户与系统之间的交互对象进行抽象。主要包括界面类。</p><h2 id="什么是领域模型："><a class="header-anchor" href="#什么是领域模型：">¶</a>什么是领域模型：</h2><p>在面向对象分析和设计的初级阶段，通常先识别出实体类，绘制初始类图，此时的类图称为领域模型，包括实体类和它们之间的相互关系。</p><h1>第八章编写高质量代码</h1><h2 id="程序复杂度怎么计算（重点是第二种，第二种有三种小方法）："><a class="header-anchor" href="#程序复杂度怎么计算（重点是第二种，第二种有三种小方法）：">¶</a>程序复杂度怎么计算（重点是第二种，第二种有三种小方法）：</h2><p><strong>基本思想</strong>：程序复杂性主要取决于程序控制流的复杂性，单一的顺序结构最简单，选择和循环结构构成的环路越多，程序越复杂。<br><strong>实质</strong>：度量程序拓扑结构的复杂性程序图：把程序看成是有一个入口、一个出口的有向图程序图的<br><strong>节点</strong>：每个语句、一个顺序流程的程序代码段、程序流程图中的每个处理符号程序图的<br><strong>有向弧</strong>：程序中的流程控制、程序流程图中连接不同处理符号的、带箭头的线段<br><strong>强连通图(Strongly Connected Graph)</strong>：是指一个有向图（Directed Graph）中任意两点v1、v2间存在v1到v2的路径（path）及v2到v1的路径的图。<br><strong>三种方法</strong>：如果程序图中每个节点都可以由入口节点到达，则<strong>图中环的个数 = 环路复杂度</strong><br>如果程序图是强连通图，则计算环路数V(G)的方法 <strong>方法一：V(G) = e–n + p（e: 弧数，n: 节点数，p: 分离部分的数目,V(G)有向图G中的环数）</strong> <strong>方法二：包括强连通域在内的环路数</strong> <strong>方法三：判定节点数 +  1</strong><br>V(G)与程序复杂性呈正比关系 一般一个模块V(G) ≤ 10</p><h1>第九章测试驱动的实现</h1><h2 id="软件测试的类型："><a class="header-anchor" href="#软件测试的类型：">¶</a><strong>软件测试的类型</strong>：</h2><p>1）从测试对象角度①单元测试 ②集成测试③功能测试 ④性能测试 ⑤安装测试2）测试技术角度①黑盒测试（功能测试）②白盒测试（结构测试）<br>3）是否运行程序角度①静态测试 ②动态测试<br>4）执行测试的方式①手工测试 ②自动化测试</p><h2 id="白盒测试："><a class="header-anchor" href="#白盒测试：">¶</a>白盒测试：</h2><p>在下一次更新中详写。</p><h2 id="自己会设计测试用例：路径覆盖："><a class="header-anchor" href="#自己会设计测试用例：路径覆盖：">¶</a>自己会设计测试用例：路径覆盖：</h2><p>在下一次更新中详写</p><h2 id="软件测试的几个阶段：-每个阶段的名称作用测试的对象"><a class="header-anchor" href="#软件测试的几个阶段：-每个阶段的名称作用测试的对象">¶</a>软件测试的几个阶段：(每个阶段的名称作用测试的对象)</h2><p><strong>1、单元测试</strong>：对软件中的最小可测试单元进行检查和验证 对象是单元。<br><strong>2、集成测试</strong>：在单元测试的基础上，将所有模块按照总体设计的要求组装成为子系统或系统进行的测试 对象是系统或者子系统<br><strong>3、确认测试</strong>：在开发过程中或结束时评估系统或组成部分的过程，目的是判断系统是否满足规定的要求。对象是系统<br><strong>4、系统测试</strong>：检测软件系统运行时与其他相关要素的协调工作情况是否满足要求。对象是系统。</p>]]></content>
      
      
      <categories>
          
          <category> 课本学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 软件工程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>武汉理工大学《软件工程》复习总括二</title>
      <link href="/2019/11/03/wu-han-li-gong-da-xue-ruan-jian-gong-cheng-fu-xi-zong-gua-er/"/>
      <url>/2019/11/03/wu-han-li-gong-da-xue-ruan-jian-gong-cheng-fu-xi-zong-gua-er/</url>
      
        <content type="html"><![CDATA[<h1>第四章需求获取</h1><p><strong>需求分析的实质</strong>：是对系统的理解与表达的过程，是一种软件工程的活动。</p><p><strong>需求分析之后建立模型的名称</strong>：分析模型或需求模型需求分析的过程：<br><strong>需求分析的过程</strong>：<img src="/2019/11/03/wu-han-li-gong-da-xue-ruan-jian-gong-cheng-fu-xi-zong-gua-er/1.png" alt="在这里插入图片描述"><br><strong>常用的需求分析的方法</strong>：<br>1：面向数据流的结构化分析方法（简称SA）<br>2：面向数据结构的Jackson方法（简称JSD）<br>3：面向对象的分析方法<br>4：建立动态模型的迁移图或Petri网等。<br><strong>软件需求的分类，根据分类的标准不同，结果也不同</strong>：<br><strong>1：按修饰对象的不同</strong>：<br><strong>Ø 产品需求</strong>：<br><strong>l 功能性需求</strong>：软件产品的功能特性<br><strong>l 非功能性需求</strong>：软件产品的质量属性，是在功能性需求满足情况下的进一步要求<br><strong>“FURPS“模型</strong>：<br><strong>功能性</strong>：需要考虑的额外的功能需求，如安全性；<br><strong>可用性</strong>：易用性、美观性、一致性和文档化；<br><strong>可靠性</strong>：指的是在特定操作环境下预期的系统故障频率、可恢复性、可预测性准确性以及平均故障时间；<br><strong>性能</strong>：响应时间、效率、资源利用率和吞吐量（在一个指定时间内系统可完成的工作量）<br><strong>可支持性</strong>：可测试性、适应性、可维护性、兼容性、可配置性、可扩展性和本地化<br><strong>Ø 过程需求</strong>——修饰或限制软件开发过程的要求<br><strong>2：按抽象层次详细程度</strong>：<br><strong>Ø 业务需求</strong><br><strong>Ø 用户需求</strong><br><strong>Ø 系统需求</strong><br><strong>Ø 软件设计规约</strong></p><p><strong>需求优先级的等级：</strong><br><strong>1、基本的</strong>：使得客户能够黑手系统并且必须实现的要求<br><strong>2：可取的</strong>：非常可取但却不是必须的那些需求<br><strong>3：可选的</strong>：在时间和资源允许的情况下，可能会实现的需求<br><strong>4：未来的</strong>：不会在系统当前版本中实现，但考虑到系统后续的版本应该记录下来的需求</p><p><strong>需求获取的技术有哪些</strong>：<br>1：面谈<br>2：问卷调查<br>3：群体诱导技术<br>4：头脑风暴<br>5：参与观察法<br>6：亲身实践<br>7：原型<br>8：情景分析<br>9：概念建模<br>10：A/B测试</p><p><strong>结构化分析的主要工具</strong>：<br>1：数据流图(DFD)<br>2:数据字典(DD)<br>3：结构化语言<br>4：判定树<br>5：判定表</p><p><strong>传统的软件建模中分析模型的核心及围绕核心的三个子模型</strong>：<br>分析模型的核心是数据字典，围绕数据字典3个层次的子模型有数据模型、功能模型和行为模型。<br><strong>数据字典</strong>：用于描述系统软件中使用或者产生的每一个数据元素，是系统数据信息定义的集合。<br><strong>数据模型</strong>：用于描述数据对象之间的关系。其应包含3种相关的信息，即数据对象、属性和关系<br><strong>功能模型</strong>：可以用数据流图描述（数据流图是一种图形化技术，可以表达软件系统必须完成的功能），所以又称数据流模型。<br><strong>行为模型</strong>：常用状态转化图（即状态图）来描述，又称状态机模型，可以理解为，在任一个时刻，系统处于有限可能的状态中的一个状态，当某一个激励条件到达时，它激发系统从一个状态转换到另一新状态。</p><h1>第五章</h1><p><strong>用例建模UML的九种图的画法，以及每种图的作用，在分析和设计的过程中怎么使用：</strong><br><img src="/2019/11/03/wu-han-li-gong-da-xue-ruan-jian-gong-cheng-fu-xi-zong-gua-er/2.png" alt="在这里插入图片描述"><br><strong>1、用例图：</strong><br><strong>作用</strong>：表示角色和用例之间的关系，其中用例代表的是一个系统或分类器的功能，外部交互者与这一分类器来进行交互呈现。<br><strong>组成/使用</strong>：由一些角色、一组用例，还可能有一些接口以及这些组成元素之间的关系构成的图，其中关系是指角色和用例之间的联系。用例通常用矩形框起来以表示系统或分类器的边界。<br><img src="/2019/11/03/wu-han-li-gong-da-xue-ruan-jian-gong-cheng-fu-xi-zong-gua-er/3.png" alt="在这里插入图片描述"><br><strong>2、类图：</strong><br><strong>作用</strong>：静态描述性模型元素相互连接的集合图，可以表示不同实体（人，事务和数据）的内部构成<br><strong>组成/使用</strong>：名称，属性和方法，他们之间的关系。#表示受保护成员，+表示公有成员，“-”表示私有成员。<br><strong><img src="/2019/11/03/wu-han-li-gong-da-xue-ruan-jian-gong-cheng-fu-xi-zong-gua-er/4.png" alt="在这里插入图片描述"><br>3、交互图</strong> ：包括顺序图和协作图，两种图在内容上是等效的，可以相互转换。<br><strong>顺序图</strong>：强调消息的时间排序<br><strong>作用</strong>：表示交互，指为得到一个期望的结果而在多个分类器角色之间进行的交互序列。<br><strong>组成/使用</strong>：顺序图有两维，垂直维代表时间，水平维代表对象。通常，垂直维自上至下代表时间向前推进。<br><strong>协作图</strong>：强调发送消息和接收消息的对象的结构组织<br><strong>作用</strong>：描述相互联系的对象之间的关系，或者分类器角色和关联角色之间的关系以下是同一个例子分别用两种图的表示<br><img src="/2019/11/03/wu-han-li-gong-da-xue-ruan-jian-gong-cheng-fu-xi-zong-gua-er/5.png" alt="在这里插入图片描述"> 顺序图<br><img src="/2019/11/03/wu-han-li-gong-da-xue-ruan-jian-gong-cheng-fu-xi-zong-gua-er/6.png" alt="在这里插入图片描述"><br>协作图<br><strong>4、状态图</strong>：描述模型元素在接收到事件后的动态行为。<br><strong>作用</strong>：描述一个类的对象在生命周期里如何从一个状态转移到另外一个状态，类的迁移由事件触发。<br><strong>组成/使用</strong>：图形中的状态和各种其他类型的顶点（伪状态）用适当的状态或者伪状态符号表示，状态之间的转换则用有向弧连接表示。<img src="/2019/11/03/wu-han-li-gong-da-xue-ruan-jian-gong-cheng-fu-xi-zong-gua-er/7.png" alt="在这里插入图片描述"></p><p><strong>5、活动图</strong>：是状态图的一种特殊的情况，其中绝大部分状态是动作或子活动状态，并且绝大部分甚至所有的转换是通过动作或者子活动的完成所触发的。<br><strong>作用</strong>：描述绝大多数甚至是所有的事件是由内部动作的完成所引起的情况。<br><strong>组成</strong>：由一条路径组成。包含并发和分叉，并发：两个活动同时发生。  分叉：选择性活动的发生<br><img src="/2019/11/03/wu-han-li-gong-da-xue-ruan-jian-gong-cheng-fu-xi-zong-gua-er/8.png" alt="在这里插入图片描述"><br><strong>6、构件图：</strong><br><strong>作用</strong>：表示构建之间的依赖关系组成：软件构建包括源代码构建、二进制代码构建和可执行构建，一些构建存在于编译时刻，一些存在于链接时刻，一些存在于运行时刻，还有一些可能存在于不止一个时刻。<br><strong>组成/使用</strong>：使用箭头表示依赖关系<br><img src="/2019/11/03/wu-han-li-gong-da-xue-ruan-jian-gong-cheng-fu-xi-zong-gua-er/9.png" alt="在这里插入图片描述"><br><strong>7、配置图：</strong><br><strong>作用</strong>：表示系统运行时的处理元素、软件构件以及基于它们的进程和对象的配置情况<br><strong>组成/使用</strong>：不处于运行状态的实体的软件构件不出现（在构件图中表示），结点可能包含构件实例，构件可能包含对象，构件与构件之间的依赖关系用箭线表示<br><strong><img src="/2019/11/03/wu-han-li-gong-da-xue-ruan-jian-gong-cheng-fu-xi-zong-gua-er/10.png" alt="在这里插入图片描述"><br>用例之间的关系</strong>：泛化、包含、扩展</p><p><strong>用例的场景：</strong><br>1：某个用例的一个实例，只描述完成给定的用例行为的若干可能途径中的一种  2：一个用例可能存在多个场景<br>3：系统会根据参与者提供的不同信息进入不同的场景<br>4：场景可以表达：正面行为需求，反面行为需求，不希望发生的交互，并行机制</p><p><strong>类与类之间的关系</strong>：<br>1：关联关系：包含自返关联、二元关联、N元关联<br>2：泛化关系<br>3：依赖关系<br>4：实现关系</p>]]></content>
      
      
      <categories>
          
          <category> 课本学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 软件工程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>武汉理工大学《软件工程》复习总括一</title>
      <link href="/2019/11/02/wu-han-li-gong-da-xue-ruan-jian-gong-cheng-fu-xi-zong-gua-yi/"/>
      <url>/2019/11/02/wu-han-li-gong-da-xue-ruan-jian-gong-cheng-fu-xi-zong-gua-yi/</url>
      
        <content type="html"><![CDATA[<h1>第一章软件工程概述</h1><p><strong>软件的本质特征：</strong><br>复杂性+一致性+可变性+不可见性<br><strong>软件危机的概念：</strong><br>软件危机是指在计算机软件的开发和维护过程中所遇到的一系列问题。<br><strong>软件工程的概念：</strong><br>1：将系统化的、规范的、可度量的方法应运与软件的开发、运行和维护的过程，即将工程化应运与软件中<br>2：对1中所述的方法的研究<br><strong>软件工程的关键元素</strong>：方法+工具+过程<br><strong>软件工程的开发策略</strong>：软件复用+分而治之+逐步演进+折中优化<br><strong>软件工程的基本原理：</strong><br>1：用分阶段的生命周期计划严格管理<br>2：坚持进行评审阶段<br>3：实行严格的产品控制<br>4：采用现代化程序设计技术<br>5：结果应能清楚的审查<br>6：开发小组人员应少而精<br>7：承认不断改进软件工程的必要性</p><h1>第二章软件过程</h1><p><strong>软件过程的定义及包含的活动：</strong><br>软件过程是指软件生成周期中的一系列相关过程，是为了获得高质量软件而实施的一系列活动。它包括问题定义、需求开发、软件设计、软件构造、软件测试等一系列软件开发的实现活动，而每一项都会产生相应的中间制品。<br><strong>软件过程常见的模型及其关系</strong>：<br>Ø <strong>瀑布模型</strong>——无法适应需求变化，计划驱动<br>特点：①. 阶段间具有顺序性和依赖性，便于分工合作；<br>②. 强调软件文档的重要性，要求每个阶段都进行仔细的验证；<br>③. 文档便于修改，并有复审质量保证。<br>缺陷：①. 划分固定，产生大量文档，增加了开发的工作量；<br>②. 开发是线性的，用户只有在整个程结束时才能看到开发成果；<br>③. 难以响应开发过程中用户的变更需求；<br>④. 早期错误难以发现适用于在软件需求明确，开发技术比较成熟，工程管理较严格的场合下使用（基本不会单独使用瀑布模型作为软件过程模型）<br>Ø <strong>原型化模型</strong>——需求不明确时选用<br>从用户需求出发快速建立一个原型，使用户通过这个原型初步表达出自己的需求，并通过反复修改完善逐渐靠近用户的全部需求，最终形成一个完全满足用户需求的新体系。<br>例：3D打印机<br>Ø <strong>迭代式开发</strong>——适应需求变化<br>开发被组织成一系列固定的短期小项目，称为一次迭代，每次迭代都包括完整的需求分析、设计、实现和测试活动。（理解：发布一系列版本）<br>增量开发：逐渐增加新的功能（缺点：增加功能的过程中可能破坏原有系统）<br>迭代开发：一次性开发出所有功能，后期再逐步完善各个功能<br>优点：①. 快速交付产品；<br>②. 快速响应需求变更；<br>③. 关注用户行为，很快得到用户反馈例：网上视频学习网站<br>Ø <strong>可转换模型</strong>——数学方法（安全、可靠、保密）<br>特点：需要一个精确表述的形式化的规格说明<br>例：汽车防抱系统，嵌入式控制系统<br><strong>关系</strong>：这些模型相互并不排斥，而且经常一起使用，尤其是对一些大型系统的开发。<br><strong>敏捷软件开发的核心价值</strong>：<br>1：“个体和交互”胜过“过程和工具”<br>2：“可以工作的软件”胜过“面面俱到的文档”<br>3：“客户合作”胜过“合同谈判”<br>4：“响应变化”胜过“遵循计划”</p><h1>第三章对象模型</h1><p><strong>面向对象方法的精华</strong>：面向对象=对象+分类+继承+消息通信<br><strong>1：对象</strong>：<br>客观世界都是由各种对象组成，任何事物都是对象，复杂的对象可以由比较简单的对象组合起来<br><strong>2：分类</strong>：<br>把所有的对象都划分为各种类，每个类都定义了一组数据和一组方法<br><strong>3：继承</strong>：<br>按照子类和父类的关系分类组成一个层次结构的系统，下层的子类与上层的父类有相同的特性<br><strong>4：消息通信</strong>：对象与对象之间只能通过传递消息进行通信。<br><strong>接口的概念</strong>：方法声明的集合<br><strong>对象属性和方法的三种访问权限</strong>：<br><strong>1：公有的（public）</strong>：其他对象可以直接访问<br><strong>2：私有的  (private)</strong>：只有特定的对象可以访问<br><strong>3：保护的(protected)</strong>：表示允许相关对象的访问<br><strong>对象的职责</strong>：即一个对象对其他对象的职责<br>1：“知道”型职责:知道各类数据和引用变量<br>2：“做”型职责：执行计算完成某项任务<br>3：“交流”型职责：和其他对象进行交流</p><p>此部分仅为第一到三章的内容，后续部分会依次更新，请您关注我的博客，在我的博客中寻找其他几章内容！！</p>]]></content>
      
      
      <categories>
          
          <category> 课本学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 软件工程 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
