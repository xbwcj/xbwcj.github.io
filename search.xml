<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>《Knowledge Lock Overcoming Catastrophic Forgetting in Federated Learning》</title>
      <link href="/2023/03/07/knowledge-lock-overcoming-catastrophic-forgetting-in-federated-learning/"/>
      <url>/2023/03/07/knowledge-lock-overcoming-catastrophic-forgetting-in-federated-learning/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h1>摘要</h1><p>  联邦学习（FL）旨在通过分散的数据来训练机器学习模型，而不需要直接的数据共享。然而，FL参与者之间数据的异质性极大地阻碍了联邦模型的竞争性能。在本文中，我们认为这个问题是知识遗忘的结果，因为FL的局部更新过程可能会导致从其他参与者那里学到的知识的灾难性遗忘。在增量学习技术的最新进展的推动下，我们通过克服由数据隔离引起的严重知识遗忘来解决这个问题。我们提出了一种新的方法，称为FedKL（带知识锁的联邦学习），其中使用知识蒸馏技术来维护先前学习的知识。我们的大量实验结果表明，FedKL比之前的方法获得了更好的性能，与流行的FL算法FedAvg相比，在CIF AR-10和CIF AR-100上的精度分别提高了3.4 $\%$ 和3.5 $\%$。此外，我们还探讨了向FedKL引入共享示例（局部数据的一部分）的好处。在实验中，我们为FedKL和基线方法每个类选择并共享10个样本。结果，FedKL在CIFAR-10上的准确率提高了2.56 $\%$ ，而不是在先前方法上的边际提高（小于1.5 $\%$ ）</p><h1>Introduction</h1><p>  联邦学习(FL)技术[31]为多方协作提供了一种安全高效的解决方案，参与者能够在不暴露私人数据的情况下协同训练一个全局模型[20]。.其基本思想是聚合模型参数的局部更新，而不是直接的数据共享[10,20]。联邦学习打破了数据合作的障碍。它在训练数据在参与者之间是独立的同分布(iid)的理想假设下工作得很好。</p><p>  然而，在大多数应用场景中，参与者的数据量和类分布差异很大，通常称为非iid或统计异质性[30,34]。结果表明，数据的异构分布往往会对联邦训练的收敛性产生负面影响，联邦模型的精度必然会降低[34]。我们考虑这个问题的角度是，由于数据的异质性，本地培训很容易忘记从其他参与者那里学到的知识。在这项工作中，我们认为解决非iid问题的瓶颈在于克服全局知识的遗忘。</p><p>  我们受到增量学习方法的启发[12,16,22]，其主要目标是在从新访问的数据中学习的同时保持之前学习过的知识。我们发现增量学习和联邦学习之间的内在相似性，即灾难性的知识遗忘是由于整个数据集不可用。实际上，主要的区别只是在于这种不可用性是由于增量学习中的“时间原因”，因为在这种情况下，一旦新数据到来，旧数据就不能被重用，而在联邦学习中是由于“空间原因”，因为训练数据是空间分散的。</p><p>  因此，这种相似性促使我们探索将增量学习策略转换为联邦学习的有效性。我们改进了增量学习中的知识维护方法，提出了一种新的知识锁方案，将训练目标解耦为一个分类项和一个知识维护项。在分类项中，局部训练由数据的原始标签监督，并应用了softmax-crossentropy损失。在知识维护方面，局部不可用类由全局模型的逻辑回归损失的蒸馏[8]来监督。此外，我们还探讨了部分数据共享对我们的方法和基线算法的影响。我们提出的范例选择算法(见算法2)已被经验证明能够在知识锁上产生显著的精度提高。</p><p>  我们进行了一系列实验来评估我们的方法FedKL （Fedrated Learning with Knowledge Lock），并将其与之前的算法进行比较。如第4节所述，在VGG[27]和ResNet[7]架构上，FedKL优于现有的算法，包括FedAvg [20]， FedProx[15]和SCAFFOLD[11]。值得注意的是，与FedAvg[20]相比，使用VGG-16[27]网络的FedKL在CIF AR-10和CIF AR-100[14]上的准确率分别提高了3.46 $\%$ 和3.51 $\%$。此外，FedKL对部分数据共享的适应性更强，在CIF AR-10和CIF AR-100[14]上的准确率分别提高了2.56 $\%$ 和2.11 $\%$。</p><h1>相关工作</h1><p>  联邦学习在可靠数据合作方面的潜力已经引起了人们对其在异构分布数据中的性能的迅速增长的兴趣。我们将现有的数据异构解决方案分为面向参数的方法和面向输出的方法。对于前者，人们试图通过约束模型参数来应对模型的异质性。例如，FedProx[15]在局部模型和全局模型的参数之间增加了欧几里得距离的正则化项来约束局部梯度体面。SCAFFOLD[11]还修改了本地反向传播的损失，引入了额外的参数来跟踪本地更新。这些方法在联邦模型的准确性、收敛性或通信效率方面取得了相当大的改进。我们重新实现FedProx[15]和SCAFFOLD[11]作为竞争基准，以便在第4节中进行比较。</p><p>  对于面向输出的方法，约束直接应用于模型输出，也就是说，它们鼓励局部模型产生与全局模型或其他更强的教师模型相似的输出。在联邦学习任务中使用知识蒸馏技术[8]已经有了一些尝试[18,19]。然而，当数据大量分布且极其异构时，这些方法无法实现与面向参数的方法一样具有竞争力的性能。这可能是因为这些方法忽略了局部未知的类，并对每个类应用相同的蒸馏损失。在联邦学习方法中也报道了元学习策略，以增强鲁棒性[3]。然而，由于缺乏对类差异的考虑，在处理更具挑战性的联邦学习场景时也很困难。</p><p>  值得注意的是，在隐私敏感性相对较低的情况下，部分数据共享是克服数据异构性的另一种有效方法[35]。例如，其中一些尝试通过共享局部数据来修改数据分布[29,33]，这也可以通过数据增强来改进[2,25]。然而，由于他们随机选择要共享的样本，这既不是足够的隐私效率，也不是有效的通信效率。相反，我们提出了一种有效的数据共享策略(参见算法2)，通过只共享很小一部分数据，我们可以获得令人满意的性能改进。</p><p>  值得注意的是，在隐私敏感性相对较低的情况下，部分数据共享是克服数据异构性的另一种有效方法[35]。例如，其中一些尝试通过共享局部数据来修改数据分布[29,33]，这也可以通过数据增强来改进[2,25]。然而，由于他们随机选择要共享的样本，这既不是足够的隐私效率，也不是有效的通信效率。相反，我们提出了一种有效的数据共享策略(参见算法2)，通过只共享很小一部分数据，我们可以获得令人满意的性能改进。</p><p>  此外，也有联邦学习研究工作转向系统设计[4,5,13,23]或个性化正则化[6,9]，重点关注局部模型和全局模型之间的权衡。一些方法[1,17]为参与者设计了一个灵活的网络，其中一部分是协作训练，另一部分是本地训练。联邦算法中还采用了多任务学习[24,28]和迁移学习[21]方法，以增强局部训练的适应性。</p><p>  最近，增量学习（也称为终身学习）已被应用于克服数据异质性的影响。Shoham et al.[26]提出了基于弹性权重巩固的FedCurv算法。在FedCurv中，参与者还上传了Fisher信息矩阵的对角线，这表明了关键的局部知识。在损失函数中加入惩罚项，使局部模型收敛到共同最优。然而，即使有带宽优化技巧，FedCurv中的通信预算也是FedAvg的三倍。Yoon et al.[32]开发了联邦加权客户端间传输(FedWeIT)方法，其中节点通过注意掩码利用来自其他节点的任务自适应参数。</p><h1>Federated Learning with Knowledge Lock</h1><h2 id="Overview"><a class="header-anchor" href="#Overview">¶</a>Overview</h2><p>  训练联邦分类器中最棘手的问题之一是，由于完整的训练数据分散分布在不同的设备上，因此无法获得。特别是，已经被广泛研究的是，设备上训练数据的异质性往往会导致预测性能的大幅下降和优化的难度。在分类任务中，这种异质性往往表现为类分布的不平衡[15,28]。通常，这是一个更尴尬的场景，每个设备上的训练样本只属于整个数据集的部分类，即，给定一个包含 $m$ 个类的完整数据集，$k$ 个设备中的每个设备只有 $m_{k}$ 类中的样本，其中 $m_{k} &lt; m$ 。换句话说，在联邦训练期间，每个设备中都有“未知”类。在本文中，我们重点研究了这种情况，并提出了我们的方法Fed-KL，该方法受到类增量学习方法的启发，以解决训练数据的异质性问题。</p><p>  给定一个分布在 $k$ 个设备上的数据集 $\mathcal{D}=\{(x_{i}, y_{i})\}_{i=1}^{n}$ ，联邦学习的目标是训练一个共享模型 $f: \mathbb{R}^{d} \rightarrow \mathbb{R}^{m}$，其中 $x_{i} \in \mathbb{R}^{d}$ 是训练数据，$y_{i} \in \mathbb{R}$ 是它的标签。具体来说，在分类任务中，模型 $f(·)$ 可以表示为表示编码器 $g(·)$，后面是具有 $m$ 个单元的全连接层，其中 $m$ 为总类数。</p><p>  为了充分利用每个设备上的数据而不直接共享它们，最常用的联邦学习方法(如FedAvg[20])以多轮更新-聚合的方式训练模型。在每一轮中，参与者（设备）在更新步骤中对其本地数据训练共享全局模型，训练周期为一定数量，然后在聚合步骤中将训练好的本地模型聚合为新的全局模型。通常，在FedAvg[20]中，网络通过直接的参数平均聚合，即给定本地网络 $f_{1}, f_{2}, \cdots ,f_{k}$ ，参数为 $\Theta_{1}, \Theta_{2}, \cdots , \Theta_{k}$，全局网络参数由：<br>$$<br>\Theta \leftarrow \frac{1}{k} \sum_{j=1}^{k} \Theta_{j} \quad\quad (1)<br>$$</p><p>  不幸的是，尽管现有的FL方法能够融合从不同参与者那里学到的知识，但它们仍然存在参与者之间训练数据的统计异质性，这使得联邦模型无法与集中式训练相比具有相当的性能。我们从知识遗忘的角度来考虑数据异质性问题，这类似于增量学习、在线学习或终身学习的主题[14,22]。具体来说，我们认为集中式训练和联合训练之间的巨大性能差距是由于（或部分由于）参与者在对他们的本地数据进行训练时忘记了从其他人那里学到的知识。因此，在本文中，知识保持被视为一个基本主题，并作为我们所提出的方法FedKL的主要策略。</p><h2 id="FedKL-with-Distillation"><a class="header-anchor" href="#FedKL-with-Distillation">¶</a>FedKL with Distillation</h2><p>  克服灾难性知识遗忘的一个非常简单但有效的方法是应用知识蒸馏[8]，假设神经网络的输出概率能够表示它已经学习到的知识[14,22]。我们遵循这一关键发现，并进一步提出了一种新的算法，FedKL-Dist（知识锁定联合学习-蒸馏），该算法在分类损失旁边增加了一个蒸馏损失项，以“锁定”从其他参与者那里学到的知识。</p><p>  形式上，一般来说，联邦分类器是由损失局部训练的<br>$$<br>\mathcal{L}_{clf}=\dfrac{1}{n_k}\sum_{i=1}^{n_k}\mathrm{SoftmaxcrossEntropy}(f_\Theta(x_i),y_i) \quad\quad (2)<br>$$</p><p>其中 $n_{k}$ 表示第 $k$ 个设备（参与者）上的训练样本数量，$f(·)$ 表示参数为 $\Theta$ 的网络。这种基本的分类损失迫使网络预测 $x_{i}$ 尽可能接近它的目标 $y_{i}$。但是，如果仅采用这种分类损失，可能会导致严重的知识遗忘。例如，正如我们在3.1节中假设的那样，总是存在“未知类”（这些类的样本不会出现在本地设备上）。因此，网络倾向于在这些类上预测所有训练样本的概率为零，这将导致在这些“未知类”上的预测性能较差。</p><p>  因此，为了约束遗忘，我们进一步对局部未知类应用蒸馏损失。假设一个设备上 $M$ 个类别中有 $M’$ 是未知的，我们首先计算所有本地可用样本的 $M’$ 类上的Sigmoid概率。形式上，我们有：<br>$$<br>\delta_{i}^{m}=\mathrm{Signoid}(f_\Theta ^{m}(x_i)) \quad\quad (3)<br>$$</p><p>式中 $\delta_{i}^{m}$ 表示全局模型 $f_{\Theta}(·)$ 预测的第 $i$ 个样本第 $m$ 个未知类别的概率。然后，定义蒸馏损失为：<br>$$<br>\mathcal{L}_{dis}=-\dfrac{1}{n_{k}|M’|}\sum_{i=1}^{n_{k}}\sum_{m\in M’}[\delta^{m}_{i}\log(\mathrm{S}(f^{m}_{\Theta}(x_{i})))+(1-\delta^{m}_{i})\log(1-\mathrm{S}(f^{m}_{\Theta}(x_{i})))], \quad\quad (4)<br>$$</p><p>其中 $S(·)$ 代表Sigmoid函数。</p><p>  直观地说，$\mathcal{L}_{dis}$ 鼓励局部优化来保持未知类上的概率，而不是强迫它们为零，而在一些增量学习算法中也出现了类似的策略，并被经验证明可以有效地防止知识遗忘[14]。</p><p><img src="/2023/03/07/knowledge-lock-overcoming-catastrophic-forgetting-in-federated-learning/1.png" alt="算法1"><br>  本地更新过程已经在算法1中进行了总结。考虑分类损失 $\mathcal{L}_{clf}$ 和蒸馏损失 $\mathcal{L}_{dis}$ 之间的权衡，我们引入了一个新的超参数 $\lambda$，局部更新的总损失可以表示为：<br>$$<br>\mathcal{L}=\mathcal{L}_{clf}+\lambda \mathcal{L}_{dis}.\quad\quad (5)<br>$$</p><p>  我们已经搜索了参数 $\lambda$，并发现将其设置为 $M’ / M$ 左右会导致更好的性能。</p><h2 id="用类样本改进FedKL"><a class="header-anchor" href="#用类样本改进FedKL">¶</a>用类样本改进FedKL</h2><p>  克服知识遗忘的另一种流行策略是用代表性样本[22]重新训练模型。具体来说，在增量学习方法中[14,22]，为了保持之前学习的知识，人们尝试存储一些有代表性的样本作为样本，然后将新的数据与这些样本相结合来训练模型。类似地，部分共享本地数据在联邦学习中也很流行，这以牺牲一部分隐私为代价提高了性能。然而，少量数据的共享并不能带来足够的性能提升，而大量数据的共享可能会违反联邦学习的基本规则。</p><p>  因此，为了效率和最小的隐私成本，选择最具代表性的样本作为样本至关重要。在本文中，我们通过它们的表示来选择例子。形式上，给定一个全局模型 $f(·)$ 和一个编码器 $g(·)$，每个样本的归一化表示可以通过：<br>$$<br>\gamma_{i}=\dfrac{g(x_{i})}{\|g(x_{i})\|_{2}}. \quad\quad (6)<br>$$</p><p>  我们假设最具代表性的样本是最接近平均表示的样本，它可以表述为：<br>$$<br>p=\mathrm{argmax}_{x\in\mathcal{X}}&lt;\gamma,g(x_{i})/\|g(x_{i})\|_{2}&gt;, \quad\quad (7)<br>$$</p><p>  其中 $\gamma=1/n_{k} \sum_{i=1}^{n_{k}}\gamma_{i},$ 表示 $n_{k}$ 表示的平均值，$\mathcal{X}$ 是 $n_{k}$ 训练样本的数据集。正如我们在算法2中所描述的，我们选择最具 $L$ 个代表性的样本作为每个类的样本，其中 $L$ 取决于隐私约束。请注意，对于一个特定的类，如果 $L$ 小于拥有这类样本的参与者数量，我们随机选择参与者共享样本。</p><p><img src="/2023/03/07/knowledge-lock-overcoming-catastrophic-forgetting-in-federated-learning/2.png" alt="算法2"></p><p><b>Warming up Epochs. </b>。值得注意的是，为了获得更高的精度，编码器 $g(·)$ 应该是一个经过良好训练的模型。例如，在实际训练中，我们可以先用算法1训练模型一段时间，然后用训练好的模型作为 $g(·)$，之后我们继续用局部数据和共享样本训练模型。</p><h1>实验</h1><h2 id="实验设置"><a class="header-anchor" href="#实验设置">¶</a>实验设置</h2><p><b>数据集</b>。我们在图像分类数据集的基线上评估我们的方法，包括:<br>  （1）MNIST手写数字数据库，包含 $60k$ 用于训练的图像和 $10k$ 用于评估的图像，来自 $10$ 个类</p><p>  （2）CIFAR-10和CIFAR-100[14]，分别是从 $10$ 和 $100$ 类中绘制的彩色图像。CIFAR-10和CIFAR-100都有 $50k$ 训练图像和 $10k$ 评估图像。</p><p>  为了模拟联邦学习中的100个参与者，我们将完整的数据集分割为100个部分。正如我们在3.1节中介绍的，每个参与者只访问部分类。具体来说，在每个设备（参与者）上，至少有两个且最多有 $M_{max}$ 类，直观上，$M_{max}$ 越高意味着异质性越低。由于我们在本文中没有研究不平衡，所以在我们的实验中，所有参与者的局部样本数量是相似的。</p><p><b>神经网络架构</b>。正如之前的工作所做的[11,15,20]，我们使用卷积神经网络（cnn）作为基础模型来评估我们的方法和基线。具体采用16层VGG网络[27]和18层ResNet[7]。</p><p><b>baseline</b>在我们的实验中，我们主要将我们的方法与以下联邦学习算法进行比较：</p><p>  FedAvg[20]。联邦平均算法(federal average, FedAvg)是联邦学习中最常用的原型算法，它在局部更新步骤中存在分类损失</p><p>  FedProx[15]。FedProx是最具竞争力的联邦学习方法之一，具有额外的损失函数 $\mu/2\|\Theta-\Theta^{t}\|^{2}$ 约束异构数据的优化</p><p>  SCAFFOLD[11]。SCAFFOLD是另一种有竞争力的联邦学习方法，它解决了集中在“客户端漂移”上的异质性问题，其中分类损失被修改以跟踪优化路径</p><p>  Centralized training。Centralized training作为联邦学习性能的上限，其中模型直接在整个数据集上进行训练。</p><p>  请注意，FedProx和SCAFFOLD中采用的策略也可以被视为克服知识遗忘的方法。然而，我们的方法与这两种算法的主要区别在于FedKL直接约束模型输出，而FedProx和SCAFFOLD专注于约束参数更新。因此，比较基于输出的方法（FedKL）和基于参数的方法（FedProx和SCAFFOLD）在维护知识方面的能力是很有价值的。</p><h2 id="预测的准确性"><a class="header-anchor" href="#预测的准确性">¶</a>预测的准确性</h2><p>  在本节中，我们将探讨我们的方法FedKL的测试准确性和第4.1节中介绍的图像分类数据集的基线。为了进行全面的比较，我们研究了它们在 $100$ 次数据分区下的性能，如表1所示。我们设置 $M_{max} = 2$ 对所有三个数据集模拟高度异构的数据分布。此外，由于存在可能影响性能的重要超参数（FedProx中的 $μ$ 和FedKL中的 $\lambda$），我们分别在两种参数设置下测试了它们的准确性。</p><p><img src="/2023/03/07/knowledge-lock-overcoming-catastrophic-forgetting-in-federated-learning/3.png" alt="$M_{max} = 2$ 下 $100$ 次数据分区的测试准确率（$\%$）报告了不同随机种子 $5$ 次重复实验的平均结果和标准差。每个网络架构的最佳结果以粗体显示。"></p><p>  如表1所述，我们的方法优于Vgg-16[27]和ResNet-18[7]网络架构的基线。值得注意的是，在采用ResNet-18的MNIST上，FedKL与集中训练的准确率差距减小到2.60 $\%$。在CIFAR-10和CIFAR-100上，FedKL的准确率分别超过62 $\%$ 和28 $\%$，显著优于之前的方法。</p><h2 id="Effect-of-Class-Exemplars"><a class="header-anchor" href="#Effect-of-Class-Exemplars">¶</a>Effect of Class Exemplars</h2><p>  进一步，我们探讨了我们提出的部分数据共享算法（见算法2）的效果。为了直接比较，我们使用与第4.2节相同的数据分区设置，即 $k = 100$, $M_{max} = 2$，并为算法2选择的每个类添加 $10$ 个全局共享样本（CIFAR-100为1个）。</p><p><img src="/2023/03/07/knowledge-lock-overcoming-catastrophic-forgetting-in-federated-learning/5.png" alt="表2。采用类样例的测试精度(%)。数据集分为 $100$ 个部分，$M_{max} = 2$，每类共享样本数量MNIST为 $10$ ,CIFAR-10为 $1$ ,CIFAR-100为 $1$ 。我们报告了不同随机种子 $5$ 次重复实验的平均结果和标准差。每个网络架构的最佳结果以粗体显示。"></p><p>  如表2所示，100个共享样本的总数分别使MNIST和CIFAR-10的准确率提高了约1 $\%$ 和3 $\%$ （10个类别中每个类别有10个样本）。此外，还表明样本共享算法在FedKL上的性能优于其他基线方法。以Vgg-16架构的CIFAR-100为例，样本共享在FedKL上提高了2.11 $\%$ 的准确率，而在FedAvg上仅提高了1.23 $\%$。</p><p>  此外，为了充分考察数据共享的效果，我们用不同数量的样本评估了数据共享的性能。如图1所示，部分数据共享显著有利于我们的方法和基线算法，这表明在低隐私敏感性的情况下范例共享的潜力。</p><p><img src="/2023/03/07/knowledge-lock-overcoming-catastrophic-forgetting-in-federated-learning/4.png" alt="图1：每个类在不同数量的共享范例下的性能。"></p><h1>结论</h1><p>  联邦学习的一个主要挑战是克服异构数据分布对模型精度和收敛性的影响。为了解决这一问题，我们提出的FedKL通过引入知识蒸馏技术来维护全局知识。FedKL可以应用于任何CNN技术，以提高其去中心化学习性能。此外，我们还探讨了向FedKL引入共享示例（局部数据的一部分）的好处。综合实验证明了该算法与现有算法相比的有效性。</p>]]></content>
      
      
      <categories>
          
          <category> 科研学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 联邦学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《Overcoming Catastrophic Forgetting with Unlabeled Data in the Wild》</title>
      <link href="/2023/02/11/overcoming-catastrophic-forgetting-with-unlabeled-data-in-the-wild/"/>
      <url>/2023/02/11/overcoming-catastrophic-forgetting-with-unlabeled-data-in-the-wild/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h1>摘要</h1><p>  以连续方式学习任务的能力对人工智能的发展至关重要。一般来说，神经网络无法做到这一点，人们普遍认为灾难性遗忘是连接主义模型不可避免的特征。我们表明，有可能克服这一限制，并训练网络，使其能够在长时间没有经历过的任务上保持专业知识。我们的方法是通过选择性地减缓对这些任务重要的权重的学习来记住旧任务。我们通过解决一组基于MNIST手写数字数据集的分类任务，并通过顺序学习几个Atari 2600游戏，证明了我们的方法是可扩展的和有效的。</p><h1>介绍</h1><p>  深度神经网络(DNNs)在许多机器学习应用中取得了显著的成功，例如分类[10]，生成[29]，物体检测[9]和强化学习[39]。然而，在任务数量不断增长的现实世界中，无法一次性给出所有任务；相反，它可能是一系列的任务。类增量学习[33]的目标是丰富模型处理这种情况的能力，目标是同时做好以前的任务和新的任务。（在类-增量学习中，每个任务中都给出一组类。在评估中，它的目标是对迄今为止学习到的任何类中的数据进行分类，没有任务边界。）特别是最近dnn在学习新任务时容易忘记之前的任务，这种现象被称为灾难性遗忘[7,28]，因此备受关注[7,28]</p><p>  灾难性遗忘的主要原因是有限的可扩展性资源：随着任务数量的增加，以前任务的所有训练数据都无法存储在有限的内存大小中。</p><p>  先前的类增量学习研究主要集中在封闭环境下的学习，即模型在训练过程中只能看到给定的标记训练数据集[3,12,23,24,33]。然而，在现实世界中，我们生活在一个连续的、大量的未标记数据流中，这些数据很容易在动态或短暂的情况下获得，例如，通过社交媒体[26]和网络数据[17]上的数据挖掘。基于此，我们建议利用如此庞大的未标记外部数据流来克服灾难性遗忘。我们注意到，我们在无标记数据上的设置类似于自学[31]，而不是半监督学习，因为我们不假设无标记数据和标记训练数据集之间存在任何相关性。</p><p><b>贡献</b>在新的类增量设置下，我们的贡献有三方面（参见图1概述）：</p><p><img src="/2023/02/11/overcoming-catastrophic-forgetting-with-unlabeled-data-in-the-wild/1.png" alt="图1。我们建议利用大量未标记的数据流进行类增量学习。在每个阶段，应用基于置信度的抽样策略来构建外部数据集。具体而言，部分未标记数据基于前阶段 $\mathcal{P}$ 学习的模型预测进行采样，以缓解灾难性遗忘，部分数据随机采样进行置信度校准。在标记的训练数据集和未标记的外部数据集的组合下，教师模型 $\mathcal{C}$ 首先学习当前任务，然后新模型 $\mathcal{M}$ 通过提取 $\mathcal{P}$、$\mathcal{C}$ 及其集合 $\mathcal{Q}$ 的知识来学习之前和当前的任务。"></p><p>  <b>$\mathcal{A}$</b>：我们提出了一种新的学习目标，称为全局蒸馏，它利用数据有效地蒸馏参考模型的知识。</p><p>  <b>$\mathcal{B}$</b>：我们设计了一个三步学习方案来提高全局蒸馏的有效性：（i）训练一个专门从事当前任务的教师，（ii）通过蒸馏前一个模型的知识来训练一个模型，教师在（i）和他们的集合中学习，以及（iii）调整以避免对当前任务过度拟合。</p><p>  <b>$\mathcal{C}$</b>：我们提出了一种基于置信度的抽样方法，以有效地利用大量未标记的数据流。</p><p>  在贡献<b>$\mathcal{A}$</b>中，全局蒸馏鼓励模型在所有先前的任务上学习知识，而先前的工作只应用了任务的局部蒸馏[3,12,24,33]。特别是，提议的全局蒸馏提取了如何在不同任务中区分类的知识，而局部蒸馏则没有。我们表明，如果一些未标记的外部数据可用，由于全局蒸馏的性能增益是特别显著的。</p><p>  在贡献<b>$\mathcal{B}$</b>中，所提出的学习方案的前两个步骤（i）和（ii）是为了保持之前任务的知识，以及学习当前任务。另一方面，最后一步（iii）的目的是为了避免对当前任务的过度拟合：由于可伸缩性的问题，在训练过程中只保留和重放了之前任务中的一小部分数据[3,30,33]。这不可避免地会导致学习模型预测的偏差，这对当前任务是有利的。为了缓解训练不平衡的问题，我们根据以前和当前任务中的数据统计对模型进行了微调。</p><p>  最后，贡献<b>$\mathcal{C}$</b>的动机来自于直觉，即由于未标记数据的数据分布与前面的任务更相似，它可以防止模型更多地发生灾难性遗忘。由于野外未标记的数据不一定与之前的任务有关，因此还远远不清楚它们是否包含缓解灾难性遗忘的有用信息。因此，我们建议通过原则性抽样策略对外部数据集进行抽样。为了从大量未标记的数据流中对有效的外部数据集进行采样，我们建议通过使用不相关数据作为分布外（out-of-distribution, OOD）（OOD是指数据分布与目前学习到的任务的分布相距甚远）样本来训练一个可信度校准模型[19,20]。我们表明，来自OOD的未标记数据也应该进行采样，以维护模型，使其更具信心校准。</p><p>  实验结果表明，本文提出的方法优于现有方法。特别地，我们表明，当未标记的外部数据可用时，所提出的方法中的性能增益更为显著。例如，在我们在ImageNet[6]上的实验设置下，与最先进的方法(E2E)[3]相比，我们使用外部数据集的方法精度提高了15.8 $\%$，遗忘减少了46.5 $\%$（精度提高了4.8 $\%$，忘记减少了6.0 $\%$）。</p><h1>2 方法</h1><p>  在本节中，我们提出了一种新的类增量学习方法。在2.1节中，我们进一步描述了场景和学习目标。在第2.2节中，我们提出了一个新的学习目标，称为全局蒸馏。在第2.3节中，我们提出了一种基于置信度的抽样策略，从大量未标记的数据流中构建外部数据集。</p><h2 id="2-1-Preliminaries-Class-Incremental-Learning（初步：类增量学习）"><a class="header-anchor" href="#2-1-Preliminaries-Class-Incremental-Learning（初步：类增量学习）">¶</a>2.1 Preliminaries: Class-Incremental Learning（初步：类增量学习）</h2><p>  形式上，设 $(x,y) \in \mathcal{D}$ 是数据集 $\mathcal{D}$ 中的数据 $x$ 及其标签 $y$，设 $\mathcal{T}$ 是将 $x$ 映射到 $y$ 的监督任务。如果 $y$ 在 $\mathcal{T}$ 的范围内，那么 $|\mathcal{T}|$ 是 $\mathcal{T}$ 中的类标签的数量，则表示 $y \in \mathcal{T}$。对于第 $t$ 个任务 $\mathcal{T}_{t}$，设 $\mathcal{D}_{t}$ 为对应的训练数据集，$\mathcal{D}_{t-1}^{cor} \subseteq \mathcal{D}_{t-1} \cup \mathcal{D}_{t-2}^{cor}$ 为一个coreset（Coreset是一个保存在有限内存中的小数据集，用于重放以前的任务。初始时 $\mathcal{D}_{0}^{cor} = \emptyset$ ），其中包含了之前任务：$\mathcal{T}_{1:(t-1)}=\{\mathcal{T}_{1}, \ldots, \mathcal{T}_{t-1}\}$，使得 $\mathcal{D}_{t}^{trn} = \mathcal{D}_{t} \cup \mathcal{D}_{t-1}^{cor}$ 为第 $t$ 阶段可用的全部标记训练数据集。设 $\mathcal{M}_{t}=\{\theta, \phi_{1: t}\}$ 是一个模型的可学习参数集，其中 $\theta$ 和 $\phi_{1: t}=\{\phi_{1}, \ldots, \phi_{t}\}$ 分别指示共享参数和特定于任务的参数。（如果给出了多个特定于任务的参数，那么所有类的logit将被连接起来进行预测，没有任务边界。请注意，任务不一定是分离的，这样一个类就可以出现在多个任务中。）</p><p>  第 $t$ 阶段的目标是训练模型 $\mathcal{M}_{t}$ 执行当前任务 $\mathcal{T}_{t}$ 和之前的任务 $\mathcal{T}_{1:(t−1)}$，没有任务边界，即 $\mathcal{T}_{1:(t)}$ 中的所有类标签在测试时都是候选。为此，从上一阶段转移了一个小的coreset $\mathcal{D}_{t-1}^{cor}$ 和之前的模型 $\mathcal{M}_{t-1}$ 。我们还假设大量未标记的数据流是可访问的，并且对一个基本的外部数据集 $\mathcal{D}_{t}^{ext}$ 进行采样，其中采样方法在2.3节中描述。请注意，我们不假设未标记的数据流与任务之间存在任何相关性。第 $t$ 阶段的结果是可以执行所有观察到的任务 $\mathcal{T}_{1:(t)}$ 的模型 $\mathcal{M}_{t}$ ，以及用于后续阶段学习的核心值 $\mathcal{D}_{t}^{cor}$ 。</p><p>  <b>学习目标</b>当数据集 $\mathcal{D}$ 被标记时，训练分类模型 $\mathcal{M}=\{\theta, \phi\}$ 的标准方法是优化交叉熵损失：<br>$$<br>\mathcal{L}_{\mathrm{cls}}(\theta, \phi ; \mathcal{D})=\frac{1}{|\mathcal{D}|} \sum_{(x, y) \in \mathcal{D}}[-\log p(y \mid x ; \theta, \phi)]<br>$$</p><p>另一方面，如果我们有一个参考模型 $\mathcal{R}=\{\theta^{\mathcal{R}}, \phi^{\mathcal{R}}\}$，数据集 $\mathcal{D}$ 不需要任何标签，因为目标标签是由 $\mathcal{R}$ 给出的：<br>$$<br>\mathcal{L}_{\text {dst}}(\theta, \phi ; \mathcal{R}, \mathcal{D}) =\frac{1}{|\mathcal{D}|} \sum_{x \in \mathcal{D}} \sum_{y \in \mathcal{T}}\left[-p\left(y \mid x ; \theta^{\mathcal{R}}, \phi^{\mathcal{R}}\right) \log p(y \mid x ; \theta, \phi)\right]<br>$$<br>其中概率可以被平滑，以便更好的蒸馏（见[11]或附录）。</p><p>  <b>以前的方法</b>在第 $t$ 阶段，训练模型 $\mathcal{M}_{t}$ 的标准方法是最小化以下分类损失：<br>$$<br>\mathcal{L}_{\mathrm{cls}}\left(\theta, \phi_{1: t} ; \mathcal{D}_{t}^{\operatorname{trn}}\right) \quad\quad\quad\quad(1)<br>$$</p><p>然而，在类增量学习中，由于核心集的大小有限，学习模型会出现灾难性遗忘。为了克服这一点，利用先前的模型 $\mathcal{P}_{t}=\{\theta^{\mathcal{P}}, \phi_{1:(t-1)}^{\mathcal{P}}\} \triangleq \mathcal{M}_{t-1}$ 来生成软标签，这是 $\mathcal{P}_{t}$ 对给定数据的知识[3,12,24,33]:<br>$$<br>\sum_{s=1}^{t-1} \mathcal{L}_{\text {dst }}\left(\theta, \phi_{s} ; \mathcal{P}_{t}, \mathcal{D}_{t}^{\mathrm{trn}}\right) \quad\quad\quad\quad(2)<br>$$<br>其中，该目标与Eq（1）共同优化。我们将这种任务型知识蒸馏称为局部蒸馏（LD），它在每个任务中传递知识。然而，因为它们是以任务明智的方式定义的，所以这个目标忽略了不同任务中类之间的区别。</p><h2 id="2-2-全局蒸馏"><a class="header-anchor" href="#2-2-全局蒸馏">¶</a>2.2 全局蒸馏</h2><p>  基于LD的局限性，我们提出了从全局上提取参考模型的知识。使用参考模型 $\mathcal{P}_{t}$，可以通过最小化以下损失实现知识的全局提炼:<br>$$<br>\mathcal{L}_{\mathrm{dst}}\left(\theta, \phi_{1:(t-1)} ; \mathcal{P}_{t}, \mathcal{D}_{t}^{\mathrm{trn}} \cup \mathcal{D}_{t}^{\text {ext }}\right) \quad\quad\quad\quad(3)<br>$$<br>然而，通过最小化Eq（3）来学习会导致偏差：由于 $\mathcal{P}_{t}$ 没有学会执行当前任务 $\mathcal{T}_{t}$，因此当仅最小化Eq（1）+（3）时，关于当前任务的知识将无法正确地学习，即当前任务的性能将不必要地牺牲。为了弥补这一点，我们引入了另一个教师模型 $\mathcal{C}_{t} = \{\theta^{\mathcal{C}}， \phi_{t}^{\mathcal{C}}\}$，专门用于当前任务 $\mathcal{T}_{t}$：<br>$$<br>\mathcal{L}_{\text {dst }}(\theta, \phi_{t} ; \mathcal{C}_{t}, \mathcal{D}_{t}^{\text {trn }} \cup \mathcal{D}_{t}^{\text {ext }})  \quad\quad\quad\quad(4)<br>$$<br>该模型可以通过最小化标准交叉熵损失来训练：<br>$$<br>\mathcal{L}_{\mathrm{cls}}(\theta^{\mathcal{C}}, \phi_{t}^{\mathcal{C}} ; \mathcal{D}_{t})  \quad\quad\quad\quad(5)<br>$$<br>注意，只使用当前任务 $\mathcal{D}_{t}$ 的数据集，因为 $\mathcal{C}_{t}$ 仅针对当前任务。为了更好地进行外部数据采样，我们在2.3节修改了这一损失。</p><p>  然而，$\mathcal{P}_{t}$ 和 $\mathcal{C}_{t}$ 分别只学会了 $\mathcal{T}_{1:(t−1)}$ 和 $\mathcal{T}_{t}$，从这两个参考模型中提取的知识无法区分 $\mathcal{T}_{1:(t−1)}$ 和 $\mathcal{T}_{t}$。为了补充缺失的知识，我们将 $\mathcal{Q}_{t}$ 定义为 $\mathcal{P}_{t}$ 和  $\mathcal{C}_{t}$ 让：<br>$$<br>p_{\max }=\max _{y} p(y \mid x, \theta^{\mathcal{P}}, \phi_{1:(t-1)}^{\mathcal{P}}),\\<br>y_{\max }=\underset{y}{\arg \max } p(y \mid x, \theta^{\mathcal{P}}, \phi_{1:(t-1)}^{\mathcal{P}}) .<br>$$</p><p>  那么 $\mathcal{Q}_{t}$ 的输出可以定义为：<br>$$<br>p(y \mid x, \theta^{\mathcal{Q}}, \phi_{1: t}^{\mathcal{Q}})= \{ \begin{matrix}<br>p_{\max } &amp; \text { if } y=y_{\max }, \\<br>\frac{1-p_{\max }-\varepsilon}{1-p_{\max }} p\left(y \mid x, \theta^{\mathcal{P}}, \phi_{1:(t-1)}^{\mathcal{P}}\right) &amp; \text { elif } y \in \mathcal{T}_{1:(t-1)}, \\<br>\varepsilon p\left(y \mid x, \theta^{\mathcal{C}}, \phi_{t}^{\mathcal{C}}\right) &amp; \text { elif } y \in \mathcal{T}_{t},<br>\end{matrix}\} \quad\quad\quad\quad(6)<br>$$</p><p>  使得 $\sum_{y} p(y \mid x, \theta^{\mathcal{Q}}, \phi_{1: t}^{\mathcal{Q}})=1$。这里，$\varepsilon$ 调整给定数据是否在 $\mathcal{T}_{1:(t-1)}$ 或 $\mathcal{T}_{t}$ 中的置信度。这个信息基本上是缺失的，但是可以通过假设所有负类的预期预测概率是相同的 $\forall y \neq y_{\max }$ 来计算，即：<br>$$<br>\mathbb{E}_{y} [p_{\varepsilon}\left(y \mid x, \theta^{\mathcal{P}}, \phi_{1:(t-1)}^{\mathcal{P}}\right)]=\mathbb{E}_{y \neq y_{\max }}\left[p_{\varepsilon}\left(y \mid x, \theta^{\mathcal{C}}, \phi_{t}^{\mathcal{C}}\right)\right]:<br>$$<br>$$<br>\varepsilon=\frac{\left(1-p_{\max }\right)\left|\mathcal{T}_t\right|}{\left|\mathcal{T}_{1: t}\right|-1} \quad\quad\quad\quad(7)<br>$$</p><p>  由于集成模型 $\mathcal{Q}_{t}$ 能够执行所有任务，所以所有参数都可以更新：<br>$$<br>\mathcal{L}_{\text {dst }}\left(\theta, \phi_{1: t} ; \mathcal{Q}_t, \mathcal{D}_{t}^{\text {ext }}\right) \quad\quad\quad\quad(8)<br>$$<br>注意，没有使用标记数据集 $\mathcal{D}_{t}^{trn}$，因为它已经在Eq（1）中用于相同范围的类。</p><p>  最后，我们的全局蒸馏（GD）模型通过优化Eq（1）+（3）+（4）+（8）进行学习：<br>$$<br>\begin{aligned}<br>\mathcal{L}_{\mathrm{cls}}\left(\theta, \phi_{1: t} ; \mathcal{D}_{t}^{\mathrm{trn}}\right) &amp; +\mathcal{L}_{\mathrm{dst}}\left(\theta, \phi_{1:(t-1)} ; \mathcal{P}_{t}, \mathcal{D}_{t}^{\mathrm{trn}} \cup \mathcal{D}_{t}^{\text {ext }}\right) \\<br>&amp; +\mathcal{L}_{\text {dst }}\left(\theta, \phi _{t} ; \mathcal{C}_t, \mathcal{D}_{t}^{\mathrm{trn}} \cup \mathcal{D}_{t}^{\text {ext }}\right) \\<br>&amp; +\mathcal{L}_{\text {dst }}\left(\theta, \phi_{1: t} ; \mathcal{Q}_{t}, \mathcal{D}_{t}^{\text {ext }}\right)<br>\end{aligned} \quad\quad\quad\quad(9)<br>$$<br>我们研究表2中每个项的贡献。<br><img src="/2023/02/11/overcoming-catastrophic-forgetting-with-unlabeled-data-in-the-wild/2.png" alt="表2。当任务规模为10时，CIFAR-100上不同参考模型学习模型的比较。“ $\mathcal{P}$ ”、“ $\mathcal{C}$ ”和“ $\mathcal{Q}$ ”分别代表先前的模型、当前任务的教师和它们的集成模型。"><br>  <b>平衡的调整</b>训练数据集中的类标签统计也是在训练过程中学习到的信息。由于之前任务的数据量远远小于当前任务的数据量，因此模型的预测会偏向于当前任务。为了消除偏差，我们在训练后以相同的学习目标进一步微调模型。在微调时，对于每个 $\mathcal{D}$ 和 $\mathcal{T}$ 的损失，我们按以下方式缩放从标记 $\mathcal{k} \in \mathcal{T}$ 的数据计算的梯度：<br>$$<br>w_{\mathcal{D}}^{(k)} \propto \frac{1}{|\{(x, y) \in \mathcal{D} \mid y=k\}|} \quad\quad\quad\quad(10)<br>$$<br>由于缩放梯度相当于多次输入相同的数据，我们将此方法称为数据加权。我们还通过将权重与 $|\mathcal{D}|/|\mathcal{T}|$ 相乘来归一化，这样如果 $\mathcal{D}$ 是平衡的，它们都是一个。</p><p>  我们只对特定任务的参数 $\phi_{1:t}$ 进行数据加权微调，因为所有的训练数据对于表示学习都是同样有用的，即共享参数 $\theta$，而训练分类器时需要去除训练数据集数据分布中的偏差，即 $\phi_{1:t}$ 。平衡微调的效果可以在表4中找到。<br><img src="/2023/02/11/overcoming-catastrophic-forgetting-with-unlabeled-data-in-the-wild/3.png" alt="表4。任务规模为10时不同平衡学习策略在CIFAR-100上的比较。“DW”、“FT-DSet”、“FTDW”分别代表式（10）中对整个训练进行数据加权的训练，对去除当前任务数据平衡的训练数据集进行微调，对数据加权进行微调。"></p><p>  <b>重量损失</b>。我们通过在损失中学习的每个任务的相对大小来平衡每个损失的贡献：对于学习 $\mathcal{T}$ 的每个损失，第 $t$ 阶段的损失权重为：<br>$$<br>w^{L}=\frac{|\mathcal{T}|}{\left|\mathcal{T}_{1: t}\right|} \quad\quad\quad\quad(11)<br>$$<br>我们注意到，损失权重可以作为超参数进行调优，但我们发现这个损失权重通常比其他值执行得更好，因为它遵循测试数据集的统计数据：所有类出现的可能性相同。</p><p>  <b>三步学习算法</b>综上所述，我们的学习策略有三个步骤：训练针对当前任务 $\mathcal{T}_{t}$ 的 $\mathcal{C}_{t}$，通过提取参考模型  $\mathcal{P}_{t}$ 、 $\mathcal{C}_{t}$ 和  $\mathcal{Q}_{t}$ 的知识来训练 $\mathcal{M}_{t}$ ，并通过数据加权对任务特定参数 $\phi_{1:t}$ 进行微调。算法1描述了三步学习方案。<br><img src="/2023/02/11/overcoming-catastrophic-forgetting-with-unlabeled-data-in-the-wild/4.png" alt="算法1"></p><p>  对于核心集管理，我们通过为每个类随机选择数据来构建一个平衡的核心集。我们注意到其他更复杂的选择算法，如羊群[33]，其性能并不明显优于随机选择，这在之前的工作中也有报道[3,42]。</p><h2 id="2-3-外部数据集采样"><a class="header-anchor" href="#2-3-外部数据集采样">¶</a>2.3 外部数据集采样</h2><p>  虽然大量未标记的数据很容易获得，但在使用它们进行知识蒸馏时存在两个问题：（a）在大规模外部数据集上的训练是昂贵的，并且（b）大多数数据不会有帮助，因为它们与模型学习的任务无关。为了克服这些问题，我们建议从大量未标记的数据流中抽取一个用于知识蒸馏的外部数据集。注意，采样的外部数据集不需要额外的永久内存；学后即弃。</p><p>  <b>取样进行置信度校准</b>为了缓解由于训练数据集不平衡而导致的灾难性遗忘，需要对之前任务中预计会出现的外部数据进行采样。由于如果数据很可能在之前的任务中，则期望之前的模型 $\mathcal{P}$ 产生高置信度的输出，因此可以使用 $\mathcal{P}$ 的输出进行抽样。然而，现代dnn是高度过度自信的[8,19]，因此，即使数据不是来自之前的任何任务，使用判别损失学习的模型也会产生高置信度的预测。</p><p>  由于大多数未标记的数据与之前的任何任务都不相关，即它们被认为来自分布外（OOD），因此避免对此类不相关的数据进行过度自信的预测是很重要的。为了实现这一点，模型应该通过学习一定数量的OOD数据以及之前任务的数据来学会自信校准[19,20]。在对OOD数据进行抽样时，我们建议对数据进行随机抽样，而不是依赖于之前模型的置信度，因为OOD在数据空间中广泛分布。这种抽样策略的效果可以在表5中找到。算法2描述了我们的抽样策略。通过验证确定OOD数据的比值（Nprev: NOOD）；详见附录。这种采样算法可能会花费很长时间，但是我们在实验中将未标记数据的检索数量限制为 $1M$，即 $N_{max} = 1M$。<br><img src="/2023/02/11/overcoming-catastrophic-forgetting-with-unlabeled-data-in-the-wild/5.png" alt="表5所示。任务规模为10时CIFAR-100上不同外部数据采样策略的比较。“Prev”列和“OOD”列描述了对前一任务数据和分布外数据的抽样方法，其中“Pred”列和“Random”列分别表示基于前一模型 $\mathcal{P}$ 预测的抽样和随机抽样。特别是，当用“Pred”对OOD进行抽样时，我们对数据进行抽样，使置信度损失 $\mathcal{L}_{cnf}$ 最小化。当只对Prev或OOD进行采样时，将匹配所采样的数据数量，以便公平比较。"></p><p><img src="/2023/02/11/overcoming-catastrophic-forgetting-with-unlabeled-data-in-the-wild/6.png" alt="算法2"></p><p>  <b>抽样置信度校准</b>对于置信度校准，我们考虑以下置信度损失 $\mathcal{L}_{cnf}$ ，使模型对与模型学习的任务不相关的数据产生置信度校准输出：<br>$$<br>\mathcal{L}_{\mathrm{cnf}}(\theta, \phi ; \mathcal{D})=\frac{1}{|\mathcal{D} | \mathcal{T}|} \sum_{x \in \mathcal{D}} \sum_{y \in \mathcal{T}}[-\log p(y \mid x ; \theta, \phi)]<br>$$</p><p>在3步学习中，只有训练 $\mathcal{C}_{t}$ 的第一步没有参考模型，所以它应该在信心损失的情况下学习。对于 $\mathcal{C}_{t}$ ，如果 $y \notin \mathcal{T}_{t}$，$(x,y)$ 来自OOD。即，通过优化前面任务 $\mathcal{D}^{cor}_{t−1}$ 和外部数据集  $\mathcal{D}^{ext}_{t}$ 的coreset下的置信度损失，模型学会了对OOD数据产生低置信度的预测，即在类标签上均匀分布的概率。因此，$\mathcal{C}_{t}$ 通过优化以下内容进行学习：<br>$$<br>\mathcal{L}_{\mathrm{cls}}\left(\theta^{\mathcal{C}}, \phi_{t}^{\mathcal{C}} ; \mathcal{D}_t\right)+\mathcal{L}_{\mathrm{cnf}}\left(\theta^{\mathcal{C}}, \phi_{t}^{\mathcal{C}} ; \mathcal{D}_{t-1}^{\text {cor }} \cup \mathcal{D}_{t}^{\text {ext }}\right) \quad\quad\quad\quad(12)<br>$$</p><p>请注意，模型 $\mathcal{M}_{t}$ 不需要额外的置信度校准，因为之前的模型 $\mathcal{P}_{t}$ 预计将在前一阶段进行置信度校准。因此，参考模型的信心校准输出被提炼为模型 $\mathcal{C}_{t}$ 。信心损失的影响如表3所示。<br><img src="/2023/02/11/overcoming-catastrophic-forgetting-with-unlabeled-data-in-the-wild/7.png" alt="表3。在CIFAR-100上，当任务规模为10时，与不同教师学习的当前任务 $\mathcal{C}$ 的模型的比较。对于“cls”，不训练 $\mathcal{C}$ ，而是通过直接优化 $\mathcal{C}$ 的学习目标进行学习。模型使用为“dst”提出的3步学习方法进行学习。信心损失被添加到 $\mathcal{C}$ 的“cnf”的学习目标中。我们在这个实验中没有使用 $\mathcal{Q}$ ，因为“cls”没有显式的 $\mathcal{C}$ 。"></p><h1>相关工作</h1><p>  <b>不断的终身学习</b>最近的许多研究都用不同的假设来解决灾难性遗忘的问题。广义上讲，有三种不同类型的工作[41]：一种是类增量学习[3,33,42]，类标签的数量不断增长。另一种是任务增量学习[12,24]，即假定任务之间的边界是明确的，并且给出了被测任务的信息。（类增量学习和任务增量学习的主要区别在于模型分别具有单头输出层和多头输出层。）最后一种可以被视为数据增量学习，即所有任务的类标签或动作集都是相同的[16,35,36]。</p><p>  这些工作可以概括为持续学习，最近关于持续学习的工作研究了两种克服灾难性遗忘的方法：基于模型的和基于数据的。基于模型的方法[1,4,14,16,21,25,27,30,34,35,36,37,43,45]通过惩罚对前一任务至关重要的参数的变化来保持对前一任务的了解，即，更新的参数被约束在原始值附近，并根据前一任务参数的重要性按比例缩小更新。然而，由于dnn有许多局部最优值，对于以前的和新的任务都会有更好的局部最优值，这是基于模型的方法无法找到的</p><p>另一方面，基于数据的方法[3,12,13,24,33]通过知识蒸馏[11]保留了之前任务的知识，使之前模型和新模型中潜在空间流形之间的距离最小。与基于模型的方法相比，它们需要提供数据来获得潜在空间的特征。因此，知识蒸馏所保留的知识量取决于前一阶段学习前一任务所使用的数据分布与后一阶段知识蒸馏所使用的数据分布的相似程度。为了保证有一定数量的相似数据，一些先前的工作[3,30,33]预留了少量的内存来保留一个coreset，而另一些工作[22,32,38,41,42]则训练了一个生成模型，并在训练新模型时回放生成的数据。请注意，基于模型的方法和基于数据的方法在大多数情况下是正交的，因此它们可以组合起来以获得更好的性能[15]。</p><p>  <b>以前作品中的知识提炼</b>我们提出的方法是一种基于数据的方法，但它与之前的工作[3,12,24,33]不同，因为他们的模型通常使用Eq（2）中的任务局部蒸馏损失进行学习。我们强调局部蒸馏只保留了之前每个任务中的知识，而全局蒸馏对所有任务进行了知识</p><p>  与我们的三步学习类似，[36]和[12]采用了两位老师一起学习的思路。然而，他们保持之前任务知识的策略是不同的:[36]应用了基于模型的方法，[12]则为任务增量学习提取了任务知识。</p><p>  另一方面，[3]也有类似的微调，但他们通过丢弃当前任务的大部分数据并更新整个网络来构建一个平衡的数据集。然而，这种欠采样牺牲了频繁类的多样性，从而降低了性能。过采样可能会解决这个问题，但它会使训练不可扩展：过采样数据集的大小与到目前为止学习的任务数量成正比。相反，我们建议应用数据加权。</p><p>  <b>可伸缩性</b>关于持续学习的早期工作是不可扩展的，因为它们保留了所有先前的模型[2,16,24,35,43]。然而，最近的研究通过最小化任务特定参数的数量来考虑可伸缩性[33,36]。此外，基于数据的方法需要保留一个核心重置或生成模型来重放以前的任务。我们的方法是一种基于数据的方法，但它没有可伸缩性问题，因为我们利用了从大量未标记数据流中采样的外部数据集。我们注意到，与coreset不同，我们的外部数据集不需要永久内存;学后即弃。</p><h1>4 实验</h1><h2 id="4-1-实验设置"><a class="header-anchor" href="#4-1-实验设置">¶</a>4.1 实验设置</h2><p>  <b>比较算法</b>为了提供性能的上限，我们比较了一种oracle方法，该方法通过优化Eq（1）来学习，同时存储之前任务的所有训练数据，并在训练期间回放它们。此外，作为基线，我们提供了在没有知识蒸馏的情况下学习的模型的性能。在之前的研究中，比较了三种最先进的方法:不遗忘学习（LwF）[24]、蒸馏与回顾（DR）[12]和端到端增量学习（E2E）[3]。为了比较公平，我们将LwF和DR用于原本在任务增量学习设置中评估的类增量设置：具体来说，我们扩大了分类损失的范围，即优化了Eq（1）+（2）和Eq（1）+（2）+（4）的复制。</p><p>  我们没有比较基于模型的方法，因为已知数据库方法在类增量学习中优于它们[22,41]，并且它们与基于数据的方法是正交的，因此它们可能与我们的方法结合以获得更好的性能[15]。</p><p>  <b>数据集</b>我们在CIFAR100[18]和ImageNet ILSVRC 2012[6]上评估比较方法，其中所有图像都被下采样到32×32[5]。对于CIFAR-100，类似于之前的工作[3,33]，我们随机统一地对类进行洗牌，并对类进行拆分，以构建一个任务序列。对于ImageNet，我们首先在每次试验中为100个随机选择的类采样500张图像，然后对类进行拆分。为了评估在有大量未标记数据流的环境下的比较方法，我们采用了两个大型数据集：TinyImages数据集[40]和整个ImageNet 2011数据集，其中包含 $80M$ 张图像。CIFAR-100和ILSVRC 2012中出现的类被排除在外，以避免任何潜在的优势。在每个阶段，我们的采样算法从它们中均匀随机地获取未标记的数据，形成一个外部数据集，直到检索到的样本数量为 $1M$。</p><p>  在前面的工作之后，我们将类分为5、10和20个类，这样分别有20、10和5个任务。对于每个任务大小，我们用不同的类顺序（在ImageNet的情况下是不同的类集）对比较的方法进行十次评估，并报告性能的平均值和标准差。</p><p>  <b>评价指标</b>我们在两个指标上报告了比较方法的性能：平均增量精度（ACC）和平均遗忘（FGT）。为了简单起见，我们假设所有类的测试数据的数量是相同的。对于来自第 $r$ 个任务 $(x,y) \in \mathcal{D}_{r}^{test}$ 的测试数据，令 $\hat{y}\left(x ; \mathcal{M}_{s}\right)$ 为第 $s$ 个模型预测的标签，使得<br>$$<br>A_{r, s}=\frac{1}{\left|\mathcal{D}_{r}^{\text {test }}\right|} \sum_{(x, y) \in \mathcal{D}_{r}^{\text {test }}} \mathbb{I}\left(\hat{y}\left(x ; \mathcal{M}_{s}\right)=y\right)<br>$$<br>测量第 $s$ 个模型在第 $r$ 个任务时的精度，其中 $s \ge r$。注意，预测是没有任务边界的：例如，在第 $t$ 阶段，随机猜测的期望精度为 $1/|\mathcal{T}_{1:t}|$ ，而不是 $1/|\mathcal{T}_{r}|$。在第 $t$ 阶段，ACC被定义为：<br>$$<br>\mathrm{ACC}=\frac{1}{t-1} \sum_{s=2}^t \sum_{r=1}^s \frac{\left|\mathcal{T}_{r}\right|}{\left|\mathcal{T}_{1: s}\right|} A_{r, s}<br>$$<br>请注意，第一阶段的性能没有被考虑，因为它不是类增量学习。ACC直接衡量整体性能，FGT通过平均性能衰减来衡量灾难性遗忘的数量：<br>$$<br>\mathrm{FGT}=\frac{1}{t-1} \sum_{s=2}^t \sum_{r=1}^{s-1} \frac{\left|\mathcal{T}_{r}\right|}{\left|\mathcal{T}_{1: s}\right|}\left(A_{r, r}-A_{r, s}\right)<br>$$<br>也就是负的后向传输[25]。注意，FGT越小越好，这意味着模型更少忘记之前的任务。</p><p>  <b>Hyperparameters超参数</b>所有比较模型的主干都是16层的宽残差网络[44]，宽度因子为2（WRN-16-2）， dropout率为0.3。请注意，这与ResNet-32[10]具有相当的性能。最后一个完全连接的层被认为是特定于任务的层，每当有一个带有新类的任务进来时，该层被扩展以生成对新类的预测。与共享层相比，任务特定层的参数数量较少（WRN-16-2中最多约为2 $\%$ ）。所有方法都使用相同大小的coreset，即2000。为了可伸缩性，采样的外部数据集的大小设置为标记数据集的大小，即算法2中的 $N_{D} = |D_{t}^{rnt}|$ 。为了进行验证，使用了一个ImageNet的分割，该分割对其他九个试验是独占的。平滑软最大概率[11]的温度对于 $\mathcal{P}$ 和 $\mathcal{C}$ 的蒸馏设置为2，对于 $\mathcal{Q}$ 设置为1。详见附录。</p><h2 id="4-2评估"><a class="header-anchor" href="#4-2评估">¶</a>4.2评估</h2><p>  <b>方法比较</b>表1和图2比较了我们提出的方法与最先进的方法。首先，即使无法访问未标记的数据，我们的方法也优于最先进的方法，这表明了所提出的3步学习方案的有效性。具体来说，除了损失函数的差异，DR没有平衡的微调，E2E缺少当前任务 $\mathcal{C}_{t}$ 的老师，用小数据集对整个网络进行微调，LwF既没有 $\mathcal{C}_{t}$ 也没有微调。与目前最先进的E2E方法相比，在任务大小为5的情况下，我们的方法在ImageNet上的ACC提高了 $4.8\%$，FGT提高了 $6.0\%$<br><img src="/2023/02/11/overcoming-catastrophic-forgetting-with-unlabeled-data-in-the-wild/8.png" alt="表1。CIFAR-100与ImageNet方法的比较。我们报告了10个CIFAR-100试验和9个ImageNet试验使用不同随机种子的均值和标准差。“↑（↓）”表示数值越高（越低）越好。"><br><img src="/2023/02/11/overcoming-catastrophic-forgetting-with-unlabeled-data-in-the-wild/9.png" alt="图2。CIFAR-100实验结果。（a，b）箭头分别表示使用无标记数据学习的平均增量精度（ACC）和平均遗忘（FGT）的性能增益。（c，d）当任务规模为10时，ACC和FGT随训练类数的变化曲线。我们报告10个试验的平均表现。"></p><p>  另一方面，如图2（a）-2（b）所示，使用未标记的外部数据集学习可以持续地提高比较方法的性能，但在GD中这种改进更为显著。例如，在任务大小为5的ImageNet的情况下，通过使用外部数据集学习，E2E提高了 $3.2\%$ 的ACC，而GD提高了 $10.5\%$。此外，FGT方面的相对性能增益更为显著：E2E健忘 $1.1\%$，而GD健忘 $43.1\%$。总体而言，通过我们提出的学习方案和与外部数据集的知识蒸馏，GD的ACC比E2E提高了 $15.8\%$，FGT提高了 $46.5\%$。</p><p>  <b>参考模型的效果</b>表2显示了不同参考模型的消融研究。如2.2节所述，由于之前的模型 $\mathcal{P}$ 不知道当前的任务，引入 $\mathcal{C}$ 的补偿提高了整体性能。另一方面， $\mathcal{Q}$ 并没有表现出比 $\mathcal{P}$ 和 $\mathcal{C}$ 的组合更好的ACC，这可能是因为，在构建 $\mathcal{Q}$ 的输出时， $\mathcal{P}$ 和 $\mathcal{C}$ 的输出的集成是基于一个假设，而这个假设并不总是正确的。<br><img src="/2023/02/11/overcoming-catastrophic-forgetting-with-unlabeled-data-in-the-wild/2.png" alt="表2。当任务规模为10时，CIFAR-100上不同参考模型学习模型的比较。“ $\mathcal{P}$ ”、“ $\mathcal{C}$ ”和“ $\mathcal{Q}$ ”分别代表先前的模型、当前任务的教师和它们的集成模型。"></p><p>然而，来自 $\mathcal{Q}$ 的知识是有用的，因此所有三个参考模型的组合显示出最佳性能</p><p>  <b>Effect of the teacher for the current task  $\mathcal{C}$</b>表3比较了不同教师对当前任务 $\mathcal{C}_{t}$ 学习的模型。除了没有 $\mathcal{C}_{t}$ 的baseline，我们还比较了Eq（5）或（12）中直接优化 $\mathcal{C}_{t}$ 的学习目标的模型，即优化损失时，模型使用硬标签学习而不是软标签学习。注意，为蒸馏引入单独的模型 $\mathcal{C}$ 是有益的，因为 $\mathcal{C}$ 可以更好地学习当前任务的知识，而不会受到其他分类任务的干扰。通过优化置信度损失来学习可以提高性能，因为如2.3节所述，置信度校准模型对外部数据进行了更好的采样。<br><img src="/2023/02/11/overcoming-catastrophic-forgetting-with-unlabeled-data-in-the-wild/7.png" alt="表3。在CIFAR-100上，当任务规模为10时，与不同教师学习的当前任务 $\mathcal{C}$ 的模型的比较。对于“cls”，不训练 $\mathcal{C}$ ，而是通过直接优化 $\mathcal{C}$ 的学习目标进行学习。模型使用为“dst”提出的3步学习方法进行学习。信心损失被添加到 $\mathcal{C}$ 的“cnf”的学习目标中。我们在这个实验中没有使用 $\mathcal{Q}$ ，因为“cls”没有显式的 $\mathcal{C}$ 。"></p><p>  <b>平衡微调的效果</b>表4显示了平衡学习的效果。首先，平衡的学习策略总体上提高了FGT。如果跳过3步学习中的微调，但在主训练（DW）中应用Eq（10）中的数据加权，则模型显示出比在特定任务参数（FT-DW）上进行平衡微调更高的FGT，如2.2节所述。注意，数据加权（FTDW）比去掉当前任务的数据来构造[3]中提出的小型平衡数据集（FT-DSet）更好，因为所有的训练数据都是有用的。<br><img src="/2023/02/11/overcoming-catastrophic-forgetting-with-unlabeled-data-in-the-wild/3.png" alt="表4。任务规模为10时不同平衡学习策略在CIFAR-100上的比较。“DW”、“FT-DSet”、“FTDW”分别代表式（10）中对整个训练进行数据加权的训练，对去除当前任务数据平衡的训练数据集进行微调，对数据加权进行微调。"></p><p>  <b>外部数据抽样的影响</b>表5比较了不同的外部数据采样策略。未标记数据在所有情况下都是有益的，但不同采样策略的性能增益是不同的。首先，观察到随机采样的数据是有用的，因为它们的预测分布将是多样化的，这样有助于学习参考模型的多样化知识，这使得模型具有信心校准。然而，随机抽样策略虽然比基于之前模型 $\mathcal{P}$ 预测的抽样具有更高的ACC，但也表现出较高的FGT。这意味着基于 $\mathcal{P}$ 预测采样的未标记数据更能防止模型的灾难性遗忘。如2.3节所述，我们提出的抽样策略中，上述两种策略的组合表现最佳。最后，基于 $\mathcal{P}$ 的预测对OOD数据进行采样是没有好处的，因为“最有可能来自OOD的数据”是没有用的。基于 $\mathcal{P}$ 预测采样的OOD数据具有近似均匀的预测分布，是局部分布的。而OOD的概念是模型学习到的数据分布的一种补充集。因此，在我们的例子中，为了更好地区分OOD，模型应该使用广泛分布在前面任务数据分布之外的数据进行学习。</p><h1>5 结论</h1><p>  我们建议利用大量未标记的数据流进行类增量学习。所提出的全局精馏旨在保持参考模型的知识无任务边界，从而实现更好的知识精馏。我们的3步学习方案有效地利用了基于置信度的抽样策略从未标记数据流中采样的外部数据集。</p><h1>附录</h1><h2 id="A-全局蒸馏示意图"><a class="header-anchor" href="#A-全局蒸馏示意图">¶</a>A.全局蒸馏示意图</h2><p><img src="/2023/02/11/overcoming-catastrophic-forgetting-with-unlabeled-data-in-the-wild/10.png" alt="图A1. 模型 $\mathcal{M}$ 如何使用全局蒸馏（GD）学习的说明。对于GD，使用了三个参考模型： $\mathcal{P}$ 是之前的模型， $\mathcal{C}$ 是当前任务的老师， $\mathcal{Q}$ 是它们的集合。"></p><h2 id="B-实验设置细节"><a class="header-anchor" href="#B-实验设置细节">¶</a>B.实验设置细节</h2><p>  <b>超参数.</b>我们使用小批量训练，每次训练的批次大小为128个，超过200个epoch，以确保收敛。初始学习率为0.1，在没有微调的情况下，在120、160、180个周期后衰减0.1。当应用微调时，首先对模型进行180个epoch的训练，学习率在120、160、170个epoch后衰减，然后对20个epoch进行微调，其中学习率从0.01开始，在10、15个epoch后衰减0.1。我们注意到，即使在对某些方法的整个网络进行微调时，20个epoch也足以实现收敛。我们用动量为0.9,L2权值衰减为0.0005的随机梯度来更新模型参数。coreset的大小设置为2000。由于可伸缩性问题，采样的外部数据集的大小被设置为标记数据集的大小。抽样中OOD数据的比例是通过对ImageNet的分割进行验证来确定的，该比例为0.7。在所有实验中，从 $\mathcal{P}$ 和 $\mathcal{C}$ 中蒸馏时平滑软最大概率的温度设置为2，从 $\mathcal{Q}$ 中蒸馏时为1。为了更具体地说明缩放概率的方法，设 $z=\{z_{y} \mid y \in \mathcal{T}\}=\mathcal{M}(x ; \theta, \phi)$ 是输出的集合（或对数）。然后，在温度为 $\gamma$ 的情况下，概率计算如下：<br>$$<br>p(y=k \mid x, \theta, \phi)=\frac{\exp (z_{k} / \gamma)}{\sum_{y^{\prime} \in \mathcal{T}} \exp \left(z_{y^{\prime}} / \gamma\right)}<br>$$</p><p>  <b>方法的可伸缩性</b>。我们注意到，所有比较的方法都是可扩展的，它们是在公平的条件下进行比较的。我们没有将生成式重放方法与我们的方法进行比较，因为众所周知，coreset方法在可伸缩设置中的类增量学习方面优于它们：特别是，据报道，在ciremote -100等自然图像数据集上，生成式模型的持续学习是一个具有挑战性的问题[22,42]。</p><h2 id="C-更多实验结果"><a class="header-anchor" href="#C-更多实验结果">¶</a>C.更多实验结果</h2><h3 id="C1-更多消融实验"><a class="header-anchor" href="#C1-更多消融实验">¶</a>C1.更多消融实验</h3><p>  <b>OOD比率的影响</b>。我们研究了可能存在于先前任务中的采样数据与OOD数据之间的比率的影响。如图C.1所示，不同数据集的最佳OOD比各不相同，但都高于0.5:具体来说，当CIFAR-100的OOD比为0.8,ImageNet的OOD比为0.7时，ACC达到最佳。另一方面，FGT的最佳OOD比是不同的:具体来说，当CIFAR-100上的OOD比为0.2,ImageNet上的OOD比为0.5时，FGT达到最佳。<br><img src="/2023/02/11/overcoming-catastrophic-forgetting-with-unlabeled-data-in-the-wild/11.png" alt="图C.1。任务规模为10时，CIFAR-100和ImageNet上的实验结果。我们报告了10个CIFAR-100试验和9个ImageNet试验中平均OOD比的ACC和FGT。"></p><p>  <b>训练数据与未标记的外部数据之间相关性的影响</b>。到目前为止，我们没有假设训练数据和外部数据之间有任何相关性。但是，在本实验中，我们基于ImageNet类标签之间的上下名关系来控制它们之间的相关性。具体来说，我们首先计算ImageNet ILSVRC 2012训练数据集中1k个类与整个ImageNet 2011数据集中其他 $21k$ 个类之间的层次距离（层次结构中类之间最短路径的长度）。注意，层次距离可以被认为是类之间的语义差异。然后，我们根据层次距离划分 $21k$ 类，这样每个分割至少有 $1M$ 图像用于模拟未标记的数据流。如图C.2所示，性能与语义相似度成正比，而语义相似度与层次距离成反比。然而，即使在最坏的情况下，未标记的数据也是有益的。<br><img src="/2023/02/11/overcoming-catastrophic-forgetting-with-unlabeled-data-in-the-wild/12.png" alt="图C.2。任务大小为10时ImageNet上的实验结果。我们报告了ACC和FGT关于在九个试验中平均的训练数据集和未标记数据流之间的层次距离。"></p><h3 id><a class="header-anchor" href="#">¶</a></h3><p><img src="/2023/02/11/overcoming-catastrophic-forgetting-with-unlabeled-data-in-the-wild/13.png" alt="表C.1。CIFAR-100与ImageNet方法的比较。我们报告了10个CIFAR-100试验和9个ImageNet试验使用不同随机种子的均值和标准差。“↑（↓）”表示数值越高(越低)越好。"></p><p><img src="/2023/02/11/overcoming-catastrophic-forgetting-with-unlabeled-data-in-the-wild/14.png" alt="图C.3。ImageNet实验结果。箭头分别显示了使用未标记数据学习ACC和FGT的性能增益。我们报告9个试验的平均表现。"></p><p><img src="/2023/02/11/overcoming-catastrophic-forgetting-with-unlabeled-data-in-the-wild/15.png" alt="图C.4。任务规模为5时CIFAR-100的实验结果。我们报告ACC和FGT的训练班数平均超过10次试验。"></p><p><img src="/2023/02/11/overcoming-catastrophic-forgetting-with-unlabeled-data-in-the-wild/16.png" alt="图C.5。任务规模为20时CIFAR-100的实验结果。我们报告ACC和FGT的训练班数平均超过10次试验。"></p><p><img src="/2023/02/11/overcoming-catastrophic-forgetting-with-unlabeled-data-in-the-wild/17.png" alt="图C.6。任务大小为5时ImageNet上的实验结果。我们报告了ACC和FGT的训练班数平均超过9次试验。"></p><p><img src="/2023/02/11/overcoming-catastrophic-forgetting-with-unlabeled-data-in-the-wild/18.png" alt="图C.7。任务大小为10时ImageNet上的实验结果。我们报告了ACC和FGT的训练班数平均超过9次试验。"></p><p><img src="/2023/02/11/overcoming-catastrophic-forgetting-with-unlabeled-data-in-the-wild/19.png" alt="图C.8。任务规模为20时ImageNet上的实验结果。我们报告了ACC和FGT的训练班数平均超过9次试验。"></p>]]></content>
      
      
      <categories>
          
          <category> 科研学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 增量学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《Overcoming catastrophic forgetting in neural networks》</title>
      <link href="/2022/12/27/overcoming-catastrophic-forgetting-in-neural-networks/"/>
      <url>/2022/12/27/overcoming-catastrophic-forgetting-in-neural-networks/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h1>摘要</h1><p>  以连续方式学习任务的能力对人工智能的发展至关重要。一般来说，神经网络无法做到这一点，人们普遍认为灾难性遗忘是连接主义模型不可避免的特征。我们表明，有可能克服这一限制，并训练网络，使其能够在长时间没有经历过的任务上保持专业知识。我们的方法是通过选择性地减缓对这些任务重要的权重的学习来记住旧任务。我们通过解决一组基于MNIST手写数字数据集的分类任务，并通过顺序学习几个Atari 2600游戏，证明了我们的方法是可扩展的和有效的。</p><h1>介绍</h1><p>  实现人工通用智能需要智能体能够学习和记住许多不同的任务Legg和Hutter[2007]。这在现实环境中尤其困难:任务的顺序可能没有明确的标记，任务可能不可预测地切换，任何单独的任务可能不会在很长时间间隔内重复出现。因此，重要的是，智能代理必须展示出持续学习的能力:即学习连续任务而不忘记如何执行先前训练过的任务的能力</p><p>  持续学习对人工神经网络提出了特别的挑战，因为之前学习过的任务(例如任务A)的知识会随着与当前任务（例如任务B）相关的信息的加入而突然丢失。这种现象被称为灾难性遗忘，特别是当网络在多个任务上进行连续训练时，因为网络中对任务A重要的权重被改变以满足任务b的目标。而机器学习特别是深度神经网络的最近进展在各个领域都取得了令人印象深刻的性能提升（例如[Krizhevsky等人，2012,LeCun等人，2015]），但在实现持续学习方面进展甚微。目前的方法通常确保来自所有任务的数据在训练期间同时可用。通过在学习过程中交错来自多个任务的数据，不会发生遗忘，因为网络的权重可以联合优化所有任务的性能。在这种机制中（通常被称为多任务学习范式），深度学习技术已被用于训练单个代理，这些代理可以成功地玩多个雅达利游戏[Rusu等人，2015,Parisotto等人，2015]。如果任务是按顺序呈现的，那么多任务学习只能在数据被情景记忆系统记录并在训练过程中回放到网络时使用。这种方法（通常称为系统级整合[McClelland et al.， 1995]）对于学习大量任务是不切实际的，因为在我们的设置中，它需要存储和回放的内存数量与任务数量成正比。缺少算法arXiv:1612.00796v2 [cs。因此，支持持续学习仍然是人工通用智能发展的关键障碍。</p><p>  与人工神经网络形成鲜明对比的是，人类和其他动物似乎能够以持续的方式学习[chichon和Gan, 2015]。最近的证据表明，哺乳动物的大脑可能通过在新皮层回路中保护之前获得的知识来避免灾难性遗忘[chichon和Gan, 2015, Hayashi-Takagi等人，2015,Y ang等人，2009,2014]。当老鼠获得一项新技能时，一定比例的兴奋性突触会增强;这表现为单个神经元树突棘体积的增加[Yang et al.， 2009]。关键是，尽管随后学习了其他任务，但这些扩大的树突棘仍然存在，这解释了几个月后表现的保留[Y ang等人，2009]。当这些棘被选择性地“擦除”时，相应的技能就被遗忘了[Hayashi-Takagi等人，2015,chicon和Gan, 2015]。这为支持保护这些强化突触的神经机制对保持任务表现至关重要提供了因果证据。总之，这些实验发现——连同神经生物学模型[Fusi等人，2005年，Benna和Fusi, 2016年]——表明哺乳动物新皮层的持续学习依赖于特定任务的突触巩固过程，关于如何执行先前获得的任务的知识持久地编码在突触的比例中，这些突触的可塑性较弱，因此在长时间尺度上稳定</p><p>  在这项工作中，我们证明了任务特定的突触巩固为人工智能的持续学习问题提供了一种新的解决方案。我们开发了一种类似于人工神经网络突触巩固的算法，我们称之为弹性权重巩固（简称EWC）。该算法根据它们对先前看到的任务的重要性，减慢了对某些权重的学习速度。我们展示了如何在监督学习和强化学习问题中使用EWC来连续训练几个任务，而不忘记旧的任务，这与之前的深度学习技术形成了鲜明的对比。</p><h1>EWC</h1><p>  在大脑中，突触巩固通过降低突触的可塑性来实现持续学习，而突触对以前学习的任务至关重要。我们实现了一种在人工神经网络中执行类似操作的算法，通过约束重要参数以保持接近其旧值。在本节中，我们将解释为什么我们希望在旧任务的附近找到一个新任务的解决方案，我们如何实现约束，最后我们如何确定哪些参数是重要的</p><p>  深度神经网络由多层线性投影和元素非线性组成。学习任务包括调整线性投影的权重和偏差 $\theta$，以优化性能。 $\theta$ 的许多配置将导致相同的性能[Nielsen, 1989, Sussmann, 1992]；这与EWC相关:过度参数化使得有可能存在一个任务 $B$ 的解 $\theta ^{*}_{B}$，它接近于任务 $A$ 之前找到的解 $\theta ^{*}_{A}$ 。因此，当学习任务 $B$ 时，EWC通过约束参数保持在以 $\theta ^{*}_{A}$ 为中心的任务A的低误差区域来保护任务 $A$ 中的性能，如图1所示。这个约束是作为一个二次惩罚来实现的，因此可以想象成一个弹簧，将参数锚定在前面的解决方案上，因此得名弹性。重要的是，对于所有参数，弹簧的刚度不应该是相同的；相反，对于任务A中对性能最重要的那些参数，它应该更大。</p><p><img src="/2022/12/27/overcoming-catastrophic-forgetting-in-neural-networks/1.png" alt="图1：弹性权重巩固（EWC）确保在任务B上训练时记住任务A。训练轨迹在示意图参数空间中表示，参数区域导致任务A（灰色）和任务B（奶油色）的良好表现。在学习了第一个任务后，参数处于 $\theta ^{*}_{A}$ 。如果我们仅根据任务B（蓝色箭头）采取梯度步骤，我们将最大限度地减少任务B的损失，但会破坏我们在任务A中所学到的东西。另一方面，如果我们用相同的系数约束每个权重（绿色箭头），所施加的限制太严格了，我们只能记住任务A，而不学习任务B。通过显式计算任务a的权重有多重要，为任务B找到一个解决方案，而不会对任务a造成重大损失（红色箭头）。"></p><p>为了证明这种约束的选择是正确的，并定义哪个权重对一个任务是最重要的，从概率的角度考虑神经网络训练是有用的。从这个角度来看，优化参数相当于在给定一些数据 $\mathcal{D}$ 的情况下找到它们的最可能值。我们可以用贝叶斯规则从参数 $p(\theta)$ 的先验概率和数据 $p(\mathcal{D} \mid \theta)$ 的概率中计算出这个条件概率 $p(\theta \mid \mathcal{D})$ :<br>$$<br>\log p(\theta \mid \mathcal{D})=\log p(\mathcal{D} \mid \theta)+\log p(\theta)-\log p(\mathcal{D}) \quad\quad\quad\quad(1)<br>$$</p><p>  请注意，给定参数 $\log p(\mathcal{D} \mid \theta)$ 的数据的对数概率仅仅是手头问题的损失函数 $-\mathcal{L}(\theta)$ 的负数。假设数据被分成两个独立的部分，一个定义任务 $A(\mathcal{D}_{A})$ ，另一个定义任务 $B(\mathcal{D}_{B})$ 。那么，我们可以重新排列方程1：<br>$$<br>\log p(\theta \mid \mathcal{D})=\log p\left(\mathcal{D}_{B} \mid \theta\right)+\log p\left(\theta \mid \mathcal{D}_{A}\right)-\log p\left(\mathcal{D}_{B}\right) \quad\quad\quad\quad(2)<br>$$</p><p>  请注意，左边仍然描述了给定整个数据集的参数的后验概率，而右边只依赖于任务B $\log p\left(\mathcal{D}_{B} \mid \theta\right)+$ 的损失函数。</p><p>  因此，关于任务A的所有信息都必须被吸收到后态分布 $p\left(\theta \mid \mathcal{D}_{A}\right)$ 中。这个后验概率必须包含关于哪些参数对任务 $A$ 重要的信息，因此是实现EWC的关键。真正的后经验概率是难以处理的，因此，在Mackay [Mackay, 1992]的拉普拉斯近似工作之后，我们将后经验近似为一个高斯分布，其平均值由参数 $\theta ^{*}_{a}$ 给出，对角精度由Fisher信息矩阵 $F$ 的对角给出。$F$ 有三个关键性质[Pascanu和Bengio, 2013]:（a）它等价于损失的二阶导数接近最小值，（b）它可以仅从一阶导数计算，因此即使对于大型模型也易于计算，（c）它保证是正半定的。注意，这种方法类似于期望传播，其中每个子任务都被视为后验因子[Eskin et al.， 2004]。根据这个近似，我们在EWC中最小化的函数 $\mathcal{L}$ 为：<br>$$<br>\mathcal{L}(\theta)=\mathcal{L}_{B}(\theta)+\sum_{i} \frac{\lambda}{2} F_{i}(\theta_{i}-\theta_{A, i}^{*})^{2}<br>$$<br>其中 $\mathcal{L}_{B}(\theta)$ 仅是任务 $B$ 的损失，$\lambda$ 设置旧任务与新任务相比的重要性，$i$ 标记每个参数</p><p>  当移动到第三个任务任务 $C$ 时，EWC将试图使网络参数接近任务 $A$ 和任务 $B$ 的学习参数。这可以通过两个单独的惩罚来强制执行，也可以通过注意两个二次惩罚的和本身就是二次惩罚来强制执行。</p><h2 id="EWC允许在监督学习环境下进行持续学习"><a class="header-anchor" href="#EWC允许在监督学习环境下进行持续学习">¶</a>EWC允许在监督学习环境下进行持续学习</h2><p>  我们首先解决弹性权值巩固是否能让深度神经网络在不发生灾难性遗忘的情况下学习一组复杂任务的问题。特别是，我们训练了一个完全连接的多层神经网络，按顺序执行几个监督学习任务。在每个任务中，我们以传统的方式训练神经网络，即通过洗牌数据和小批量处理。然而，在每个任务上进行固定数量的训练后，我们不允许在该任务的数据集上进行进一步的训练</p><p>  我们从MNIST [LeCun等人，1998年]数据集中的手写数字分类问题构建了任务集，根据以前在持续学习文献中使用的方案[Srivastava等人，2013年，Goodfellow等人，2014年]。对于每个任务，我们生成一个固定的、随机的排列，所有图像的输入像素将被打乱。因此，每一项任务与原始MNIST问题的难度相同，尽管每一项任务都需要不同的解决方案。所使用的设置的详细描述可以在附录4.1中找到。</p><p>  用简单随机梯度下降（SGD）训练这个任务序列会导致灾难性的遗忘，如图2A所示。蓝色曲线表示在两个不同任务的测试集上的表现。当训练机制从第一个任务（A）的训练切换到第二个任务（B）的训练时，任务B的表现迅速下降，而任务A的表现则急剧上升。随着训练时间的增加，以及后续任务的增加，任务A的遗忘进一步加剧。这个问题不能通过对每个权重使用固定的二次约束正则化网络来解决（绿色曲线，$L2$ 正则化）：在这里，任务A的性能下降得不那么严重，但由于约束对所有权重的保护是平等的，因此无法正确地学习任务B，几乎没有多余的容量用于学习B。但是，当我们使用EWC时，考虑到每个权重对任务A的重要性，网络可以很好地学习任务B，而不会忘记任务A（红色曲线）。这正是图1中所描述的预期行为。</p><p>  以前解决深度神经网络持续学习问题的尝试依赖于网络超参数的仔细选择，以及其他标准的正则化方法，以减轻灾难性遗忘。然而，在这项任务中，他们只在最多两个随机排列上取得了合理的结果[Srivastava et al.， 2013, Goodfellow et al.， 2014]。使用类似于[Goodfellow等人，2014]的交叉验证超参数搜索，我们将传统的退出正则化与EWC进行了比较。我们发现，仅使用dropout正则化的随机梯度下降是有限的，并且它不能扩展到更多的任务(图2B)。相比之下，EWC允许按顺序学习大量的任务，错误率只有适度的增长</p><p>  鉴于EWC允许网络有效地将更多功能压缩到具有固定容量的网络中，我们可能会问，它是否为每个任务分配了完全独立的网络部分，或者是否通过共享表示以更有效的方式使用容量。为了评估这一点，我们通过测量任务对各自Fisher信息矩阵之间的重叠来确定每个任务是否依赖于相同的权重集（见附录4.3）。较小的重叠意味着这两个任务依赖于不同的权重集（即EWC为不同的任务细分了网络的权重）;较大的重叠表明两个任务都使用了权重（即EWC支持共享表示）。图2C显示了重叠作为深度的函数。作为一个简单的控制，当一个网络在两个彼此非常相似的任务上训练时（MNIST的两个版本，其中只有几个像素排列)，任务依赖于整个网络中相似的权重集（灰色曲线）。当两个任务之间的差异更大时，网络开始为这两个任务分配独立的容量（即权重）（黑线）。然而，即使对于较大的排列，接近输出的网络层也确实被两种任务重用。这反映了这样一个事实，即排列使输入域非常不同，但输出域（即类标签）是共享的。</p><h2 id="EWC允许在强化学习环境中持续学习"><a class="header-anchor" href="#EWC允许在强化学习环境中持续学习">¶</a>EWC允许在强化学习环境中持续学习</h2><p>  接下来，我们测试了弹性权重巩固是否可以在要求更高的强化学习(RL)领域支持持续学习。在RL中，智能体动态地与环境交互，以制定最大化累积未来奖励的策略。我们询问了深度Q网络(DQNs)——一种在如此具有挑战性的RL设置中取得了令人印象深刻的成功的架构[Mnih等人，2015]——是否可以与EWC一起利用，成功地支持经典Atari 2600任务集中的持续学习[Bellemare等人，2013]。具体来说，每个实验都由DQN从人类或更高水平的游戏中随机选择10个游戏组成。在训练时，智能体在很长一段时间内暴露在每场游戏的经验中。游戏的呈现顺序是随机的，允许玩家多次回到相同的游戏中。我们还会定期测试智能体在这10个游戏中的每一个游戏中的得分，而不允许智能体在这些游戏上进行训练(图3A)。</p><p>  值得注意的是，以前用于持续学习的强化学习方法要么依赖于向网络添加容量[Ring, 1998年，Rusu等人，2016]，要么依赖于在单独的网络中学习每个任务，然后用于训练一个可以玩所有游戏的单一网络[Rusu等人，2015年，Parisotto等人，2015年]。相比之下，这里提出的EWC方法利用具有固定资源(即网络容量)的单个网络，并且具有最小的计算开销</p><p>  除了使用EWC来保护之前获得的知识外，我们还使用RL领域来解决成功的持续学习系统所需的更广泛的需求:特别是，需要更高级别的机制来推断当前正在执行的任务，检测和合并遇到的新任务，并允许在任务之间快速灵活地切换[Collins和Frank, 2013]。在灵长类动物的大脑中，前额叶皮层被广泛认为通过维持任务背景的神经表征来支持这些能力，这些任务背景对低水平区域的感觉处理、工作记忆和行动选择施加自上而下的门控影响[O 'Reilly和Frank, 2006, Mante等人，2013,Miller和Cohen, 2001, Doya等人，2002]。</p><p>  受到这一证据的启发，我们使用了一种与[van Hasselt等人，2016]中描述的非常相似的代理，但差异很少:(a)具有更多参数的网络，（b）更小的过渡表，©每层的任务特定偏差和增益，(d) Atari中的完整动作集，（e）任务识别模型，以及（e）EWC惩罚。超参数的全部细节在附录app:atari中描述。这里我们简要描述了对代理的两个最重要的修改:任务识别模块和EWC惩罚的实现</p><p>  我们将任务上下文作为隐马尔可夫模型的潜变量。因此，每个任务都与观察结果的底层生成模型相关联。我们方法的主要区别在于，我们允许添加新的生成模型，如果它们通过使用受勿忘我过程启发的训练过程，比现有的模型池更好地解释最近的数据<a href="%E8%A7%81%E9%99%84%E5%BD%954.2">Kieran et al.， 2016</a>。</p><p>  为了应用EWC，我们在每个任务开关处计算Fisher信息矩阵。对于每一项任务，都添加了一个惩罚，锚点由参数的当前值给出，权重由Fisher信息矩阵乘以比例因子λ给出，该比例因子由超参数搜索优化。我们只在经历了至少2000万帧的游戏中添加了EWC惩罚</p><p>  我们还允许DQN代理为每个推断任务维护单独的短期记忆缓冲区:这允许使用经验重放机制从策略外学习每个任务的动作值[Mnih等人，2015]。因此，整个系统具有两个时间尺度上的记忆:在短时间尺度上，经验重放机制允许DQN中的学习基于交错和不相关的经验[Mnih等人，2015]。在更长的时间尺度上，通过使用EWC可以巩固任务之间的专门知识。最后，我们允许少量网络参数是针对游戏的，而不是跨游戏共享。特别是，我们允许网络的每一层都有偏差，每个元素都有特定于每个游戏的乘法增益</p><p>  我们在图3中比较了使用EWC的代理（红色）和不使用EWC的代理（蓝色）的性能。我们用所有10款游戏的人类标准化得分来衡量玩家的表现。我们对随机种子和10场比赛的选择进行平均（见附录4.2）。我们还将每个游戏的人类标准化分数剪辑为1。因此，我们对性能的衡量是一个最大为10的数字（至少在所有游戏中是人类的水平），其中0表示代理与随机代理一样好。如果我们依赖于[Mnih等人，2015]中的普通梯度下降方法，智能体永远不会学会玩多个游戏，而忘记旧游戏所造成的伤害意味着人类归一化的总分数仍然低于1。然而，通过使用EWC，代理确实学会了玩多种游戏。作为对照，我们还考虑了如果我们显式地向智能体提供真实的任务标签(图3B，棕色)，而不是依赖于通过FMN算法学习的任务识别（红色），对智能体的好处。这里的改善不大。</p><p>  虽然用EWC增强DQN代理可以让它在不遭受灾难性遗忘的情况下连续学习许多游戏，但它无法达到训练10个独立DQN所能获得的分数（见附录4.2中的图1）。其中一个可能的原因是我们基于参数不确定性的近似（即Fisher信息）整合了每款游戏的权重。因此，我们试图以经验来检验我们估计的质量。为此，我们在一个游戏中训练一个智能体，并测量扰动网络参数如何影响智能体的分数。不管智能体是在哪个游戏上训练，我们观察到相同的模式，如图3C所示。首先，与均匀扰动(黑色)相比，代理对由费雪信息对角线逆形成的参数扰动（蓝色）总是更稳健。这证明了Fisher的对角线可以很好地估计某个参数的重要性。在我们的近似范围内，零空间中的扰动应该对性能没有任何影响。然而，根据经验，我们观察到这个空间（橙色）中的摄动与逆费雪空间中的摄动具有相同的效果。这表明我们对某些参数不重要过于自信:因此，当前实现的主要限制可能是它低估了参数的不确定性。<br>$$<br>\mathrm{P}(\mathrm{B} \mid \mathrm{A})=\mathrm{P}(\mathrm{A} \mid \mathrm{B}) \frac{\mathrm{P}(\mathrm{B})}{\mathrm{P}(\mathrm{A})}<br>$$</p><p>$$<br>\mathrm{P}(\theta \mid \mathrm{D})=\mathrm{P}(\mathrm{D} \mid \theta) \frac{\mathrm{P}(\theta)}{\mathrm{P}(\mathrm{D})}<br>$$</p><p>$$<br>\log \mathrm{P}(\theta \mid \mathrm{D})=\log \left(\mathrm{P}(\mathrm{D} \mid \theta) \frac{\mathrm{P}(\theta)}{\mathrm{P}(\mathrm{D})}\right)=\log \mathrm{P}(\mathrm{D} \mid \theta)+\log \mathrm{P}(\theta)-\log \mathrm{P}(\mathrm{D})<br>$$</p>]]></content>
      
      
      <categories>
          
          <category> 科研学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 联邦学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《Learning without Forgetting》</title>
      <link href="/2022/12/21/learning-without-forgetting/"/>
      <url>/2022/12/21/learning-without-forgetting/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h1>LEARNING WITHOUT FORGETTING</h1><p><img src="/2022/12/21/learning-without-forgetting/2.png" alt="图2.说明我们的方法（e）和与（b-d）比较的方法。展示了训练中使用的图像和标签。在联合训练中，不同任务的数据交替使用"><br>  给定一个具有共享参数 $θ_{s}$ 和特定任务参数 $θ_{o}$ 的CNN（图2（a）），我们的目标是为新任务添加特定任务参数 $θ_{n}$，并学习在旧任务和新任务中工作良好的参数，仅使用新任务的图像和标签（即不使用现有任务的数据）。算法如图3所示，网络结构如图2（e）所示。<br><img src="/2022/12/21/learning-without-forgetting/1.png" alt="图3.LEARNING WITHOUT FORGETTING伪代码"></p><p>  首先，我们从原始网络中记录每个新任务图像上的响应 $\mathbf{y}_{o}$，用于旧任务（由 $θ_{s}$ 和 $θ_{o}$ 定义）的输出。我们的实验涉及分类，因此响应是每个训练图像的标签概率集。每个新类的节点被添加到输出层，与下面的层完全连接，随机初始化权值 $θ_{n}$。新参数的数量等于新类的数量乘以最后一个共享层中的节点数量，通常只占参数总数的很小一部分。在我们的实验（第4.2节）中，我们还比较了为新任务修改网络的替代方法。</p><p>  接下来，我们训练网络以最小化所有任务的损失，并使用随机梯度下降正则化 $\mathcal{R}$。正则化 $\mathcal{R}$ 对应于 $0.0005$ 的简单权值衰减。训练时，我们首先冻结 $θ_{s}$ 和 $θ_{o}$，训练 $θ_{n}$收敛（热身步骤）。然后，我们联合训练所有权值 $θ_{s}$，$θ_{o}$ 和 $θ_{n}$，直到收敛（联合优化步骤）。预热步骤极大地提高了微调的旧任务性能，但对我们的方法或比较的减少遗忘学习都不是那么重要（见表2（b））。我们在“不忘学习”（以及大多数比较方法）中仍然采用这种技术，以进行轻微的增强和公平的比较。</p><p>  为了简单起见，我们表示了单个例子的损失函数、输出和基本真理。总损耗是在训练中对一批图像进行平均。对于新任务，损失鼓励预测 $\hat{\mathbf{y}}_{n}$ 与真值 $\mathbf{y}_{n}$ 一致。我们的实验任务是多类分类，所以我们使用常见的[3]，[27]多项logistic损失：<br>$$<br>\mathcal{L}_{n e w}(\mathbf{y}_{n}, \hat{\mathbf{y}}_{n})=-\mathbf{y}_{n} \cdot \log \hat{\mathbf{y}}_{n}<br>$$<br>其中 $\hat{\mathbf{y}}_{n}$ 为网络的softmax输出，$\mathbf{y}_{n}$ 为one-hot ground真值标签向量。如果有多个新任务，或者如果任务是多标签分类，我们对每个标签做出正确/错误的预测，我们将计算新任务和标签之间的损失之和</p><p>  对于每个原始任务，我们希望每个图像的输出概率接近原始网络的记录输出。我们使用由Hinton等人发现的知识蒸馏损失，[11]可以很好地鼓励一个网络的输出接近另一个网络的输出。这是一个修正的交叉熵损失，它增加了较小概率的权重：<br>$$<br>\begin{aligned}<br>\mathcal{L}_{\text {old }}(\mathbf{y}_{o}, \hat{\mathbf{y}}_{o}) &amp; =-H(\mathbf{y}_{o}^{\prime}, \hat{\mathbf{y}}_{o}^{\prime}) \\<br>&amp; =-\sum_{i=1}^{l} y_{o}^{\prime(i)} \log \hat{y}_{o}^{\prime(i)}<br>\end{aligned}<br>$$<br>其中 $l$ 为标签的数量，$y_{o}^{\prime(i)}$，其中 $\hat{y}_{o}^{\prime(i)}$ 为记录的和当前概率的修改版本 $y_{o}^{(i)}$, $\hat{y}_{o}^{(i)}$ :<br>$$<br>y_{o}^{\prime(i)}=\frac{(y_{o}^{(i)})^{1 / T}}{\sum_{j}(y_{o}^{(j)})^{1 / T}}, \quad \hat{y}_{o}^{\prime(i)}=\frac{(\hat{y}_{o}^{(i)})^{1 / T}}{\sum_{j}(\hat{y}_{o}^{(j)})^{1 / T}}<br>$$</p><p>  如果有多个旧任务，或者如果一个旧任务是多标签分类，我们取每个旧任务和标签的损失之和。Hinton et al.[11]建议设置 $T&gt;1$，这将增加较小logit值的权重，并鼓励网络更好地编码类之间的相似性。我们使用 $T=2$ 根据网格搜索在一个持有的集合，这与作者的建议一致。在实验中，使用知识蒸馏损耗可以得到比其他合理损耗稍好的性能。因此，将原始任务的输出约束为与原始网络相似是很重要的，但相似度度量并不重要。</p><p>  $\lambda_{o}$ 是一个损失平衡权重，在我们的大多数实验中设置为 $1$。增大 $\lambda$ 将有利于旧任务的性能而不是新任务的性能，因此我们可以通过改变 $\lambda$ 得到旧任务-新任务性能线。（图7）</p><p><b>与联合训练的关系</b>。如前所述，联合训练和我们的方法之间的主要区别是需要旧的数据集。联合训练在训练中使用旧任务的图像和标签，而Learning without Forgetting不再使用它们，而是使用新的任务图像 $X_{n}$ 和记录的响应 $Y_{o}$ 作为替代。这消除了需要和存储旧数据集的需要，为我们带来了共享 $θ_{s}$ 的联合优化的好处，也节省了计算量，因为图像 $X_{n}$ 只需要在新任务和旧任务中通过一次共享层。然而，来自这些任务的图像的分布可能非常不同，这种替代可能会降低性能。因此，联合训练的表现可以看作是我们方法的一个上限。</p><p><b>效率的比较</b>。使用神经网络计算最昂贵的部分是通过共享参数 $\theta _{s}$ 进行评估或反向传播，特别是卷积层。对于训练，特征提取是最快的，因为只调整了新的任务参数。LwF比微调稍慢，因为对于旧的任务，它需要通过  $\theta _{o}$ 反向传播，但只需要评估和反向传播 $\theta _{s}$ 一次。联合训练是最慢的，因为不同的图像用于不同的任务，每个任务都需要通过共享参数进行单独的反向传播。</p><p>  所有方法评估测试图像所花费的时间大致相同。但是，复制网络并对每个任务进行微调需要 $m$ 倍的时间来评估，其中 $m$ 是任务的总数。</p><p>  <br>  <br>  <br>  <br>  <br>  <br>  <br>  <br>  <br>  <br>  <br>  <br>  <br>  <br>  <br>  <br>  </p>]]></content>
      
      
      <categories>
          
          <category> 科研学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 联邦学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《Accelertion of federated learning with alleviaed forgetting in local training》</title>
      <link href="/2022/12/20/accelertion-of-federated-learning-with-alleviaed-forgetting-in-local-training/"/>
      <url>/2022/12/20/accelertion-of-federated-learning-with-alleviaed-forgetting-in-local-training/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h1>摘要</h1><p>  联邦学习（FL）可以对机器学习模型进行分布式优化，同时通过在每个客户端上独立训练本地模型，然后在中央服务器上聚合参数来保护隐私，从而产生有效的全局模型。虽然已经提出了各种FL算法，但当数据在不同客户端之间不是独立和同分布（非i.i.d）时，它们的训练效率仍然很低。我们观察到，现有方法的缓慢收敛速度（至少部分）是由每个单独客户端在局部训练阶段的灾难性遗忘问题引起的，这导致了其他客户端上关于之前训练数据的损失函数的大幅增加。在这里，我们提出了FedReg，一种在局部训练阶段通过正则化局部训练参数来缓解知识遗忘的算法，该算法对全局模型学习到的先前训练数据的知识进行编码。我们的综合实验表明，FedReg不仅显著提高了FL的收敛速度，特别是当神经网络结构较深且客户端数据非常非i.i.d时。，但在分类问题中也能更好地保护隐私，对梯度反演攻击更健壮。该代码可在:<br>  <br>  <br>  <br>  <br>  <br>  <br>  <br>  <br>  <br>  <br>  <br>  <br>  <br>  <br>  <br>  <br>  <br>  <br>  <br>  <br>  <br>  <br>  <br>  <br>  </p>]]></content>
      
      
      <categories>
          
          <category> 科研学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 联邦学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《SS-IL Separated Softmax for Incremental Learning》</title>
      <link href="/2022/12/11/ss-il-separated-softmax-for-incremental-learning/"/>
      <url>/2022/12/11/ss-il-separated-softmax-for-incremental-learning/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h1>摘要</h1><p>  我们考虑了类增量学习（CIL）问题，其中学习代理不断地从增量到达的训练数据批量中学习新的类，并旨在对迄今为止学习的所有类进行良好的预测。该问题的主要挑战是灾难性遗忘，而对于基于样本-记忆的CIL方法，众所周知，遗忘通常是由于新类和旧类（在样本-记忆中）之间的数据不平衡而注入的分类评分偏差造成的。虽然已经提出了通过一些额外的后处理来纠正这种评分偏差的方法，如评分重标或平衡微调，但尚未对这种偏差的根本原因进行系统的分析。为此，我们分析了通过结合所有旧类和新类的输出分数来计算softmax概率可能是偏差的主要原因。然后，我们提出了一种新的方法，称为分离softmax增量学习（SS- il），该方法由分离softmax输出层（SS）和任务知识蒸馏（TKD）相结合来解决这种偏差。通过我们在几个大型CIL基准数据集上的广泛实验结果，我们表明，我们的SS-IL通过在旧类和新类中获得更平衡的预测分数，无需任何额外的后处理，实现了强大的最先进的准确性。</p><h1>介绍</h1><p>  增量或持续学习，即智能体不断学习新的训练数据的增量到达，是人工智能和机器学习中的重大挑战之一。这种设置没有假设旧训练数据的完全可用性，近年来尤其从实际应用的角度受到越来越多的关注。原因是存储所有的训练数据很容易变得大规模，对于内存和计算受限的应用程序（如移动电话或机器人）来说，在一批处理中存储所有的训练数据通常是不现实的。因此，在不访问目前接收到的全部数据的情况下，持续而有效地更新学习代理是必不可少的。</p><p>  这种代理的一个可行的候选者是基于端到端学习的深度神经网络（DNN）模型。近年来，基于dnn的增量学习方法在许多不同的应用中取得了成功[14,3,7]，近年来也得到了积极的追求。尽管有一些有希望的结果，但它们也有一个关键的局限性：灾难性遗忘，这是指在使用新数据对模型进行简单的微调后，旧数据上的泛化性能严重退化的问题。</p><p>  在本文中，我们关注基于dnn的类增量学习（CIL），我们指的是学习一个分类器来从每个增量训练数据中分类新的对象类，并在迄今为止学习到的所有类上测试分类器。在几种不同的提出的方法中，基于样本记忆的方法[28,8,31,35,4,5]，允许将少量来自旧类的训练数据存储在单独的存储器中，已被证明可以有效地减轻灾难性遗忘。</p><p>  在CIL中使用范例内存的主要挑战是解决新类和旧类（在范例内存中）的训练数据点之间严重的数据不平衡。也就是说，对这种不平衡的数据进行天真的微调，会使预测分数向新学习的类严重倾斜，因此，对旧类的准确性会急剧下降，导致显著的遗忘。最近，一些最先进的方法[8,31,35,4,5]提出在学习分类模型后，通过一些额外的后处理步骤，如评分重新缩放或平衡微调来纠正这种评分偏差。</p><p>  虽然上述方法在一定程度上提高了准确性，但我们认为，这些方法缺乏系统地分析产生这种偏差的主要原因，并且他们的一些方案的组成部分，如知识蒸馏(KD)[15]，被天真地使用，没有任何适当的理由[31,23,35,20]。为此，在本文中，我们首先分析了这种分类评分偏差的根本原因，然后提出了一种合理的方法来减轻这种原因。也就是说，我们认为偏差是由普通交叉熵损失中使用的softmax概率总是通过合并所有类的输出分数来计算的这一事实注入的，这使得由于数据不平衡而对旧类的输出分数进行了严重的惩罚。此外，我们表明，如果预测偏差已经存在于模型中，那么幼稚地使用通用KD（GKD）方法，该方法还结合了所有旧类的输出分数来计算软目标，可能会保留偏差，甚至会损害精度。</p><p>  为了解决上述问题，我们提出了用于增量学习的Separated-Softmax（SS-IL），它由两个主要组件组成。首先，设计分离的softmax（SS）输出层，相互阻塞新旧类之间的分数梯度流动，从而缓解旧类输出概率的不平衡惩罚;其次，我们展示了Task-wise KD (TKD)[25]，它也以任务分离的方式计算精析的软目标，特别适合我们的SS层，因为它试图保留Task-wise知识，而不保留可能在任务之间保留的预测偏差。为了展示我们方法的有效性，我们在几个大型CIL基准上进行了广泛的实验验证，并通过重现所有的SS-IL与最近的强基准进行了比较。因此，我们令人信服地表明，我们的SS-IL通过充分平衡新旧类的预测分数，无需任何额外的后处理，实现了强大的最先进的准确性。</p><p>  总之，我们的贡献有三方面:</p><ol><li>我们提出了一种新的分离softmax(SS)层，它可以防止在整个梯度步骤中过度惩罚旧的类分数。</li><li>我们表明，在CIL中使用GKD可以保持模型的偏差，而TKD在与具有相同直觉的SS特别结合时可以带来协同作用。</li><li>我们对我们的SS-IL进行了广泛的实验验证，在几个大规模的基准上进行了各种CIL场景，并与最近的、全部再现的最先进的基线进行了相当的比较。</li></ol><p>  最近，已经有大量的工作来解决持续/增量学习中的灾难性遗忘问题。对于一般的持续学习，有三种主要的方法;1)基于正则化的[22,34,9,2,19]，2)基于动态架构的[29,33,24,17]，3)基于范例/重放记忆的[26,10,11,30,22,32,30,21,32]方法。要了解更详细的调查，请参阅[27]。</p><p>  对于CIL，特别是基于样本记忆的方法与知识蒸馏（KD）相结合，已被证明是有效的。现将近期具有代表性的工作总结如下。</p><p><b>使用范例记忆和偏差校正：</b>在CIL中，早期基于样本记忆的方法，如iCaRL[28]和EEIL[8]，已经显示出了优越的结果。iCaRL使用最接近样本均值（NME）对样本进行分类，EEIL还利用平衡微调，通过平衡训练批次进一步微调网络。后来，Javed等人[18]指出，使用范例内存的方法会导致数据集不平衡，因此在最终的FC层中已被证明存在偏差问题。为了解决这种不平衡学习问题，提出了几种偏差去除技术。另一种平衡微调方法UW[23]利用基于训练数据统计的损失加权梯度缩放。BiC[31]通过额外训练一个偏差校正层来校正分数的偏差，Wa[35]根据每个权重的范数来校正FC层中的偏差权重。此外，利用模型输出的统计特性，IL2M[4]对输出的softmax概率进行了校正，ScaIL[5]对分类器的权重进行了缩放。</p><p><b>知识蒸馏（KD）：</b>KD作为一种保存和利用从旧类中学习到的信息的流行技术，在CIL中被广泛应用，以减轻遗忘。然而，正如引言中提到的，几个版本的KD在不同的方法中被混淆使用，没有适当的理由。如LwF[25]、iCaRL[28]、EEIL[8]采用TKD形式，而BiC[31]、UW[23]、W A[35]采用GKD形式。每种方法都只是简单地做出了选择，除了一些直观的论据之外，没有对选择进行分析或证明（例如，[23]证明了GKD，但是，没有适当的比较或证据）。</p><p>  除了上述方法外，LUCIR[16]和PODNet[13]考虑了一种略有不同的设置，在这种设置中，初始模型使用大量基类进行训练，以获得有用的特征表示，它们利用特征蒸馏来保留这些表示，同时学习未来的类。但是，我们认为它们的设置更有限，我们比较了它们在纯CIL设置下的表现，以便与其他基线进行公平的比较。</p><h1>Preliminaries</h1><h2 id="符号和问题设置"><a class="header-anchor" href="#符号和问题设置">¶</a>符号和问题设置</h2><p>  在CIL中，我们假设每个增量到达的训练数据，通常称为增量任务，由以前没有学习过的 $m$ 个新类的数据组成。更正式地说，增量任务 $t$ 的训练数据表示 $\mathcal{D}_{t}={(\boldsymbol{x}_{t}^{(i)}, y_{t}^{(i)})}_{i=1}^{n_{t}}$，其中 $\boldsymbol{x}_{t}^{(i)}$, $y_{t}^{(i)}$ 和 $n_{t}$ 分别表示任务 $t$ 的输入数据，对应的（整数值）目标标签和对应任务的训练样本数量。到任务 $t$ 为止的类的总数用 $\mathcal{C}_{t}=m\cdot t$ 表示，这导致标记 $y_{t}^{(i)} \in \{C_{t-1}+1, \ldots, C_{t}\} \triangleq \mathcal{C}_{t}$。在学习每个增量任务时，我们假设分配了一个单独的范例内存 $\mathcal{M}$ 来存储旧类的范例数据。也就是说，在学习增量任务 $t$ 时，我们从每个学习的类中存储 $\left\lfloor\frac{|\mathcal{M}|}{C_{t-1}}\right\rfloor$ 个数据点，直到增量任务 $t−1$。因此，随着增量任务的增长，为每个类存储的样本数据点的数量随t线性减少，我们假设 $\left | \mathcal{M}  \right | \ll n_{t}$ 增量任务总数用 $T$ 表示。</p><p>  我们的分类模型由一个具有深度卷积神经网络（CNN）架构的特征提取器和分类层组成，分类层是具有softmax输出的最终全连接（FC）层。我们用 $\theta$ 表示分类模型的参数。在增量任务 $t$ 下，使用 $\mathcal{D}_{t} \cup \mathcal{M}$ 中的数据点来学习模型的参数 $θ_{t}$ 。学习后，对给定样本 $\boldsymbol{x}_{text}$ 的类预测由：<br>$$<br>\hat{y}_{\text {test }}=\arg \max _{y \in \mathcal{C}_{1: t}} z_{t y}\left(\boldsymbol{x}_{\text {test }}, \boldsymbol{\theta}_{t}\right)  \quad\quad\quad\quad(1)<br>$$<br>其中 $z_{t y}\left(\boldsymbol{x}_{\text {test }}, \boldsymbol{\theta}_{t}\right)$ 是类 $y \in \{1, \ldots, C_{t} \} \triangleq \mathcal{C}_{1: t}$ 时模型 $θ_{t}$ 的输出分数（在softmax之前）。即在测试时，对最终的FC层进行整合，并对 $\mathcal{C}_{1: t}$ 中所有类进行预测，就像使用普通的多类分类器一样。</p><h2 id="知识蒸馏"><a class="header-anchor" href="#知识蒸馏">¶</a>知识蒸馏</h2><p>  如前所述，在CIL中使用的KD的两种主要变体是通用KD（GKD）和任务型KD（TKD），对于学习任务 $t$ 的每种方法定义的损失函数如下:对于一个输入数据 $\boldsymbol{x}\in \mathcal{D}_{t}\cup \mathcal{M} $，<br>$$<br>\mathcal{L}_{\mathrm{GKD}, t}(\boldsymbol{x}, \boldsymbol{\theta}) \triangleq \mathcal{D}_{K L}\left(\boldsymbol{p}_{1: t-1}^{\tau}\left(\boldsymbol{x}, \boldsymbol{\theta}_{t-1}\right) | \boldsymbol{p}_{1: t-1}^{\tau}(\boldsymbol{x}, \boldsymbol{\theta})\right)\quad\quad\quad\quad(2)<br>$$</p><p>$$<br>\mathcal{L}_{\mathrm{TKD}, t}(\boldsymbol{x}, \boldsymbol{\theta}) \triangleq \sum_{s=1}^{t-1} \mathcal{D}_{K L}\left(\boldsymbol{p}_{s}^{\tau}\left(\boldsymbol{x}, \boldsymbol{\theta}_{t-1}\right) | \boldsymbol{p}_{s}^{\tau}(\boldsymbol{x}, \boldsymbol{\theta})\right)\quad\quad\quad\quad(3)<br>$$</p><p>其中，$\mathcal{D}_{KL}(\cdot ||\cdot)$ 是Kullback-Leibler散度，$\tau$ 是温度缩放参数，$θ$ 是为任务 $t$ 学习的模型参数，$θ_{t−1}$ 是到任务 $t−1$ 学习的模型参数。在（2）和（3）中，定义概率向量 $\boldsymbol{p}_{s}^{\tau}(\boldsymbol{x}, \boldsymbol{\theta}) \in \Delta^{m} \text { and } \boldsymbol{p}_{1: s}^{\tau}(\boldsymbol{x}, \boldsymbol{\theta}) \in \Delta^{C_{s}}$ 的第 $c$ 个分量分别为<br>$$<br>\begin{aligned}<br>p_{s, c}^{\tau}(\boldsymbol{x}, \boldsymbol{\theta}) &amp; =\frac{e^{z_{s c}(\boldsymbol{x}, \boldsymbol{\theta}) / \tau}}{\sum_{k \in \mathcal{C}_{s}} e^{z_{s k}(\boldsymbol{x}, \boldsymbol{\theta}) / \tau}} \text { and } \<br>p_{1: s, c}^{\tau}(\boldsymbol{x}, \boldsymbol{\theta}) = \frac{e^{z_{s c}(\boldsymbol{x}, \boldsymbol{\theta}) / \tau}}{\sum_{k \in \mathcal{C}_{1: s}} e^{z_{s k}(\boldsymbol{x}, \boldsymbol{\theta}) / \tau}},<br>\end{aligned}<br>$$<br>即 $\boldsymbol{p}_{s}^{\tau}(\boldsymbol{x}, \boldsymbol{\theta})$ 是计算softmax概率时仅使用任务 $s$ 的输出分数得到的概率向量， $\boldsymbol{p}_{1:s}^{\tau}(\boldsymbol{x}, \boldsymbol{\theta})$ 是计算softmax概率时使用任务1到s的所有输出分数得到的概率向量。因此，最小化（2）或（3）都将导致对过去的模型 $θ_{t−1}$ 进行正则化，但（2）使用所有过去任务的全局softmax概率，$\boldsymbol{p}_{t-1}^{\tau}(\boldsymbol{x}, \boldsymbol{\theta_{t-1}})$，而（3）使用任务级softmax概率，$\{\boldsymbol{p}_{s}^{\tau}(\boldsymbol{x}, \boldsymbol{\theta}_{t-1})\}_{s=1}^{t-1}$，分别为每个任务获得。在最近的CIL基线中，（2）在[31,23,35]中使用，（3）在[25,8]中使用。（2）和（3）的区别如图1所示。<br><img src="/2022/12/11/ss-il-separated-softmax-for-incremental-learning/1.png" alt="图1：图示 $\mathcal{L}_{GKD,t}(\boldsymbol{x}, \boldsymbol{\theta})$（左）和 $\mathcal{L}_{TKD,t}(\boldsymbol{x}, \boldsymbol{\theta}) $（右）"></p><h1>Motivation</h1><p>  如引言中所述，之前的几项工作[8,23,16,31,4,35,5]指出，基于样本记忆的CIL的主要挑战是解决由于数据不平衡引起的分类评分偏差。在这里，我们考虑一个简单的例子，并给出一个令人信服的论点，说明为什么这种评分偏差会被注入，以及为什么天真地使用GKD不能修复这种偏差</p><p>  即，首先注意，典型的CIL方法所使用的学习任务 $t$ 的普通交叉熵损失可以表示为：</p><p>$$<br>\mathcal{L}_{\mathrm{CE}, t}((\boldsymbol{x}, y), \boldsymbol{\theta})=\mathcal{D}_{K L}(\boldsymbol{y}_{1: t} | \boldsymbol{p}_{1: t}(\boldsymbol{x}, \boldsymbol{\theta})) \quad\quad\quad\quad(4)<br>$$</p><p>其中，$\boldsymbol{y}_{1: t}$ 是 $\mathbb{R}^{\mathcal{C}_{t}}$ 中的一个one-hot向量，在 $y$ 坐标处的值为 $1$，$\boldsymbol{p}_{1: t}(\boldsymbol{x}, \boldsymbol{\theta})$ 是 $\boldsymbol{p}_{1: t}^{\tau}(\boldsymbol{x}, \boldsymbol{\theta})$ 。现在，为了系统地分析典型CIL方法中常见的预测偏差的根本原因，我们使用一种简单的CIL方法进行了一项实验，该方法使用以下损失：<br>$$<br>\mathcal{L}_{\mathrm{CE}, t}((\boldsymbol{x}, y), \boldsymbol{\theta})+\mathcal{L}_{\mathrm{GKD}, t}(\boldsymbol{x}, \boldsymbol{\theta}) \quad\quad\quad\quad(5)<br>$$</p><p>其中 $(\boldsymbol{x}, y) \in \mathcal{D}_{t} \cup \mathcal{M}$ 用于学习任务 $t$。即在学习任务 $t$ 的同时，通过 $\mathcal{L}_{\mathrm{GKD}}$ 保留过去的知识。如图2所示，我们对ImageNet数据集 $m = 100$ 和 $| \mathcal{M} |=10k$ 进行了实验，因此总共有 $10$ 个任务。</p><p><img src="/2022/12/11/ss-il-separated-softmax-for-incremental-learning/2.png" alt="图2。左：基于CIL模型对测试数据的预测的混淆矩阵（跨任务）。右：$θ_{t−1}$ 对 $\mathcal{D}_{t}$ 的Top1预测之比。我们不表示被预测的类，而是表示被预测的类所属的任务。请注意，右图中的虚线区域表示最新旧任务的比例，它表示 $\mathcal{L}_{\mathrm{GKD}}$ 对soft targets的偏差。所有的结果都在ImageNet-1K数据集上，$m=100$，$| \mathcal{M} |=10k$。"></p><h2 id="由普通交叉熵引起的偏差"><a class="header-anchor" href="#由普通交叉熵引起的偏差">¶</a>由普通交叉熵引起的偏差</h2><p>  图2的左图显示了学习所有任务后测试样本在任务级的混淆矩阵。它清楚地显示了常见的预测偏差；也就是说，对过去任务的大多数预测都过度偏向最近的任务（任务10）。我们认为，这种偏差的根本原因可以在输出分数的梯度中找到：<br>$$<br>\frac{\partial \mathcal{L}_{\mathrm{CE}, t}((\boldsymbol{x}, y), \boldsymbol{\theta})}{\partial z_{t c}}=p_{1: t, c}(\boldsymbol{x}, \boldsymbol{\theta}) - \mathbb{1}_{\{c=y\}} \quad\quad\quad\quad(6)<br>$$<br>其中，$\mathbb{1}_{\{c=y\}}$ 是 $c=y$ 的指标。注意，由于 $c \ne y$ 时（6）始终为正，我们可以很容易地观察到，当用 $\mathcal{D}_{t}\cup \mathcal{M}$ 中的数据更新模型时，$\mathcal{D}_{t}$ 中新类的丰富样本进行梯度下降时，旧类的分类分数会继续下降。因此，我们认为，这些不平衡的梯度下降步骤的分类分数的老类，使显着的分数倾向于新类。梯度下降步骤的玩具插图如图3所示。<br><img src="/2022/12/11/ss-il-separated-softmax-for-incremental-learning/3.png" alt="图3。对于 $m=2$ 和 $T=2$ 在不平衡的 $\mathcal{D}_{t}\cup \mathcal{M}$ 上的梯度下降步骤的玩式说明。由于不平衡的梯度下降步骤，类 $c ∈ \mathcal{C}_{1}$ 的分数继续下降。"></p><h2 id="GKD保留偏置"><a class="header-anchor" href="#GKD保留偏置">¶</a>GKD保留偏置</h2><p>  现在，如上所述，以前的一些工作使用GKD来保存从过去的任务中学习到的知识。然而，当来自交叉熵损失的梯度导致前一节中提到的显著偏差时，我们认为使用GKD会在旧模型中保留这种偏差，甚至可能会损害性能。在（2）中定义的 $\mathcal{L}<em>{\mathrm{GKD}}$ 中，$\boldsymbol{p}</em>{1: t-1}^{\tau}\left(\boldsymbol{x}, \boldsymbol{\theta}<em>{t-1}\right)$ 是从旧模型θt−1计算出来的软目标，用于知识蒸馏。现在，图2(右)表明，由于交叉熵学习引起的偏差，这个软目标可能严重倾斜。也就是说，该图显示了 ${1, \cdots , t−1}$，即新任务数据点 $x ∈ \mathcal{D}_{t}$ 为时，旧模型预测的 $θ</em>{t−1}$ 作为输入，对于每个新任务 $t$ （横轴）。</p><p>  我们可以观察到，预测压倒性地偏向最近的旧任务（即任务 $t−1$），这是由于在学习任务 $t−1$ 期间产生的偏差与交叉熵损失。这表明软目标 $\boldsymbol{p}<em>{1: t-1}^{\tau}\left(\boldsymbol{x}, \boldsymbol{\theta}</em>{t-1}\right)$ 也会严重偏向最近的旧任务（任务 $t−1$），因此，当它被用于GKD损失（2）时，它将保留这种偏差，并可能严重惩罚旧任务的输出概率。因此，这可能会使偏见或对旧任务的遗忘更加严重。以上两个观察结果表明，产生预测偏差的主要原因可能是通过将新旧任务结合在一起来计算softmax概率。基于此，我们在下一节中提出了用于增量学习的Separated-softmax （SS-IL）。</p><h1>主要方法</h1><p>  我们的SS-il主要由两个组件组成，它们都是由上一节构建的直觉驱动的:（1）分离的softmax（SS）输出层和（2）任务型KD（TKD）。我们注意到，在LwF[25]中首次提出使用TKD治疗CIL，但正如我们在实验中所示，当与我们的SS层结合时，TKD尤其强大。为了简单解释，在增量任务 $t$ 处，设 $\mathcal{P}<em>{t}$ 表示之前任务的类 （$\mathcal{C}</em>{1:t−1}$） ，$\mathcal{C}<em>{t}$ 表示新任务（$\mathcal{C}</em>{t}$）的类。</p><p>  （1）Separated-Softmax (SS) layer:对于 $(\boldsymbol{x},y)∈ \mathcal{D}_{t} ∪ \mathcal{M}$，我们将交叉熵损失函数修改为，定义一个独立的softmax输出层<br>$$<br>\begin{aligned}<br>\mathcal{L}<em>{\mathrm{CE}-\mathrm{SS}, t}((\boldsymbol{x}, y), \boldsymbol{\theta})= &amp; \mathcal{L}</em>{\mathrm{CE}, t-1}((\boldsymbol{x}, y), \boldsymbol{\theta}) \cdot \mathbb{1}\left{y \in \mathcal{P}<em>{t}\right}+ \<br>&amp; \mathcal{D}</em>{K L}\left(\boldsymbol{y}<em>{t} | \boldsymbol{p}</em>{t}(\boldsymbol{x}, \boldsymbol{\theta})\right) \cdot \mathbb{1}\left{y \in \mathcal{N}<em>{t}\right},<br>\end{aligned}<br>$$<br>其中 $\boldsymbol{y}</em>{t}$ 表示 $\mathbb{R}^{\left|\mathcal{N}<em>{t}\right|}$ 中的one-hot向量，$\boldsymbol{p}</em>{t}(\boldsymbol{x}, \boldsymbol{\theta})$ 为 $τ=1$ 的 $(\boldsymbol{x}, y) \in \mathcal{M}$ 或者 $(\boldsymbol{x}, y) \in \mathcal{D}<em>{t}$。即，根据 $(\boldsymbol{x}, y) \in \mathcal{M}$ 或者 $(\boldsymbol{x}, y) \in \mathcal{D}</em>{t}$，分别仅使用 $\mathcal{P}<em>{t}$ 或 $\mathcal{N}</em>{t}$ 的输出分数分别计算softmax概率，并分别计算交叉熵损失。虽然（7）是对普通交叉熵（4）的简单修改，但当 $(x,y) ∈ \mathcal{D}<em>{t}$ 时，我们现在可以观察到 $\frac{\partial \mathcal{L}</em>{\text {CESS }}}{\partial z_{t c}}=0$ 对于 $c ∈ \mathcal{P}<em>{t}$。因此，$\mathcal{N}</em>{t}$ 中新类样本的梯度对 $\mathcal{P}_{t}$ 中旧类的分类分数不会有过大的惩罚作用。</p><p>  （2）Task-wise KD：为了防止保留GKD中存在的偏差，我们重新研究了LwF[25]中使用的Task-wise蒸馏(TKD)。利用与SS层类似的直觉，我们可以很容易地看到，使用TKD（3）进行知识蒸馏是很自然的，TKD（3）对每个任务也使用了separed-softmax。也就是说，在TKD中，由于软目标 $\left.\left{\boldsymbol{p}<em>{s}^{\tau}(\boldsymbol{x}, \boldsymbol{\theta})\right)\right}</em>{s=1}^{t-1}$ 仅在每个任务中计算，因此TKD不会受到旧模型 $θ_{t−1}$ 中可能存在的任务级偏差的影响，这与第4.2节所示的GKD相反。因此，我们可以预期TKD特别适合SS层，这将在我们的实验结果中得到证明。</p><p>  实现细节说明：当天真地将 $\mathcal{D}_{t} ∪ \mathcal{M}$ 中的随机小批用于模型的SGD更新时，也会使小批中的类比对新类的影响更大。小批量中的这种不平衡预计会淡化我们SS层中旧类的模型更新，因为（7）第一部分的梯度几乎不会产生。因此，我们额外实现了经验重放（ER）[11]技术，该技术在一个小批量中保留了来自旧类和新类的比例，以确保来自 $m$ 的样本的最小比例。我们经验地发现，使用ER对SS-IL的预测更加平衡。关于ER使用的详细分析见补充资料。</p><p>  <b>SS-IL的最终损失函数</b>：通过组合 $\mathcal{L}_{CE-SS,t}$ 在（7）和 $\mathcal{L}_{TKD,t}$ 在（3）， SS-IL的总体损失为：<br>$$\mathcal{L}<em>{\mathrm{SS}-\mathrm{IL}, t}((\boldsymbol{x}, y), \boldsymbol{\theta})=\mathcal{L}</em>{\mathrm{CE}-\mathrm{SS}, t}((\boldsymbol{x}, y), \boldsymbol{\theta})+\mathcal{L}_{\mathrm{TKD}, t}(\boldsymbol{x}, \boldsymbol{\theta})<br>$$<br>而最小化损失的小批量SGD是用ER完成的。图4说明了我们的方法，训练算法在补充材料中进行了总结。我们在实验结果中表明，SS有效地平衡了新旧班级之间的分数，从而纠正了预测偏差。最后，详细的结果表明，我们的SS-IL在各种大规模基准数据集和许多不同的增量场景中达到了最先进的精度。</p><p>  </p><p>  </p><p>  </p><p>  </p>]]></content>
      
      
      <categories>
          
          <category> 科研学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 联邦学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《A Memory T ransformer Network for Incremental Learning》</title>
      <link href="/2022/12/06/a-memory-t-ransformer-network-for-incremental-learning/"/>
      <url>/2022/12/06/a-memory-t-ransformer-network-for-incremental-learning/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h1>摘要</h1><p>  我们学习类增量学习，一种训练设置，在这种训练中，随着时间的推移观察新的数据类别，以便模型从中学习。尽管问题的表述很简单，但将分类模型天真地应用到类增量学习中会导致对以前见过的类的“灾难性遗忘”。现有最成功的方法之一是使用范例记忆，它通过将过去数据的子集保存到记忆库中，并在训练未来任务时利用它来防止遗忘，从而克服了灾难性遗忘的问题。在本文中，我们提出了提高该记忆库的利用率的方法:我们不仅像现有的工作一样将其作为附加训练数据的来源，而且还将其显式地集成到预测过程中。我们的方法，内存转换网络（MTN），学习如何结合和聚合来自内存中最近邻居的信息与转换器，以做出更准确的预测。我们进行了大量的消融实验来评估我们的方法。我们证明MTN在具有挑战性的ImageNet-1k和google - landmark -1k增量学习基准测试中取得了最先进的性能。</p><h1>介绍</h1><p>  增量学习（IL）的目标是设计能够在输入数据随着时间推移逐渐可用时动态学习的模型。主要的挑战是适应新的输入数据，同时仍然能够记住之前所学的知识。在过去，许多IL方法被提出来避免灾难性遗忘，这是指增量学习者在之前学习的任务[11]上表现不佳的情况。接近灾难性遗忘的一种流行且成功的方法是，从早期任务中保留一小部分训练数据，即存储在记忆库中的范例，并在训练新任务时使用它们来扩充数据，这被称为彩排[27]。这种方法已被证明对IL非常有效，因此，使用记忆来学习模型参数是大多数现代方法的关键组成部分[1,2,14,15,26,38]。</p><p>  在我们的工作中，我们建议更进一步，不仅将样本作为额外训练数据的来源，而且将其用于预测过程本身。我们的假设是查询（例如一个训练或测试示例）和记忆库的不同元素产生一个强信号，这有助于增量学习者对给定的查询做出更健壮和更准确的预测。为此，我们提出了内存转换网络（MTN），它在预测查询向量之前先观察其本地特征邻域。MTN是一种轻量级的转换器，它通过直接建模查询与存储库中范例的特征表示之间的关系，对给定查询进行类预测。由于对整个存储库的条件反射对计算量的要求过高，我们选择只向MTN输入一个减少的样本集（用最近邻搜索选择）。</p><p>  我们的方法允许显式地记住罕见的模式，而不是隐式地记住模型参数。查询的预测不再仅仅依赖于它的表示，它还依赖于它在保存的范例中的最近的邻居。这使得模型更加准确，直接从内存中检索角落案例，而不是专门使用模型参数来记忆它们。</p><p>  MTN的灵感来自于最近语言建模[39]和视频识别[36]中的内存转换器架构，其中转换器的输入序列遵循自然顺序，例如单词和帧的序列。在这些框架中，记忆通常包含以前见过的单词和框架，这有助于系统做出预测。MTN将记忆转换器扩展到图像分类中，我们在特征空间中根据查询的局部邻域定义输入序列。</p><p>  综上所述，我们的贡献如下:</p><ol><li>我们相信我们的方法是第一个提出了一个带有外部记忆的类增量学习模型。为此，我们利用一个将查询输入特性与内存范例特性结合起来的转换器。</li><li>与现有的类增量学习不同，我们不仅在模型训练过程中利用范例，而且通过输出分布依赖于外部记忆直接在决策过程中利用范例。</li><li>我们的方法在ImageNet-1k[8]和Google-Landmarks1k[35]数据集上取得了最先进的结果。</li></ol><h1>相关工作</h1><p>  在本节中，我们将讨论以前关于类增量学习的工作，看看基于范例的方法和无范例的方法。我们还讨论了文献中使用记忆增强神经网络的方法，这是我们方法的核心。</p><p>  类增量学习中一个流行的方法是保留任务训练数据的子集作为范例，并在后续任务中使用它来减轻增量训练进行时的遗忘效应。在当前任务训练集中添加范例是防止遗忘的一种有效方法，称为彩排[27]。Rebuffi等人提出了iCaRL方法[26]，该方法通过一种称为羊群的简单算法选择范例，并在推理时使用每个类的平均范例向量，而不是训练时使用的分类器。最近的工作扩展了iCaRL，并专注于纠正增量训练中对任务的模型偏差。一些方法训练附加参数[38,41]，而另一些方法修正损失函数[1,2,14,23,32]。也有人探索优化范例本身[19]，以及架构更改，如将模型分成稳定的和可塑的权重[20]，以允许快速的任务训练和过去任务的稳定知识。为了减少将样例保存为图像的内存占用，Iscen等人[15]存储特征而不是图像，通过一个单独的网络调整旧的样例特征以与新特征兼容。</p><p>  在类增量学习中也有很多工作没有使用范例。Li和Hoiem[17]在通常的分类损失的基础上增加了知识蒸馏损失，这有助于防止在看到任务时遗忘。Lopez-Paz和Ranzato[21]在给定的任务中修改梯度更新，也将更新传递给前面看到的类。还有很多方法学习生成模型来模拟过去任务中的数据，这种方法被称为伪排练。多种方法探索使用对抗训练来生成伪样本[29,37]，[40]，而Smith等人使用模型反演来生成假数据</p><p>  Santoro等人[28]描述了一种记忆增强神经网络（MANN），在该网络中，为了提高神经网络在元学习上的性能，学习了一个可微分的外部记忆。在NLP领域有许多工作允许模型利用知识的外部存储器[10,13]。Liu等人在无监督学习中使用了通过Hopfield网络实现的记忆和预训练特征。Mi和falings[22]在推荐系统的应用中使用MANN增量学习。有许多工作类似于最近邻的学习版本，通过计算查询点与特征和标签支持集的相似度来预测标签[24,31,34]。最近也有一些研究使用变压器来处理查询点与内存的关系。Gordo等人[12]应用一个转换器进行检索，适应一个查询点及其最近的邻居。Doersch等人[7]在元学习中使用转换器，根据与记忆中标记点的空间感知相似性预测查询的标签。</p><h1>方法</h1><p>  在本节中，我们首先描述问题定义和本文中使用的表示法。然后在第3.2节介绍我们的方法</p><h2 id="背景和符号"><a class="header-anchor" href="#背景和符号">¶</a>背景和符号</h2><p>  我们定义数据集 $\mathcal{D}=\{(x, y) \mid x \in \mathcal{X}, y \in \mathcal{Y}\}$ 它由一组图像 $\mathcal{X}$ 和它们对应的标签 $\mathcal{Y}$ 组成。我们假设标签 $\mathcal{Y}$ 属于 $\mathcal{C}$ 中的一个类， $\mathcal{C}$ 是 $\mathcal{C}$ 类的集合。</p><p>  在增量学习中，我们不能一次访问整个数据集。相反，数据集被分割成 $T$ 个任务，每个任务包含不相连的类，例如 $\mathcal{C}^{1},\mathcal{C}^{2},\dots ,\mathcal{C}^{T}$。其中 $\mathcal{C} = \mathcal{C}^{1} \cup \mathcal{C}^{2} \cup \dots \cup \mathcal{C}^{T}$ 并且 $\mathcal{C}^{i} \cap \mathcal{C}^{j} = \emptyset$ 其中 $i \ne j$。在任务 $t$ 中，我们只能访问类集 $\mathcal{C}^{t}$ 对应的数据，任务 $t$ 中的训练数据定义为 $\mathcal{D}^{t}=\{(x, y) \mid x \in \mathcal{X}^{t}, y \in \mathcal{Y}^{t}\}$ 。</p><p>  学习的模型通常由两部分组成:特征提取器和分类器。特征提取器 $f$ 将每个输入图像 $x$ 映射到 $d$ 维特征向量，即 $\mathbf{q}:=f(x) \in \mathbb{R}^{d}$。因此，对每个向量 $\mathbf{q}$ 应用分类器 $g:=\mathbb{R}^{d} \to \mathbb{R}^{c}$，得到类预测分数，或logits，用 $\mathbf{z}$ 表示，即 $\mathbf{z}:=g(\mathbf{q}) \in \mathbb{R}^{C}$。我们将特征提取器和分类器中使用的模型参数表示为 $\theta$。</p><p>  在类增量学习中，最近表现最好的方法是基于排练的方法[1,2,14,15,26,38]。也就是说，它们依赖于保存类训练数据的某个子集 $\mathcal{M}^{t} \subset \mathcal{D}^{t}$，即范例。范例可以存储为图像[1,2,14,26,38]或特征[15]。为了防止灾难性遗忘，任务 $t$ 中的训练数据包含前几个任务中的所有范例，即 $\mathcal{D}_{train}^{t} = \mathcal{D}^{t} \cup \mathcal{M}^{1:t-1}$。一般情况下，保存的样本数量明显少于样本总数，即 $|\mathcal{M}^{t}|&lt;&lt;|\mathcal{D}^{t}|$。</p><p>  最近的研究表明，通常交叉熵损失的替代方法可以使类增量学习中的训练受益[1,2,14]。在本研究中，我们使用了separated-softmax loss（分离式softmax-loss）[1]，它将分类损耗分为两项:<br>$$<br>\mathcal{L}_{S S}(x, y)=\delta_{y \in \mathcal{C}^{\prime}} \mathcal{D}_{K L}(\mathbf{y}^{t} \| \mathbf{p}^{t})+ \<br>\delta_{y \in \mathcal{C}^{1: t-1}} \mathcal{D}_{K L}(\mathbf{y}^{1: t-1} \| \mathbf{p}^{1: t-1}), \quad\quad\quad\quad(1)<br>$$</p><p>其中 $\mathbf{y}^{t}$ 是任务类 $\mathcal{C}^{t}$ 中给出标签 $y$ 的唯一热点向量，$\mathbf{p}^{t} = softmax(\mathbf{z}^{t})$ 是由模型生成的相同类的概率向量。我们还增加了分类损失（Eq.（1））与任务上的蒸馏损失，这已被证明在类增量学习中有效地防止遗忘[4,17]:<br>$$<br>\mathcal{L}_{T K D}(x)=\sum_{i=1}^{t-1} \mathcal{D}_{K L}\left(\mathbf{p}_{t-1}^{i} \| \mathbf{p}^{i}\right)<br>$$</p><p>其中，$\mathbf{p}_{t-1}^{i}$ 是用前一个任务训练结束时冻结的模型参数 $\theta _{t−1}$ 得到的。我们的总体目标是:<br>$$<br>\mathcal{L}(x, y)=\mathcal{L}_{S S}(x, y)+\mathcal{L}_{T K D}(x) .<br>$$</p><h2 id="内存转换器网络（Memory-Transformer-Network）"><a class="header-anchor" href="#内存转换器网络（Memory-Transformer-Network）">¶</a>内存转换器网络（Memory Transformer Network）</h2><p>  现有的工作只是在训练模型分类器 $g$ 时使用样例来防止灾难性的遗忘。我们的工作是基于这样的观察，因为我们必须保持样例的记忆，我们也可以利用样例记忆来改进我们的预测</p><p>  我们通过矩阵 $V$ 定义内存，这个矩阵是一个 $M×d$ 矩阵，包含特征向量，用特征提取器 $f$ 提取每个范例的特征向量。为了简单起见，我们将任务索引 $t$ 放在这里。通常情况下，记忆 $V$ 是用来在增量学习的训练过程中学习模型参数 $\theta$ 的同时，预习前面任务中的信息。内存在实际预测期间没有影响，它遵循分布 $p_{\theta}(y|\mathbf{q})$，其中 $y$ 是给出相应查询向量 $\mathbf{q}$ 的类标签</p><p>  然而，考虑到神经网络在增量学习时表现出的灾难性遗忘效应，我们在预测过程中显式地使用 $V$。也就是说，我们将输出标签分布建模为 $p_{\theta}(y|\mathbf{q},V)$，它现在依赖于记忆 $V$ 以及查询 $x$。$T$ 将范例记忆 $V$ 合并到预测过程中，我们在特征提取和logit预测之间添加了一层，这样我们就为输入 $\mathbf{q}$ 和范例记忆 $V: \tilde{\mathbf{q}}=h(\mathbf{q}, V)$ 适应特征。然后将输出 $\tilde{\mathbf{q}}$ 传递给分类器以生成logits:$\mathbf{z}=g(\tilde{\mathbf{q}})$。MTN模型如图2所示</p><p><img src="/2022/12/06/a-memory-t-ransformer-network-for-incremental-learning/1.png" alt="图2:MTN模型的概述。提供一个查询特征向量作为MTN的输入，以及一组示例内存特征向量中的k个最近邻居。MTN通过学习建模查询向量与其最近邻居之间的关系来适应输入查询特性。然后，我们将适应的特征传递给线性头来计算一组类分数（或logits）。"></p><p>  Transformer[33]是 $h$ 的自然候选。Transformers 将一组（可能是有序的）输入特征映射到一组输出特征，其中每个输出特征通过自我注意依赖于所有输入特征。这符合将范例记忆集 $V$ 纳入预测过程的建模需求。我们希望适应的特征能够注意到 $\mathbf{q}$，也能注意到范例记忆的特征 $V$。</p><p>  范例记忆集 $V$ 可以增长到数以万计的范例数量级。因此，我们只提供 $V$ 的一个子集作为给定输入 $\mathbf{q}$ 的上下文。为了为给定输入 $\mathbf{q}$ 选择 $V$ 的一个子集，我们首先从内存V中找到k个最近邻（kNNs） $\mathbf{q}$。因此，变压器的任务是对输入特征向量 $\mathbf{q}$ 进行局部注意，依赖于它在特征空间中的局部邻域。</p><h1>实验</h1><p>  在本节中，我们首先描述实验设置和实现细节。然后我们将我们的方法与现有的类增量学习方法进行比较。我们还介绍了MTN消融的详细研究，显示了不同设计选择的影响。</p><h2 id="实验设置"><a class="header-anchor" href="#实验设置">¶</a>实验设置</h2><p><b>数据集</b>我们分别在ImageNet-1K[8]和谷歌两个数据集上评估基于类的增量学习的MTN。按照标准实践[26]，我们使用平均增量精度作为评估指标，这是在T个任务中平均的前1个精度。现在我们将更详细地描述这两个数据集</p><p>  ImageNet-1k包含 $C=1000$ 个类、$120$ 万张训练图像和 $50k$ 张验证图像。我们遵循Hou等人[14]的工作，并使用以下方法创建增量任务，第一个任务包含一半的类，即 $C/2=500$ 个类。然后把剩下的班分成 $L=50$、$L=100$ 或 $L=250$ 个类的任务</p><p>  谷歌landmark-1k是来自谷歌Landmarks v2[35]的 $C=1000$ 类的一个子集，它最初包含 $203,094$ 个类。我们遵循Ahn等人[1]的工作，按照每个类的图像样本最大的顺序对 $1000$ 个类进行抽样。与ImageNet-1K类似，第一个任务包含 $500$ 个类，其他任务分为 $L=50$、$L=100$ 或 $L=250$ 个类。</p><p><b>特性表征</b>除了ResNet-18骨干网从头学到的特征外，我们还使用固定的特征，而不是通过整个骨干网反向传播，类似于[16,40]。然而，我们认为，使用预先训练的特征，以监督的方式进行分类[16,40]可能会混淆我们的结果，因为主干在任何时候都知道所有任务。因此，我们选择使用从没有(或弱)监督中获得的表示[6,25]。事实上，这些表示已经被证明是通用的和优秀的“现成”描述符，在大量的任务和数据集上表现良好，无需任何微调[3,42]。</p><p>  在实践中，我们使用 $d=512$ 的CLIP [25] （ViT-B/32）特征和 $d=2048$ 的SimCLR [6] （ResNet-50）特征作为固定特征。对于学习到的特征，我们使用使用SS-IL[1]学习并在每个任务中更新的ResNet-18 （$d=512$）骨干。在学习了特征提取器并冻结其权重之后，我们将分别学习分类器。</p><p>  在实践中，我们使用 $d=512$ 的CLIP [25] （ViT-B/32）特征和 $d=2048$ 的SimCLR [6] （ResNet-50）特征作为固定特征。对于学习到的特征，我们使用使用SS-IL[1]学习并在每个任务中更新的ResNet-18（d=512）骨干。在学习了特征提取器并冻结其权重之后，我们将分别学习分类器。</p><p><b>实现细节</b>用SGD优化器训练MTN，学习率为 $0.1$，权值衰减为 $0.0001$，动量为 $0.9$。我们将每个任务训练模型 $10$ 个周期。我们使用的批大小为 $128$，但也从以前的任务中添加 $32$ 个样本到每个批次[1]。我们使用Ringbuffer方法[5]来选择范例并构造内存。内存中有 $M$ 个向量在任意点处。随着新类的出现，现有类中的一些样例将被删除，以保持 $M$ 大小的内存。我们通常将内存大小设置为 $M =20k$，并选择每个查询的前 $k=10$ 个最近的邻居作为转换器的输入。我们的默认设置是一个 $4$ 层，$4$ 个注意头的多头自注意变压器，输出通道尺寸为 $128$。我们将发布代码和检查点来再现我们的结果。</p><h2 id="与现有的方法比较"><a class="header-anchor" href="#与现有的方法比较">¶</a>与现有的方法比较</h2><p>  我们演示了MTN在ImageNet-1k和landmark-1k上的有效性，这是table 1中的两个标准增量学习基准。为了公平比较并避免混淆因素，我们在与MTN类似的设置中重新实现了不同的方法[1,14,23,26,38]，即使用来自SimCLR和CLIP的固定预训练特征</p><p>  我们提出并报告一个简单的基线，“memk-NN”，它在样本的记忆范围内基于最近邻搜索进行分类。我们设 $k=10$ 为“mem-NN”，因为它提供了最高的性能。我们看到我们的MTN方法比“memk-NN”baseline执行得更好，比SimCLR特征（R50（SimCLR））+21%，展示了学习如何从记忆中聚合信息进行预测的有效性</p><p>  总的来说，我们在Table 1中观察到，当在固定表示的基础上进行训练时，MTN的性能显著优于其他现有方法，并为ImageNet-1k上的类增量学习设置了新的最先进的技术。与SS-IL的比较尤其有趣，因为我们重用了SS-IL提出的框架和损耗来训练MTN，如3.1节所述。具体地说，SS-IL和我们的唯一区别是分类器的体系结构;SS-IL使用的是线性层，而我们使用的是一个以存储器的元素为条件的变压器。我们在表1中观察到MTN优于SS-IL，显示了我们基于内存转换器的预测过程的有效性</p><p>  在所有的方法中，我们看到在ImageNet-1k上使用SimCLR特性比在landmark -1k上性能更好。我们假设，这是由于ResNet-50是在ImageNet-1k数据集（没有标签）上预先训练的，因此表示更适合于该数据集上的分类，而应用于landmark-1k时存在域差异。</p><p>  我们还表明MTN可以很容易地应用于从零开始学习的特征。对于ResNet-18实验，我们使用SS-IL从头训练特征提取器，并在每个任务结束时冻结特征提取器。然后我们学习MTN作为分类器。T able 1显示MTN在ResNet-18 SS-IL特征(?-R18(SS-IL)相比没有MTN的SS-IL达到+4%的相对改善。应用于SS-IL之上的MTN并不比ImageNet-1k上的最新基线表现更好，但我们注意到MTN架构不是特定于SS-IL的，可以应用于任何其他数量的方法，如PODNet[9]或CCIL[23]。另一方面，MTN在SS-IL特征上的应用使Landmarks-1k数据集达到了新的水平</p><p>  在图3中，我们展示了每个任务后所有可见类的准确性。我们报告MTN和两个基线的结果:“memk-NN”（使用内存的元素进行基于最近邻居的预测）和SS-IL[1]，它的训练方式与我们的相似，但使用线性分类器而不是MTN。我们看到SS-IL基线的准确性开始时与MTN的水平相似，但随着任务被看到，下降的速度更快，这表明它严重遭受灾难性遗忘。另一方面，MTN的精度下降速度并不快，因此总体增量精度较高。</p><h2 id="消融实验"><a class="header-anchor" href="#消融实验">¶</a>消融实验</h2><p>  在本节中，我们将评估MTN的不同设计选择，如从内存中选择的最近邻居的数量、变压器体系结构的细节或位置编码的使用。我们还提出了MTN学习内容的定性评价。所有实验都采用SimCLR特性(?-R50 (SimCLR))</p><p><b>kNN的数量</b>在MTN中输入的最近邻居的数量是不同的。更精确地说，超参数 $k$ 用于获取给定查询向量 $\mathbf{q}$ 的k-NN列表NN $(\mathbf{q};V)$。如上所述，从内存中选择 $k$ 个最近邻是必要的，并保持MTN的效率，因为将 $M=20k$ 个向量的整个内存输入到变压器是不切实际的。图4（左）显示了不同 $k$ 对MTN的影响。我们观察到MTN的精度在不同 $k$ 下保持稳定，只要它“足够大”（大于10）。我们所有的实验中，我们设 $k=10$ 。</p><p><b>内存大小</b>我们改变内存 $M$ 的大小，并报告MTN和SS-IL的平均增量精度，如图4（右）。我们观察到MTN对于较小的 $M$ 更加稳健，而对于较小的 $M$, SS-IL的准确性迅速下降。这表明MTN是需要非常小内存占用的高效增量学习系统的优秀候选人。</p><p><b>Transformer的大小</b>我们将在图4（左）中检查不同变压器架构的效果。我们将在 $3$ 个体系结构之间进行比较。“小”架构有 $2$ 层，$1$ 个注意头，输出通道尺寸为 $64$。“介质”架构有 $4$ 层和注意头，输出通道尺寸为 $128$。“大”架构有 $12$ 层和注意头部，输出通道尺寸为 $768$。“小”对于不同的事物来说没有那么强大 $k$ 的值。从“中等”到“大”没有任何好处，因为它们的表现相似。$W$ 使用我们所有实验的“介质”。</p><p><b>定性的结果</b>我们提出了一个定性的可视化来检查MTN的行为，并获得它如何学习做出更准确的预测的直觉。在图5中，我们观察了经过MTN处理后不同范例之间的相似性是如何演变的。对于每个查询图像，我们在内存中的范例中显示它的 $5$ 个最近的邻居，即MTN处理之前，并报告基于这些邻居投票的预测。这类似于基线“mem.k-NN”在Table1。然后，在图5的“MTN”行中，我们展示了这些示例与由MTN处理的查询的相似性。具体来说，我们在直方图中显示了mtn适应的查询向量q和mtn适应的k-NN特征 $\tilde{V}_{\mathrm{NN}_{M}(\mathbf{q})}$ 之间的相似性。我们在图5中观察到MTN调整后的特征如何具有更好的相似性</p><p>  对于以错误标签作为多数的邻居集，MTN能够成功地增加具有正确标签的邻居的权重。我们发现有趣的是，即使图像在高层次上是相似的，MTN分配大部分的权重给一个或两个图像。这表明MTN能够成功地区分混淆的例子。</p><h1>结论</h1><p>  我们介绍了MTN方法，它通过在预测过程中直接利用样本来提高基于样本的类增量学习性能。我们证明了MTN允许在各种基准测试中持续改进类增量学习性能。实验表明，具有通用预训练特征的MTN与最先进的方法具有竞争力。在未来的工作中，我们希望用未锁定的特性评估MTN，并以增量的方式训练完整的端到端体系结构。$\mathcal{D}_{K L}(\cdot \| \cdot)$</p>]]></content>
      
      
      <categories>
          
          <category> 科研学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 联邦学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《Rethinking Architecture Design for Tackling Data Heterogeneity in Federated Learning》</title>
      <link href="/2022/11/28/rethinking-architecture-design-for-tackling-data-heterogeneity-in-federated-learning/"/>
      <url>/2022/11/28/rethinking-architecture-design-for-tackling-data-heterogeneity-in-federated-learning/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h1>摘要</h1><p>  联合学习是一种新兴的研究范式，能够在不同组织之间协同训练机器学习模型，同时保持每个机构的数据保密。尽管最近取得了一些进展，但仍存在一些根本性挑战，如缺乏收敛性，以及跨现实世界异构设备的联合学习中存在灾难性遗忘的可能性。在本文中，我们证明了基于注意力的体系结构（例如Transformers）对分布变化具有相当的鲁棒性，因此改进了异构数据的联合学习。具体来说，我们对一系列联邦算法、真实世界基准和异构数据分割中的不同神经架构进行了第一次严格的实证研究。我们的实验表明，用变形金刚简单地替换卷积网络可以大大减少对先前设备的灾难性遗忘，加速收敛，并达到更好的全局模型，尤其是在处理异构数据时。我们将在https://github.com/Langqiong/ViT-FL-main 上发布我们的代码和预训练模型。鼓励未来在稳健架构中进行探索，作为当前优化领域研究工作的替代方案。</p><h1>介绍</h1><p>  联合学习（FL）是一种新兴的研究范式，用于在分布在多个异构设备上的私有数据上训练机器学习模型[43]。FL将每个设备上的数据保密，并旨在训练仅通过通信参数而不是数据本身更新的全局模型。因此，它为跨多个机构的协作机器学习提供了一个机会，而不会有私人数据泄露的风险[24，32，49]。事实证明，这在医疗保健[4，7]、从移动设备学习[17，34]、智能城市[24]和通信网络[45]等领域尤其有用，在这些领域，保护隐私至关重要。尽管FL提供了丰富的机会，但在FL能够容易地应用于真实世界的数据分布之前，仍有一些基本的研究问题需要解决。大多数当前旨在跨非IID设备学习单个全局模型的方法遇到了挑战，例如并行FL方法的非保证收敛和模型权重发散[20，33，62]，以及串行FL方法的严重灾难性遗忘问题[7，16，53]。</p><p>  虽然大多数研究工作集中于改进FL中的优化过程，但我们的论文旨在通过重新思考联邦模型中的架构选择提供一个新的视角。我们假设Transformer架构[13，56]特别适合于异构数据分布，因为它们对分布变化具有惊人的鲁棒性[3]。这一特性导致变形金刚在自我监督学习中的流行，其中异质性通过未标记的预训练数据和标记的测试数据之间的分布变化来表现[12]，以及在图像和文本等基本异质输入模式上的多模态学习[23，55]。为了研究这一假设，我们在一套联邦算法、真实世界基准和异构数据分割中对几种神经架构进行了首次大规模的经验基准测试。为了表示Transformer网络，我们在跨越图像分类[29，36]和医学图像分类[26]的图像任务上使用Vision Transformer[13]的标准实现。</p><p>  我们的结果表明，VIT-FL在具有最异构设备拆分的设置中表现特别好，VIT-FL和具有ResNets[18]的FL之间的差距随着异构性的增加而显著增加。为了进一步了解这些结果，我们发现，改进的主要来源在于Transformer模型对异构数据的鲁棒性增强，这减少了在使用完全不同的新设备进行训练时对以前设备的灾难性遗忘。一起，变形金刚收敛更快，并达到适合所有设备的更好的全球模型。通过与专门针对异构数据设计的FL方法进行全面比较，我们发现VIT-FL在不使用训练启发式、额外的超参数调整或额外的训练/微调的情况下提供了即时改进。我们得出结论，Transformer模型应被视为未来研究中FL问题的自然起点。</p><h1>相关工作</h1><p><img src="/2022/11/28/rethinking-architecture-design-for-tackling-data-heterogeneity-in-federated-learning/1.png" alt="图1：具有标签分布偏差的CIFAR-10[29]的非IID数据分区上的典型串行FL方法CWT[7]和并行FL方法FedA VG[42]的简化示意图。Wt，在客户端i的t轮训练期间识别模型权重（共涉及K个客户端）。在右边，我们展示了使用UMAP的ViT FedA VG和ResNet FedA VG的特征嵌入可视化[41]。我们发现，与ResNet FedA VG学习的特性相比，ViT FedAVG学习的特性更加明显。我们的实验（第4.2节）支持VIT-FL在异构数据上的优势，我们提供了解释其有效性的实证分析（第4.3节）"></p><p>  <b>联合学习</b> 联合学习（FL）旨在跨大规模分布式设备在私有数据上训练机器学习模型[43]。为了实现跨异构设备的有效分布式训练，出现了两类方法：（1）并行FL方法涉及同步或异步并行训练每个本地客户端（如经典的FedA VG[43]），而（2）串行方法以串行和循环方式训练每个客户端（如循环权重转移（CWT）[7]和分割学习[57]）。FedA VG[43]和CWT[7]的示意性描述如图1所示。在其核心，FL在跨客户端的训练数据分布中存在数据异质性挑战，这导致并行FL方法[20，33，62]的非保证收敛和模型权重发散，以及串行FL方法的严重灾难性遗忘问题[7，16，53]。在经典并行FedA VG算法[43]的最新发展中，包括使用服务器动量（FedA VGM）来减轻每个客户端的分布变化和不平衡[21]，在所有用户之间全局共享小数据子集（FedA VGA共享）[62]，使用本地目标的近端术语（FedProx）来减少严重异构设备下的潜在权重差异[33]，以及通过匹配和平均隐藏元素（FedMA）以分层方式构建共享全局模型[59]。</p><p>  同时，最近的几项努力旨在缓解连续和连续学习中的灾难性遗忘：限制对先前看到的任务或客户端重要的权重的更新（弹性权重合并（EWC）[28]），应用深度生成回放模拟来自先前客户端或任务的数据[54]，以及应用循环加权目标来减轻标签分布偏度的性能损失[2]等。然而，所有这些方法主要集中于改进优化算法，而没有研究架构设计中提高数据分布变化鲁棒性的潜力。在我们的工作中，我们发现，简单的架构选择实际上会产生很大的影响，应该成为与优化方法并行的一个活跃的研究领域，而优化方法一直是当前工作的主要焦点</p><p>  <b>Transformer</b> Transformer架构最初被提出用于序列到序列机器翻译[56]，随后在许多NLP任务中建立了最先进的性能，尤其是在自我监督范式中训练时[12]。最近，变形金刚也被发现广泛适用于涉及图像和视频的任务。例如，Parmar等人[46]在图像的局部邻域中应用了自我关注，而视觉变换器（ViT）[13]通过将具有全局自我关注的变换器直接应用于全尺寸图像，实现了ImageNet分类的最先进水平</p><p>  相对于经典的语言架构（即LSTM[19]）和视觉架构（即CNN[18,31]），其有趣的性能提升激发了人们对理解其有效性背后原因的兴趣。在几个特别相关的发现中，ViTs对严重的闭塞、扰动、域移位[3，44]以及合成和自然对抗示例[40，47]具有高度鲁棒性。此外，最近的研究表明，Transformer也适用于异构和多模态数据[23，37，55]。受这些发现的启发，我们假设ViTs在适应FL中的数据异质性方面也会非常有效，并提供详细的实证分析来验证这一假设。</p><h1>Transformers in Federated Learning</h1><p>  在本节中，我们将介绍Transformer架构和联合学习方法的背景知识。</p><h2 id="Vision-Architectures（视觉架构）"><a class="header-anchor" href="#Vision-Architectures（视觉架构）">¶</a>Vision Architectures（视觉架构）</h2><p>  <b>CNN</b>对于基于卷积的体系结构，我们使用ResNet-50[18]，它包含一系列卷积、ReLU、池和批处理归一化层。ResNet-50是最受欢迎的图像分类体系结构之一，是对图像数据进行FL处理的标准体系结构[1,35]。</p><p>  <b>Transformers.</b>作为比较，我们使用视觉Transformers.(ViT)[13]，它不使用传统的卷积层。而是通过以下两个步骤提取图像特征:</p><ol><li><b>图像顺序化</b>在[13]之后，我们首先执行标记化，将输入 $\mathbf{x}$ 重塑为一个扁平的 $2D$ 补丁序列 $\left\{\mathbf{x}_{p}^{i} \in \mathbb{R}^{P^{2} \cdot C} \mid i=1, . ., N\right\}$，其中每个patch的大小为 $P × P$, $N=\frac{HW}{p^{2}} $ 为图像patch的个数（即输入序列长度）。</li><li><b>Patch Embedding</b>我们使用可训练的线性投影将向量化的补丁 $\mathbf{x}_{p}$ 映射到潜在的d维嵌入空间。为了编码空间信息，我们学习特定的位置嵌入，这些位置嵌入被添加到补丁嵌入中，以保留如下的位置信息。</li></ol>$$\mathbf{z}_{0}=\left[\mathbf{x}_{\text {class }} ; \mathbf{x}_{p}^{1} \mathbf{E} ; \mathbf{x}_{p}^{2} \mathbf{E} ; \cdots ; \mathbf{x}_{p}^{N} \mathbf{E}\right]+\mathbf{E}_{\text {pos }}  \quad\quad\quad\quad(1)$$其中 $\mathbf{x}_{class}$ 代表类令牌。$\mathbf{E} \in \mathbb{R}^{\left(P^{2} \cdot C\right) \times D}$ 和 $\mathbf{E}_{pos} \in \mathbb{R}^{\left(P^{2} \cdot C\right) \times D}$ 分别表示斑块嵌入投影和位置嵌入。变压器编码器由L层多头自注意（MSA）和多层感知器（MLP）块组成（式（2）（3））。因此 $\ell -th$ 层的输出可以写成:$$\mathbf{z}_{\ell}^{\prime}=\operatorname{MSA}\left(\operatorname{LN}\left(\mathbf{z}_{\ell-1}\right)\right)+\mathbf{z}_{\ell-1} \quad\quad\quad\quad(2)$$$$\mathbf{z'}_{\ell}^{\prime}=\operatorname{MLP}\left(\operatorname{LN}\left(\mathbf{z'}_{\ell}\right)\right)+\mathbf{z'}_{\ell} \quad\quad\quad\quad(3)$$其中 $LN(·)$ 表示层归一化算子，第 $L$ 层输出的第一个元素 $\mathbf{z}_{L}^{0}$ 是对应的图像表示 $\mathbf{y}$。<p><b>Hybrid Model（混合模型）</b>最后，我们还实验了[13]中提出的CNN-Transformer混合（vi-h）模型，其中ResNet-50（GN）首先被用作特征提取器，为输入生成特征映射。这里的GN表示用组规范化替换ResNet-50中的所有批处理规范化层。patch embedding应用于从CNN feature map中提取的 $1×1$ patch，而不是从原始图像中提取。</p><h2 id="Federated-Learning-Methods（联邦学习方法）"><a class="header-anchor" href="#Federated-Learning-Methods（联邦学习方法）">¶</a>Federated Learning Methods（联邦学习方法）</h2><p>  我们使用最流行的并行方法之一(FedA VG[43])和串行方法(CWT[7])作为训练算法（参见图1中的示意图描述）。</p><p><b>Federated Averaging。</b>FedAVG将每个客户端的局部随机梯度下降（SGD）与平均[43]的迭代模型相结合。具体来说，在每一轮通信中随机抽样一小部分本地客户机，服务器将当前全局模型发送到每一个这些客户机。然后，每个选定的客户端对其本地训练数据执行本地SGD的 $E$ epoch，并将本地梯度发送回中央服务器进行同步聚合。然后，服务器应用平均梯度来更新其全局模型，并重复此过程。</p><p><b>Cyclic Weight Transfer 循环重量转移。</b>与FedAVG中对每个本地客户端进行同步、并行的训练不同，CWT中的本地客户端采用串行、循环的方式进行训练。在每一轮训练中，CWT在一个本地客户端上用它的本地数据训练一个全局模型，训练时间为若干个epoch $E$，然后循环地将这个全局模型转移到下一个客户端进行训练，直到所有本地客户端都在[7]上训练过一次。然后，训练过程在客户机中重复循环，直到模型收敛或达到预定义的通信轮数。</p><h1>实验</h1><p>  我们的实验旨在回答以下对FL方法的实际部署非常重要的研究问题，同时也帮助我们理解（远景）Transformer架构。</p><ol><li>Transformers是否能够在FL设置中学习更好的全局模型，而不是在cnn中学习FL任务的实际方法（第4.2节）？</li><li>Transformer是否特别能够处理异构数据分区（第4.3.1节）？</li><li>与cnn相比，变压器是否降低了通信成本（第4.3.2节）？</li><li>在FL（章节4.4）中部署transformer有哪些实用的提示？</li></ol><p>  附录中包含实验代码，将在盲审后对外公布。</p><h2 id="实验设置"><a class="header-anchor" href="#实验设置">¶</a>实验设置</h2><p>  接下来[7,20]，我们在我们的研究中对Kaggle糖尿病视网膜病变竞赛数据集（称为Retina）[26]和CIFAR-10数据集[29]上评估不同的滤泡方法。具体地说，我们将视网膜数据集中的标签二值化为健康（阳性）和患病（阴性），随机选择6000张平衡的图像用于训练，3000张图像作为全局验证数据集，3000张图像作为[7]后的全局测试数据集。我们使用CIFAR-10中的原始测试集作为全局测试数据集，从原始训练数据集中提取5000张图像作为全局验证数据集，并使用剩下的45000张图像作为训练数据集。视网膜和CIFAR-10数据集的详细图像预处理步骤见附录A.1。我们为Retina和CIFAR-10模拟了三组数据分区:一个iid数据分区，和两个标签分布倾斜的非iid数据分区。视网膜和CIFAR-10数据集中的每个数据分区分别包含4个和5个模拟客户端。我们使用每两个客户端之间的平均Kolmogorov-Smirnov （KS）统计量来衡量标签分布的偏度。$KS = 0$ 表示IID数据分区，而 $KS = 1$ 表示非常非IID数据分区。详细的数据分区见附录A.1。</p><p>  我们使用线性学习率预热和衰减调度器的ViT-FL。采用线性预热衰减和阶跃衰减两种方法选择具有cnn的FL的学习率调度器。采用梯度裁剪（全局范数1）来稳定训练。我们将所有FL方法中的本地训练周期设置为1，并将总通信次数设为100，除非另有说明。更多实现细节见附录A.2。</p><h2 id="结果"><a class="header-anchor" href="#结果">¶</a>结果</h2><p><img src="/2022/11/28/rethinking-architecture-design-for-tackling-data-heterogeneity-in-federated-learning/3.png" alt="表1:视网膜数据集的预测精度（$\%$）。箭头表示ResNet对应的性能w.r.t的变化。视觉转换器(包括ViT和ViT-H)表现出一贯的强大性能，特别是在non-iid数据分区中。"></p><p><img src="/2022/11/28/rethinking-architecture-design-for-tackling-data-heterogeneity-in-federated-learning/4.png" alt="表2:CIFAR-10数据集的预测精度（$\%$）。箭头表示ResNet对应的性能w.r.t的变化。视觉转换器（ViT和ViT-h）在IID和non-IID数据分区中表现出一贯的强大性能，特别是在最异构的分裂（split -3）上有显著的改进。"><br><b>不同神经结构和（理想的）集中训练的FL的比较</b>:无论采用哪种架构（R-50、ViT和ViT-h）作为骨干网（表1和表2），在IID设置上，CWT和FedA VG都能取得与在中央托管数据（称为Central）上训练的模型相当的结果。然而，我们观察到在异构数据分区上，特别是在极端异构数据分区上（Split 3, CIFAR-10的kS-1）， R50-CWT和R50-FedAVG的测试准确性显著降低。通过简单地用ViT（或vita-h）替换ResNet-50, CWT和FedAVG成功地保持了模型的准确性，即使是在高度异构的non-iid设置中。特别是在CIFAR-10数据集的高异质性Split-3、KS-1上，vitc-cwt和vitc-fedaVG相对于R50-CWT和R50-FedAVG提高了 $77.79\%$ 和 $37.35\%$ 的测试精度。因此，ViT特别适用于FL中的异构数据。</p><p><b>与现有FL方法的比较</b>:我们还将vita -FL与几种最先进的基于优化的FL方法进行了比较:FedA VGM[21]、FedProx[33]和FedA VG-Share[61]。我们使用ResNet-50作为所有比较的FL方法的骨干网。我们通过网格搜索在Split-2数据集上优化最佳参数（包括学习速率、FedAVGM的动量参数 $β$ 和FedProx的近项惩罚常数 $\mu$），并将相同的参数应用于所有剩余的数据分区。我们允许每个客户在FedAVG-Share中彼此共享 $5\%$ 的数据。</p><p><img src="/2022/11/28/rethinking-architecture-design-for-tackling-data-heterogeneity-in-federated-learning/2.png" alt="图2:与以ResNet-50为骨干的基于最新优化的联邦学习方法的比较。基于Vision transformer的FL方法（viT-cwt和viT-fedaVG）在non-iid数据分区中优于其他方法。"><br>  如图2所示，在non-iid数据分区中，vitl-FL的性能明显优于所有其他FL方法。FedProx[33]和FedAVGM[21]在高度异构的数据分区上都遭受了严重的性能下降，尽管仔细调优了优化参数。同样，FedAVGShare在高度异构的数据分区Split-3上也会出现性能下降，即使 $5\%$ 的本地数据在所有客户机之间共享（CIFAR-10数据集上Split-3是 $94.4\%$，而Split-1上是 $97\%$ ）。我们得出的结论是，简单地使用transformer可以改进最近为联邦优化设计的几种方法，这些方法通常需要仔细调优优化参数。</p><h2 id="Transformers的有效性分析"><a class="header-anchor" href="#Transformers的有效性分析">¶</a>Transformers的有效性分析</h2><p>鉴于在一套FL基准上的这些有前景的实证结果，并与最先进的FL方法进行比较，我们现在进行仔细的实证分析，以揭示到底是什么促进了Transformers性能的提高。</p><h3 id="Transformers在non-iid设置下泛化效果更好"><a class="header-anchor" href="#Transformers在non-iid设置下泛化效果更好">¶</a>Transformers在non-iid设置下泛化效果更好</h3><p>  FL的分布式特性意味着跨客户端的数据分布可能存在很大的异质性。先前的研究表明，用FedAVG或CWT训练FL模型会分别引发权值发散和灾难性遗忘等问题[28,53]。我们认为，在cnn中使用的局部卷积（已被证明更多地依赖于局部高频模式[15,25,58]）可能对异构设备特别敏感。这一问题在医疗保健数据的FL中尤其普遍，因为不同机构捕获的输入图像可能由于不同的医疗成像协议而在局部模式（强度、对比度等差异）上存在显著差异[16,50]，以及由于用户在说[30]、打字[17]和写[27]时的特性而在用户之间的自然数据分割中存在显著差异。另一方面，与cnn相比，ViTs对局部模式的偏好明显减少，而是使用自我注意来学习全局相互作用[48]，这可能有助于它们对分布偏移和对抗性扰动的惊人鲁棒性[3,44]。为了对transformer跨异构数据的泛化能力进行更深入的分析，我们设计了以下两个经验实验:</p><p><b>跨异构设备的灾难性遗忘：</b>通过cnn学习的模型在非分布数据上的效果通常较差。这种现象在连续FL法CWT中尤为严重。由于其顺序和串行的训练策略，在CWT范式中训练cnn通常会导致对非iid数据分区的灾难性遗忘:在具有不同数据分布的新客户机上进行几次更新后，模型在以前客户机上的性能突然下降[3,44]。</p><p>  这导致了较差和较慢的收敛，这在外语学习中是不可取的。在迁移学习文献中也发现了类似的遗忘问题[8,11,52]。</p><p><img src="/2022/11/28/rethinking-architecture-design-for-tackling-data-heterogeneity-in-federated-learning/5.png" alt="图3:左:随着更多的客户端参与CIFAR-10数据集Split 3上的CWT学习，客户端3验证数据集（与其本地训练数据集共享相同的数据分布）上的预测精度的变化。右:放大左图中的红色矩形。并给出了不同客户的培训顺序。在高度异构的数据分布下，R50-CWT的顺序训练策略往往会对之前的客户端造成灾难性的遗忘ViT-CWT具有较强的泛化能力，有助于缓解这一问题。"><br>  我们在CIFAR-10数据集的Split-3上评估了CWT，以显示这种灾难性的遗忘现象。在图3中，我们绘制了随着更多客户端参与CWT学习，Client-3验证数据集（与训练数据集共享相同的数据分布）上预测精度的变化。当在Client-3上将一个训练良好的模型转移到Client-4时，前一个Client-3验证数据集上的预测精度突然急剧下降（从 $&gt; 98\%$ 下降到 $&lt; 1\%$ 的精度）。然而，以ViT为骨干（ViT-cwt）训练的模型能够将知识从Client-3转移到Client-4，同时只丢失Client-3上的少量信息（准确率保持在 $98\%$）。因此，vit可以更好地泛化到新的数据分布，而不会忘记旧的数据分布。关于灾难性遗忘的其他分析，请参考附录B.1。</p><p><b>VIT-FL在现实联邦数据集上的推广：</b>一个训练有素的联邦模型应该能很好地处理其他未见客户端的非分布测试数据集。为了测试transformer的可泛化性，我们进一步将其应用到现实世界的联邦CelebA数据集[36]，并将其与ResNet对应数据集FedProx[33]和FedAVG-Share进行比较[61]。CelebA是一个大型的人脸属性数据集，拥有超过200K的名人图像，并由图像上的名人参与。我们使用LEAF基准[5]提供的特别设计的CelebA联邦版本，该版本根据图片中的名人划分设备。在[5]之后，我们测试二元分类任务（smile的存在），丢弃少于10个样本的客户机，并使用总客户机的 $5\%$ 进行训练。结果共429个客户，平均 $22.6±6.23$ 个样本，共9704个样本。</p><p>  我们报告了在图4中所有本地客户端测试数据的联合上，使用不同的FL方法训练的模型的平均值(带标准差)、最大值和最小测试精度。我们的VIT-FL方法（VIT-cwt为 $91.8±0.57\%$，ViT-fedaVG为 $91.9±0.42\%$）的性能与集中训练（$91.17\%$）相当，优于最先进的FL方法，结果的标准差也小得多。使用ResNet-50，虽然一些客户端在其他客户端的测试数据集上取得了很好的性能，但仍然有一些客户端受到了影响（例如，FedProx的最大精度是 $93.1\%$，但最小精度下降到 $75.5\%$）。在FedAVG-Share中，在所有客户端中共享 $5\%$ 的数据有助于改善泛化，但仍显示出较大的偏离平均值（$90.8±2.21$）。相比之下，在ViTl-FL下训练的模型在低方差的所有客户端上表现出一贯的强大性能。这再次表明Transformers比cnn的同类学会了更好的全局模型。</p><h3 id="Transformers收敛更快，更优"><a class="header-anchor" href="#Transformers收敛更快，更优">¶</a>Transformers收敛更快，更优</h3><p>  强大的FL方法不仅应该在IID和non-IID数据分区上健壮地执行，而且还应该具有较低的通信成本，以便能够在通信受限的带宽上进行部署。通信成本是由直到收敛的通信轮数和通信参数（通常是模型参数）的数量决定的。我们计算需要通信轮数，以实现预定义的目标测试集精度的95%的中央训练ResNet-50的预测精度。具体而言，我们将Retina和CIFAR-10数据集的目标精度分别设置为 $77.5\%$ 和 $91.5\%$。我们将串行CWT方法上的一个通信轮定义为一个跨所有联邦本地客户机的完整训练周期。</p><p>  如图5和表3所示，所有评估的FL方法在同构数据分区上快速收敛到目标测试性能。然而，R50FedA VG和R50-CWT的收敛速度随着异构性的增加而下降，甚至在高度异构的数据分区上达到一个平台（永远达不到目标精度）。相比之下，vitl-fl在异构数据分区上仍然能快速收敛。例如，在CIFAR-10上的异构数据分区Split-2和Split-3上，由于严重的灾难性遗忘，R50-CWT完全发散，而ViT-cwt在经过3轮和72轮通信后达到了目标性能。虽然VIT比ResNet-50有更多的模型参数（例如，vit16的85.8M和ResNet-50的23.5M），但由于通信轮数少，ViT-fl仍然降低了总通信成本。</p><h2 id="FL中Transformers实际使用的要点"><a class="header-anchor" href="#FL中Transformers实际使用的要点">¶</a>FL中Transformers实际使用的要点</h2><p><b>局部训练周期：</b>在FL中，使用E表示局部模型经过其局部数据集的轮数是标准的。已知 $E$ 强烈影响FedAVG[43]和CWT[7]的性能。我们进一步开展了一项实验研究，研究了本地培训阶段 $E$ 对ViT-FL的影响，为实际使用提供指导。我们认为对于ViT-fedaVG, $E \in \{1,5,10\}$，对于ViT-CWT, $E \in \{1,5\}$。从图6中，我们发现ViT表现出与CNN相似的现象，即较大的 $E$ 加速了ViT-fedaVG在异构数据分区上的收敛，但可能导致异构数据分区上的最终性能下降。类似地，ViT-CWT还支持每个客户机之间的频繁传输速率，如R50-CWT[7]在non-iid数据分区上。</p><p>  因此，我们建议用户在同构数据或轻度异构数据分区上使用大 $E$，以降低通信成本，但在高度异构的数据分区上建议使用小 $E$ （ViT-fedaVG  $E≤5$ ,CWT $E = 1$)。</p><p><b>其他训练技巧：</b>相对较小的学习率和梯度范数剪辑是稳定CWT中ViT训练的必要条件，特别是在高度异构的数据分区中。梯度范数剪辑也有助于训练跨异构数据的cnn的FL，因为它已被证明可以减少局部更新和当前全局模型[33]之间的权重差异。请参考附录B.2了解更多一般提示和实验分析。</p><h1>结论</h1><p>  尽管最近在FL方面取得了进展，但在收敛和灾难性遗忘方面仍然存在挑战，特别是在处理异构数据时。不同于以往专注于改进优化过程的方法，我们通过重新思考联邦模型中的体系结构设计提供了一个新的视角。利用变压器对异构数据和分布偏移的鲁棒性，我们进行了广泛的实验分析，并证明了变压器在减轻灾难性遗忘、加速收敛和达到并行和串行FL方法的更好最优方面的优势。我们发布我们的代码和模型，以鼓励在当前优化方面的研究工作的同时，在健壮架构方面进行未来的开发</p><p><b>局限性：</b>尽管Transformers在FL取得了很好的成绩，但仍存在一些挑战。首先，由于ViT在每轮中有更多的通信参数（更大的模型尺寸），因此在同构数据分区上的通信成本高于其cnn对应的通信成本。对于异构数据，vit的通信成本较低，这使得它们在这种设置下具有吸引力（因为更快的收敛减少了轮数)。因此，如果通信带宽有限，类似于稀疏变压器[9]的模型可能是一个明智的选择。更多的本地训练周期也可以用来减少通信成本，但可能会在异构数据设置中降低最终性能。进一步分析用更多的局部训练时间保持模型质量是未来工作的一个重要领域。未来的工作还应该使变压器适应图像检测和分割的FL任务</p><p><b>更广泛的影响：</b>FL为大规模分布式培训提供了工具，但同时也要求在现实环境中进行谨慎部署。我们在这里列出了几个潜在的问题:</p><p><b>隐私：</b>根据敏感设备数据进行预测可能存在隐私风险。虽然联邦学习使数据在每个设备上都是私有的，而不发送到其他位置，但它是在分布式模型训练[6]时揭示私有特性。因此，在收集设备数据并将其应用于FL算法之前，获得用户的同意仍然是至关重要的</p><p><b>安全性：</b>在整个培训过程中交流模型更新可能会向第三方或中央服务器[14]透露敏感信息。FL也可能对来自对手的外部安全攻击很敏感[38]。</p><p><b>公平：</b>由于数据集不平衡，可能存在曝光偏倚的风险，特别是在涉及个人医疗保健数据时。用有偏见的数据训练的模型已被证明可以放大潜在的社会偏见，特别是当它们与预测目标[60]相关时</p><p>  总之，我们相信我们的发现可以帮助提高FL对异构数据的鲁棒性和收敛性。未来的工作应该进一步量化性能、通信、隐私、安全和公平之间的权衡。我们的方法的灵活性还提供了与这些领域的进步集成的机会，如差异隐私[10]、对抗训练[34]、公平代表学习[39]、模型压缩[51]和顺序学习[28]。</p>]]></content>
      
      
      <categories>
          
          <category> 科研学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 联邦学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《A distillation-based approach integrating continual learning and federated learning for pervasive services》</title>
      <link href="/2022/11/09/a-distillation-based-approach-integrating-continual-learning-and-federated-learning-for-pervasive-services/"/>
      <url>/2022/11/09/a-distillation-based-approach-integrating-continual-learning-and-federated-learning-for-pervasive-services/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h1>摘要</h1><p>  联邦学习（Federated Learning）是一种新的机器学习范式，它增强了边缘设备的使用，正受到普及社区的广泛关注，以支持智能服务的发展。然而，这种方法仍然需要适应普及领域的特殊性。特别是，需要解决与持续学习相关的问题。在本文中，我们提出了一种基于蒸馏的方法来处理联合学习场景中的灾难性遗忘。具体来说，人类活动识别任务用作演示域。</p><h1>介绍</h1><p>  普及计算促进了我们生活空间中连接设备的集成，以帮助我们进行日常活动。这些设备收集数据，可以运行一些本地计算，并进一步向用户提供建议，通过服务对环境采取行动，或者只向全球服务器提供信息</p><p>  我们现在看到了基于机器学习（ML）技术的智能服务的出现[5]。此外，当前的实现实际上是基于分布式架构的，在分布式架构中部署模型并经常在云中执行。然而，这种方法并不适用于普适计算。它在安全性、性能和成本方面受到了重大限制。</p><p>  谷歌最近提出了联合学习（FL）[4][12]，这是一种新的机器学习范式，增强了边缘设备的使用。FL鼓励在边缘设备上计算本地模型，并将它们发送到云服务器，在那里它们被聚合为更通用的模型。新模型被重新分配给设备，作为下一次局部学习迭代的引导模型。FL降低了通信成本并提高了安全性，因为只有模型在边缘和云之间交换[21]。它作为一种有前途的范例，可以满足基于ML的普及应用程序的挑战，立即引起了人们的关注。尽管如此，这种方法仍需要适应具体情况无处不在的领域。</p><p>  在大多数当前解决方案中，FL使用静态本地数据进行操作，在每个客户的整个培训过程中保持不变[11]。然而，在现实世界场景中，边缘的新数据不断可用，模型必须适应它，同时不要忘记过去的经验。这需要使用持续学习（CL）方法[27]。使用深度神经网络的CL的主要挑战是灾难性遗忘[24]，[18]，这是由于根据新任务而不是旧任务优化整个网络而引起的。</p><p>  CL的特点是学习过程的顺序性，具体地说是任务的顺序。任务t是一组与其他（以前或将来）任务中的类不相交的类，其中t是其任务ID[22]。大多数CL方法处理任务增量学习（任务IL）场景[19]，其中关于示例的任务ID的信息在测试时已知。然而，更具挑战性的场景是分类学习（IL类），其中模型必须在测试时区分所有任务的所有类别[22]。</p><p>  在我们的工作中，我们关注CL的IL类场景，它与FL场景相结合。我们的目的是解决以下研究问题：FL有助于防止客户的灾难性遗忘吗？FL能否帮助分享所有客户的过去知识，提高他们在未知任务上的表现？我们可以利用全局服务器来提高客户端的性能吗？</p><p>  我们决定在移动设备上使用人类活动识别分类（HAR）作为我们的演示领域[25；7]，因为“自然”客户端：智能手机的可用性，并且该领域没有像图像分类那样得到很好的研究。由于某些类（运动动作）的密切关系，这种选择具有挑战性。</p><p>  在本文中，我们提出了一种基于蒸馏的方法，用于处理联邦连续学习（FCL）中用于HAR分类任务的灾难性遗忘。在第2节中，我们给出了FL和CL字段的一些背景，并描述了当前关于FCL问题的现有解决方案。在第3节中，我们介绍了我们的工作方法。在第4节中，我们提出了我们的方法。在第5节中，我们展示了实验结果并对其进行了讨论。在第5节中，我们以一个结论结束了这项工作。</p><h1>背景以及相关工作</h1><h2 id="联邦学习"><a class="header-anchor" href="#联邦学习">¶</a>联邦学习</h2><p>  传统FL的主要目标是在服务器上实现良好的全局模型。FL过程包括多轮通信：1）客户端的本地训练（使用静态本地数据集），之后客户端将其更新的参数发送到服务器；2） 服务器将获取的模型的参数进行聚合，以定义一个新的全局模型，该模型将进一步重新分发给所有客户端，成为下一轮通信中的初始模型</p><p>  FedAvg[23]使用客户模型的简单加权平均值，其中权重基于本地数据集中的示例数。FedPer[3]在基础层和专门化层上分离客户机模型的层，并仅向服务器发送基础层。FedMa[28]通过融合相似的神经元进行分层聚集</p><p>  在我们的工作中，我们希望在服务器和客户端上都实现良好的性能，因为它们是模型的主要潜在用户。为此，我们将使用FedAvg作为基本方法。它显示了HAR[11]上的竞争结果，并且不需要对移动设备至关重要的大量计算资源。</p><h2 id="连续学习"><a class="header-anchor" href="#连续学习">¶</a>连续学习</h2><p>  在单机上的标准CL中，模型从一系列任务中迭代学习。在每个培训课程中，模型只能按顺序访问序列中的一个任务培训结束后，任务不再可用。</p><p>  对于IL类场景，CL方法可分为三大类[22]：基于正则化的方法，其计算先前任务的权重的重要性并惩罚模型更改权重（EWC[18]、MAS[1]、PathInt[13]、LwF[20]），基于范例的方法，存储先前任务的范例（iCarl[26]、EEIL[10]），以及明确处理对最近学习的任务的偏差的偏差校正方法（BiC[30]、LUCIT[15]、IL2M[6]）。</p><h2 id="联邦连续学习"><a class="header-anchor" href="#联邦连续学习">¶</a>联邦连续学习</h2><p>  在FCL中，每个客户端都有其可私人访问的任务序列。每一轮客户机根据其序列对某个任务训练其本地模型，然后将其参数发送到服务器。</p><p>  据我们所知，迄今为止，只有少数基于联合学习和持续学习的融合的作品被提出。</p><p>  对于FL中的任务IL图像分类问题，最近提出了FedWeIT[32]。它基于将模型参数分解为密集的全局参数和稀疏的任务自适应参数，这些参数在所有客户端之间共享。LFedCon2[9]和FedCL[31]处理FL中的单任务场景。LFedCon2使用传统分类器代替DNN，并提出了一种基于集成再训练的概念漂移处理算法。FedCL采用EWC[18]（基于正则化的CL方法，该方法不能显示IL类场景的最佳结果[22]）。FedCL旨在提高联邦模型的泛化能力。</p><p>  然而，这些方法都不能解决我们在工作中研究的FL中的类IL CL场景问题。这就是为什么我们的研究在这一领域做出了新的贡献。</p><h1>方法论</h1><h2 id="人类活动识别-UCI数据集"><a class="header-anchor" href="#人类活动识别-UCI数据集">¶</a>人类活动识别 UCI数据集</h2><p>  我们使用UCI HAR数据集[2]，HAR研究社区广泛使用该数据集来测量和比较最新的结果。在30名志愿者的帮助下，将三星Galaxy S II放在腰部，收集UCI数据集。一个数据示例包含128个加速度计和陀螺仪记录（均为3维）。数据集中有6个类（每个类的示例数在括号中）：0–行走（1722），1–向上行走（1544），2–向下行走（1406），3–坐下（1777），4–站立（1906），5–躺着（1944）。</p><p>  为了理解类之间的关系，我们对实验中使用的神经网络的最后一层（激活前）上的UCI数据集进行了主成分分析（PCA）[17]（详见第3.3节）。为了更好地表示，我们随机选择了每个类的200个示例，并在三维空间中绘制了该数据的前3个主要成分（见图1）。</p><p><img src="/2022/11/09/a-distillation-based-approach-integrating-continual-learning-and-federated-learning-for-pervasive-services/1.png" alt="图1:UCI数据集在所用神经网络的最后一层上的PCA，前3个主成分的3D空间。"></p><p>  我们可以看到，“躺着”这个班与其他班很有区别。“坐着”和“站着”的位置很近。行走动作定位在一起，但很难单独区分。在我们的实验中，这样的数据分布将对某些类的灾难性遗忘产生影响。例如，在图2中，我们将看到，即使在培训课程中没有这个类的例子，也不会忘记第5类（铺设）。</p><h2 id="假定，设想"><a class="header-anchor" href="#假定，设想">¶</a>假定，设想</h2><ol><li>我们专注于类增量CL场景，因为我们希望在所有可见的类中进行分类，这在现实世界中更常见</li><li>在每一轮通信中，每个客户机在与某个任务相对应的新本地数据集上训练自己的模型，就像在现实世界中，在与服务器通信之间的一段时间内在移动设备上收集新数据一样。除非另有说明，否则前几轮的数据不可用</li><li>由于我们仍在工作中坚持任务的定义，任何任务都不能在每个客户端的私有任务序列中多次看到。然而，与来自不同客户端的私有序列的任务相对应的类集可以重叠。</li><li>为了简单起见，客户的行为是同步的，因此他们都参与了每一轮交流。</li></ol><h2 id="FCL问题定义"><a class="header-anchor" href="#FCL问题定义">¶</a>FCL问题定义</h2><p>  在上述假设下，我们给出了以下符号：</p><ol><li>每个客户端 $k \in \{1, 2, \cdots, K\}$ 具有其私人可访问的 $n_{k}$ 个任务序列 $T_{k}$：$$ \mathcal{T}_{k}=\left[\mathcal{T}_{k}^{1}, \mathcal{T}_{k}^{2}, \ldots, \mathcal{T}_{k}^{t}, \ldots, \mathcal{T}_{k}^{n_{k}}\right], \mathcal{T}_{k}^{t}=\left(C_{k}^{t}, D_{k}^{t}\right) $$ 其中 $t \in \{1, \cdots , n_{k} \}$ 并且 $C_{k}^{t}$ 是表示客户端 $k$ 的任务 $t$ 的一组类。$D_{k}^{t} = \{ X_{k}^{t} , Y_{k}^{t} \}$ 是对应于 $C_{k}^{t}: C_{k}^{i} \cap C_{k}^{j}=\varnothing$ 的训练数据，如果 $i \ne j$</li><li>用于客户端 $k$ 的每个任务 $\mathcal{T}_{k}^{t}$ 在 $r_{k}^{t}$ 次通信循环期间被训练，并且 $\sum_{t=1}^{n_{k}} r_{k}^{t}=R$，其中 $R$ 是服务器和客户端之间的通信循环的总数；</li><li>在通信期间，$r$ 轮客户端 $k$ 使用训练数据 $D_{kr} = \{X_{kr}, Y_{kr}\}$ : $$D_{k r}=D_{k r}^{t} \subset D_{k}^{t}, \quad \sum_{t=1}^{t-1} r_{k}^{t} < r \leqslant \sum_{t=1}^{t-1} r_{k}^{t}+r_{k}^{t}$$ 其中 $D_{k^{\prime} r^{\prime}} \cap D_{k^{\prime \prime} r^{\prime \prime}}=\emptyset$ 如果 $k' \ne k''$ 并且 $r' \ne r''\left(1 \leqslant k^{\prime}, k^{\prime \prime} \leqslant K, 1 \leqslant r^{\prime}, r^{\prime \prime} \leqslant R\right)$</li></ol><h2 id="客户场景。训练和测试装置"><a class="header-anchor" href="#客户场景。训练和测试装置">¶</a>客户场景。训练和测试装置</h2><p>  作为FL中的主要CL场景，我们观察客户端的行为，该客户端在通信轮次总数的一半中学习一个任务，而另一半则学习一个新任务。</p><p>  在上面的符号中，客户机1总共搞学习 $n_{1} = 2$ 个任务：$\mathcal{T}_{1}^{1}=\left( C_{1}^{1}, D_{1}^{1} \right)$ 和 $\mathcal{T}_{1}^{2}=(C_{1}^{2}, D_{1}^{2})$，其中 $C_{1}^{1} = {1}$ 和 $C_{1}^{2} = {2}$ 分别是“向上走”和“向下走”类；$\mathcal{T}_{1} = \left[ \mathcal{T}_{1}^{1}, \mathcal{T}_{1}^{2} \right]$，$r_{1}^{1} = r_{1}^{2} = R/2$</p><p>  为了简单起见，我们假设所有其他 $K− 1$ 个客户端的行为类似，我们可以通过单个广义客户端来概括它们在FL过程中的影响。我们还假设所有客户端在每一轮都包含相同数量的示例，因此客户端数据集的大小不会影响遗忘。这就是为什么在服务器聚合期间，广义客户端和观测客户端的权重为 $1/K*(K-1)$ 和 $1/K$。</p><p>  通用客户端对数据进行在线学习（每轮训练相同的任务），这些数据平衡良好，包含所有课程。因此，它总共学习了 $n_{g}=1$ 个任务：$\mathcal{T}_{g}^{1}=(C_{g}^{1}，D_{g}^{1})$，其中 $C_{g}^{1}=\{0, 1, 2, 3, 4, 5\}$，有它的私人可访问任务序列 $\mathcal{T}_{g}=[\mathcal{T}_{g}^{1}]$，因此 $r_{g}^{1}＝R$。</p><p>  为了构建训练和测试集，我们根据每个客户端所需的CL场景，从UCI HAR数据集中随机选择了示例。如上所述，对于每个客户端 $k$ 和每个通信循环 $r$，我们构建了一个相同大小的训练组 $D_{kr}$。</p><p>  我们在所有客户端和服务器的公共测试集上评估模型的性能。测试数据集包括每个类的100个示例（总共600个示例）。</p><h2 id="神经网络架构"><a class="header-anchor" href="#神经网络架构">¶</a>神经网络架构</h2><p>  服务器和所有客户端使用相同的神经网络架构。由于我们应该考虑移动设备上的小处理能力，我们希望限制我们使用的模型的复杂性和大小。我们对卷积神经网络（CNN）中的不同架构进行了比较，这些架构用于其他最先进的研究[16]，[29]</p><p>  通过集中式方法，我们在用于我们工作中所有实验的数据集UCI HAR上训练模型：70%的数据是训练集，15%是验证集，15%为测试集。使用大小为32且辍学率为0.5的小批量SGD训练模型。</p><p><img src="/2022/11/09/a-distillation-based-approach-integrating-continual-learning-and-federated-learning-for-pervasive-services/2.png" alt="表1:UCI HAR测试集上不同模型架构的比较。"><br>  表1显示，196-16C_4M_1024D模型给出了最佳结果，我们在实验中使用了它。该模型包括196个16x1卷积层的滤波器，然后是1x4最大池化层，然后是1024个密集层单元，最后是softmax层。</p><h2 id="实验设置"><a class="header-anchor" href="#实验设置">¶</a>实验设置</h2><p>  所有实验都是在Python 3上使用TensorFlow 2库编写的，并在CPU Intel（R）Xeon（R）2.30GHz（2个CPU核，12GB可用RAM）上运行</p><p>  我们对所有模型使用相同的初始权重，这些模型是通过在一个平衡良好的小数据集（每个类10个示例）上预先训练所选CNN（参见第3.5节）而获得的</p><p>  我们为客户端的本地培训运行 $R=8$ 个通信轮和 $E=10$ 个周期。我们假设我们有 $K=5$ 个客户（一个客户概括了其中 $K− 1$ 个客户的影响）。每个客户端 $k$ 和每轮 $r$ 的数据集大小为 $|D_{kr}|=120$ 。我们使用了学习率 $η=0.01$，辍学率等于 $0.5$ ，批量大小 $B=32$ 和SGD优化器。</p><h1>问题演示</h1><p><img src="/2022/11/09/a-distillation-based-approach-integrating-continual-learning-and-federated-learning-for-pervasive-services/3.png" alt="图2：标准CL（a）和FCL（b）中灾难性遗忘的演示。"></p><p>  在标准CL（图2（a））中，我们可以看到，当学习任务更改为 $C_{2}={2}$ 时，类 $C_{1}={1}$ 立即被遗忘。但我们总能在第5课（铺设）上看到良好的表现，即使我们在训练中没有这些课的例子。正如我们在图1中看到的，铺设类与其他类非常不同，因此最初的预训练模型成功地学习了它，用于分类的模型参数没有随着进一步的训练而改变。</p><p>  在FCL（图2（b））中，当我们获得新数据时，我们使用了模型的简单微调。我们仍然可以看到客户端1立即灾难性地忘记了任务1。但是联合学习成功地将关于其他静态动作的知识传递给了客户端1（类3-5）。</p><h1>提议</h1><p>  为了处理FCL中的灾难性遗忘，我们提出了一种基于蒸馏的方法，其灵感来自无F组织学习（LwF）[20]，使用客户端上的先前模型和当前服务器模型作为当前客户端模型的教师。</p><h2 id="Baseline-FLwF"><a class="header-anchor" href="#Baseline-FLwF">¶</a>Baseline: FLwF</h2><p>  知识蒸馏最初是为了将知识从大模型（教师）转移到小模型（学生）[8]，[14]。在LwF中，作者在CL中使用这种技术来防止学习新任务时的漂移。为此，建议在训练模型时使用蒸馏损失</p><p>  首先，我们在FCL中实现了标准的LwF方法。在一个标准的LwF中，有一个教师模型（过去的客户模型 - round $r − 1$） 和一个学生模型（当前客户模型–第 $r$ 轮）。当我们使用在所有类上预先训练的初始模型时，每个客户机从一开始就对所有 $n=6$ 个类有一定的了解。</p><p>  教师分类器的输出逻辑（来自轮 $r-1$ 的客户端的过去模型） 在FCL中表示为 $\mathbf{o}^{r-1}(x)=\left[o_{1}^{r-1}(x), \ldots, o_{n}^{r-1}(x)\right]$，其中 $x$ 是网络的输入，并且学生分类器的输出逻辑（循环 $r$ 的当前客户端模型）是 $\mathbf{o}^{r}(x)=\left[o_{1}^{r}(x), \ldots, o_{n}^{r}(x)\right]$。</p><p>  FCL中LwF方法的客户 $k$ 和通信轮 $r$ 的蒸馏损失定义为：<br>$$<br>L_{\text {dis_cl }}\left(D_{k r} ; \theta_{r}^{k}, \theta_{r-1}^{k}\right)=\sum_{x \in X_{k r}} \sum_{i=1}^{n}-\pi_{i}^{r-1}(x) \log \left[\pi_{i}^{r}(x)\right]<br>$$<br>其中 $θ_{r}^{k}$ 是通信轮 $r$ 中客户端 $k$ 的当前（学生）模型的权重（$θ_{r−1}^{k}$–先前（教师）模型），$D_{kr}=\{ X_{kr}，Y_{kr} \}$ 是客户端 $k$ 在通信循环 $r$ 期间使用的数据集，$π_{i}^{r’}(x)$ 是网络的温度标度逻辑：<br>$$<br>\pi_{i}^{r^{\prime}}(x)=\frac{e^{o_{i}^{r^{\prime}}(x) / T}}{\sum_{j=1}^{n} e^{o_{j}^{r^{\prime}}(x) / T}},<br>$$<br>其中 $T$ 是温度缩放参数[14]。温度标度的对数 $π_{i}^{r−1}(x)$ 指教师模型的预测（$\mathbf{o}^{r−1}(x)$）和 $π_{i}^{r}(x)$ 是指学生模型（$\mathbf{o}^{r}(x)$ 的预测。</p><p>  FCL中的分类损失（软最大交叉熵）为：<br>$$<br>L_{\text {class }}\left(D_{k r} ; \theta_{r}^{k}\right)=\sum_{(x, y) \in D_{k r}} \sum_{i=1}^{n}-y_{i} \log \frac{\exp \left(o_{i}^{r}(x)\right)}{\sum_{j=1}^{n} \exp \left(o_{j}^{r}(x)\right)},<br>$$<br>其中 $(x,y) \in D_{kr} = \{X_{kr}, Y_{kr}\}$ 并且 $D_{kr} \subset D_{kr}^{t}$；$x$ 是训练样本的输入特征向量，$y$ 对应于集合 $C_{k}^{t}$ 的某个类，并表示为对应于 $x:y \in \{0,1\}^{n}=6$ 的一个one-hot真值标签向量。</p><p>  每个客户 $k$ 的FCL标准LwF中的最终损失由分类损失和蒸馏损失组成，该分类损失和精馏损失是用当前客户 $k$ 的模型计算得出的，用于第 $r$ 轮，而客户 $k$ 的先前模型用于 $r-1$ 轮:<br>$$<br>L_{F L w F}=\alpha L_{\text {class }}+(1-\alpha) L_{\text {dis_cl }}<br>$$<br>其中 $α$ 是正则化每个项影响的标量</p><p>  我们将此实现命名为无遗忘联合学习（FLwF）。</p><h2 id="提案：FLwF-2T"><a class="header-anchor" href="#提案：FLwF-2T">¶</a>提案：FLwF-2T</h2><p>  我们建议利用服务器，它对所有客户机都有一般的了解。我们的目标是将服务器用作第二个教师，并将其知识发送给学生客户端模型。第一位老师（过去的客户模型）可以提高学生（客户）的特殊性表现，使其在之前学习的任务上仍然表现出色。第二个教师（服务器）可以通过从所有其他客户机转移知识来改进客户机模型的一般特征，并避免客户机模型在其新任务上的过度拟合。</p><p>  我们表示 $\hat{\mathbf{o}}^{r-1}(x)=\left[\hat{o}_{1}^{r-1}(x), \ldots, \hat{o}_{n}^{r-1}(x)\right]$ 作为全局模型（服务器网络）的输出（在激活之前），该输出是在循环 $r-1$ 之后训练的客户端模型的聚合之后获得的。服务器网络的温度缩放逻辑定义如下：<br>$$<br>\hat{\pi}_{i}^{r-1}(x)=\frac{e^{\hat{o}_{i}^{r-1}(x) / T}}{\sum_{j=1}^{n} e^{\hat{o}_{j}^{r-1}(x) / T}}<br>$$</p><p>  服务器模型的蒸馏损失定义为：<br>$$<br>L_{\text {dis_serv }}\left(D_{k r} ; \theta_{r}^{k}, \theta_{r-1}\right)=\sum_{x \in X_{k r}} \sum_{k=1}^{n}-\hat{\pi}_{k}^{r-1}(x) \log \left[\pi_{k}^{r}(x)\right]<br>$$<br>其中 $θ_{r}^{k}$ 是通信轮 $r$ 中客户端 $k$ 的当前（学生）模型的权重，$θ_{r−1}$ 是服务器上的全局模型的权重，该权重是在 $r-1$轮 通信之后通过客户端模型的聚合而获得的。</p><p>  建议方法的最终损失为：<br>$$<br>L_{F L w F-2 T}=\alpha L_{\text {class }}+\beta L_{\text {dis_cl }}+(1-\alpha-\beta) * L_{\text {dis_serv }}<br>$$<br>其中 $α$ 和 $β$ 是正则化项影响的标量。$L_{\text {dis_cl}}$ 是指第4.1节中定义的过去模型（教师1）的蒸馏损失，$L_{\text{dis-serv}}$ 是指服务器模型（教师2）。</p><p>  对于第一轮通信，模型仅使用服务器模型作为教师。</p><p>  所提出的解决方案有助于从服务器转移知识，并减少对先前学习的任务的遗忘。这是一种基于正则化的方法，因为它可以在学习新任务时防止激活漂移。我们将其命名为 <b>Learning without Forgetting - 2 Teachers (LwF-2T)</b>。算法1中给出了FLwF-2T的伪码，其中损失函数 $L_{\text{FLwF−2T}}(\cdots;b)$ 在批次 $b$ 上计数。</p><p><img src="/2022/11/09/a-distillation-based-approach-integrating-continual-learning-and-federated-learning-for-pervasive-services/4.png" alt="FLwF-2T算法"></p><h2 id="度量"><a class="header-anchor" href="#度量">¶</a>度量</h2><p>  为了评估模型性能，我们使用了分成两组的几个指标。在第一组中，我们使用描述流程FL方面的指标：模型的通用性和客户特定知识的性能。第二组度量从流程的CL侧评估对所学任务的遗忘。让我们将 $a_{t,d}^{kr}$ 定义为客户机 $k$ 学习任务 $t(d \le t)$ 后任务 $d$ 在通信循环 $r$ 期间训练的模型的精度。为了计算 $a_{t,d}^{kr}$，我们从测试集中获取与任务 $d$ 对应的所有类的示例，计算在通信循环 $r$ 期间训练的模型在学习任务 $t$ 后对它们的精度。在客户端 $k$ 或服务器的通信循环 $r$ 之后，在整个测试集上计算精度 $a_{0}^{kr}$。</p><p>  <b>FL 指标</b>：</p><p>  为了评估模型的通用性，我们计算了一般的精度（<b>general accuracy</b>，$A_{gen}^{k}$）：<br>$$<br>A_{g e n}^{k}=\frac{1}{R} \sum_{r=1}^{R} a_{0}^{k r}<br>$$<br>我们计算指定客户机、通用客户机和服务器的一般精度。</p><p>  为了根据客户的特定知识评估模型的性能，我们计算个人准确度（<b>personal accuracy</b>）（$A_{per}^{k}$）：<br>$$<br>A_{p e r}^{k}=\frac{1}{R} \sum_{r=1}^{R} a_{p e r}^{k r}<br>$$<br>其中，准确度 $a_{per}^{kr}$ 是根据客户 $k$ 在轮次 $1, \cdots ,r$ 期间已经学习的类计算的。我们仅针对具有特定CL场景的客户计算个人准确度。</p><p>  <b>CL 指标</b>：</p><p>  为了评估模型如何忘记先前学习的任务，我们在任务 $t$ 中使用所有之前学习的任务和遗忘的平均准确性[22]。我们仅针对具有特定CL场景的客户端计算它们。</p><p>  <b>A verage accuracy at task</b>：$t(A_{t}^{k})$ 根据客户 $k$ 在任务之前学到的所有知识计算：<br>$$<br>A_{t}^{k}=\frac{1}{t} \sum_{d=1}^{t} a_{t, d}^{k}, \quad a_{t, d}^{k}=a_{t, d}^{k}=\frac{1}{r_{k}^{t}} \sum_{r^{\prime}=\left(r_{k}^{t}\right)^{\prime}}^{\left(r_{k}^{t}\right)^{\prime}+r_{k}^{t}} a_{t, d}^{k r^{\prime}},<br>$$<br>它可以平均为 $F_{t}^{k}=\frac{1}{t-1} \sum_{d=1}^{t-1} f_{t, d}^{k}$。$F_{t}^{k}$ 越高，模型越容易忘记。</p><h1>实验</h1><p>  在本节中，我们介绍了比较FCL中不同方法的实验。我们还建议对一般客户使用不同的培训策略，并使用学习前任务的范例。</p><h1>方法的比较</h1><p>  我们比较了FCL中的以下方法：简单连续学习中的微调（CL-FT）、联合连续学习中微调（FCL-FT）、联合学习而不遗忘（FLwF）、我们提出的使用两名教师的方法（FLwF-2T）以及针对每个客户机的所有培训数据（所有数据）的离线学习。表2、3第一部分和第二部分的结果</p><p>  对于方法FLwF和FLwF2，我们使用了其他实验中常用的温度缩放参数 $T=2$[30]，[10]，[22]。通过使用网格搜索，我们发现对于FLwF $α=0.001$ 和对于FLwF $α=0.001$，$β=0.7$ 表现出最佳性能。</p><h2 id="通用客户端：微调"><a class="header-anchor" href="#通用客户端：微调">¶</a>通用客户端：微调</h2><p>  在这个实验中，我们建议根据当前的本地数据选择一种训练策略。如果当前通信轮次中的本地数据平衡良好且包含所有类，我们建议使用简单的微调，不依赖于受其他客户端影响的服务器全局模型，如果没有，我们建议实施CL方法来记住我们的过去，并尽可能多地从服务器传递可用的知识</p><p>  我们定义了当客户端1使用FLwF（FLwF2T）并且广义客户端使用FT作为FLwF/FT（FLwF1T/FT）时的策略。结果见表2、3的第四部分<br><img src="/2022/11/09/a-distillation-based-approach-integrating-continual-learning-and-federated-learning-for-pervasive-services/5.png" alt="表2:FL指标：客户机1（$k=1$）、通用客户机（$k=g$）和服务器（$k=Server$）的一般准确性（$A_{gen}{k}$）和个人准确性（$A_{per}{k}$）。"><br><img src="/2022/11/09/a-distillation-based-approach-integrating-continual-learning-and-federated-learning-for-pervasive-services/6.png" alt="表3:CL度量：客户端1（$k=1$）和任务2（$t=2$）的任务 $t(A_{t}^{k})$ 和遗忘（$F_{t}^{k}$）的平均准确度。"></p><h2 id="保存示例"><a class="header-anchor" href="#保存示例">¶</a>保存示例</h2><p>  在这个实验中，我们将学习任务的范例保存在记忆中。对于每一轮，我们实现了以下过程：如果任务是新的，我们将与此任务对应的10个示例保存在内存中；如果模型已经学习了这个任务，我们会用与这个任务对应的新示例刷新内存。我们实施了随机抽样策略，因为它显示了与另一种策略相比的竞争结果，该策略用于课堂增量CL，并且不需要太多计算资源[22]。结果见表2、3的第三部分和第五部分。</p><h2 id="结果讨论"><a class="header-anchor" href="#结果讨论">¶</a>结果讨论</h2><p><img src="/2022/11/09/a-distillation-based-approach-integrating-continual-learning-and-federated-learning-for-pervasive-services/7.png" alt="图3：根据FLwF-2T/FT（我们的）所示的表2和表3得出的最佳结果。"><br>  如果我们比较图2（a）和图3（b），我们会发现我们的持续学习方法（FLwF-2T）既有助于提高模型的通用性，也有助于保持学习任务中的知识。</p><p>  首先，我们比较了客户机1及其特定CL场景的FCL中的不同方法。对于客户端1，我们的方法FLwF-2T显示了所有方法中最佳的总体精度（表2）。它在通用客户端和服务器上的性能也优于FLwF。与FCL-FT相比，FLwF-2T的个人准确性降低了，这是因为FT时显著的过度拟合。我们还可以看到，使用CL方法的FL有助于处理灾难性遗忘。我们的方法FLwF-2T在处理客户机1的灾难性遗忘方面显示了最好的结果（表3）。</p><p>  然后，我们建议根据当前通信回合的本地数据使用不同的训练策略。我们可以看到，在比较的方法中，我们在客户机1上使用FLwF-2T和在通用客户机上使用FT的方法在捕获模型的通用性和保持过去的知识方面都显著优于所有其他方法。因此，根据本地数据为不同的客户实施不同的培训策略可以提高FCL的总体性能。</p><p>  然后，我们进行了实验，从学习的任务中保存样本。我们可以看到，添加示例可以提高所有观察到的方法的CL度量的性能。当我们不为客户选择培训策略时，FLwF-2T+ex显示了所有添加示例的方法中的最佳结果。当用FT训练广义客户端时，FLwF/FT+ex比FLwF-2T/FT+ex对广义客户端和服务器显示出更好的泛化精度。</p><p>  根据表2、3（图3），FLwF-2T/FT——我们提出的方法——显示了FL和CL度量的所有观察方法中的最佳结果。</p><h1>结论</h1><p>  我们的研究提出了一种融合两种学习范式的新框架：联邦学习和类增量学习，并将其应用于普适计算（移动设备上的HAR）</p><p>  我们表明，在FL中，如果没有CL方法，也会发生对所学任务的快速遗忘。我们看到，UCI HAR数据集中类之间的密切关系会影响对其中一些类的更快遗忘。最后，我们针对FCL提出了基于蒸馏的方法FLwF-2T，该方法不需要高计算和存储资源，这对于移动设备至关重要，因为它只使用客户端的过去模型和服务器发送的全局模型进行训练，在每一轮通信中，客户端都可以使用该模型。我们表明，它允许为具有特定CL场景的客户增加一般知识，并保留其过去的私人知识。</p>]]></content>
      
      
      <categories>
          
          <category> 科研学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 联邦学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《Continual Federated Learning Based on Knowledge Distillation》</title>
      <link href="/2022/10/29/continual-federated-learning-based-on-knowledge-distillation/"/>
      <url>/2022/10/29/continual-federated-learning-based-on-knowledge-distillation/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h1>摘要</h1><p>  联合学习（FL）是一种很有前途的方法，可以在多个客户端拥有的分散数据上学习共享的全局模型，而不会暴露其隐私。在现实场景中，客户端累积的数据随时间的分布而变化。因此，在学习新任务时，全球模型往往会忘记从以前的任务中获得的知识，显示出“灾难性遗忘”的迹象。以前的集中学习研究使用数据重放和参数正则化等技术来减轻灾难性遗忘。不幸的是，这些技术无法充分解决FL中的非平凡问题。我们提出了带蒸馏的连续联合学习（CFeD）来解决FL下的灾难性遗忘。CFeD在客户端和服务器上执行知识蒸馏，每一方独立拥有一个未标记的代理数据集，以缓解遗忘。此外，CFeD为不同的客户分配了不同的学习目标，即学习新任务和回顾旧任务，旨在提高模型的学习能力。结果表明，我们的方法在减轻灾难性遗忘方面表现良好，并在两个目标之间实现了良好的权衡。</p><h1>介绍</h1><p>  联合学习（FL）[McMahan et al.，2017]被提出作为一种解决方案，使用多个客户拥有的分散数据学习共享模型，而不公开其私人数据。图1展示了一个FL任务的示例，以推断移动设备用户的使用习惯。敏感数据，即屏幕时间的时间直方图流，在移动设备（客户端）上收集并用于训练本地模型。同时，在中央服务器上构建了一个全局模型，它利用客户端提交的本地模型，而不访问任何客户端的私有数据。在每一轮培训期间，服务器首先向客户端广播全局模型。然后，每个客户端独立地使用其本地数据来更新模型，并将模型更新提交给服务器。最后，服务器聚合这些更新，为下一轮生成新的全局模型。</p><p><img src="/2022/10/29/continual-federated-learning-based-on-knowledge-distillation/1.png" alt="图1：（a）预测移动设备使用习惯的FL系统概述。（b） 特定用户在两周内的屏幕时间分布。（c） 从新数据中学习时，本地更新遭受灾难性遗忘。"></p><p>  尽管FL可以很好地保护数据隐私，但由于以下问题，其性能在实践中存在风险。首先，由于网络故障，参与一轮培训的客户可能在下一轮中不可用，从而导致连续轮之间的培训数据分布发生变化。其次，在客户端积累的数据可能会随着时间的推移而变化，甚至可能被视为具有不同数据分布或不同标签的新任务，这对模型的适应性提出了重大挑战。此外，由于原始数据的不可访问性，最小化新任务的损失可能会增加旧任务的损失。这些问题都导致全球模式的表现不佳，这一现象被称为“灾难性遗忘”。</p><p>  具体而言，FL系统中的灾难性遗忘主要表现为两类，即任务内遗忘和任务间遗忘。（1） 当两个不同的客户子集参与两个连续的轮次时，会发生任务内遗忘。在图1（a）中，由于客户端不参与训练轮，新的全局模型可能会忘记在前几轮中从该客户端获得的知识，从而在其本地数据上表现不佳。（2） 当客户积累具有不同域或不同标签的新数据时，会发生任务间遗忘。如图1（c）所示，由于第2周屏幕时间数据的不同分布，新的全局模型在第1周数据上的性能下降。应该指出，非IID问题给这两种遗忘带来了更多挑战。</p><p>  在本文中，我们提出了一个名为连续联合学习与提取（CFeD）的框架，以减轻学习新任务时对任务内和任务间类别的灾难性遗忘。具体地说，CFeD以两种方式利用基于未标记的公共数据集（称为代理数据集）的知识蒸馏。首先，在学习新任务时，每个客户机通过其自己的代理数据集将旧任务的知识从收敛于最后一个任务的模型转移到新模型中，以减轻任务间遗忘。同时，CFeD将这两个目标分配给不同的客户以提高性能，称为客户划分机制。其次，服务器还维护另一个独立的代理数据集，通过将当前和上一轮学习的知识提取到新的聚合模型中，从而在每一轮中微调聚合模型，称为服务器提取。</p><p>  本文的主要贡献如下：</p><ol><li>我们将持续学习扩展到联合学习场景，并定义了持续联合学习（CFL），以解决学习一系列任务时的灾难性遗忘问题。（第3节）</li><li>我们提出了一个名为CFeD的CFL框架，它使用基于代理数据集的知识蒸馏来减轻服务器端和客户端的灾难性遗忘。在每一轮中，通过指派客户学习新任务或回顾旧任务来减轻任务间遗忘。通过在服务器端应用蒸馏方案来减轻任务内遗忘。（第4节）</li><li>我们通过改变数据分布并在文本和图像分类数据集上添加类别来评估CFL的两种场景。CFeD在克服遗忘方面优于现有的FL方法，而不会牺牲学习新任务的能力。（第5节）</li></ol><h1>相关工作</h1><h2 id="连续学习"><a class="header-anchor" href="#连续学习">¶</a>连续学习</h2><p>  连续学习（CL）旨在解决学习一系列任务时的稳定性-可塑性困境[Delange等人，2021]。稳定性强的模型几乎不会忘记什么，但在新任务中表现不佳。相比之下，可塑性更好的模型可以快速适应新任务，但往往会忘记旧任务。现有的CL方法通常可以分为三类：数据重放、参数分离和参数正则化。</p><p>  数据重放方法的核心思想[Chaudhry等人，2018]是存储和重放旧任务的原始数据，以缓解遗忘。然而，存储旧数据直接违反了隐私和存储限制。参数分离方法[Hung等人，2019]通过分配模型参数的不同子集来处理每个任务，从而克服灾难性遗忘。然而，随着新任务的到来，分离方法将导致参数的无限增加，这在FL中可能很快变得不可接受。</p><p>  基于正则化的方法通过惩罚重要参数的更新[Kirkpatrick等人，2017]或向目标函数添加知识蒸馏[Hinton等人，2015]损失来限制更新过程[Li和Hoiem，2017；Lee等人，2019；Zhang等人，2020]，以从旧模型中学习知识。然而，很难准确评估其重要性。一些基于蒸馏的方法基于新的任务数据执行蒸馏，但当域变化很大时，其效率显著下降。其他利用未标记外部数据的方法解决了上述困难。CFeD采用公共数据集的知识蒸馏，并提出了一种客户端划分机制，以减少时间和计算成本。</p><h2 id="联邦学习"><a class="header-anchor" href="#联邦学习">¶</a>联邦学习</h2><p>  因此，他们研究的目的是为参与的客户收集当地模型。虽然我们的工作看起来很相似，但我们的研究问题却截然不同，因为我们的目标是学习更好的全球模型。因此，我们决定在实验研究中不与之比较</p><p>  基于FedAvg，[Jeong等人，2018]提出了联邦蒸馏框架，以提高通信效率，而不影响性能。还有几项工作利用从公共数据集构建的额外数据集[Li和Wang，2019；Lin等人，2020]或原始本地数据[Itahara等人，2020]来帮助蒸馏。与上述工作相比，我们提出的减轻灾难性遗忘的方法是正交的，可以与它们一起工作。</p><h1>问题定义</h1><p>  在FL中，一个中央服务器和 $K$ 个客户机通过 $R$ 轮通信协作训练任务 $\mathcal{T} $ 的模型。优化目标可以写为：<br>$$<br>\min _{\theta} \sum_{k=1}^{K} \frac{n_{k}}{n_{a}} \mathcal{L}\left(\mathcal{T}^{k} ; \boldsymbol{\theta}\right) \quad\quad (1)<br>$$<br>其中 $\mathcal{T}^{k}$ 是指第 $k$ 个客户端处的 $n_{k}$ 个训练样本的集合，$n_{a}$ 是所有 $n_{k}$ 个的总和。</p><p>  在这里，我们定义了连续联合学习（CFL），它旨在通过服务器和客户端之间的迭代通信，在客户端积累的一系列任务 $\{ \mathcal{T}_{1}, \mathcal{T}_{2}, \mathcal{T}_{3}, ···\}$ 上训练全局模型。</p><p>  当第 $t$ 个任务到达时，以前任务的数据将无法用于训练。我们将从先前任务中获得的全局模型参数定义为 $\theta _{t-1}$，并且新任务为 $\mathcal{T}_{t}=\bigcup_{k=1}^{K} \mathcal{T}_{t}^{k}$，其中 $\mathcal{T}_{t}^{k}$ 包含在每个客户端处新收集的数据。</p><p>  目标是训练新任务 $\mathcal{T}_{t}$ 和旧任务 $\{ \mathcal{T}_{1}, ···, \mathcal{T}_{t-1} \}$ 损失最小的全局模型。 优化目标是通过迭代的服务器-客户端通信，最大限度地减少 $K$ 个客户端在直到时间 $t$ 的所有本地任务上的损失。全局模型参数可按如下方式获得：<br>$$<br>\boldsymbol{\theta}_{t}=\arg \min _{\boldsymbol{\theta}} \sum_{k=1}^{K} \frac{n_{k}}{n_{a}} \sum_{i=1}^{t} \mathcal{L}\left(\mathcal{T}_{i}^{k} ; \boldsymbol{\theta}\right) \quad\quad (2)<br>$$</p><p>  在任务 $\mathcal{T}_{t}$ 的全局模型预计在历史任务上不会比在时间 $t-1$ 时实现更高的损失。这就是：$\sum_{i=1}^{t-1} \mathcal{L}\left(\mathcal{T}_{i} ; \boldsymbol{\theta}_{t}\right) \leq \sum_{i=1}^{t-1} \mathcal{L}\left(\mathcal{T}_{i} ; \boldsymbol{\theta}_{t-1}\right)$</p><p>  然而，在现实场景中，由于访问先前数据的限制，CFL在“任务内”和“任务间”级别遭受灾难性遗忘。形式上，任务内遗忘意味着在第 $r$ 轮更新后，全局模型在第 $k$ 个客户端的本地数据集上获得比第 $(r-1)$ 轮更高的损失：$\mathcal{L}\left(\mathcal{T}_{t}^{k} ; \boldsymbol{\theta}_{t, r}\right)&gt;\mathcal{L}\left(\mathcal{T}_{t}^{k} ; \boldsymbol{\theta}_{t, r-1}\right)$，尤其是当跨客户端的分发是非IID时。然后，任务间遗忘是指在时间t时模型在旧任务上的损失高于在时间 $t-1$ 的损失:$\sum_{i=1}^{t-1} \mathcal{L}\left(\mathcal{T}_{i}^{k} ; \boldsymbol{\theta}_{t}\right)&gt;\sum_{i=1}^{t-1} \mathcal{L}\left(\mathcal{T}_{i}^{k} ; \boldsymbol{\theta}_{t-1}\right)$</p><h1>建议的方法</h1><p>  为了解决FL中的灾难性遗忘，我们提出了一个名为CFeD（带蒸馏的连续联合学习）的框架。如图2所示，核心思想是使用最后一个任务的模型来预测代理数据集，并将输出作为伪标签来执行知识蒸馏，以审查不可用数据的知识。为了提高全局模型的学习能力并充分利用计算资源，可以将学习新任务和查看旧任务分配给不同的客户端，而那些没有足够新任务数据的客户端只能查看旧任务。此外，提出了一种服务器蒸馏机制来缓解非IID数据中的任务内遗忘。对聚合的全局模型进行微调，以模拟上一轮的全局模型和当前轮的局部模型的输出。</p><p>  代理数据集应该从公共数据集中收集，以保护隐私，并覆盖尽可能多的特征或与旧任务相似，以确保蒸馏的有效性。由于模型参数没有增加，与FedAvg相比，没有额外的通信成本。</p><p><img src="/2022/10/29/continual-federated-learning-based-on-knowledge-distillation/2.png" alt="图2：带知识蒸馏的连续联合学习"></p><h2 id="客户蒸馏"><a class="header-anchor" href="#客户蒸馏">¶</a>客户蒸馏</h2><p>  CFeD的蒸馏过程假设在第 $k$ 个客户端存在代理数据集 $X_{s}^{k}$。对于每个样本 $\mathrm{x}_{s} \in X_{s}^{k}$，其在时间 $t$ 的标签为 $\mathbf{y}_{\mathrm{s}, t}=f\left(\mathbf{x}_{\mathrm{s}} ; \boldsymbol{\theta}_{t-1}\right)$。 因此，我们获得了样本对的perclient集合 $\mathcal{S}_{t}^{k}=\{\left(\mathbf{x}_{\mathrm{s}}, \mathbf{y}_{\mathrm{s}, t}\right), \forall \mathbf{x}_{\mathrm{s}} \in X_{s}^{k} \}$。添加蒸馏项 $\mathcal{L}_{d}\left(\mathcal{S}_{t}^{k} ; \boldsymbol{\theta}\right)$ 以近似旧任务 $\sum_{i=1}^{t-1} \mathcal{L}\left(\mathcal{T}_{i}^{k} ; \boldsymbol{\theta}\right)$ 的损失。对于时间t的特定替代样本对 $\left(\mathbf{x}_{\mathrm{s}}, \mathbf{y}_{\mathrm{s}, t}\right)$，蒸馏损失可以形式化为交叉熵损失的修正版本：<br>$$<br>\mathcal{L}_{d}\left(\left(\mathbf{x}_{\mathrm{s}}, \mathbf{y}_{\mathrm{s}, t}\right) ; \boldsymbol{\theta}\right)=-\sum_{i=1}^{l} y_{\mathrm{s}, t}^{\prime(i)} \log \hat{y}_{\mathrm{s}, t}^{\prime(i)} \quad\quad (3)<br>$$</p><p>  其中 $l$ 是目标类的数量，${y’}_{s,t}^{(i)}$ 是修改的代理标签，而 ${\hat{y}’ }_{s,t}^{(i)} $ 是代理样本 $\mathbf{x}_{s}$ 上模型的修改输出。后两者定义为：<br>$$<br>y_{\mathrm{s}, t}^{\prime(i)}=\frac{\left(y_{\mathrm{s}, t}^{(i)}\right)^{1 / T}}{\sum_{j=1}^{l}\left(y_{\mathrm{s}, t}^{(j)}\right)^{1 / T}}, \quad \hat{y}_{\mathrm{s}, t}^{(i)}=\frac{\left(\hat{y}_{\mathrm{s}, t}^{(i)}\right)^{1 / T}}{\sum_{j=1}^{l}\left(\hat{y}_{\mathrm{s}, t}^{(j)}\right)^{1 / T}} \quad\quad (4)<br>$$</p><p>  其中 ${\hat{y}}_{s,t}^{(i)} $ 是 $f(\mathbf{x}_{\mathrm{s}} ; \boldsymbol{\theta})$ 的第 $i$ 个元素，$t$ 是蒸馏温度，更大的 $t$ 值可以放大次要的逻辑，从而增加教师模型提供的信息。</p><p>  基于上述概念，CFeD通过用perclient代理数据集上的蒸馏损失替换等式2中旧任务上的未知损失来计算时间 $t$ 时所有客户端上的总损失。正式而言，总损失为：<br>$$<br>\begin{aligned}<br>&amp; \sum_{k=1}^{K} \frac{n_{k}}{n_{a}}\left(\mathcal{L}\left(\mathcal{T}_{t}^{k} ; \boldsymbol{\theta}\right)+\sum_{i=1}^{t-1} \mathcal{L}\left(\mathcal{T}_{i}^{k} ; \boldsymbol{\theta}\right)\right) \\<br>\propto &amp; \sum_{k=1}^{K} \frac{n_{k}}{n_{a}}\left(\mathcal{L}\left(\mathcal{T}_{t}^{k} ; \boldsymbol{\theta}\right)+\lambda_{d} \mathcal{L}_{d}\left(\mathcal{S}_{t}^{k} ; \boldsymbol{\theta}\right)\right)<br>\end{aligned}  \quad\quad (5)<br>$$</p><p>  其中 $\lambda_{d}$（默认值0.1）是一个预定义的参数，用于对蒸馏损失进行加权。</p><h2 id="客户划分机制"><a class="header-anchor" href="#客户划分机制">¶</a>客户划分机制</h2><p>  在现实场景中，不同的客户端以不同的速度积累数据。当一些客户准备学习一项新任务时，其他客户可能没有为新任务获得足够的数据，因此无法在学习中有效利用</p><p>  为了利用未充分利用的计算资源，CFeD将学习新任务和回顾旧任务视为两个单独的目标。该框架将两个目标中的一个分配给每个客户，因此一些客户只能执行审查。此外，这种划分可以改进模型在不同目标上的探索，并帮助模型脱离先前的局部极小值。形式上，关于除法机制，等式5可以扩展并改写为：<br>$$<br>\sum_{k=1}^{N} \frac{n_{k}}{n_{a}} \mathcal{L}_{d}\left(\mathcal{S}_{t}^{k} ; \boldsymbol{\theta}\right)+\sum_{k=N+1}^{K} \frac{n_{k}}{n_{a}} \mathcal{L}\left(\mathcal{T}_{t}^{k} ; \boldsymbol{\theta}\right)  \quad\quad (6)<br>$$</p><p>  其中 $N=\lceil\alpha K\rceil, \alpha \in[0,1]$。 我们引入了一个因子 $\alpha$ 来描述每轮参与审查的客户比例。</p><h2 id="服务器提取"><a class="header-anchor" href="#服务器提取">¶</a>服务器提取</h2><p>  尽管客户端划分机制允许利用空闲的计算资源，但我们必须注意，一些活跃的客户端现在没有学习新任务。这可能会损害新任务的性能，并导致严重的任务内遗忘，尤其是在非IID场景中。原因是，由于不同客户端上的数据标签可能彼此不相交，因此适合一个客户端（学习新任务）的全局模型可能很容易在另一个客户端中表现出遗忘（回顾旧任务，或学习不同标签数据上的新任务）</p><p>  为了减轻新任务的这种性能下降，客户机可以采用简单的解决方案来增加其本地训练迭代。然而，增加非IID数据的局部更新的周期数很容易导致过度拟合和全局模型的性能不稳定。</p><p>  受集中式训练中的小批量迭代更新范式的激励[Toneva等人，2018]，我们提出了服务器蒸馏（SD），以减轻任务内遗忘并稳定服务器端聚合模型的性能。在我们的方法中，服务器还维护一个轻量级的、未标记的公共数据集 $X_{\mathrm{s}}$，类似于客户端上的代理数据集。在第 $r$ 轮聚合从客户端收集的 $K$ 个本地模型之后，服务器将 $X$ 分成 $K+1$ 个批次，将其中 $K$ 个分配给接收的本地模型，将一个分配给最后一轮的全局模型，并在其分配的批次上收集上述模型的输出 $\hat{Y}_{s,r}$，以构建用于服务器蒸馏的标记样本对集，表示为 $\mathcal{S}_{r}=\{\left(\mathbf{x}_{\mathrm{s}}, \mathbf{y}_{\mathrm{s}, r}\right), \forall \mathbf{x}_{\mathrm{s}} \in X_{s}\}$。接下来，将用蒸馏损失 $\mathcal{L}_{d}(\mathcal{S}_{r} ; \boldsymbol{\theta})$ 迭代更新聚合全局模型 $\boldsymbol{\theta}_{r}$。</p><p>  通过服务器蒸馏，全局模型能够进一步从局部模型和先前的全局模型中检索知识，从而减轻任务内遗忘。</p><h1>实验</h1><p>  在本节中，我们评估了在文本和图像分类任务中从传统连续学习方法扩展的CFeD和基线方法。</p><h2 id="数据和任务"><a class="header-anchor" href="#数据和任务">¶</a>数据和任务</h2><p>  我们考虑了文本和图像分类数据集：THUCNews[Li et al.，2006]包含了2005年至2011年期间从新浪新闻RSS收集的14类中文新闻数据。SogouCS[SogouLabs，2012]包含了2012年的18类511218个中文新闻数据，Sina2019包含了我们自己从新浪消息中获取的2019年的30533个中文新闻。NLPIR微博语料库[NLPIR，2017]由从新浪微博和腾讯微博这两个中国社交媒体网站获取的23万个样本组成。我们将其用作跨不同任务的代理数据集。CIFAR10[Krizhevsky，2009]包含60000张图像，共10类。CIFAR-100[Krizhevsky，2009]包含60000个图像和100个类。Caltech-256[Griffin等人，2007]包含30608个图像，其中256个类作为图像分类中的替代数据集。代码所在的网址为： <a href="https://github.com/lianziqt/CFeD">https://github.com/lianziqt/CFeD</a>.</p><p>  我们设计了在两种不同场景中学习的任务序列：域IL表示输入分布在序列中不断变化的情况；类IL表示新类在序列中递增出现的情况。文本数据集中的任务用“Tx”表示，而图像数据集中的那些任务用“Ix”表示。其中“x”是任务序列ID。实验中的大多数任务序列都很短，包含两个或三个分类任务。</p><h2 id="比较实验"><a class="header-anchor" href="#比较实验">¶</a>比较实验</h2><p>  我们选择以下方法进行评估：</p><p>  （1）微调：一种简单的方法，按顺序训练任务模型。（2）FedAvg：一种FL方法，每个客户端按顺序学习任务，服务器从客户端聚合本地模型。（3）MultiHead：一种CL方法，为每个任务训练单独的分类器，要求任务标签在推断阶段指定输出。FedMultiHead表示应用于客户的FedAvg和MultiHeat。（4）EWC：一种基于正则化的方法[Kirkpatrick等人，2017]，使用Fisher信息矩阵来估计参数的重要性。FedEWC表示FedAvg，EWC适用于客户。（5）LwF：一种基于蒸馏的方法。LwF利用新的任务数据来执行蒸馏，而不是未标记的数据。FedLwF表示FedAvg，LwF应用于客户端。（6）DMC:Deep Model Consolidation[Zhang et al.，2020]，一种IL类CL方法，首先只为新任务训练单独的模型，然后利用未标记的公共数据集来提取旧任务和新任务的知识，以获得新的组合模型。FedDMC表示FedAvg，DMC适用于客户。（7）CFeD：我们的方法，CFeD（C）表示我们方法CFeD的集中式版本。</p><p>  请注意，MultiHead和FedMultiHead在推理过程中需要任务标签来知道当前输入属于哪个任务。此外，在域IL场景中，多个分类器不可避免地会带来更多参数。由于这些附加信息，它们的性能可以被视为其他方法的目标。</p><h2 id="实验设置"><a class="header-anchor" href="#实验设置">¶</a>实验设置</h2><p>  我们使用TextCNN[Kim，2014]或ResNet-18[He等人，2016]，然后使用完全连接层进行分类。每个任务训练模型 $R=20$ 回合。对于每个客户端中的本地更新，学习时期在域IL中为10，在类IL中为40。除非另有说明，EWC方法的约束因子 $\lambda$ 设置为100000。蒸馏温度默认设置为2</p><p>  对于FL的配置，我们假设有100个客户机，并且只有随机10%的客户机被抽样参加每一轮培训。训练数据集和代理数据集都被随机分成200个碎片（IID）或按类别（非IID）排序。在每个实验中，每个客户端都选择每个任务上的两个数据碎片作为本地数据集，并选择代理数据集的两个碎片作为本地代理。特别是，服务器还选择两个碎片用于非IID分发中的服务器蒸馏。以上所有选择都是随机进行的。</p><p>  对于每个任务，我们选择 $70\%$ 的数据作为训练集，$10\%$ 作为验证集，其余的作为测试集。在每轮训练结束时，在测试集上评估全局模型。所有实验用不同的随机种子重复3次。</p><h2 id="实验结果"><a class="header-anchor" href="#实验结果">¶</a>实验结果</h2><p><b>对缓解任务间遗忘的影响</b></p><p><img src="/2022/10/29/continual-federated-learning-based-on-knowledge-distillation/3.png" alt="表1：不同方法（FL的全局模型）学习任务的平均准确率（$\%$）。前7行来自集中式方法，而后7行来自FL方法。∗ 指示具有附加信息（任务标签）的方法。DMC仅适用于IL类场景。"></p><p>  我们评估了集中式和FL场景中的所有方法。表1总结了模型在域IL（左）和类IL（右）中顺序学习第二和第三个任务后，学习过的任务的平均准确性。通过比较不同方法的结果，可以看出CFeD的平均精度超过了其他基线</p><p>  在域IL场景（表1的左半部分）中，CFeD的平均精度超过FedAvg、FedLwF、FedEWC和FedDMC方法，接近FedMultiHead。此外，在模型不断学习Domain-T2后，所有方法的平均精度都有所提高。结果表明，Domain-T1的新任务可能会覆盖旧任务的一些特征，这有助于模型审查旧知识，值得注意的是，CFeD仍然优于其他基线</p><p>  在IL类场景（表1的右半部分）中，我们可以看到FedAvg和FedEWC的平均准确性都显著下降。原因是旧任务的标签不可用，模型很快就超过了新任务。相比之下，CFeD优于其他基线，这表明了利用代理数据集为旧任务获取合理的软标签的好处。</p><p>  我们注意到，FedDMC在IL类中的性能显著下降。由于DMC的模型合并仅使用代理数据进行提取，即没有新的任务数据来学习新任务和审查旧任务，因此其性能受到代理数据集大小的显著限制（在我们的情况下，每个客户端只有2300个代理样本）。为了验证这一点，我们构造了一个变体FedDMCfull，其中每个客户端都可以访问整个代理数据集。在这样的设置下，FedDMCfull实现了相当大的改进，因为每个客户端都有更多的数据。然而，我们的方法仍然优于它，显示了CFeD对代理数据集大小的鲁棒性。</p><p><b>对缓解任务内遗忘的影响</b></p><p><img src="/2022/10/29/continual-federated-learning-based-on-knowledge-distillation/4.png" alt="图3:FedA VG（蓝色）、CFeD（绿色）和CFeD+SD（红色）在非IID分发下的域IL和类IL场景中的性能"></p><p>  为了说明我们提出的方法对任务内遗忘的影响，我们比较了三种方法：FedA-VG、CFeD和CFeD与服务器提取（即CFeD+SD），这两种方法都适用于域IL和非IID分布的类IL场景。图3显示了学习过程中模型对新任务的准确度和对学习任务的平均准确度。结果表明，CFeD+SD在不牺牲学习新任务的能力的情况下提高了缓解性能。此外，所有方法在非IID分布中的性能都显著下降，但CFeD+SD比其他两种方法更稳定。由于客户划分和服务器蒸馏，CFeD+SD在不牺牲可塑性的情况下实现了更高的平均精度。</p><p><b>客户划分机制</b></p><p><img src="/2022/10/29/continual-federated-learning-based-on-knowledge-distillation/5.png" alt="表2：不同学习率下学习任务的平均准确率（Avg，$\%$）和新任务的测试准确率（new，$\%$）。"></p><p>  为了评估客户划分机制的效果，表2显示了新任务准确度和平均准确度的更详细结果，以说明CFeD在稳定性和可塑性之间的权衡（见第2.1节）。可以看出，在IL类中，CFeD（C）也面临着可塑性和稳定性之间的两难境地：${CFeD(C)}_{lr=1e-6}$ 不能很好地学习新任务。相比之下，由于客户划分机制，${CFeD(C)}_{lr=1e-3}$ 在可塑性和稳定性之间取得了良好的平衡。</p><p><b>不同的代理数据大小</b></p><p>  为了了解代理数据大小如何影响CFeD的性能，我们将分配给每个客户端的代理数据的碎片数（$\beta$）从2（默认值）更改为40。为了减少实验时间，我们将本地epoch数设置为10。</p><p><img src="/2022/10/29/continual-federated-learning-based-on-knowledge-distillation/6.png" alt="图4：不同替代比率的结果（$\beta$ 表示所选碎片的数量）"><br>  实验结果如图4所示。通常，当 $\beta$ 增加时，新任务的性能降低。然而，旧任务的情况并非如此。当 $\beta$ 从2变化到40时，旧任务的性能先提高后降低。T1的最佳值约为10，T2为20。值得注意的是，当 $\beta$ 较小时，两个任务的平均精度都达到峰值，然后随着学习的进行而缓慢降低（左下角子图）。这表明模型快速学习新任务（达到峰值），然后逐渐忘记旧任务，这抵消了从新任务中获得的性能。当我们放大 $\beta$ 。但由于T2中的大量任务，推迟的效果并不明显。</p><h1>结论</h1><p>  在本文中，我们解决了一系列任务的联合学习中的灾难性遗忘问题。我们提出了一个名为CFeD的持续联合学习框架，它利用基于代理数据集的知识蒸馏来解决这个问题。我们的方法允许客户通过基于自己的替代数据集优化蒸馏损失来回顾在旧模型中学到的知识。服务器还执行蒸馏以减轻任务内遗忘。为了进一步提高模型的学习能力，客户可以被分配学习新任务或单独回顾旧任务。实验结果表明，我们提出的方法在减轻灾难性遗忘方面优于基线，并在稳定性和可塑性之间实现了良好的折衷。对于未来的工作，我们将进一步改进我们的方法，以克服非IID数据中的任务内遗忘，并降低其培训成本。</p>]]></content>
      
      
      <categories>
          
          <category> 科研学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 联邦学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《Federated Continual Learning with Weighted Inter-client Transfer》</title>
      <link href="/2022/10/14/federated-continual-learning-with-weighted-inter-client-transfer/"/>
      <url>/2022/10/14/federated-continual-learning-with-weighted-inter-client-transfer/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h1>摘要</h1><p>  人们对持续学习和联合学习的兴趣大增，这两者在现实世界的深度神经网络中都很重要。然而，对于每个客户从一个私有的本地数据流中学习一连串的任务的场景，人们却没有做什么研究。这种联合持续学习的问题给持续学习带来了新的挑战，比如利用其他客户的知识，同时防止不相关知识的干扰。为了解决这些问题，我们提出了一个新的联合持续学习框架，即联合加权客户间传输（FedWeIT），它将网络权重分解为全局的联合参数和稀疏的特定任务参数，每个客户通过采取其特定任务参数的加权组合，从其他客户那里接收选择性的知识。FedWeIT最大限度地减少了不兼容的任务之间的干扰，并且在学习过程中允许客户之间的积极知识转移。我们将FedWeIT与现有的联合学习和持续学习方法在不同程度的客户端之间的任务相似性进行了验证，我们的模型明显优于它们，并大大降低了通信成本。代码可以在https://github.com/wyjeong/FedWeIT.上看到。</p><h1>介绍</h1><p>  持续学习（Thrun，1995；Kumar &amp; Daume III，2012；Ruvolo &amp; Eaton，2013；Kirkpatrick等人，2017；Schwarz等人，2018）描述了一种学习场景，即模型在一连串的任务上持续训练；它受到人类学习过程的启发，因为一个人在其一生中会学习执行大量具有多样性的任务，利用过去的知识来学习新的任务，而不会忘记之前学习的任务。持续学习是一个长期研究的课题，因为拥有这样的能力就有可能建立一个通用的人工智能。然而，在用传统模型如深度神经网络（DNNs）来实现它时，存在一些关键的挑战，如灾难性遗忘，它描述了在训练期间为过去的任务学习的参数或语义表示漂移到新任务的方向。这个问题已经被之前的各种工作解决了（Kirkpatrick等人，2017；Shin等人，2017。 Riemer等人，2019）。更多最新的工作解决了其他问题。 如可扩展性或秩序稳健性（Schwarz等人，2018。Hung等人，2019；Y oon等人，2020）。</p><p>  然而，所有这些模型从根本上说都是有限的，因为这些模型只能从它的直接经验中学习它们只能从它们所训练的任务序列中学习。相反，人类可以通过不同的方式（如口头交流、书籍或各种媒体）从他人的间接经验中学习。那么在持续学习框架中实现这样的能力不是很有好处吗？这样，在不同机器上学习的多个模型可以从其他客户已经经历过的任务知识中学习。这里出现的一个问题是，由于单个客户的数据隐私和高昂的通信费用，客户之间或服务器和客户之间可能无法直接沟通数据。联合学习（McMahan等人，2016；Li等人，2018；Yurochkin等人，2019）是一种学习范式，通过交流参数而不是原始数据本身来解决这个问题。我们可以有一个服务器，接收在多个客户端上本地训练的参数，将其汇总为一个单一的模型参数，并将其发回给客户端。在我们关于从间接经验中学习的直觉的激励下，我们解决了联合持续学习（FCL）的问题，在这个问题上，我们用在私人任务序列上训练的多个客户端进行持续学习，这些客户端通过一个全球服务器交流它们的特定任务参数。</p><p><img src="/2022/10/14/federated-continual-learning-with-weighted-inter-client-transfer/1.png" alt="图1.概念。在医院学习疾病预测任务序列的持续学习者可能希望利用其他医院的相关任务参数。FCL允许通过任务分解参数的通信进行客户间知识转移。"></p><p>  图1描述了FCL的一个示例场景。假设我们正在建立一个医院网络，每个医院都有一个疾病诊断模型，该模型不断学习进行给定的CT扫描，对新类型的疾病进行诊断。然后，在我们的框架下，任何诊断模型如果了解了一种新类型的疾病（例如，COVID-19），将把特定任务的参数传送到全球服务器，后者将把它们重新分配到其他医院，供本地模型使用。这使得所有参与者都能从新的任务知识中获益，而不损害数据隐私。</p><p><img src="/2022/10/14/federated-continual-learning-with-weighted-inter-client-transfer/2.png" alt="图2.联合持续学习的挑战。由于分享不相关的知识而受到其他客户的干扰，可能会阻碍目标客户的最佳培训（红色），而来自其他客户的相关知识将有助于他们的学习（绿色）。"></p><p>  然而，联合式持续学习的问题也带来了新的挑战。首先，不仅有来自持续学习的灾难性遗忘，而且还有来自其他客户的潜在干扰的威胁。图2用一个简单的实验结果来描述这一挑战。在这里，我们训练一个MNIST数字识别的模型，同时将另一个在不同数据集上训练的客户端的参数进行交流。当从另一个客户端传输的知识与目标任务（SVHN）相关时，模型开始有很高的准确性，收敛速度更快，达到更高的准确性（绿线），而如果传输的知识来自与目标任务高度不同的任务（CIFAR-10，红线），模型的表现就不如基础模型。因此，我们需要有选择地利用其他客户的知识，以尽量减少客户间的干扰，最大限度地提高客户间的知识转移。联合学习的另一个问题是有效的通信，因为在利用其他客户的知识时，通信成本可能会变得过大，因为在与边缘设备合作的实际场景中，通信成本可能是主要瓶颈。因此，我们希望知识能被尽可能紧凑地表示出来。</p><p>  为了应对这些挑战，我们提出了一个新的联合持续学习框架，即联合加权客户间传输（FedWeIT），它将本地模型参数分解为密集的基础参数和稀疏的任务适应性参数。FedWeIT减少了不同任务之间的干扰，因为基础参数将编码任务的通用知识，而特定任务的知识将被编码到任务适应性参数中。当我们利用通用知识时，我们也希望客户端能选择性地利用在其他客户端获得的特定任务知识。为此，我们允许每个模型采取从服务器广播的任务适应性参数的加权组合，这样它就可以选择对手头任务有帮助的特定任务知识。Fed-WeIT的通信效率很高，因为任务自适应参数是高度稀疏的，只需要在创建时进行一次通信。此外，当通信效率不是一个关键问题时，如在跨语境的联合学习中（Kairouz等人，2019），我们可以使用我们的框架，根据其任务适应性参数的注意力权重来激励每个客户端。我们在多个不同的场景上验证了我们的方法，这些场景中各客户端的任务相似度不同，与各种联合学习和本地持续学习模型相比，我们的方法也是如此。结果表明，我们的模型比所有的基线获得了明显的优越性能，能更快地适应新的任务，并在很大程度上降低了通信成本。本文的主要贡献如下：</p><ol><li><b>我们引入了一个新的问题--联合持续学习（FCL）</b>，即多个模型在分布式客户端上持续学习，这带来了新的挑战，如防止客户端间的干扰和客户端间的知识转移。</li><li><b>我们为联合持续学习提出了一个新颖的、具有通信效率的框架，</b>它允许每个客户端自适应地更新联合参数，并有选择地利用来自其他客户端的过去知识。稀疏参数的交流，有选择地利用其他客户端的知识。</li></ol><h1>相关工作</h1><p>  <b>持续学习</b> 虽然持续学习（Kumar &amp; Daume III, 2012; Ruvolo &amp; Eaton, 2013）是一个长期研究的话题，有大量的文献，但我们只讨论最近的相关工作。<b>基于正则化</b> EWC（Kirkpatrick等人。 2017）利用Fisher信息矩阵来限制 模型参数的变化，从而使模型找到解决办法，对之前和当前的任务都有好处。 和IMM（Lee等人，2017）提议将多个任务的后分布作为高斯的混合物来学习。稳定的SGD（Mirzadeh等人，2020）通过控制基本的超参数和每次新任务到来时逐渐降低学习率，显示出令人印象深刻的性能增益。<b>基于架构</b> DEN（Y oon等人，2018）通过迭代神经元/滤波器的修剪和分割，扩大必要的网络规模来解决这个问题，RCL（Xu &amp; Zhu，2018）使用强化学习来解决同样的问题。APD（Y oon等人，2020）将参数加法分解为共享参数和特定任务参数，以尽量减少网络复杂性的增加。<b>基于Coreset</b> GEM变体（Lopez-Paz和Ranzato，2017；Chaudhry等人，2019）使实际数据集和存储的外显记忆的损失最小。FRCL（Titsias等人，2020）通过复杂构建的诱导点来记忆以前任务的近似后验。据我们所知，现有的方法都没有考虑到深度神经网络持续学习的可交流性，我们解决了这个问题。CoLLA（Rostami等人，2018）旨在解决多代理终身学习与稀疏字典学习，它没有一个中央服务器来指导客户之间的协作，并且由一个简单的字典学习问题制定，因此不适用于现代神经网络。另外，CoLLA仅限于同质客户的同步训练。</p><p>  <b>联邦学习：</b> 联邦学习是一种不同隐私下的分布式学习框架，旨在学习服务器上的全局模型，同时聚合客户端在其私有数据上学习的参数。FedAvg（McMahan et al.，2016）通过基于训练的数据点数量计算多个客户的加权平均值，汇总了在多个客户中训练的模型。FedProx（Li et al.，2018）使用近端术语对本地模型进行培训，限制其更新接近全球模型。FedCurv（Shoham et al.，2019）旨在通过采用修订版的EWC，最小化联合学习期间客户之间的模型差异。最近的工作（Y urochkin等人，2019年；Wang等人，2020年）通过利用贝叶斯非参数方法引入了精心设计的聚合策略。联合学习的一个关键挑战是降低通信成本。TW AFL（Chen et al.，2019）通过执行逐层参数聚合来解决这个问题，其中浅层在每一步聚合，而深层在循环的最后几步聚合。（Karimireddy et al.，2020）提出了一种快速收敛算法，通过牺牲局部最优性来最小化客户端不同任务之间的干扰。这与个性化联合学习方法相反（Fallah等人，2020；Lange等人，2020，Deng等人，2020），后者更强调本地模型的性能。FCL是两者的平行研究方向，据我们所知，我们的研究是第一个在联合学习框架下考虑客户任务增量学习的工作。</p><h1>联邦持续学习——FedWeIT</h1><p>  基于间接经验的人类学习过程，我们引入了一种新的联合学习环境下的持续学习，我们称之为联合持续学习（FCL）。FCL假设多个客户端在与全局服务器通信学习到的参数时，接受了来自私有数据流的一系列任务的培训。我们首先在第3.1节中正式定义了问题，然后在第3.2节中提出了简单的解决方案，将现有的联合学习和持续学习方法直接结合起来。然后，在第3.3节和第3.4节之后，我们讨论了联合持续学习带来的两个新挑战，并提出了一个新的框架，联邦加权客户端间传输（FedWeIT）可以有效地处理这两个问题，同时降低客户端到服务器的通信成本。</p><h2 id="问题定义"><a class="header-anchor" href="#问题定义">¶</a>问题定义</h2><p>  在标准的持续学习（在单台机器上）中，模型从任务序列 $\{\mathcal{T}^{(1)}, \mathcal{T}^{(2)}, \ldots, \mathcal{T}^{(T)}\}$ 迭代学习，其中 $\mathcal{T}^{(t)}$ 是 $t^{th}$ 任务 $\mathcal{T}^{(t)}=\{\boldsymbol{x}_{i}^{(t)}, \boldsymbol{y}_{i}^{(t)}\}_{i=1}^{N_{t}}$ 的标记数据集，由 $N_{t}$ 对实例 $x_{i}^{(t)}$ 及其对应的标签 $y_{i}^{(t)}$ 组成。假设最现实的情况，我们考虑任务序列是一个到达顺序未知的任务流的情况，这样模型只能在任务 $t$ 的训练期访问 $\mathcal{T}^{(t)}$，而训练期过后将无法访问。给定 $\mathcal{T}^{(t)}$ 和迄今为止所学的模型，任务 $t$ 的学习目标如下：最小化 $\mathcal{L}\left(\boldsymbol{\theta}^{(t)} ; \boldsymbol{\theta}^{(t-1)}, \mathcal{T}^{(t)}\right)$，其中 $\boldsymbol{\theta}^{(t)}$ 是任务 $t$ 的一组模型参数。</p><p>  现在，我们将传统的持续学习扩展到具有多个客户端和全局服务器的联合学习环境。假设我们有 $C$ 客户机，其中每个客户机 $c_{c} \in \{c_{1}, \cdots, c_{C}\}$ 在私有可访问的任务序列 $\{\mathcal{T}^{(1)}, \mathcal{T}^{(2)}, \ldots, \mathcal{T}^{(T)}\} \subseteq \mathcal{T} $ 上训练模型。请注意，在步骤 $t$ 中跨客户端接收的任务 $\mathcal{T}_{1:C}^{(t)}$ 之间没有关系。现在的目标是通过与全局服务器通信模型参数，在其私有任务流上有效地训练 $C$ 连续学习模型，全局服务器聚合从每个客户端发送的参数，并将其重新分发给客户端。</p><h2 id="交流式持续学习"><a class="header-anchor" href="#交流式持续学习">¶</a>交流式持续学习</h2><p>  在传统的联合学习环境中，学习是通过多轮本地学习和参数聚合完成的。在每一轮通信中，客户端 $c_{c}$ 和服务器 $s$ 执行以下两个过程：本地参数传输和参数聚合和广播。在局部参数传输步骤中，对于随机选择的客户端子集，在第 $r$ 轮，$\mathcal{C}^{(r)} \subseteq \{c_{1}, c_{2}, \cdots ,c_{C} \}$，每个客户端 $c_{c} \in \mathcal{C}^{(r)}$ 向服务器发送更新的参数 $\boldsymbol{\theta}^{(r)}$。并非每个客户端都进行服务器客户端传输，因为某些客户端可能暂时断开连接。然后，服务器将从客户端发送的参数 $\boldsymbol{\theta}_{c}^{(r)}$ 聚合为单个参数。这种聚合最流行的框架是FedAvg（McMahan等人，2016）和FedProx（Li等人，2018）。然而，用这两种算法对局部任务序列进行朴素的联合连续学习可能会导致灾难性遗忘。一个简单的解决方案是使用基于正则化的方法，例如弹性重量合并（EWC）（Kirkpatrick et al.，2017），这使得模型能够获得对之前和当前任务都是最优的解决方案。还有其他先进的解决方案（Nguyen等人，2018年；Chaudhry等人，2019年）可以成功地防止灾难性遗忘。然而，在客户端级别防止灾难性遗忘是来自联合学习的一个正交问题。</p><p>  因此，我们重点关注在这个联合持续学习环境中新出现的挑战。在联合持续学习框架中，将参数聚合为全局参数 $\boldsymbol{\theta}_{G}$ 允许跨客户进行客户间知识转移，因为客户机 $c_{i}$ 第 $q$ 轮学习的任务 $\mathcal{T}_{i}^{(q)}$ 可能与在客户机 $c_{j}$ 第 $r$ 轮学习的 $\mathcal{T}_{j}^{(r)}$ 相似或相关。然而，使用单个聚合参数 $\boldsymbol{\theta}_{G}$ 在实现这一目标方面可能是次优的，因为来自无关任务的知识可能没有用处，甚至通过将其参数更改为不正确的方向来阻碍每个客户的培训，我们将其描述为客户间干扰。另一个同样重要的问题是通信效率。从客户端到服务器的参数传输以及从服务器到客户端的参数传输都会产生很大的通信成本，这对于持续学习设置来说是个问题，因为客户端可能会在可能无限的任务流上进行培训。</p><h2 id="联合加权客户端间传输"><a class="header-anchor" href="#联合加权客户端间传输">¶</a>联合加权客户端间传输</h2><p>  那么，我们如何才能最大限度地实现客户之间的知识转移，同时最小化客户之间的干扰和通信成本？我们现在描述我们的模型，联邦加权客户间转移（FedWeIT），它可以解决持续学习方法与联邦学习框架的简单结合所产生的这两个问题</p><p>  如前所述，问题的主要原因是，在多个客户机上学习的所有任务的知识都存储在一组参数 $\boldsymbol{\theta}_{G}$ 中。然而，为了有效地进行知识传授，每个客户都应该有选择地仅利用在其他客户处接受培训的相关任务的知识。这种选择性迁移也是最大限度地减少客户间干扰的关键，它将忽略可能干扰学习的无关任务的知识。</p><p>  我们通过将参数分解为具有不同角色的三种不同类型的参数来解决这个问题：全局参数（ $\boldsymbol{\theta}_{G}$ ），用于捕获所有客户端的全局和通用知识；本地基本参数（ $\boldsymbol{B}$ ），用于捕捉每个客户端的通用知识；任务自适应参数（ $\boldsymbol{A}$ ），用于每个客户端的每个特定任务，由Yoon等人推动（2020年）。然后，持续学习客户端 $c_{c}$ 任务 $t$ 的一组模型参数 $\boldsymbol{\theta}_{c}^{(t)}$ 定义如下：<br>$$<br>\boldsymbol{\theta}_{c}^{(t)}=\boldsymbol{B}_{c}^{(t)} \odot \boldsymbol{m}_{c}^{(t)}+\boldsymbol{A}_{c}^{(t)}+\sum_{i \in \mathcal{C}_{\backslash c}} \sum_{j&lt;|t|} \alpha_{i, j}^{(t)} \boldsymbol{A}_{i}^{(j)}  \qquad (1)<br>$$<br>式中 $\boldsymbol{B}_{c}^{(t)} \in \{\mathbb{R}^{I_{l} \times O_{l}}\}_{l=1}^{L}$ 是客户端中所有任务共享的 $c^{th}$ 客户端的基本参数集，$\boldsymbol{m}_{c}^{(t)} \in \{\mathbb{R}^{O_{l}} \}_{l=1}^{L}$ 是一组稀疏向量掩码，它允许为任务 $t$ 自适应变换 $\boldsymbol{B}_{c}^{(t)}$， $\boldsymbol{A}_{c}^{(t)} \in \{\mathbb{R}^{I_{l} \times O_{l}}\}_{l=1}^{L}$ 是客户端 $c_{c}$ 的稀疏任务自适应参数集。这里，$L$ 是神经网络中的层数，$I_{l}$、$O_{l}$ 分别是层 $l$ 权重的输入和输出维数。</p><p><img src="/2022/10/14/federated-continual-learning-with-weighted-inter-client-transfer/3.png" alt="图3.FedWeIT的更新。（a）客户端发送稀疏的联邦参数 $\boldsymbol{B}_{c}^{(t)} \odot \boldsymbol{m}_{c}^{(t)}$。然后，服务器将聚合的参数重新分配给客户端。（b）知识库存储以前的任务和客户端的自适应参数，每个客户端都有选择地使用它们，并带有一个注意掩码。"></p><p>  第一个术语允许有选择地利用全球知识。我们希望每个客户端的基本参数 $\boldsymbol{B}_{c}^{(t)}$ 能够捕获所有客户端所有任务的通用知识。在图3（a）中，我们在每一轮 $t$ 使用上一次迭代的全局参数对其进行初始化， $\boldsymbol{B}_{G}^{(t-1)}$ 用于聚合从客户端发送的参数。这使得 $\boldsymbol{B}_{c}^{(t)}$ 也可以从有关所有任务的全局知识中受益。然而，由于 $\boldsymbol{B}_{G}^{(t-1)}$ 还包含与当前任务无关的知识，而不是按原样使用它，我们学习稀疏掩码 $\boldsymbol{m}_{c}^{(t)}$ 以仅为给定任务选择相关参数。这种稀疏的参数选择有助于最小化客户端间干扰，并允许高效通信。第二个术语是任务自适应参数 $\boldsymbol{A}_{c}^{(t)}$ 。由于我们对这些参数进行了额外分解，这将学习捕获第一个术语未捕获到的关于任务的知识，从而捕获关于任务 $\mathcal{T}_{c}^{(t)}$ 的特定知识。最后一个术语描述了加权客户机间知识转移。我们有一组从服务器传输的参数，其中包含来自所有客户端的所有任务自适应参数。为了有选择地利用来自其他客户的这些间接经验，我们进一步对这些参数分配注意力 $\alpha_{c}^{(t)}$，以加权组合它们。通过学习这种注意力，每个客户只能选择有助于学习给定任务的相关任务自适应参数。虽然我们将 $\boldsymbol{B}_{i}^{(j)}$ 设计为高度稀疏，但在实践中使用全参数存储器的 $2-3 \% $，发送所有任务知识是不可取的。因此，我们在知识库的所有时间步长上传输随机抽样的任务自适应参数，我们在实践中发现，这些参数可以取得很好的效果。</p><p>  <b>训练：</b>我们通过优化以下目标来学习可分解参数 $\boldsymbol{\theta}_{c}^{(t)}$ ：<br>$$<br>\begin{aligned}<br>\underset{\boldsymbol{B}_{c}^{(t)}, \boldsymbol{m}_{c}^{(t)}, \boldsymbol{A}_{c}^{(1: t)}, \boldsymbol{\alpha}_{c}^{(t)}} {\operatorname{minimize}} \mathcal{L}(\boldsymbol{\theta}_{c}^{(t)} ; \mathcal{T}_{c}^{(t)})+\lambda_{1} \Omega({\boldsymbol{m}_{c}^{(t)}, \boldsymbol{A}_{c}^{(1: t)}}) +\lambda_{2} \sum_{i=1}^{t-1} \|\Delta \boldsymbol{B}_{c}^{(t)} \odot \boldsymbol{m}_{c}^{(i)}+\Delta \boldsymbol{A}_{c}^{(i)} \|_{2}^{2}  \qquad (2)<br>\end{aligned}<br>$$<br>其中 $\mathcal{L}$ 是损失函数， $\Omega{(\cdot)}$ 是所有任务自适应参数和掩蔽变量的稀疏诱导正则化项（我们使用 $\ell_{1}$ -范数正则化），以使它们稀疏。第二个正则化项用于追溯更新过去的任务自适应参数，这有助于任务自适应参数通过反映基本参数的变化来保持目标任务的原始解。在这里 $\Delta \boldsymbol{B}_{c}^{(t)}=\boldsymbol{B}_{c}^{(t)}-\boldsymbol{B}_{c}^{(t-1)}$ 是当前时间步和以前时间步的基本参数之间的差异，以及 $\Delta \boldsymbol{A}_{c}^{(i)}$ 是任务 $i$ 在当前时间步和上一时间步的任务自适应参数之间的差异。这种规则化对于防止灾难性遗忘至关重要。$\lambda_{1}$ 和 $\lambda_{2}$ 是控制这两个正则化子效果的超参数。</p><h2 id="通过稀疏参数实现高效通信"><a class="header-anchor" href="#通过稀疏参数实现高效通信">¶</a>通过稀疏参数实现高效通信</h2><p>  FedWeIT通过服务器到客户端的通信进行学习。如前所述，这里的一个关键挑战是降低通信成本。我们描述了每一步在客户端和服务器上发生的情况。</p><p>  <b>客户端：</b>在每轮 $r$ 中，每个客户端 $c_{c}$ 使用服务器发送的全局参数的非零组件部分更新其基本参数；也就是说，$\boldsymbol{B}_{c}(n)=\boldsymbol{\theta}_{G}(n)$，其中 $n$ 是全局参数的非零元素。使用方程2对模型进行训练后，得到一个新任务的稀疏化基本参数 $\widehat{\boldsymbol{B}}_{c}^{(t)}=\boldsymbol{B}_{c}^{(t)} \odot \boldsymbol{m}_{c}^{(t)}$ 和任务自适应参数 $\boldsymbol{A}_{c}^{(t)}$ ，两者都发送到服务器，与原始FCL基线相比，成本更低。虽然原始FCL基线需要 $|\mathcal{C}| \times R \times|\boldsymbol{\theta}|$ 用于客户端到服务器的通信，但FedWeIT需要 $|\mathcal{C}| \times (R \times| \hat{\boldsymbol{B}}| + |\boldsymbol{A}|)$，其中 $R$ 是每个任务的通信轮数，$|\cdot|$ 是参数的数量。</p><p>  <b>服务器：</b>服务器首先通过加权平均值聚合所有客户端发送的基本参数：$\boldsymbol{\theta}_{G}=\frac{1}{\mathcal{C}} \sum_{\mathcal{C}} \widehat{\boldsymbol{B}}_{i}^{(t)}$。然后，它向所有客户端广播 $\boldsymbol{\theta}_{G}$。$t-1$ 的任务适应性参数，$\{\boldsymbol{A}_{i}^{(t-1)} \}_{i=1}^{\mathcal{C}_{\backslash c}}$ 在训练任务 $t$ 期间，每个客户端都会广播一次。虽然简单的FCL基线需要 $|\mathcal{C}| \times R \times|\boldsymbol{\theta}|$ 作为服务器到客户机的通信成本，但FedWeIT需要 $|\mathcal{C}| \times (R \times |\boldsymbol{\theta}_{G}| + (|\mathcal{C}|−1) \times |\mathcal{A}|)$，其中 $\boldsymbol{\theta}_{G}$，$\boldsymbol{A}$ 高度稀疏。我们在算法1中描述了FedWeIT算法。</p><p><img src="/2022/10/14/federated-continual-learning-with-weighted-inter-client-transfer/4.png" alt="算法1"></p><h1>实验</h1><p>  我们验证了我们的 <b>FedWeIT</b> 在不同的任务序列配置下与baseline，即Overlapped-CIFAR-100和NonIID-50）的对比。<b>1）OverlappedCIFAR-100：</b>我们将100类CIFAR-1000数据集分组为20个非iid超类任务。然后，我们从20个任务中随机抽取10个任务，并分割实例，为每个任务重叠的客户端创建一个任务序列。<b>2） NonIID-50：</b>我们使用了以下八个基准数据集：MNIST（LeCun等人，1998）、CIFAR-10/-100（Krizhevsky&amp;Hinton，2009）、SVHN（Netzer等人，2011）、FashionMNIST（Xiao等人，2017）、Not-MNIST（Bulatov，2011），FaceScrub（Ng&amp;Winkler，2014）和TrafficSigns（Stallkamp等人，2011年）。我们将8个数据集中的类划分为50个非IID任务，每个任务由5个与用于其他任务的类分离的类组成。这是一个大规模实验，包含28万张来自8个异构数据集的293类图像。在生成和处理任务后，我们将其随机分配给多个客户端，如图4所示。我们遵循了最近工作中的准确性和遗忘指标（Chaudhry等人，2020；Mirzadeh等人，2020年；2021）。</p><p><img src="/2022/10/14/federated-continual-learning-with-weighted-inter-client-transfer/5.png" alt="图4.任务序列的配置：我们首先以non-IID方式（（a）和（b））将数据集 $D$ 拆分为多个子任务。然后，我们将它们分发给多个客户端（C#）。来自多个数据集（彩色圆圈）的混合任务分布在所有客户端（（c））。"></p><p><b>实验设置：</b>我们使用LeNet的修改版本（LeCun et al.，1998）对Overlapped-CIFAR-100和NonIID-50数据集进行实验。此外，我们使用ResNet-18（He et al.，2016）和NonIID-50数据集。我们遵循了（Serrá等人，2018）和（Y oon等人，2020）的其他实验设置。有关任务配置、指标、超参数和更多实验结果的详细描述，请参阅补充文件</p><p><b>baseline和我们的模型</b><b>1）STL：</b>每个到达任务的单任务学习。<b>2）EWC：</b>每个客户与EWC的个人持续学习（Kirkpatrick等人，2017）<b>3）Stable-SGD：</b>每个客户与StableSGD（Mirzadeh et al.，2020）的个人持续学习。<b>4）APD：</b>每个客户与APD的个人持续学习（Yoon等人，2020年）<b>5）FedProx：</b>FCL使用FedProx（Li等人，2018）算法。<b>6）Scaffold：</b>使用Scaffold（Karimireddy等人，2020）算法的FCL。<b>7）FedCurv：</b>FCL使用FedCurv（Shoham等人。，2019）算法。<b>8） FedProx-[model]：</b>FCL，经过培训在[model]中使用FedProx算法。<b>9）FedWeIT：</b>我们的FedWeIT算法。</p><h2 id="实验结果"><a class="header-anchor" href="#实验结果">¶</a>实验结果</h2><p><img src="/2022/10/14/federated-continual-learning-with-weighted-inter-client-transfer/7.png" alt="表2.FCL期间，5个客户端的两个数据集的平均每任务性能（fraction=1.0）。在完成所有学习阶段后，我们通过3个单独的实验测量了任务准确性和模型大小。我们还测量了训练每个任务的C2S/S2C通信成本。"><br>  我们首先根据单任务学习（STL）、持续学习（EWC、APD）、联合学习（FedProx、Scaffold、FedCurv）和朴素联合持续学习（基于FedProx-based）基线，在Overlapped-CIFAR-100和NonIID-50任务序列上验证我们的模型。表2显示了两个数据集完成（联合）持续学习后的最终平均每任务性能。我们观察到，与没有联合学习的相同方法相比，基于联邦Prox的联合持续学习（FCL）方法降低了持续学习（CL）方法的性能。这是因为在不相关任务中学习到的所有客户端参数的聚合会导致对每个任务的学习产生严重干扰，从而导致灾难性遗忘和次优任务适应。Scaffold在FCL上表现不佳，因为它在局部梯度上的规则化对FCL有害，因为所有客户都从不同的任务序列中学习。虽然FedCurv减少了参数的任务间差异，但它不能最小化任务间干扰，这导致它的性能低于单机CL方法。另一方面，FedWeIT在两个数据集上都显著优于单机CL基线和原始FCL基线。即使客户数量较多（C=100），FedWeIT也始终优于所有基线（图5）。这在很大程度上归功于FedWeIT有能力选择性地利用其他客户的知识，以快速适应目标任务，并获得更好的最终绩效。</p><p><img src="/2022/10/14/federated-continual-learning-with-weighted-inter-client-transfer/6.png" alt="图5.培训最后两个（$9^{th}$ 和 $10^{th}$）任务期间的平均任务适应，有5个和100个客户。"></p><p>  快速适应新任务是客户间知识转移的另一个明显优势。为了进一步证明我们的方法在大型网络中的实用性，我们使用ResNet-18（表3）对Non-IID数据集进行了实验，在该数据集上，FedWeIT仍然显著优于最强基线（FedProx APD），同时使用较少的参数。<br><img src="/2022/10/14/federated-continual-learning-with-weighted-inter-client-transfer/8.png" alt="表3.使用ResNet-18的NonIID-50数据集的FCL结果。"></p><p><b>FedWeIT的效率：</b>我们还在表1至表3中报告了作为网络容量函数的精度，我们通过使用的参数数量来测量。我们观察到，与FedProx APD相比，FedWeIT在使用较少参数的情况下获得了更高的精度。这种效率主要来自重用其他客户端的任务自适应参数，这在单机CL方法或朴素的FCL方法中是不可能的。</p><p><img src="/2022/10/14/federated-continual-learning-with-weighted-inter-client-transfer/9.png" alt="表1.在具有100个客户端的FCL期间，重叠CIFAR-100上的平均每任务性能。"><br>我们还检查了每种方法的通信成本（传输的非零参数的大小）。表2重新报告了训练每个任务时的客户端到服务器（C2S）/服务器到客户端（S2C）通信成本。FedWeIT仅使用密度模型中 $\hat{\boldsymbol{B}}$ 和 $\boldsymbol{A}$ 的 $30\%$ 和 $3\%$ 的参数。我们观察到，尽管由于参数的高度稀疏性，FedWeIT广播任务自适应参数，但它比FCL基线的通信效率明显更高。图7显示了根据top-κ%信息参数的传输，精确度作为C2S成本的函数。由于FedWeIT有选择地利用从其他客户那里学到的特定于任务的参数，因此与APD基线相比，它的性能更佳，尤其是在模型参数通信稀疏的情况下。</p><p><img src="/2022/10/14/federated-continual-learning-with-weighted-inter-client-transfer/10.png" alt="图7.准确度高于客户端到服务器成本。我们向原始网络报告相对通信成本。所有结果均为5个客户的平均值。"></p><p><b>灾难性遗忘：</b>此外，我们还研究了在持续学习过程中过去任务的表现如何变化，以了解每种方法的灾难性遗忘的严重程度。图6显示了在后续任务培训结束时，FedWeIT和FCLbaseline在第3、第6和第8个任务上的性能。我们观察到，由于客户之间的干扰，与使用EWC进行本地持续学习相比，原始FCL基线遭受更严重的灾难性遗忘，其中来自其他客户的无关任务的知识覆盖了过去任务的知识。相反，我们的模型没有显示出灾难性遗忘的迹象。这主要是由于通过全局任务自适应参数有选择地利用从其他客户那里学到的先验知识，从而有效地减轻了客户之间的干扰。FedProx APD也不会遭受灾难性遗忘，但由于知识转移无效，它们的性能较差。</p><p><b>加权客户间知识转移</b>通过分析方程式1中的注意力 $\boldsymbol{\alpha}$，我们检查了每个客户端从其他客户端选择的任务参数。图8显示了为MNIST的第0次分割和CIF AR-100的第10次分割学习的注意权重的示例。我们观察到，大量的注意被分配给来自同一数据集（CIFAR-100使用来自具有不相交类的CIFAR-1000任务的参数）或来自类似数据集（MNIST使用来自交通标志和SVHN的参数）的任务参数。这表明，FedWeIT有效地选择了有益的参数，以最大限度地实现客户间知识转移。这是一个令人印象深刻的结果，因为它不知道参数是在哪些数据集上训练的。</p><p><img src="/2022/10/14/federated-continual-learning-with-weighted-inter-client-transfer/11.png" alt="图8.NonIID-50的客户端间传输。我们比较了第一个FC层的关注度，它给出了从其他客户端传输的任务自适应参数的权重。"></p><p><b>异步联合连续学习</b>我们现在考虑异步联合持续学习场景下的FedWeIT，其中每个任务的客户端之间没有同步。这是一个更现实的场景，因为在联合持续学习期间，每个任务可能需要不同的训练回合来汇聚。这里，异步意味着每项任务需要不同的培训成本（即时间、时期或轮次）。在异步联合学习场景下，FedWeIT将任何可用的任务自适应参数从知识库传输到每个客户端。在图9中，我们绘制了同步/异步联合连续学习期间所有任务的平均测试精度。在异步FedWeIT中，每个任务都需要不同的训练回合，并以异步方式接收新任务和任务自适应参数，异步FedWeIT的性能几乎与同步FedWeIT的性能相似。</p><p><img src="/2022/10/14/federated-continual-learning-with-weighted-inter-client-transfer/12.png" alt="图9.FedWeIT在Noniid 50数据集上进行异步联邦持续学习。我们测量每个客户的所有任务的测试精度。"></p><h1>结论</h1><p>  我们解决了一个新的联合持续学习问题，它在每个客户机上持续学习本地模型，同时允许它利用来自其他客户机的间接经验（任务知识）。这带来了新的挑战，如客户间知识转移和防止无关任务之间的客户间干扰。为了应对这些挑战，我们将每个客户端的模型参数进一步分解为在所有客户端共享的全局参数，以及针对每个任务的稀疏本地任务自适应参数。此外，我们允许每个模型有选择地更新全局任务共享参数，并有选择地利用来自其他客户端的任务自适应参数。针对现有的联合学习和持续学习基线，在不同客户端任务相似性下对我们的模型进行了实验验证，结果表明，我们的模型在降低通信成本的情况下显著优于基线。我们相信，联合持续学习是一个对持续学习和联合学习研究社区都非常感兴趣的实际重要主题，这将导致新的研究方向。</p><h1>致谢</h1><p>  这项工作得到了三星电子公司三星研究资助中心（编号：SRFC-IT150251）、三星先进技术研究所、三星电子有限公司、韩国国家研究基金会（NRF）下一代信息计算开发项目的支持，该基金会由韩国科学、信息通信技术与未来规划部（编号：NRF）资助2016M3C4A7952634），由韩国政府（MSIT）资助的韩国国家研究基金会（NRF）拨款（2018R1A5A1059921），以及由DAPA和ADD资助的人工智能应用研究中心（CARAI）拨款（UDI190031RD）。</p><h1>补充文件</h1><p><b>组织：</b>我们提供了主文档中未涵盖的深入描述和解释，并在本补充文档中报告了更多实验，其组织结构如下：</p><ol><li><b>A部分：</b>我们进一步描述了实验细节，包括网络架构、训练配置、遗忘措施和数据集。</li><li><b>B部分：</b>我们报告了其他实验结果，例如通信频率的有效性（第B.1节）和重叠CIFAR-100数据集的消融研究（第B.2节）。</li></ol><h2 id="实验细节"><a class="header-anchor" href="#实验细节">¶</a>实验细节</h2><p>  我们进一步提供了详细的实验设置，包括网络架构、超参数和数据集配置的描述。</p><p><img src="/2022/10/14/federated-continual-learning-with-weighted-inter-client-transfer/13.png" alt="表A.4.基础网络架构（LeNet）的实施细节。注意，$T$ 表示每个客户机顺序学习的任务数。"><br><b>网络体系结构：</b>我们使用LeNet的修改版本和传统的ResNet-18作为主干网络架构进行验证。在LeNet中，前两层是由20个和50个带5×5卷积核的滤波器组成的卷积神经层，然后是两个完全连接的层，每个层有800和500个单元。校正线性单元激活和局部响应归一化随后应用于每个层。我们在每个卷积层之后使用2×2 max-pooling。所有层都基于方差缩放方法进行初始化。LeNet架构的详细描述如表A.4所示。</p><p><b>配置：</b>我们使用具有自适应学习速率衰减的Adam优化器，该优化器每5个周期将学习速率衰减3倍，验证损失没有连续下降。当学习率达到 $\rho$ 时，我们提前停止培训，开始学习下一个任务（如果有）。LeNet的实验有5个客户端，我们在每个新任务开始时用 $1e^{-3} \times 1/3$ 初始化，并且 $\rho=1e^{−7}$。Mini-batch大小为100，每个任务的轮数为20，每轮的历元为1。ResNet-18的设置相同，不包括初始学习率 $1e^{−4}$。在对20和100个客户进行实验的情况下，我们设置了相同的设置，但将minibatch大小从100减少到10，初始学习率为 $1e^{−4}$。在每轮沟通中，我们分别使用客户分数0.25和0.05。我们设置 $\lambda_{1}=[1e^{−1}、4e^{−1}]$ 对于所有实验，$\lambda_{2}=100$。此外，我们在FedProx使用 $\mu=5e^{−3}$ ，而 $\lambda=[1e^{−2}，1.0]$ 适用于EWC和FedCurv。我们将注意力参数 $\boldsymbol{\alpha}_{c}^{(t)}$ 的和初始化为1，$\alpha_{c, j}^{(t)} \leftarrow 1 / \mid \boldsymbol{\alpha}_{c}^{(t)}$。</p><p><b>Metrics：</b>我们根据持续学习文献（Chaudhry等人，2019年；Mirzadeh等人，2020年），在两个指标上评估所有方法。</p><ol><li><b>平均精度：</b>我们通过 $A_{t}=\frac{1}{t} \sum_{i=1}^{t} a_{t, i}$，来测量任务 $t$ 持续学习完成后所有任务的平均测试准确度，其中at，i是任务i学习任务t后的测试准确度。</li><li><b>平均遗忘：</b>我们将遗忘作为持续训练期间最小任务准确度之间的平均差异进行测量。更正式地说，对于 $T$ 任务，遗忘可以定义为：$F=\frac{1}{T-1} \sum_{i=1}^{T-1} \max _{t \in 1, \ldots, T-1}(a_{t, i}-a_{T, i})$</li></ol><p><img src="/2022/10/14/federated-continual-learning-with-weighted-inter-client-transfer/14.png" alt="表A.5.非IID-50任务的数据集详细信息。我们提供了NonIID-50数据集的数据集详细信息，包括8个异构数据集、子任务数、每个子任务的类以及训练集、有效集和测试集的实例。"></p><p><b>数据集：</b>我们创建Overlapped-CIFAR-100和NonIID-50数据集。对于Overlapped-CIFAR-100，我们基于20个超类生成20个非iid任务，其中包含5个子类。我们根据客户机的数量（5、20和100）分割20个任务的实例，然后在所有客户机上分发任务。对于NonIID-50数据集，我们使用8个异构数据集，共创建50个非iid任务，如表A.5所示。然后我们任意选择10个无重复的任务，并将它们分配给5个客户端。根据我们的基本LeNet架构，数据集上的单任务学习平均性能为 $85.78±0.17(\%)$。</p><p><img src="/2022/10/14/federated-continual-learning-with-weighted-inter-client-transfer/19.png" alt="图A.10.使用20个客户端和100个客户端与FedWeIT和APD进行的任务适应比较。我们将每个客户10个任务中的最后5个任务可视化。重叠的CIF AR-100数据集在根据客户端数量（20和100）拆分实例后使用。"></p><h2 id="其他实验结果"><a class="header-anchor" href="#其他实验结果">¶</a>其他实验结果</h2><p>  我们还包括了关于通信轮频率的定量分析和跨客户数量的其他实验结果。</p><h3 id="通信频率的影响"><a class="header-anchor" href="#通信频率的影响">¶</a>通信频率的影响</h3><p><img src="/2022/10/14/federated-continual-learning-with-weighted-inter-client-transfer/15.png" alt="图A.11.每轮的Epochs数我们在重叠CIF AR-100上显示了与5个客户机的每轮通信的训练时间数相关的误差条。所有模型都传输大量的局部基参数和高度稀疏的任务自适应参数。所有结果均为5名客户的平均准确度，我们进行了3次单独试验。每个点上的红色箭头表示性能的标准偏差。"><br>  通过比较模型的性能，我们对通信频率的影响进行了分析，通过每个通信回合的训练时间来衡量。我们运行4个不同的FedWeIT，每轮1、2、5和20个训练时段。图A.11显示了我们的FedWeIT变体的性能。由于客户端经常通过与中央服务器通信来更新模型参数，因此模型在保持较小网络容量的同时获得了更高的性能，因为通信频繁的模型在传递客户端间的知识时有效地更新了模型参数。然而，与通信稀疏的模型相比，它需要更高的通信成本。例如，每轮训练1个时间段的模型可能需要比每轮训练20个时间段模型大16.9倍的整体通信成本。因此，联邦持续学习的模型性能和通信效率之间存在权衡，而FedWeIT变体始终优于（联邦）持续学习基线。</p><h3 id="模型组分的消融研究"><a class="header-anchor" href="#模型组分的消融研究">¶</a>模型组分的消融研究</h3><p>  我们进行了一项消融研究，以分析FedWeIT的每个组成部分的作用。我们比较了模型的四种不同变体的性能。<b>w/o通信</b>描述了不传输基本参数<b>B</b>且仅与任务自适应参数通信的模型。<b>w/o A communication</b>是不通信任务自适应参数的模型。<b>w/o A</b>是仅通过局部基参数的稀疏传输来训练模型的模型，而<b>w/o m</b>是没有稀疏矢量掩码的模型。如表B.7所示，如果不与<b>B</b>或<b>A</b>沟通，与完整模型相比，该模型的性能显著降低，因为它们无法从客户间知识转移中获益。由于灾难性遗忘，模型<b>w/o A</b>的性能非常低，而模型<b>w/o</b>稀疏掩码<b>m<b>的精度较低，容量和成本较大，这表明了执行选择性传输的重要性。</b></b></p><p><img src="/2022/10/14/federated-continual-learning-with-weighted-inter-client-transfer/16.png" alt="表B.7.分析WeIT参数分解有效性的烧蚀研究。在NonIID50数据集上执行的所有实验。"></p><h3 id="带有规范化条件的消融研究"><a class="header-anchor" href="#带有规范化条件的消融研究">¶</a>带有规范化条件的消融研究</h3><p>  我们还通过删除表B.8中建议的正则化项来执行附加分析。如第3.3节所述，如果没有“1”项，该方法可以获得更好的性能，但需要更大的内存.如果没有“2”项，该方法将遭受遗忘。</p><p><img src="/2022/10/14/federated-continual-learning-with-weighted-inter-client-transfer/17.png" alt="表B.8.知识转移的消融研究（NonIID-50）"></p><h3 id="遗忘分析"><a class="header-anchor" href="#遗忘分析">¶</a>遗忘分析</h3><p>  在图B.12中，我们展示了基线模型的遗忘性能，例如Local EWC/APD、FedAvgEWC/APD和FedProx EWC、APD，以及我们的方法FedWeIT。如图所示，基于EWC的方法，包括本地持续学习以及FedAvg和FedProx的组合，在学习新任务时表现出性能下降。例如，FedAvg EWC（红色，带倒三角形标记）的性能从Task 1快速下降到Task 10，这在第一行最左侧的图中显示。另一方面，基于APD的方法和我们的方法都显示出令人信服的能力，无论客户后来学习了多少任务和哪些任务，都可以防止灾难性遗忘。我们在主文档的表2中提供了相应的结果。</p><p><img src="/2022/10/14/federated-continual-learning-with-weighted-inter-client-transfer/18.png" alt="图B.12.遗忘分析在NonIID-50联合持续学习期间，除最后一项任务（第1至第9项）外，所有任务的任务数量不断增加，绩效也随之发生变化。我们观察到，我们的方法在任何任务上都不会出现任务遗忘。"></p>]]></content>
      
      
      <categories>
          
          <category> 科研学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 联邦连续学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《RSA- Byzantine-Robust Stochastic Aggregation Methods for Distributed Learning from Heterogeneous Datasets_1》</title>
      <link href="/2022/10/07/rsa-byzantine-robust-stochastic-aggregation-methods-for-distributed-learning-from-heterogeneous-datasets-1/"/>
      <url>/2022/10/07/rsa-byzantine-robust-stochastic-aggregation-methods-for-distributed-learning-from-heterogeneous-datasets-1/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h1>前缀基础知识</h1><p>  <b>Byzantine Worker：</b>Byzantine worker是指在训练过程中可以随意作恶的worker。比如随意掉线，或者上传恶意、随机的更新梯度。从而达到是的模型优化无法收敛的目的。Byzantine worker 可能上传随机的梯度，也可能故意上传和正确梯度相反的梯度。无论何种方式，这都会导致模型divergence。</p><h1>摘要</h1><p>  在本文中，我们提出了一类鲁棒随机次梯度方法，用于在存在未知数量Byzantine Worker的异构数据集中进行分布式学习。Byzantine Worker在学习过程中，可能会由于数据损坏、通信失败或恶意攻击，向master发送任意错误的消息，从而使学习到的模型产生偏差。该方法的关键是在目标函数中加入正则化项，以增强学习任务的鲁棒性，减轻Byzantine attacks的负面影响。由此产生的基于次梯度的算法被称为Byzantine-鲁棒随机聚合方法，证明了我们今后使用的RSA。与大多数现有算法相比，RSA不依赖于数据在工作者上是独立和同分布的假设，因此适用于更广泛的应用程序。从理论上，我们证明：i) RSA收敛到一个近似最优解，学习误差依赖于Byzantine Worker的数量；ii) RSA在Byzantine attacks下的收敛速度与随机梯度下降法相同，后者不受Byzantine attacks。从数值上讲，在真实数据集上的实验证实了RSA的具有竞争力的性能和与最先进的替代方案相比的复杂性降低。</p><h1>介绍</h1><p>  过去十年见证了智能手机和物联网（IoT）设备的扩散。它们每天都会产生大量数据，人们可以从中学习网络物理系统的模型，并做出决策来改善人类的福利。然而，要求将培训数据集中在一台机器或数据中心的标准机器学习方法可能不适合此类应用，因为从分布式设备收集并存储在云中的数据会导致重大隐私风险（Sicari等人，2015年）。为了缓解用户隐私问题，谷歌提出了一种称为联合学习的新分布式机器学习框架，并于近期流行起来（McMahan and Ramage 2017；Smith et al.2017）。联合学习允许将培训数据保存在业主设备的本地。数据样本和计算任务分布在多个工人之间，例如智能家居中的物联网（IoT）设备，这些设备被编程为协作学习模型。流行机器学习算法的并行实现，例如随机梯度下降（SGD），应用于从分布式数据中学习（Bottou 2010）。</p><p>  然而，联合学习仍然面临两个重大挑战：高通信开销和严重的安全风险。虽然最近开发了几种方法来解决分布式学习的通信瓶颈（Li等人2014；Liu等人2017；Smith等人2017；Chen等人2018b），但安全问题尚未得到充分解决。在联合学习应用程序中，许多设备可能高度不可靠，甚至很容易被黑客破坏。我们称这些装置为Byzantine Worker。在这种情况下，学习者缺乏安全的训练能力，因此容易失败，更不用说对抗性攻击（Lynch 1996）。例如，作为大规模机器学习的主力军，SGD甚至容易受到一名Byzantine Worker的攻击（Chen、Su和Xu，2017）。</p><p>  在这种背景下，本论文研究了一般Byzantine故障模型下的分布式机器学习，其中Byzantine Worker可以任意修改从他们自己传送给主机的消息。有了这样的模型，它对通信故障或攻击就没有任何限制。我们的目标是开发高效的分布式机器学习方法，为这种环境量身定制，并提供可证明的性能保证。</p><h1>相关工作</h1><p>  Byzantine-robust分布式学习近年来受到了越来越多的关注。大多数现有算法都扩展了SGD，以纳入拜占庭稳健设置，并假设数据在工人身上独立且相同分布（i.i.d.）。在这种假设下，正规工人计算的随机梯度大概分布在真实梯度周围，而拜占庭工人发送给主人的随机梯度可能是任意的。因此，掌握者能够应用稳健估计技术来聚集随机梯度。典型的梯度聚集规则包括几何中值（Chen、Su和Xu 2017）、边缘修剪平均值（Yin et al.2018a；Xie、Koyejo和Gupta 2018b）、维度中值（Xie、Koyejo、Gupta 2018；Alistarh、Allen Zhu和Li 2018）等。一种称为Krum的更复杂算法选择一个梯度，该梯度与给定数量的最近梯度的欧氏距离之和最小（Blanchard等人，2017年）。针对高维学习，在（Su和Xu 2018）中开发了一种迭代滤波算法，在高维范围内实现了最佳错误率。这些现有算法的主要缺点来自i.i.d.假设，这在异构计算单元的联合学习中是不可能的。实际上，将这些算法推广到非身份证设置并不简单。此外，其中一些算法依赖于复杂的梯度选择子程序，如Krum和几何中值子程序，这会导致较高的计算复杂性</p><p>  在此背景下的其他相关工作包括（Yin等人2018b）针对拜占庭攻击下非凸优化问题的逃逸鞍点，以及（Chen等人2018a）利用基于梯度编码的算法进行鲁棒学习。然而，（Chen et al.2018a）中的方法需要重新定位数据点，这在联合学习范式中不容易实现。利用额外数据，（Xie、Koyejo和Gupta 2018c）研究了可信赖的基于分数的计划，即使只有一名非Byzantine Worker，也能保证有效学习，但在实践中可能并不总是有额外数据可用。我们的算法还与在（Ben Ameur，Bianchi，and Jakubowicz 2016；Xu，Li，and Ling 2018）中研究的稳健分散优化有关，这些算法考虑在具有不可靠节点的分散网络上优化静态或动态成本函数。相反，这项工作的重点是拜占庭稳健随机优化。</p><h2 id="贡献"><a class="header-anchor" href="#贡献">¶</a>贡献</h2><p>  本文的贡献总结如下</p><p>  c1）我们开发了一类稳健的随机方法，简称RSA，用于异构数据集和拜占庭攻击下的分布式学习。RSA有几个变体，每个变体都是为“$\ell_{p}$-范数正则化鲁棒化目标函数”量身定制的</p><p>  c2）在收敛速度以及拜占庭攻击所导致的错误方面，严格地为最终的RSA方法建立了性能</p><p>  c3）使用MNIST数据集进行了广泛的数值测试，以证实RSA在拜占庭攻击和运行时的分类精度方面的有效性。</p><h1>分布式SGD</h1><p>  我们考虑一个一般的分布式系统，由一个master和 $m$ 个worker组成，其中 $q$ 个worker是Byzantine的（任意行为）。目标是找到以下问题的优化器：<br>$$<br>\min _{\tilde{x} \in \mathbb{R}^{d}} \sum_{i=1}^{m} \mathbb{E}\left[F\left(\tilde{x}, \xi_{i}\right)\right]+f_{0}(\tilde{x}) \qquad(1)<br>$$<br>这里 $\tilde{x} \in \mathbb{R}^{d}$ 是优化变量，$f_{0}(\tilde{x})$ 是正则化项，$F(\tilde{x}, \xi_{i})$ 是worker $i$ 相对于随机变量 $\xi_{i}$ 的损失函数。与之前的工作不同，之前的工作假设worker之间的分布式数据是i.i.d.，我们考虑了一个更实际的情况：$\xi_{i} \sim \mathcal{D}_{i}$，其中 $\mathcal{D}_{i}$ 是workers $i$ 上的数据分布，可能与其他workers上的分布不同。</p><p>  在主-辅结构中，在分布式SGD算法的时间 $k+1$ 时，每个辅 $i$ 从主节点接收当前模型 $\tilde{x}^{k}$，从分布 $\mathcal{D}_{i}$ 中针对随机变量 $\xi_{i}^{k}$ 采样一个数据点，并计算局部经验损失的梯度 $\nabla F\left(\tilde{x}^{k}, \xi_{i}^{k}\right)$ 请注意，此采样过程可以很容易地推广到小批量设置，其中每个worker采样多个i.i.d.数据点，并计算本地经验损失的平均梯度。master收集并聚合workers发送的梯度，并更新模型。其在 $k+1$ 时的更新是：<br>$$<br>\tilde{x}^{k+1}=\tilde{x}^{k}-\alpha^{k+1}\left(\nabla f_{0}\left(\tilde{x}^{k}\right)+\sum_{i=1}^{m} \nabla F\left(\tilde{x}^{k}, \xi_{i}^{k}\right)\right) \qquad(2)<br>$$<br>其中，$\alpha^{k+1}$ 是 $k+1$ 时的递减学习率。分布式SGD在算法1中概述。</p><p><b>SGD容易受到Byzantine attacks：</b>虽然SGD在传统的大规模机器学习环境中表现良好，但在Byzantine workers在场的情况下，其表现将显著下降（Chen、Su和Xu，2017）。假设一些workers是Byzantine，他们可以根据其他workers发送的信息报告任意消息或策略性地发送精心设计的消息，从而偏见学习过程。具体来说，如果worker $m$ 是Byzantine，在时间 $k+1$，它可以选择以下两种攻击之一：<br>  a1）发送 $\nabla F\left(\tilde{x}^{k}, \xi_{m}^{k}\right)=\infty$</p><p>  a2）发送 $\nabla F\left(\tilde{x}^{k}, \xi_{m}^{k}\right)=-\sum_{i=1}^{m-1} \nabla F\left(\tilde{x}^{k}, \xi_{i}^{k}\right)$<br>在任何情况下，SGD更新（2）中使用的聚合梯度 $\sum_{i=1}^{m} \nabla F\left(\tilde{x}^{k}, \xi_{i}^{k}\right)$ 将为无穷大或为空，因此学习的模型 $\tilde{x}^{k}$ 将不会收敛或收敛到错误的值。图1显示了SGD在Byzantine attacks攻击下的运作情况。</p><p><img src="/2022/10/07/rsa-byzantine-robust-stochastic-aggregation-methods-for-distributed-learning-from-heterogeneous-datasets-1/1.png" alt="图1:SGD的操作。有 $m$ 名workers，$r$ 为固定workers，其余 $q=m−r$ 是Byzantine。主进程将当前迭代发送给工作进程，而常规工作进程将返回随机梯度。红色魔鬼标记表示Byzantine workers发送给主人的错误信息。"></p><p>  与使用（2）中的简单平均值不同，稳健的梯度聚集规则已被纳入到SGD中（Blanchard et al.2017；Chen，Su，and Xu 2017；Xie，Koyejo，and Gupta 2018a；Yin et al.2018b；2018a）。然而，在联合学习环境中，由于难以区分统计异质性和Byzantine attacks，这些聚合规则变得不那么有效。接下来，我们开发了一个与SGD相对应的版本，以解决从分布式异构数据中进行健壮学习的问题。</p><h1>RSA用于强健的分布式学习</h1><p>  请注意，由于Byzantine workers的存在，解决（1）是没有意义的，它最小化了所有workers的本地预期损失的总和，而没有区分正规workers和Byzantine workers，因为Byzantine workers总是会阻止学习者访问他们的本地数据并找到最佳解决方案。相反，一个不那么雄心勃勃的目标是找到一个解决方案，最大限度地减少正规workers当地预期成本函数加上正规化术语的总和：<br>$$<br>\tilde{x}^{*}=\arg \min _{\tilde{x} \in \mathbb{R}^{d}} \sum_{i \in \mathcal{R}} \mathbb{E}\left[F\left(\tilde{x}, \xi_{i}\right)\right]+f_{0}(\tilde{x})  \qquad(3)<br>$$<br>这里我们用 $|\mathcal{B}|=q$ 和 $|\mathcal{R}|=m-q$ 表示 $\mathcal{B}$ 是Byzantine workers的集合，$\mathcal{R}$ 是regular worker的集合。让每个regular worker $i$ 有局部迭代 $x_{i}$，并且master有局部迭代 $x_{0}$，我们得到（3）的等价形式：<br>$$<br>\begin{array}{l}<br>\min _{x:=\left[x_{i} ; x_{0}\right]} \sum_{i \in \mathcal{R}} \mathbb{E}\left[F(x_{i}, \xi_{i})\right]+f_{0}(x_{0}) \qquad (4a)\\<br>\text{s,to} \qquad x_{0}=x_{i}, \forall i \in \mathcal{R} \qquad (4b)<br>\end{array}<br>$$<br>其中 $x:=\left[x_{i} ; x_{0}\right] \in \mathbb{R}^{(|\mathcal{R}|+1) d}$ 是一个向量，用于堆叠regular workers的局部变量 $x_{i}$ 和主变量 $x_{0}$。公式（4）与中的共识优化概念一致，例如（Shi等人，2014）。</p><h2 id="ell-1-norm-RSA"><a class="header-anchor" href="#ell-1-norm-RSA">¶</a>$\ell_{1}$-norm RSA</h2><p>  由于master无法获得Byzantine workers的身份，因此不可能通过迭代更新 $\tilde{x}$ 或 $x$ 直接求解（3）或（4）。因此，我们引入了（4）的“$\ell_{1}$-范数正则形式：<br>$$<br>x^{*}:=\underset{x:=\left[x_{i} ; x_{0}\right]}{\arg \min } \sum_{i \in \mathcal{R}}\left(\mathbb{E}\left[F\left(x_{i}, \xi_{i}\right)\right]+\lambda\left\|x_{i}-x_{0}\right\|_{1}\right)+f_{0}\left(x_{0}\right) \qquad (5)<br>$$<br>其中 $\lambda$ 为正常数。代价函数（5）中的第二项是“$\ell_{1}$-范数惩罚”，其最小化迫使每个 $x_{i}$ 接近主变量 $x_{0}$。接下来，我们将展示这种松弛形式如何在Byzantine attacks下带来鲁棒学习的优势。</p><p>  在Byzantine workers身份被揭示的理想情况下，我们可以应用随机次梯度方法求解（5）。优化只涉及regular workers和master。在 $k+1$ 时，regular worker $i$ 的 $x_{i}^{k+1}$ 和主设备的 $x_{0}^{k+1}$ 的更新由以下提供：<br>$$<br>\begin{array}{l}<br>x_{i}^{k+1}=x_{i}^{k}-\alpha^{k+1}(\nabla F(x_{i}^{k}, \xi_{i}^{k})+\lambda \operatorname{sign}(x_{i}^{k}-x_{0}^{k})) \qquad (6a) \\<br>x_{0}^{k+1}=x_{0}^{k}-\alpha^{k+1}(\nabla f_{0}(x_{0}^{k})+\lambda(\sum_{i \in \mathcal{R}} \operatorname{sign}(x_{0}^{k}-x_{i}^{k}))) \qquad (6b)<br>\end{array}<br>$$<br>其中，$\operatorname{sign}(\cdot)$ 是元素级符号函数。给定一个 $a \in \mathbb{R}$、 当 $a&gt;0$ 时，符号 $\operatorname{sign}(a)$ 等于1，当 $a&lt;0$ 时为-1，且当 $a=0$ 时，为 $[−1，1]$ 其中的任意值。在时间 $k+1$ 时，每个worker $i$ 将本地迭代 $x_{i}^{k}$ 发送给主进程，而不是在分布式SGD中发送其本地随机梯度。主节点聚合了工作节点发送的模型，以更新自己的模型 $x_{0}^{k+1}$。从这个意义上讲，（6）中的更新基于模型聚合，不同于SGD中的梯度聚合。<font color="red">注意，这里所说的模型聚合是因为前面的worker和master传送的不是梯度，而是优化局部变量。</font></p><p>  现在让我们考虑一下（6）中的更新在Byzantine workers面前的表现。regular worker $i$ 的更新与（6a）相同，即：<br>$$<br>x_{i}^{k+1}=x_{i}^{k}-\alpha^{k+1}(\nabla F(x_{i}^{k}, \xi_{i}^{k})+\lambda \operatorname{sign}\left(x_{i}^{k}-x_{0}^{k}\right)) \qquad (7)<br>$$<br>如果worker $i$ 是Byzantine的，那么它不会将从（6a）计算出的值 $x_{i}^{k}$ 发送给master，而是发送一个任意变量 $z_{i}^{k} \in \mathbb{R}^{d}$。master无法区分regular worker 发送的 $x_{i}^{k}$ 或拜占庭工人发送的 $z_{i}^{k}$。因此，在 $k+1$ 时master的更新不再是（6b），而是：<br>$$<br>\begin{aligned}<br>x_{0}^{k+1}=x_{0}^{k}-\alpha^{k+1}(\nabla f_{0}(x_{0}^{k})+\lambda(\sum_{i \in \mathcal{R}} \operatorname{sign}(x_{0}^{k}-x_{i}^{k})+\sum_{j \in \mathcal{B}} \operatorname{sign}(x_{0}^{k}-z_{j}^{k})))<br>\end{aligned} \qquad (8)<br>$$<br>我们将此算法称为 $\ell_{1}$-范数RSA（Byzantine-robust随机聚合）。</p><p><b>$\ell_{1}$-norm RSA对Byzantine attacks非常强大</b>由于在（5）中引入了“$\ell_{1}$-norm正则化术语”，“$\ell_{1}$-normal RSA对Byzantine attacks具有鲁棒性。正则化项允许每个 $x_{i}$ 不同于 $x_{0}$，并且偏差由参数 $\lambda$ 控制。当任何workers是Byzantine workers且行为任意时，此修改使目标函数更加稳健。从算法的角度来看，我们可以从更新（8）中观察到，无论regular worker和Byzantine workers发送给主master值有多大差异，他们对 $x_{0}^{k+1}$ 的影响都是相似的。因此，只有Byzantine workers的数量会影响RSA更新（8），而不是Byzantine workers发送的恶意消息的数量。从这个意义上说，$\ell_{1}$-norm RSA对Byzantine workers的任意攻击非常强大。这与即使是一个Byzantine workers也容易受到伤害的SGD形成鲜明对比。</p><h2 id="ell-p-norm-RSA的推广"><a class="header-anchor" href="#ell-p-norm-RSA的推广">¶</a>$\ell_{p}$-norm RSA的推广</h2><p>  除了解决$\ell_{1}$-norm正则化问题（5）外，我们还可以解决以下 $\ell_{p}$-norm正则问题：<br>$$<br>x^{*}:=\underset{x:=[x_{i} ; x_{0}]}{\arg \min } \sum_{i \in \mathcal{R}}(\mathbb{E}[F(x_{i}, \xi_{i})]+\lambda\|x_{i}-x_{0}\|_{p})+f_{0}(x_{0}) \qquad (9)<br>$$<br>其中 $p \ge 1$。类似于（5）中的 $\ell_{1}$-正则化目标情况，$\ell_{p}$ 规范处罚有助于减轻Byzantine workers的负面影响。</p><p>  与 $\ell_{1}$-norm RSA类似，$\ell_{p}$-norm RSA”仍然使用次梯度递归进行操作。对于每个regular worker $i$，其在 $k+1$ 时的本地更新为：<br>$$<br>x_{i}^{k+1}=x_{i}^{k}-\alpha^{k+1}(\nabla F(x_{i}^{k}, \xi_{i}^{k})+\lambda \partial_{x_{i}} \|x_{i}^{k}-x_{0}^{k} \|_p)  \qquad (10)<br>$$<br>其中 $\partial_{x_{i}} \|x_{i}^{k}-x_{0}^{k} \|_p$ 是 $\|x_{i}-x_{0}^{k} \|_p$ 在 $x_{i}=x_{i}^{k}$ 的次梯度。同样，对于master，其在 $k+1$ 时的更新为：<br>$$<br>x_{0}^{k+1}=x_{0}^{k}-\alpha^{k+1}(\nabla f_{0}(x_{0}^{k})+\lambda(\sum_{i \in \mathcal{R}} \partial_{x_{0}} \|x_{0}^{k}-x_{i}^{k} \|_{p} +\sum_{j \in \mathcal{B}} \partial_{x_{0}} \|x_{0}^{k}-z_{j}^{k} \|_{p}))  \qquad (11)<br>$$<br>其中 $\partial_{x_{0}} \|x_{0}^{k}-x_{i}^{k} \|_{p}$ 和 $\partial_{x_{0}} \|x_{0}^{k}-z_{j}^{k} \|_{p}$ 分别是 $\|x_{0}^{k}-x_{i}^{k} \|_{p}$ 和 $\|x_{0}^{k}-z_{j}^{k} \|_{p}$ 在 $x_{0} = x_{0}^{k}$ 的次梯度。</p><p>  为了计算 $\ell_{p}$-norm RSA中涉及的次梯度，我们将依赖以下命题。</p><p><b>Proposition 1.</b>让 $p \ge 1$ 和 $b$ 满足 $1/b +1/p =1$ 。对于 $x \in \mathbb{R}^{d}$，我们有次微分 $\partial_{x}\|x\|_{p}=\{z \in \mathbb{R}^{d}: \quad\langle z, x\rangle=\|x\|_{p},\|z\|_{b} \leq 1 \}$</p><p>  在这里和之后，我们通过使用 $\partial$ 表示次梯度和次微分。Proposition 1的证明见补充文件。</p><p><img src="/2022/10/07/rsa-byzantine-robust-stochastic-aggregation-methods-for-distributed-learning-from-heterogeneous-datasets-1/2.png" alt="算法2"></p><p>  算法2中总结了在Byzantine attacks下用于鲁棒分布式随机优化的 $\ell_{1}$-norm RSA和 $\ell_{p}$-norm RSA并如图2所示。<br><img src="/2022/10/07/rsa-byzantine-robust-stochastic-aggregation-methods-for-distributed-learning-from-heterogeneous-datasets-1/3.png" alt="图2：RSA的操作。有 $m$ 名workers，$r$ 为regular workers，其余 $q=m−r$ 还是Byzantine。master将其局部变量发送给workers，而regular workers将返回其局部变量。红色魔鬼标记表示Byzantine workers发送给master的错误信息。"></p><p><b>备注 1（模型与梯度聚合）</b>大多数现有的Byzantine-robust方法都是基于梯度聚合的。由于每个worker都使用相同的迭代计算其梯度，因此这些方法不存在共识问题（Blanchard et al.2017；Chen，Su and Xu 2017；Xie，Koyejo and Gupta 2018a；Yin et al.2018b；2018a，Xie，Koyejo，and Gupta 2018c）。然而，为了实现有效的梯度聚合，这些方法要求workers中存储的数据是i.i.d.，这在联合学习环境中是不切实际的。在假设所有workers的数据都是i.i.d.的情况下，regular workers计算的随机梯度也是i.i.d.，他们的期望是相等的。利用这一先验知识，master能够应用稳健估计技术来汇总从regular workers和Byzantine workers收集的随机梯度。当i.i.d.假设不成立时，即使是regular workers计算的随机梯度在预期上也是不同的，因此基于梯度聚合的方法不再有效。相比之下，所提出的RSA方法利用模型聚合来寻找共识模型，并且不依赖于i.i.d.假设。另一方面，现有的梯度聚合方法通常需要设计非平凡的子程序来聚合梯度，因此会产生相对较高的复杂性（Xie、Koyejo和Gupta 2018a；Blanchard et al.2017；Su和Xu 2018）。相比之下，所提出的RSA方法具有更低的复杂性，这与在Byzantine-free环境中工作的标准分布式SGD相同。我们将在数值试验中证明RSA在计算时间方面的优势，并与几种最先进的替代方案进行比较。</p><h1>收敛性分析</h1><p>  本节分析了建议的RSA方法的性能，并在补充文档中给出了证明。我们做出以下假设。</p><p><b>假设1.（强凸性）</b>局部代价函数 $\mathbb{E}[F(\tilde{x},\xi _{i}  )] $ 和正则化项 $f_{0}(\tilde{x})$ 分别与常数 $\mu _{i} $ 和 $\mu _{0} $ 强凸。</p><p><b>假设2.（Lipschitz连续梯度）</b>局部代价函数 $\mathbb{E}[F(\tilde{x},\xi _{i}  )] $ 和正则化项 $f_{0}(\tilde{x})$ 分别具有常数 $L_{i}$ 和 $L_{0}$ 的Lipschit连续梯度。</p><p><b>假设3.（有界方差）</b>对于每个worker $i$，数据采样是跨时间的i.i.d.，因此 $\xi _{i}^{k} \sim \mathcal{D} _{i}$。$\nabla F(\tilde{x},\xi _{i})$ 的方差上界为 $\delta _{i}^{2} $，即 $\mathbb{E}\left[\left\|\nabla \mathbb{E}\left[F\left(\tilde{x}, \xi_i\right)\right]-\nabla F\left(\tilde{x}, \xi_i\right)\right\|^2\right] \leq \delta_i^2$。</p><p>  请注意，假设1-3是基于随机梯度的方法性能分析的标准（Nemirovski等人，2009年），它们适用于广泛的机器学习问题，如 $\ell_{2}$-正则最小二乘法和逻辑回归</p><p>  我们从研究 $\ell_{p}$-norm 正则化问题（9）开始，说明了（9）的最优解是合意的，并且与（3）的最佳解相同的条件。</p><p><b>定理1.</b>假设假设1和假设2成立。如果$\lambda \geq \lambda_{0}:=\max _{i \in \mathcal{R}}\|\nabla \mathbb{E}[F(\tilde{x}^{*}, \xi_{i})]\|_{b}$ 、$p \ge 1$ 并且 $b$ 满足 $1/b+1/p=1$，那么我们有 $x^{*}=[\tilde{x}^{*}]$, 其中 $\tilde{x}^{*}$ 和 $x^{*}$ 分别是（3）和（9）的最优解。</p><p>  定理1断言，如果选择惩罚常数 $\lambda$ 足够大，正则化问题（9）的最优解与（3）的最佳解相同。接下来，我们将检查RSA迭代在Byzantine attacks下关于（9）的最优解的收敛性。</p><p><b>定理2.</b>假设假设1、2和3成立。将 $\ell_{p}$ 的步长设置为 $\alpha ^{k+1} =\min \{\underline{\alpha },\frac{\bar{\alpha } }{k+1} \}$，其中 $\underline{\alpha}$ 和 $\bar{\alpha}$ 取决于 $\{ \mu_{0}, \mu_{i}, L_{0}, L{i} \}$。那么，对于满足 $\min \{k: \underline{\alpha } \ge \frac{\bar{\alpha}}{k+1} \}$ 的 $k_{0}$，我们有：<br>$$<br>\mathbb{E} \|x^{k+1} - x^{*} \|^{2} \leq(1-\eta \underline{\alpha})^{k} \|x^{0} - x^{*} \|^{2}+\frac{\underline{\alpha} \Delta_{0}+\Delta_{2}}{\eta}, k &lt; k_{0}  \qquad (12)<br>$$<br>并且<br>$$<br>\mathbb{E} \|x^{k+1} - x^{*} \|^{2} \leq \frac{\Delta_{1}}{k+1}+\bar{\alpha} \Delta_{2}, k \geq k_{0}<br>$$<br>其中 $\eta$，$\Delta_{1}$ 和 $\Delta_{2}=\mathcal{O}(\lambda^{2} q^{2})$ 是某些正常数</p><p>  定理2表明局部迭代序列次线性收敛到正则化问题（9）的近似最优解。渐近次优缺口二次依赖于Byzantine workers的数量 $q$。根据定理1和2，我们可以得出以下定理。</p><p><b>定理3.</b>在与定理2中相同的假设下，如果我们选择 $\lambda \ge \lambda _{0}$ 根据定理1，则对于足够大的 $k \ge k_{0}$，我们有：<br>$$<br>\mathbb{E} \|x^{k} - [\tilde{x}^{*}] \|^{2} \leq \frac{\Delta_{1}}{k+1}+\bar{\alpha} \Delta_{2}<br>$$<br>如果我们选择 $0 &lt; \lambda &lt; \lambda_{0}$ 并且假设（9）和（3）的优化器之间的差是由 $\|x^{*} - [\tilde{x}^{*}] \|^{2} \le \Delta_{3}$，然后是 $k \ge k_{0}$ 我们有：<br>$$<br>\mathbb{E} \|x^{k} - [\tilde{x}^{*}] \|^{2} \leq \frac{2 \Delta_{1}}{k+1}+2 \bar{\alpha} \Delta_{2}+2 \Delta_{3}<br>$$</p><p>  定理3暗示局部迭代序列也次线性收敛到原（3）的近似最优解。在适当选择的 $\lambda$ 下，极限中的次优差距与Byzantine workers的数量成正比。注意，由于 $\mathcal{O}(1/k)$ 步长对其初始值非常敏感（Nemirovski等人，2009），我们使用 $\mathcal{O}(1/ \sqrt{k})$ 数值测试中的步长。其相应的理论主张和收敛性分析在补充文件中给出</p><p>  关于惩罚常数 $\lambda$ 和 $p$ 范数的最佳选择，下面是注释。</p><p><b>备注2.（ $\lambda$ 和 $p$ 的最佳选择）.</b>在RSA中选择不同的惩罚常数 $\lambda$ 和 $\ell_{p}$ norm 通常会导致不同的性能。对于固定 $\lambda$，如果使用带小 $p$ 的范数 $\ell_{p}$，则对偶范数 $\ell_{b}$ 具有大 $b$，从而在定理1中导致小 $\lambda_{0}$。因此，局部解可能是一致的。通过数值试验，RSA与 $\ell_{\infty}$ 标准不提供竞争绩效，而具有 $\ell_{1}$ 和 $\ell_{2}$ 标准的绩效很好。另一方面，对于固定的 $p$，较小的 $\lambda$ 不能保证局部解之间的一致性，但它给出了一个较小的次优间隙 $\Delta_{2}$。我们建议使用相对小于 $\lambda_{0}$ 的 $\lambda$，稍微牺牲共识，但减少次优差距。</p><h1>数值实验</h1><p>  在本节中，我们评估了所提出的RSA方法对Byzantine attacks的鲁棒性，并将其与几个基准算法进行了比较。我们在MNIST数据集上进行实验，该数据集有 $60k$ 个训练样本和 $10k$ 个测试样本，并使用softmax回归，其 $\ell_{2}$ 范数正则化项 $f_{0}(\tilde{x})=\frac{0.01}{2} \|\tilde{x} \|^{2}$。我们在一台使用Intel i7-6700 CPU@3.40GHz的计算机上启动了20个workers进程和1个master进程。在i.i.d.案例中，训练样本随机平均分配给workers。在异质情况下，每两名workers平均分享一位数的训练样本。在每次迭代中，每个regular worker 都会在一小批32个样本上估计其局部梯度。测试数据集的第1精度（在RSA中用 $x_{0}$ 进行评估，在基准算法中用 $\tilde{x}$ 进行评估）用作性能指标。该代码可从https://github.com/liepill/rsa-byzantine获取</p><h2 id="基准算法"><a class="header-anchor" href="#基准算法">¶</a>基准算法</h2><p>  我们使用无攻击的SGD迭代（2）作为预言机，称为 <b>Ideal SGD</b>。注意，该方法不受Byzantine worker数量 $q$ 的影响。其他基准算法实现以下随机梯度聚合递归：<br>$$<br>\tilde{x}^{k+1}=\tilde{x}^{k}-\alpha^{k+1} \tilde{\nabla}(\tilde{x}^{k})<br>$$<br>式中 $\tilde{\nabla}(\tilde{x}^{k})$ 是一种依赖于算法的聚合随机梯度，它在master发送给workers的 $\tilde{x}^{k}$ 点处近似于梯度方向。让worker $i$ 发送给master的消息为 $v_{i}^{k}$ 如果 $i$ 是正则的，则为随机梯度 $\nabla F(\tilde{x}^{k}, \xi _{i}^{k})$；如果 $i$ 是Byzantine的，则是任意的。基准算法使用不同的规则来计算聚集的随机梯度。</p><p><b>GeoMed（Chen，Su，Xu 2017）.</b>$\{v_{i}^{k} : i \in [m]\}$ 的几何中值表示为：<br>$$<br>\operatorname{GeoMed}({v_{i}^{k}})=\underset{v \in \mathbb{R}^{d}}{\arg \min } \sum_{i=1}^{m} \|v-v_{i}^{k}\|_{2} \qquad (17)<br>$$<br>在实验中，我们使用快速Weiszfeld算法（Weiszfield and Plastria 2009）计算几何中值。</p><p><b>Krum（Blanchard等人，2017年）.</b>Krum计算 $\tilde{\nabla}(\tilde{x}^{k})$ 依据：<br>$$<br>\operatorname{Krum}({v_{i}^{k}})=v_{i^{*}}^{k}, i^{*}=\underset{i \in[m]}{\arg \min } \sum_{i \rightarrow j} \|v_{i}^{k}-v_{j}^{k} \|^{2} \qquad (18)<br>$$<br>其中 $i \rightarrow j (i \ne j)$ 选择 $\{v_{j}^{k} : j \in [m]\}$ 中 $v_{i}^{k}$ 的 $m-q-2$ 近邻索引 $j$ ，用欧氏距离测量。注意 $q$，Byzantine workers的数量，必须在Krum提前知道。</p><p><b>中位数（Xie、Koyejo和Gupta 2018a）.</b>边际中值聚合规则返回矢量 $\{v_{i}^{k} : i \in [m]\}$ 的元素中值。</p><p><b>SGD（Bottou，2010年）.</b>经典SGD聚合 $\{v_{i}^{k} : i \in [m]\}$ 通过返回平均值，因此对拜占庭攻击不可靠。</p><p>  在接下来的实验中，基准算法的步长都是手动调整到最佳的。</p><h2 id="没有Byzantine-attacks"><a class="header-anchor" href="#没有Byzantine-attacks">¶</a>没有Byzantine attacks</h2><p>  在本测试中，我们考虑了无Byzantine workers的学习，并在图3中显示了所有算法的性能。$\ell_{1}$-norm RSA选择参数 $\lambda = 0.1$，步长 $\alpha^{k}=0.003\ \sqrt{k}$。RSA和GeoMed接近Ideal SGD，明显优于Krum和Median。因此，增强RSA中的成本，虽然会带来偏差，但在常规情况下不会牺牲性能。</p><h2 id="Same-value-attacks"><a class="header-anchor" href="#Same-value-attacks">¶</a>Same-value attacks</h2><p>  相同的值攻击将Byzantine worker $i$ 发送的消息设置为 $v_{i}^{k}=c\mathbf{1}$。这里是 $\mathbf{1} \in \mathbb{R}^{d}$ 是一个全一向量，$c$ 是一个常数，我们将其设置为100。我们考虑了两个不同数量的Byzantine workers，$q=4$ 和 $q=8$，并在图4中演示了性能。$\ell_{1}$-norm RSA选择正则化参数 $\lambda=0.07$ 和步长 $\alpha^{k}=0.001/ \sqrt{k}$、 当 $q=4$ 时，RSA和GeoMed仍接近理想SGD，并优于Krum和Median当 $q$ 到 $q=8$ 时，Krum与Median的表现比 $q=4$ 差，而RSA与GeoMed几乎与 $q=4$ 相同。请注意，由于softmax回归的特殊结构，SGD仍能在相同的值攻击下工作。</p><h2 id="Sign-flipping-attacks"><a class="header-anchor" href="#Sign-flipping-attacks">¶</a>Sign-flipping attacks</h2><p>  符号翻转攻击会翻转消息的符号（渐变或局部迭代）并放大其大小。具体来说，Byzantine worker $i$ 首先计算真值 $\hat{v}_{i}^{k}$，然后将 $v_{i}^{k}=\sigma \hat{v}_{i}^{k}$；发送给master，其中 $\sigma$ 是一个负常数。我们在设置 $q=4$ 和 $q=8$ 时测试 $\sigma = -4$，如图5所示。参数为 $\lambda=0.07$ 和 $\alpha=0.001/ \sqrt{k}$ 时 $q=4$，同时当 $\lambda=0.01$ 和 $\alpha=0.0003$ 时，$q=8$。毫不奇怪，SGD在这两种情况下都失败了。GeoMed、Median和 $\ell_{1}$-normal RSA显示出类似的性能，并且Median比其他Byzantine-robust算法稍差。</p><h2 id="Gaussian-attacks"><a class="header-anchor" href="#Gaussian-attacks">¶</a>Gaussian attacks</h2><p>  Gaussian attacks将Byzantine worker $i$ 发送的消息设置为 $v_{i}^{k}$，其中每个元素都遵循高斯分布 $\sim N(0，g^{2})$。如图6所示，当 $q=4$ 和 $q=8$ 时，我们测试 $g=10000$。在 $\ell$-norm RSA中，参数为 $\lambda =0.07$ 和 $\alpha=0.002/ \sqrt{k}$ 对于 $q=4$ 和 $q=8$。在这两种情况下，SGD都失败了。RSA和GeoMed的业绩相似，表现优于Median和Krum。</p><h2 id="实时比较"><a class="header-anchor" href="#实时比较">¶</a>实时比较</h2><p>  我们在图7中显示了在参数 $c=100$ 和 $q=8$ Byzantine workers的相同值攻击下算法的运行时间。每种算法的总迭代次数为 $5000$ 次。虽然这些算法没有在联合学习平台上实现，但比较清楚地表明，在处理Byzantine attacks时，每次迭代都会产生额外的计算成本。由于难以计算几何中值，GeoMed的每次迭代计算成本最高。$\ell_{1}$-norm RSA和Median都比Ideal SGD稍慢，但比Krum快。与理想SDG相比，RSA的唯一计算开销在于计算轻量级的符号函数。因此，与其他复杂的梯度聚集方法相比，RSA在计算复杂性方面具有优势。</p><h2 id="超参数-lambda-的影响"><a class="header-anchor" href="#超参数-lambda-的影响">¶</a>超参数 $\lambda$ 的影响</h2><p>  我们改变了超参数 $\lambda$，并展示了它如何影响性能。我们使用 $c=100$ 的相同值攻击，改变 $\lambda$，运行RSA $5000$ 次迭代，并在图8中描述了最终的top1精度。Byzantine workers的数量为 $q=8$，步长手动调整到最佳。注意，当 $\lambda$ 很小时，regular worker往往依赖自己的数据，因此网络上的信息融合速度很慢，这会导致收敛速度慢和误差大。另一方面，正如我们在收敛分析中所研究的那样，较大的 $\lambda$ 也会导致显著的误差。</p><h2 id="具有不同范数的RSA"><a class="header-anchor" href="#具有不同范数的RSA">¶</a>具有不同范数的RSA</h2><p>  现在我们比较用不同范数规范化的RSA方法。图9和图10分别显示了无Byzantine attacks和 $q=8$ 和 $c=100$ 的相同值攻击下的结果。我们考虑两个性能指标，regular workers本地迭代的top-1精度和方差。一个小的差异意味着regular workers会得出类似的解决方案。如果没有Byzantine workers，$\ell_{1}$ 的 $\lambda =0.1$，$\alpha^{k}=0.001/ \sqrt{k}$、$\ell_{2}$ 是 $\lambda =1.4$ 且 $\alpha^{k}=0.001/ \sqrt{k}$、而 $\ell_{\infty}$ 为 $\lambda=51$ 且 $\alpha^{k}=0.0001/ \sqrt{k}$、在相同的值攻击下，$\ell_{1}$ 表示 $\lambda=0.07$，$\alpha^{k}=0.001/ \sqrt{k}$、 $\ell_{2}$ 是 $\lambda=1.2$ 且 $\alpha^{k}=0.001/ \sqrt{k}$、 而 $\ell_{\infty}$ 为 $\lambda=20$ 且 $\alpha^{k}=0.0001/ \sqrt{k}$、在这两种情况下，$\ell_{1}$-norm RSA和 $\ell_{2}$-norm RSA在top-1精度方面接近，并且两者都优于 $\ell_{\infty}$-RSA标准。这一观察结果与我们的收敛分析一致，即 $\ell_{\infty}$-norm RSA需要一个较大的 $\lambda$ 来确保一致性，这反过来会导致较大的错误。事实上，我们故意为 $\ell_{\infty}$-norm RSA以减少错误，但牺牲一致性。因此，关于regular workers的本地迭代的方差 $\ell_{\infty}$-norm RSA最大，而 $\ell_{2}$-norm RSA小于 $\ell_{1}$-norm RSA。</p><h2 id="异构数据"><a class="header-anchor" href="#异构数据">¶</a>异构数据</h2><p>  为了显示RSA在异构数据集上的健壮性，我们以这种方式重新分发MNIST数据：每两个工作人员都与关于同一手写数字的数据关联。在实验中，每个Byzantine workers $i$ 传输 $v_{i}^{k}=v_{r}^{k}$，其中worker $r$ 是regular workers之一。我们在实验中设置 $r=1$。结果如图11所示。当 $q=4$ 时，两个手写数字的数据在实验中不可用，因此最佳准确度约为 $0.8$。当 $q=8$ 时，最佳准确度大约为 $0.6$。$\ell_{1}$-norm RSA的参数为 $\lambda=0.5$ 和 $\alpha^{k}=0.0005/ \sqrt{k}$、注意，当 $q=4$ 时，Krum失败了，而RSA的表现优于GeoMed和Median。当 $q$ 增加到 $8$ 时，GeoMed、Krum和Median都失败了，但RSA仍然表现良好，达到了接近最佳的精度。</p><h1>结论</h1><p>  本文讨论了Byzantine attacks下的分布式学习。虽然现有的工作主要集中在i.i.d.数据的情况下，并且依赖于昂贵的梯度聚合规则，但我们开发了一种有效的SGD变体，用于在Byzantine attacks下从异构数据集进行分布式学习。由此产生的基于SGD的方法（我们称之为RSA）以 $O(1/k)$ 收敛速度收敛到接近最优解，其中最优差距取决于Byzantine workers的数量。在Byzantine-free环境下，SGD和RSA都以次线性收敛速度收敛到最优解。从数字上看，对真实数据的实验证实了RSA与最先进的替代方案相比具有竞争力。</p>]]></content>
      
      
      <categories>
          
          <category> 科研学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 联邦学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《Practical and Private （Deep） Learning without Sampling or Shuffling》</title>
      <link href="/2022/09/24/practical-and-private-deep-learning-without-sampling-or-shuffling/"/>
      <url>/2022/09/24/practical-and-private-deep-learning-without-sampling-or-shuffling/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h1>摘要</h1><p>  我们利用小批处理梯度来考虑差分隐私（DP）训练模型。现有的先进技术，差分隐私随机梯度下降（DP-SGD），需要通过采样或变换来放大隐私，以获得最佳的隐私/准确性/计算权衡。不幸的是，在重要的实际场景中，对精确采样和变换的精确要求很难实现，特别是联邦学习（FL）。我们设计并分析了遵循-the-regulizizedleader（DP-ftrl）的DP变体，它（从理论上和经验上）优于放大的DP-sgd，同时允许更灵活的数据访问模式DP-FTRL不使用任何形式的隐私放大。</p><h1>介绍</h1><p>  差分私有随机梯度下降（DP-SGD）[1,6,65]已经成为训练私有（深度）学习模型的最先进方法[1,25,27,50,55,68]。它通过在有噪声的小批梯度<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="梯度计算在训练示例的子集上，也称为小批处理。">[1]</span></a></sup>上运行随机梯度下降[59]来运行，并对噪声进行校准，以确保差别隐私。隐私分析大量使用诸如通过采样/变换放大隐私[1,6,24,30,40,69,73]等工具，以获得最佳的隐私/效用权衡。这种放大工具要求每个小批次都是训练数据的完美（一致）随机子集。这种假设会使实际部署变得异常困难，特别是在像联邦学习（FL）这样的分布式环境中，人们几乎无法控制在任何时候看到训练数据的哪个子集[5,38]。<br>  我们提出了一种新的基于在线学习[32,62]的DP算法，差分隐私跟踪-正则化leader（DP-ftrl），该算法可以与DP-sgd在隐私/效用/计算权衡这几方面进行竞争，且不依赖隐私放大。DP-FTRL在所有隐私级别上都显著优于未放大的DP-SGD。在更高的精度/更低的隐私机制下，DP-FTRL甚至优于放大的DP-SGD。我们强调，在ML应用程序的环境中，使用DP机制（即使具有较大的ε）实际上比使用非DP机制在隐私方面要好得多[35,52,64,67]。<br><b>隐私放大及其风险：</b>在高级别上，DP-SGD可以被认为是一个迭代的嘈杂状态更新过程，用于在小批量训练数据上操作的 $T$ 步。对于时间步长 $t \in [T]$，来自大小为 $n$ 的数据集 $D$ 的一个大小为 $k$ 的任意小批，设 $\sigma _{t}$ 为第 $t$ 次更新所需噪声的标准差，以满足 $\epsilon _{t}$ 差分隐私。如果在每个时间步骤 $t$ <sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="我们也可以用泊松采样[1,49,73]创建一个小批次，只是批次大小现在是一个随机变量。为了简单起见，我们将重点放在固定批处理设置上。">[2]</span></a></sup> 从数据集 $D$ 按照 $u.a.r$ 和 $i.i.d$选择小批量，然后通过采样[1,6,40,69]进行隐私放大，可以将噪声缩小到 $\sigma _{t}·(k/n)$，同时仍然保证 $\epsilon _{t}$ 差分隐私<sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="类似的论点也适用于通过洗牌放大[24,30]，当数据在每个时代的开始一致洗牌时。本文不考虑迭代[28]的隐私放大，因为它只适用于光滑凸函数。">[3]</span></a></sup>。当 $k \ll n$ 时，这种放大对于DP-SGD在实践中获得最先进的模型[1、55、68]至关重要。</p><p>  此类部署有两个主要瓶颈：i）对于大型数据集，在每一轮（或epoch）中实现统一的小批量采样/混排在计算和/或工程复杂性方面可能非常昂贵；ii）在分布式环境中，如联合学习（FL）[45]，由于每个时间步长的可用总体差异很大，均匀采样混排可能不可行。我们的工作肯定地回答了以下问题：<b>我们能否设计一种不依赖隐私放大的算法，从而允许以任意顺序访问数据，同时提供与DP-SGD竞争的隐私/效用/计算权衡？</b></p><p><b>DP-FTRL和无放大模型训练：</b>DP-FTRL可被视为追随正则化领导（FTRL）算法的差分私有变体[17,44,72]。DP-FTRL的主要思想是使用树聚合技巧[14,23]在小批梯度的总和中添加噪声，以确保隐私。关键是，它通过添加跨时间步的相关噪声（而不是独立噪声）来偏离DP-SGD。DP-FTRL的这一特性使得它可以在不依赖隐私放大的情况下获得强大的隐私/效用权衡。</p><p><b>联邦学习（FL）和DP-FTRL：</b>之前的工作[5,57]详细介绍了在联邦学习的现实应用中，在获得强大的隐私保障方面所面临的挑战，这些隐私保障包括参与客户端有限的可用性。尽管存在诸如随机签到[5]之类的技术，可获得FL设置的隐私放大，但实现此类技术可能仍然需要客户跟踪在其可用期间服务器上完成的培训轮数，以便能够统一随机化他们的参与。另一方面，由于DP-FTRL（算法1）的隐私保证不依赖于任何类型的隐私放大，它不需要任何局部/中心随机性，除了模型更新的噪声之外。<br>  附录A和第2节分别描述了额外的相关工作和背景。</p><h2 id="问题制定"><a class="header-anchor" href="#问题制定">¶</a>问题制定</h2><p>  假设我们有一个数据样本流 $D=\left[d_{1}, \ldots, d_{n}\right] \in \mathcal{D}^{n}$，其中 $\mathcal{D}$ 为数据样本的定域，损失函数 $\ell: \mathcal{C} \times \mathcal{D} \rightarrow \mathbb{R}$，其中 $\mathcal{C} \in \mathbb{R} ^{p}$ 为所有模型的空间。我们考虑以下两个问题设置。</p><p><b>Regret最小化：</b>在每个时间步 $t \in \left[n \right]$，同时观察样本 $\left[d_{1}, \ldots, d_{t−1} \right]$ 时，算法 $\mathcal{A}$ 输出一个模型 $\theta_{t} \in \mathcal{C}$，用于对样例 $d_{t}$ 进行预测。$\mathcal{A}$ 的性能是用regret来衡量的，而不是任意的事后比较器 $\theta ^{*} \in \mathcal{C}$ ：</p><p>$$<br>R_{D}\left(\mathcal{A} ; \theta^{*}\right) = \frac{1}{n} \sum_{t=1}^{n} \ell \left(\theta_{t} ; d_{t}\right)-\frac{1}{n} \sum_{t=1}^{n} \ell \left(\theta^{*} ; d_{t}\right)<br>$$</p><p>  如果 $R_{D}\left(\mathcal{A} ; \theta^{*}\right)=o(1)$，我们认为算法 $\mathcal{A}$ 是low-regret。为了保证low-regret算法，我们假设 $\|\nabla \ell(\theta ; d)\|_{2} \leq L$ 对于任何数据样本 $d$，以及任何模型 $\theta \in \mathcal{C}$。我们考虑两种对抗性遗憾，其中数据样本 $d_{t}$ 基于过去的输出 $\{ θ_{1}, \cdots , θ_{t} \}$ [32]和随机后悔[33]，其中 $D$ 中的数据样本是从某个固定分布 $\tau$ 中 $i.i.d.$ 抽取的。</p><p><b>过剩风险最小化：</b>在此背景下，我们着眼于最小化过剩人口风险的问题。假设数据集 $D$ 从分布τ $\tau$ 采样 $i.i.d.$，算法 $\mathcal{A}$ 输出 $\hat{\theta } \in \mathcal{C} $，我们要最小化：<br>$$<br>\operatorname{PopRisk}(\mathcal{A})=\mathbb{E}_{d \sim \tau} \ell(\widehat{\theta} ; d)-\min _{\theta \in \mathcal{C}} \mathbb{E}_{d \sim \tau} \ell(\theta ; d)<br>$$<br>  本文的所有算法都保证了差分隐私[21,22]和Rényi差分隐私[51]（详见第2节）。单个数据记录的定义可以是一个训练示例（又称示例级隐私），也可以是来自一个人的一组训练示例（又称用户级隐私）。除了FL设置中的经验评估外，我们关注的是示例级别的隐私<br><b>定义1.1</b>（差分隐私[21,22]）。随机化算法 $\mathcal{A}$是 $(ε, δ)$-差分私有的如果对于一个记录中不同的任何相邻数据集 $D$， $D’$，对于 $\mathcal{A}$ 的输出范围内的任何事件 $\mathcal{S}$，我们有：<br>$$<br>\operatorname{Pr}[\mathcal{A}(D) \in \mathcal{S}] \leq e^{\varepsilon} \cdot \operatorname{Pr}\left[\mathcal{A}\left(D^{\prime}\right) \in \mathcal{S}\right]+\delta<br>$$<br>其中概率除以 $\mathcal{A}$ 的随机性。<br><img src="/2022/09/24/practical-and-private-deep-learning-without-sampling-or-shuffling/1.png" alt="表1:最著名的regret guarantees。在这里，高概率意味着wp至少为 $1−β$ 除以算法的随机性。预期的regret是对数据集的随机选择和算法的随机性的预期。"></p><h2 id="我们的贡献"><a class="header-anchor" href="#我们的贡献">¶</a>我们的贡献</h2><p>  我们在本文中的主要贡献是一个私有在线学习算法:差分私有跟随正则化leader（DP-FTRL）（算法1）。我们提供了基于DP-FTRL的更紧密的隐私/效用权衡（见表1摘要），并展示了如何轻松地适应训练（联邦）深度学习模型，与DP-SGD相比，隐私/效用/计算权衡有时甚至更好。我们将这些贡献总结如下。</p><p><b>DP-FTRL算法：</b>我们提供了DP-FTRL，它是follow-the-regulizedleader（FTRL）算法[32,44,47,62]的差分私有变体，用于在线凸优化（OCO）。我们还提供了一种名为动量DP-FTRL的变体，在实践中具有优越的性能。[3]提供了一个特定于线性损耗的DP-FTRL实例。[63]提供了一种类似于DP-FTRL的算法，其中使用了正则化损失的二次近似，而不是仅仅线性化损失。</p><p><b>Regret guarantees：</b>在对抗性OCO设置（章节4.1）中，与之前的工作[3,37,63]相比，DP-FTRL具有以下主要优势。首先，它将[63]中最著名的regret guarantee提高了 $\sqrt{\varepsilon } $（来自 $\widetilde{O}\left(\sqrt{\frac{\sqrt{p}}{\varepsilon^{2} n}}\right) \text { to } \widetilde{O}\left(\sqrt{\frac{\sqrt{p}}{\varepsilon n}}\right)$，当 $ε≤1$ 时）。这种改进是显著的，因为它区分了中央私有OCO和局部私有[26,40,70]OCO<sup id="fnref:4"><a href="#fn:4" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="尽管在文献中没有正式说明，但一个简单的论证表明，本地私有SGD[19]可以实现与[63]中相同的遗憾。">[4]</span></a></sup>。其次，与[63]不同，DP-FTRL（及其分析）扩展到无约束设置 $\mathcal{C}=\mathbb{R}^{p}$。此外，在复合损失[18,44,46,72]的情况下，即损失函数的形式为 $\ell\left(\theta ; d_{t}\right)+r_{t}(\theta)$，$r: \mathcal{C} \rightarrow \mathbb{R}^{+} \text {( e.g.,} \|\cdot\|_{1} )$ 是凸正则化器，DP-FTRL对损失 $\ell (\theta ; d_{t})$ 的形式：（ regret没有 $r_{t}$ 的界限）$+\frac{1}{n} \sum_{t=1}^{n} r_{t}\left(\theta^{*}\right)$。</p><p>  在随机OCO设置（第4.2节）中，我们证明对于最小二乘损失（其中 $\ell\left(\theta ; d_{t}\right)=\left(y_{t}-\left\langle\mathbf{x}_{t}, \theta\right\rangle\right)^{2}$ ，并且 $d_{t}=\left(\mathbf{x}_{t}, y_{t}\right)$）和线性损失（当 $\ell\left(\theta ; d_{t}\right)=\left\langle d_{t}, \theta\right\rangle$）， DP-FTRL的一个变体以算法随机性的 $1-β$ 概率实现 $O\left(\left(\frac{1}{\sqrt{n}}+\frac{\sqrt{p}}{\varepsilon n}\right) \cdot \text { polylog}\left(\frac{1}{\delta}, n, \frac{1}{\beta}\right)\right)$ 形式的 regret。我们的保证是严格的高概率保证，也就是说， regret只取决于 $\text { polylog}(1/β)$。</p><p><b>人口风险保证：</b>在第4.3节中，使用标准的在线到批转换[13,61]，我们获得了DP-FTRL的人口风险保证。对于一般的Lipschitz凸损失，定理C.5中的DP-FTRL的总体风险与[6，附录F]中的相同（直到对数因子），但DP-FTRL的优点是它是一个单遍算法（在数据集 $D$ 上），而不是需要对数据进行 $n$ 遍。因此，我们为单个通过算法提供了最知名的总体风险保证。虽然[7,9,29]的结果具有更紧密（且最优）的过剩人口风险 $\widetilde{\Theta}(1 / \sqrt{n}+\sqrt{p} /(\varepsilon n))$，但它们要么需要凸函数的光滑性，对于单遍算法来说，要么需要对数据进行n次遍历。对于线性损失和最小二乘损失等有限类别，DP-FTRL可以通过更严格的随机遗憾保证来实现最优总体风险。DP-FTRL是否能在一般凸集下获得最优的超额种群风险是一个有待解决的问题。</p><p><b>经验贡献：</b>在第5节中，我们研究了DP-FTRL和DP-SGD在隐私/效用/计算之间的一些权衡。我们在四个基准数据集上进行实验：MNIST、CIFAR-10、EMNIST和StackOverflow。我们首先修正可用的计算技术，并观察隐私/效用的权衡。我们发现在中等到较大的 $ε$ 条件下，DP-FTRL比DP-SGD获得了更好的效用。在无法确保放大的情况下（例如，由于实际/实施限制），DP-FTRL比未放大的DP-SGD提供了更好的性能。此外，我们还证明，在不需要任何放大的情况下，在适当增加计算成本的情况下，DP-FTRL可以达到放大的DP-SGD的性能。接下来，当需要一个实用目标时，我们关注这两种技术的隐私/计算权衡。我们表明，与DP-SGD相比，DP-FTRL可以在各种精度目标上提供更好的权衡，当数据集的大小变得有限时，这可以显著节省隐私/计算成本。</p><p>  为了阐明DP-FTRL（通过比较）与DP-SGD的经验有效性，在第3.2节中，我们展示了DP-SGD的变体（带有相关噪声）可以被视为无约束环境（$\mathcal{C} = \mathbb{R} ^{p}$）下DP-FTRL的等价公式。在传统DP-SGD[6]的情况下，当 $t=\omega(n)$ 时，每步 $t \in [n]$ 增加的噪声尺度与DP-FTRL渐近相同。</p><h1>背景</h1><p><b>差分隐私：</b>在整个论文中，我们使用了近似差分隐私[21,22]和Rényi差分隐私（RDP）[1,51]的概念。对于有意义的隐私保障，$ε$ 被假设为一个小常数，$δ \ll 1 / |D|$ 。</p><p><b>定义2.1</b>（RDP[1,51]）。随机算法 $\mathcal{A}$ 是 $(α,ε)$-RDP如果对于任意一对相邻的数据集 $D$，$D’$在一个记录中不同，我们有：</p><p>$$<br>\frac{1}{\alpha-1} \log \underset{o \sim \mathcal{A}(D)}{\mathbf{E}}\left(\frac{\operatorname{Pr}(\mathcal{A}(D)=o)}{\operatorname{Pr}\left(\mathcal{A}\left(D^{\prime}\right)=o\right)}\right)^{\alpha} \leq \varepsilon<br>$$</p><p>  Abadi等人[1]和Mironov[51]已经表明 $(α,ε)$-RDP算法保证 $\left(\varepsilon+\frac{\log (1 / \delta)}{\alpha-1}, \delta\right)$-微分隐私。后续的工作[4,12]提供了更紧密的转换。我们在实验中使用了[12]中的转换<br>  为了回答 $\ell _{2}$ 灵敏度 $L$ 的查询 $f(D)$，即 $\max _{\text {neighboring } D, D^{\prime}} | f(D)-f(D^{\prime}) |_{2} \leq L$，高斯机制[22]返回 $f(D)+\mathcal{N}\left(0, L^{2} \sigma^{2}\right)$，从而保证 （$\sqrt{1.25 \log (2 / \delta)} / \sigma, \delta$）-差分隐私[20,22]和 $(α,α/2σ^{2})$-RDP[51]。</p><p><b>DP-SGD与隐私放大：</b>差分私有随机梯度下降（DP-SGD）是解决私有优化问题的常用算法。其基本思想是强制单个梯度的有界 $\ell_{2}$ 范数，并在SGD更新中使用的梯度中添加高斯噪声。具体来说，考虑一个数据集 $D = \{ d_{1}, \cdots , d_{n} \}$ 和一个形式为 $\sum_{i=1}^{n} \ell\left(\theta ; d_{i}\right)$ 的损失函数 $\ell$。DP-SGD使用更新规则<br>$$<br>\theta_{t+1} \leftarrow \theta_{t}-\frac{\eta}{|\mathcal{B}|}\left(\sum_{i \in \mathcal{B}} \operatorname{clip}\left(\nabla_{\theta} \ell\left(\theta_{t} ; d_{i}\right), L\right)+\mathcal{N}\left(0, L^{2} \sigma^{2}\right)\right)<br>$$<br>其中 $\operatorname{clip}（v, L）$ 将 $v$ 投影到半径为 $L$ 的 $\ell_{2}$-ball上，$\mathcal{B} \in [n]$ 表示一个小批量数据。</p><p>  通过对高斯机理的分析，我们知道这种更新步骤相对于小批 $\mathcal{B}$ 保证了 $(α,α/2σ^{2})$-RDP。通过并行合成，运行一个不相交的小批次保证了 $(α,α/2σ^{2})$-RDP。另一方面，以前的工作[1,6,69]已经表明，如果从 $[n]$ 中均匀随机选择 $\mathcal{B}$，或者如果我们使用泊松抽样来收集一批样本 $\mathcal{B}$，那么一步就可以保证 $\left(\alpha, O\left(\alpha / 2 \sigma^{2} \cdot(|\mathcal{B}| / n)^{2}\right)\right)$-RDP</p><p><b>基于树的聚合：</b>考虑私有地释放数据流的前缀和的问题，即给定一个流 $D = (d_{1}, d_{2}, \cdots , d_{T})$ 使得每个 $d_{i} \in \mathbb{R}^{p}$ 有 $\ell_{2}$ 范数以 $L$ 为界，我们的目标是对所有 $t \in [1,t]$ 在差分隐私下释放 $s_{t}=\sum_{i=1}^{t} d_{i}$。Chan等[14]，Dwork等[23]提出了一种基于树的聚合算法来解决这一问题。考虑一个完整的二叉树 $\mathcal{T}$，叶节点从 $d_{1}$ 到 $d_{T}$，内部节点是它子树中所有叶节点的和。为了释放确切的前缀和 $s_{t}$，我们只需要对 $O(\log(t))$ 个节点求和。为了保证释放树 $\mathcal{T}$ 时的差分隐私性，由于 $\mathcal{T}$ 中的 $\log(T)$ 节点中出现任何 $d_{i}$，利用复合，我们可以添加 $L \sqrt{\log (T) \log (1 / \delta)} / \varepsilon$ 标准差的高斯噪声来保证 $(ε,δ)$-微分隐私性。</p><p>  Smith和Thakurta[63]利用这种聚合算法为私人在线学习构建了一种近乎最优的算法。Smith和Thakurta[63]结果的一个重要方面是，它表明即使对于自适应选择的序列 $\{d_{t}\}_{t}^{T}=1$，隐私保证也成立，这对于模型训练任务是至关重要的。</p><h1>隐私Follow-The-Regularized-Leader</h1><p>  在本节中，我们将提供DP-FTRL算法（算法1）的形式化描述及其隐私分析然后，我们证明了差分私有随机梯度下降（DP-SGD）的变体[6,65]可以被视为在适当学习率选择下DP-FTRL的实例化。</p><p>  关键的是，我们对DP-FTRL的隐私保证在数据 $D$ 以任意（甚至是恶意选择的）顺序处理时仍然有效，并且不依赖于损失函数的凸性。效用担保，即后悔担保和超额风险担保要求凸损失（即 $\ell( \cdot ; \cdot)$ 第一个参数是凸的）。在下面的演示中，为了简洁起见，我们假设损失是可微的。通过标准使用次微分，这些论证扩展到不可微凸损失[32,62]。</p><h2 id="算法描述"><a class="header-anchor" href="#算法描述">¶</a>算法描述</h2><p>  DP-FTRL的主要思想基于三个观察结果：i）对于在线凸优化，对给定损失函数 $\ell(θ;d_{t})$（即时间步 $t$ 处的损失），它足以使算法在 $θ_{t}$ 处的损失（时间步 $t$ 处的模型输出）的线性化上操作：$ \widetilde{\ell}\left(\theta ; d_{t}\right)=\left\langle\nabla_{\theta} \ell\left(\theta_{t} ; d_{t}\right), \theta-\theta_{t}\right\rangle$, ii） $λ$ 的适当选择下，优化为 $\theta_{t+1}=\arg \min _{\theta \in \mathcal{C}} \sum_{i=1}^{t} \tilde{\ell}\left(\theta ; d_{t}\right)+\frac{\lambda}{2}|\theta|_{2}^{2}$ 除以 $θ \in \mathcal{C}$ 给出了 $t+1$ 步的一个很好的模型，并且iii）对于所有 $t \in [n]$，我们可以私下跟踪 $\sum_{i=1}^{t} \tilde{\ell}\left(\theta ; d_{t}\right)$ 使用现在标准的树聚合协议[14,23]。虽然在[63]中以follow-the-approximate-leader的名义使用了这种想法的变体，但一个关键的区别是，他们使用了正则化损失的二次近似，即 $\ell\left(\theta ; d_{t}\right)+\frac{\lambda}{t}|\theta|_{2}^{2}$。这一公式导致了更复杂的算法、次优regret分析和无法维持复合损耗引入的结构特性（如稀疏性）[18,44,46,72]。<br><img src="/2022/09/24/practical-and-private-deep-learning-without-sampling-or-shuffling/2.png" alt="算法1 $\mathcal{A}_{FTRL}$：差分隐私Follow-The-Regularized-Leader （DP-FTRL）"><br>  在本文的后面部分，我们提供了两种DP-FTRL变体（动量DP-FTRL和最小二乘损失的DP-FTRL），它们在某些问题设置中具有更好的隐私/效用权衡。</p><p>  DP-FTRL的形式化描述在算法1中。有三个函数，InitializeTree, AddToTree，GetSum，它对应于树聚合算法。在高层次上，InitializeTree初始化树数据结构 $\mathcal{T}$, AddToTree允许在 $\mathcal{T}$ 中添加新的渐变 $\nabla_{t}$, GetSum私有地返回前缀和 $\sum_{i=1}^{t} \nabla_{t}$。关于形式化算法的描述，请参考附录B.1。</p><p>  可以看出，DP-FTRL中由于隐私性引入的误差主要是在每个 $t \in [n]$ 处估计 $\sum_{i=1}^{t} \nabla_{t}$ 时的误差。由[63]可知，对于（自适应选择的）向量序列 $\{ \nabla_{t} \} _{t=1}^{n}$，如果对每个 $t \in [n]$ 执行AddToTree $(\mathcal{T},t,\nabla_{t})$，则可以写 $\operatorname{GetSum}(\mathcal{T}, t)=\sum_{i=1}^{t} \nabla_{i}+\boldsymbol{b}_{t}$，其中 $\boldsymbol{b}_{t}$ 为正态分布，均值为零，并且 $\forall t \in[n],\|\boldsymbol{b}_{t} \|_{2} \leq L \sigma \sqrt{p\lceil\lg (n)\rceil \ln (n / \beta)}$ w.p.至少为 $1−\theta$。</p><p><b>动量变量：</b>我们发现使用一个附加的动量项 $γ \in [0,1]$，将算法1中的第7行替换为<br>$$<br>\boldsymbol{v}_{t} \leftarrow \gamma \cdot \boldsymbol{v}_{t-1}+\boldsymbol{s}_{t}, \theta_{t+1} \leftarrow \arg \min _{\theta \in \mathcal{C}}\left\langle\boldsymbol{v}_{t}, \theta\right\rangle+\frac{\lambda}{2}\|\theta\|_{2}^{2}<br>$$</p><p>相比原始算法，可以获得更好的经验隐私/效用权衡。在本文中，我们将这种变体称为动量DP-FTRL，或DP-FTRLM。虽然我们没有为这个变量提供正式的后悔保证，但我们猜想，优越的经验性能是由于以下原因。树聚合算法添加的噪声总是以 $O(\sqrt{p \ln (1 / \delta)} \cdot \ln (n) / \varepsilon)$ 为界。然而，时间步 $t$ 和 $t+1$ 的噪声可能相差 $O(\sqrt{\ln(n)})$。这在输出模型之间与DP-SGD相比产生了突然的跳跃。动量可以平滑这些跳跃。</p><p><b>隐私分析：</b>在定理3.1中，我们提供了算法1及其动量变量的隐私保证（证明见附录B.2）。在附录D中，我们将其扩展到数据集 $D$ 上的多次传递，批处理大小为 $&gt; 1$。</p><p><b>定理3.1（隐私保证）</b>如果 $| \nabla_{\theta} \ell (\theta ; d) | _{2} \leq L$ 对于所有 $d \in \mathcal{D}$ 和 $\theta \in \mathcal{C}$，那么算法1（及其动量变量）保证 $\left(\alpha, \frac{\alpha[\lg (n)]}{2 \sigma^{2}}\right)$-Rényi差分隐私，其中 $n$ 为 $D$ 中的样本数。设 $\sigma \frac{\sqrt{2\lceil\lg (n)\rceil \ln (1 / \delta)}}{e}$，可以保证 $(ε, δ)$-差分隐私，当 $ε \leq 2 \ln(1/δ)$ 时。</p><h2 id="比较DP-SGD和DP-FTRL的噪声"><a class="header-anchor" href="#比较DP-SGD和DP-FTRL的噪声">¶</a>比较DP-SGD和DP-FTRL的噪声</h2><p>  在本节中，我们使用非私有SGD和FTRL[46]的等价性来建立noisy-SGD和DP-FTRL的变体之间的等价性，从而使DP-SGD和DP-FTRL具有可比性。<br>  设 $D = { d_{1}, \cdots, d_{n} }$ 为大小为 $n$ 的数据集。考虑更新规则为 $\theta_{t+1} \leftarrow \theta_{t}-\eta \cdot\left(\nabla_{\theta} \ell\left(\theta_{t} ; d_{t}\right)+\boldsymbol{a}_{t}\right)$，其中 $\eta$ 是学习率，$\boldsymbol{a_{t}}$ 是一些随机噪声。DP-SGD可以视为一种特殊情况，其中 $d_{t}$ 从 $D$中均匀随机采样，$\boldsymbol{a}_{t}$ 从 $\mathcal{N}\left(0, \widetilde{O}\left(\frac{L^{2}}{n \varepsilon^{2}}\right)\right)$ 中i.i.d.抽取。如果我们展开递归关系，我们可以看到加到 $\theta_{t+1}$ 估计中的噪声总量是 $\eta \sum_{i=1}^{t} \boldsymbol{a}_{t}=\mathcal{N}\left(0, \widetilde{O}\left(\frac{\eta^{2} L^{2} t}{n \varepsilon^{2}}\right)\right)$。设 $\boldsymbol{b}_{t}$ 为AFTRL算法的树聚合算法在时间步 $t$ 处添加的噪声。我们可以证明DP-FTRL可以写成与上述通用noisy-sgd公式相同的形式，其中i）噪声 $\boldsymbol{a}_{t}=\boldsymbol{b}_{t} -\boldsymbol{b}_{t-1}$ 处，ii）数据样本 $d_{t}$ 从 $D$ 中按顺序抽取，iii）学习率 $\eta$ 设为 $1/λ$，其中 $λ$ 是算法 $\mathcal{A}_{FTRL}$ 中的正则化参数。在这种noisy-SGD变体中，添加到模型中的总噪声为 $\boldsymbol{b}_{t}=\mathcal{N}\left(0, \widetilde{O}\left(\frac{\eta^{2} \cdot L^{2}}{\varepsilon^{2}}\right)\right)$。</p><p>  在相同形式的更新规则下，我们可以粗略地（因为在DP-FTRL情况下噪声不是独立的）比较两种算法。当 $t = Ω(n)$ 时，放大后的DP-SGD的噪声与DP-FTRL的噪声匹配达到了 $\text{polylog} (n)$ 的倍数。因此，我们预期（并得到了总体风险保证和实验的证实）采样的DP-SGD和DP-FTRL表现相似。（在附录B.3中，我们提供了一个正式的等价。）</p><h1>Regret and Population Risk Guarantees</h1><p>  在本节中，我们考虑损失函数 $\ell$ 的第一个参数为凸时的设置，并提供DP-FTRL：i）一般凸损失的对抗性后悔保证，ii）最小二乘和线性损失的更紧密随机后悔保证，iii）通过在线到批转换的总体风险保证。我们的所有保证都是高概率比算法的随机性，即w.p.至少为 $1−β$，误差仅依赖于多对数 $(1/β)$。</p><h2 id="对抗性Regret（综合）损失"><a class="header-anchor" href="#对抗性Regret（综合）损失">¶</a>对抗性Regret（综合）损失</h2><p>  这里的定理给出了算法1对选择损失函数 $\ell (\theta; d_{t})$ 基于 $[\theta_{1}, \cdots, \theta_{t}]$，但不知道算法的内部随机性。参见附录C.1，了解定理4.1的更一般版本及其证明</p><p><b>定理4.1（Regret guarantee）</b>设 $\theta$ 是 $\mathcal{C}$ 中的任意模型，$[\theta_{1}, \cdots, \theta_{n}]$ 为算法$\mathcal{A}_{FTRL}$ （算法1）的输出，设 $L$ 为损失函数的 $\ell _{2}$-Lipschitz常数的一个界。从定理3.1中优化 $λ$ 并插入噪声尺度 $\sigma$ 以保证 $(ε,δ)$-微分隐私，我们得到对于任意 $\theta^{*} \in \mathcal{C}$, w.p在 $\mathcal{A}_{FTRL}$ 的随机性上至少为 $1−β$，即regret</p><p>$$<br>R_{D}\left(\mathcal{A}_{\text {FTRL }} ; \theta^{*}\right)=O\left(L \| \theta^{*} \|_{2} \cdot \left(\frac{1}{\sqrt{n}}+ \sqrt{\frac{p^{1 / 2} \ln ^{2}(1 / \delta) \ln (1 / \beta)}{\varepsilon n}} \right)\right)<br>$$</p><p><b>综合损失的扩展：</b>复合损失[18,44,46]指的是在每一轮中，算法提供函数 $f_{t}(\theta) = \ell (\theta; d_{t}) + r_{t}(\theta)$ 而 $r_{t}: \mathcal{C} \leftarrow \mathbb{R}^{+}$ 是一个不依赖于数据样本 $d_{t}$ 的凸正则化。$\ell_{1}$-正则化子，$r_{t}(θ) = | θ |_{1}$，可能是最重要的实际例子，在高维统计（例如，在LASSO方法中）[10]中发挥关键作用，以及在像点击率（CTR）预测等应用中，非常稀疏的模型需要效率[48]。为了对复合损耗进行运算，我们简单地将算法 $\mathcal{A}_{FTRL}$ 的第7行替换为</p><p>$$<br>\theta_{t+1} \leftarrow \arg \min _{\theta \in \mathcal{C}}\left\langle\boldsymbol{s}_{t}, \theta\right\rangle+\sum_{i=1}^{t} r_{i}(\theta)+\frac{\lambda}{2}\|\theta\|_{2}^{2},<br>$$</p><p>该公式可以在许多重要的情况下以封闭的形式求解，例如 $\ell _{1}$ 正则化。我们得到推论4.2，类似于非私有情况下的[46，定理1]。由于我们只对算法 $\mathcal{A}_{FTRL}$ 中的损失进行线性化处理，因此除了凸性外，我们不需要对正则化器进行任何假设（例如Lipschitzness）。值得一提的是，[63]与这类担保从根本上是不相容的。</p><p><b>推论4.2</b>设 $θ$ 是 $\mathcal{C}$ 中的任意模型，$[θ_{1}, \cdots , θ_{n}]$ 为算法 $\mathcal{A}_{FTRL}$ （算法1）的输出，$L$ 为损失函数的 $\ell _{2}$-Lipschitz常数的一个界。对于任意 $θ^{*} \in \mathcal{C}$，算法的随机性至少为 $1−β$，假设 $0 \in \mathcal{C}$，我们有：</p><p>$$<br>R_{D}\left(\mathcal{A}_{\text {FTRL }} ; \theta^{*}\right) \leq \frac{L \sigma \sqrt{p\lceil\lg n\rceil \ln (n / \beta)}+L^{2}}{\lambda}+\frac{\lambda}{2 n} \|\theta^{*} \|_{2}^{2}+\frac{1}{n} \sum_{t=1}^{n} r_{t}\left(\theta^{*}\right).<br>$$</p><h2 id="最小平方损失的随机Regret"><a class="header-anchor" href="#最小平方损失的随机Regret">¶</a>最小平方损失的随机Regret</h2><p>  在这种情况下，在数据集 $d_{i} = {d_{1}, \cdots, d_{n}}$ 中，对于每个数据样本 $d_{i}=\left(\mathbf{x}_{i}, y_{i}\right)$（其中 $\mathbf{x}_{i} \in \mathbb{R}^{p}$ 并且 $y_{i} \in \mathbb{R}$）对应的损失采用最小二乘形式<sup id="fnref:5"><a href="#fn:5" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="定理4.3中类似的论证可以用在损失函数是线性的情况下，$\ell (θ;d) = \left\langle θ,d \right\rangle$, $d \in \mathbb{R}\^{p}$, 并且 $\\| d \\|_{2} ≤ L$。">[5]</span></a></sup>：$\ell\left(\theta ; d_{i}\right)=\left(y_{i}-\left\langle\mathbf{x}_{i}, \theta\right\rangle\right)^{2}$。我们还假设每个数据样本di是从某个固定分布τ的i.i.d.绘制的</p><p>  直接修改DP-FTRL, AFTRL-LS（附录C.2中的算法2），可以实现以下保证。</p><p><b>定理4.3（最小二乘损失的随机regret）</b>设 $D = { (\mathbf{x}_{1}, y_{1}), \cdots，(\mathbf{x}_{n}, y_{n}) } \in \mathcal{D}_{n} $ 是从 $\tau$ 中按照 i.i.d.抽取得到的数据集，令 $L=\max _{\mathbf{x} \in \mathcal{D}} |\mathbf{x}|_{2}$，令 $\max _{y \sim \mathcal{D} } |y|≤1$ 的数据集。令 $θ^{∗} \in \mathcal{C}$ ，$\mu = \max_{\theta \in \mathcal{C} } | \theta  | _{2}$，$\rho=\max {\mu, \mu^{2} }$。在输出 $[θ_{1}, \cdots, θ_{n}]$ 时，$\mathcal{A}_{FTRL-LS}$ 提供 $(ε, δ)$-差分隐私。s.t. w.p.对于任意 $θ^{*} \in \mathcal{C}$, $\mathbb{E}_{D}\left[R_{D}\left(\mathcal{A}_{\text {FTRL-LS }} ; \theta^{*}\right)\right]=O\left(L^{2} \rho^{2}\left(\sqrt{\frac{\ln (n)}{n}}+\frac{\sqrt{p \ln ^{5}(n / \beta) \cdot \ln (1 / \delta)}}{\varepsilon n}\right)\right)$ 输出 $[θ_{1}, \cdots, θ_{n}]$ 至少为 $1-\beta $</p><p>  论文[3]的参数可以扩展为仅在期望中显示类似的regret保证，而我们的是一个高概率保证。</p><h2 id="在线到批量转换带来的过度风险"><a class="header-anchor" href="#在线到批量转换带来的过度风险">¶</a>在线到批量转换带来的过度风险</h2><p>  利用在线到批的转换[13,61]，由定理4.1，我们可以得到一个总体风险保证 $O\left(\left(\sqrt{\frac{\ln (1 / \beta)}{n}}+\sqrt{\frac{p^{1 / 2} \ln ^{2}(1 / \delta) \ln (1 / \beta)}{\varepsilon n}}\right)\right)$，$β$ 为失效概率。（正式声明见附录C.3）对于最小二乘和线性损失，使用定理4.3中的regret保证和在线到批转换，实际上可以实现最优的总体风险（直到对数因子）$O\left(\sqrt{\frac{\ln (n) \ln (1 / \beta)}{n}}+\frac{\sqrt{p \ln ^{5}(n / \beta) \cdot \ln (1 / \delta)}}{\varepsilon n}\right)$。</p><h1>实证评估</h1><p>  我们在四个基准数据集上提供了DP- ftrl（算法1）的经验评估，并在三个不同的轴上比较了它与最先进的DP-sgd的性能：（1）<b>隐私</b>，测量为机制上的 $（ε,δ）$-DP保证，（2）<b>效用</b>，测量为DP保证下最终训练模型的（预期）测试集精度，以及（3）<b>计算成本</b>，我们用小批量大小和训练迭代次数来衡量。</p><p>  首先，我们在固定的计算成本下评估每种技术提供的隐私/效用权衡。其次，我们评估每种技术在固定效用目标下所能提供的隐私/计算权衡。这方面的一个自然应用是像FL这样的分布式框架，其中隐私预算和期望的效用阈值可以固定，目标是用最少的计算满足这两个约束。计算成本在FL中至关重要，因为随着小批规模和/或培训轮数的增加，寻找可用客户可能变得具有挑战性</p><p>  我们给出了以下结果：（1） DP-FTRL比未放大的DPSGD提供更好的隐私/效用权衡，（2）在计算成本适度增加的情况下，DP-FTRL（不使用任何隐私放大）可以在所有隐私制度下匹配放大的DP-SGD的隐私/效用权衡，进一步（3）对于隐私预算较大的制度，即使在相同的计算成本下，DP-FTRL比放大的DP-SGD获得更高的精度，（4）对于现实数据集大小，与DP-SGD相比，DP-FTRL可以提供更好的隐私/计算权衡。</p><h2 id="实验装置"><a class="header-anchor" href="#实验装置">¶</a>实验装置</h2><p><b>数据集：</b>我们对三个图像分类任务进行了评估，MNIST [43]， CIFAR-10 [42]， EMNIST （ByMerge split） [16];以及StackOverflow数据集[53]上的下一个单词预测任务。由于StackOverflow自然是由用户键控的，我们假设在联邦学习设置中进行训练，即使用联邦平均优化器对StackOverflow中的用户进行训练。因此，与其他三个数据集的示例级隐私相比，隐私保证是用户级的（参见定义1.1）。</p><p>  对于所有使用DP的实验，我们在MNIST和CIFAR-10上设置隐私参数 $δ$ 为 $10^{−5}$，在EMNIST和StackOverflow上设置隐私参数 $δ$ 为 $10^{−6}$,s.t $δ &lt; n^{−1}$，其中 $n$ 为StackOverflow中的用户数量（或其他数据集中的示例数量）。</p><p><b>模型架构：</b>对于所有的图像分类任务，我们使用小型卷积神经网络，就像之前的工作[55]一样。对于StackOverflow，我们使用[58]中描述的单层LSTM网络。详情见附录E.1。</p><p><b>优化器：</b>我们考虑DP-FTRL与小批量模型更新和多阶段。我们在附录d中提供了两个扩展的隐私分析。我们也考虑了它的动量变体DP-FTRLM。我们发现动量为0.9的DPFTRLM总是优于DP-FTRL。类似地，对于DP-SGD[31]，我们考虑它的动量变量（DP-SGDM），并报告每个任务中性能最好的变量。参见附录E.2，了解两种技术的两种优化器的比较。</p><p><img src="/2022/09/24/practical-and-private-deep-learning-without-sampling-or-shuffling/3.png" alt="图3：DP-SGD（私有基线）、无放大DP-SGD（标签“DP-SGD（无放大器）”）和MNIST上的DP-FTRLM（小批大小250）、CIFAR-10（小批大小500）和EMNIST（小批大小500）的隐私/准确性权衡。标签中的“4x”表示计算成本的4倍（通过将批大小增加4倍）。“DP-SGD 4x”的结果推迟到附录F。"></p><h2 id="用固定计算权衡隐私-效用"><a class="header-anchor" href="#用固定计算权衡隐私-效用">¶</a>用固定计算权衡隐私/效用</h2><p>  在图3中，我们展示了在固定计算成本下的准确性/隐私权衡（通过改变噪声乘数）。由于DP-FTRL和DP-SGD都需要从每个样本中裁剪梯度，并在每次迭代中向聚合更新添加噪声，我们考虑迭代次数和小批大小作为计算成本的代理。对于每个实验，我们进行了5个独立的试验，并绘制了不同隐私水平下最终测试准确性的平均值和标准差。我们在附录F.1中提供了所有技术的超参数调优细节</p><p>  DP-SGD是用于私有深度学习的最先进技术，通过子采样（或变换）进行放大是其隐私分析的关键组成部分。因此，我们以一个固定的计算成本作为我们的基线（如红线所示），以放大的DP-SGD（或性能更好时的动量变化）。对于MNIST，我们将（小批处理中的样本，训练迭代）固定为（250,4800），CIFAR-10固定为（500,10000），EMNIST固定为（500,69750）。我们的目标是在以任意顺序处理数据（也就是说，不依赖任何放大）的情况下实现平等或更好的权衡。</p><p>  没有任何隐私放大的DP-SGD（标记为“DP-SGD （no-amp）”）无法实现这一目标:对于所有数据集，我们发现，在图3中 $ε$ 最高的DP-SGD （no-amp）的精度比在 $ε$ 最低的DP-SGD基线的精度更差。此外，如果我们将计算量增加四倍（将迷你批处理大小增加四倍），“DP-SGD（无放大器）4x”的隐私/效用权衡仍然比私有基线严重得多对于DP-FTRLM（蓝色），在与我们的DP-SGD基线相同的计算成本下，随着隐私参数 $ε$ 的增加，DP-FTRLM的相对性能在每个数据集都有所提高，甚至在ε值较大的情况下优于基线。此外，如果我们将DP-FTRLM的批大小增加4倍，其隐私-效用权衡几乎总是匹配或优于放大的DP-SGD基线，肯定地回答了本文的主要问题。特别是对于CIFAR-10（图3b），“DP-FTRLM 4x”即使对于最低的 $ε$ 也比DP-SGD基线提供了更好的性能<sup id="fnref:6"><a href="#fn:6" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="为了完整起见，我们在附录F.2中提供了DP-SGD（无放大器）、DP-SGD（无放大器）4x和DP-SGD 4x的完整性能图。">[6]</span></a></sup>。</p><p>  在图4a中，我们观察到使用用户级DP的StackOverflow的类似结果。我们将计算成本固定为100每轮客户（也称为报告目标），1600轮培训。DP-SGDM（或者更准确地说case，带有服务器动量的DP-FedAvg）也是我们的基线（红色）。DP-SGDM无隐私放大（DP-SGDM no-amp），隐私/准确性的权衡永远不会与DP-SGD基线相匹配，并且在较低的ε时显著恶化。由于报告目标增加了4倍，DP-SGDM无放大器几乎匹配DP-SGD基线的隐私/效用权衡，在更大的 $ε$ 情况下优于它。</p><p>  对于DP-FTRLM，在与DP-SGD基线相同的计算成本下，它优于较大ε的基线，而对于4倍增长的报告目标，它提供了严格意义上更好的隐私/效用权衡。我们得出的结论是，DP-FTRL比未放大的DP-SGD提供了更好的隐私/效用权衡，在计算成本略有增加的情况下，它可以与DP-SGD的性能相当，而不需要隐私放大。</p><p><img src="/2022/09/24/practical-and-private-deep-learning-without-sampling-or-shuffling/4.png" alt="图4:（a）通过改变噪声倍增器和批处理大小，在不同隐私epsilon下StackOverflow的准确性（b）测试DP-SGDM和DP-FTRLM与StackOverflow的各种噪声乘数的准确性。（c）对于StackOverflow数据集上的两个固定精度目标（见图例），用户级隐私 $ε$（当 $δ≈1/population$）和计算成本（报告目标）之间的关系。"></p><h2 id="隐私-计算与固定效用的权衡"><a class="header-anchor" href="#隐私-计算与固定效用的权衡">¶</a>隐私/计算与固定效用的权衡</h2><p>  对于足够大的数据集/人口，更好的隐私和准确性之间的权衡基本上总是可以以增加计算为代价实现的。因此，在本节中，我们通过固定效用（精度）目标来分割隐私/效用/计算空间，并评估需要多少计算量（报告目标）才能为StackOverflow实现不同的ε。我们的非私有基线达到了25.15%的精度，我们将24.5%（2.6%的相对损失）和23%（8.6%的相对损失）作为我们的精度目标。请注意，从图4a所示的准确性-隐私权衡来看，DP-SGD或DP-FTRL即使达到23%，也会导致对报告目标考虑的非常大的隐私 $ε$。</p><p>  对于每个目标，我们以固定的计算成本调优DP-SGDM和DP-FTRLM的超参数（详见附录G.2），以获得每种技术的最大噪声尺度，同时确保训练的模型满足精度目标。具体来说，我们固定了1600轮培训中每轮100个客户端的报告目标，并为15个噪声倍增器调整DP-SGD和DP-FTRL，范围从DP-SGD的（0,0.3）和DP-FTRL的（0,1.13）。在此报告目标下，对于噪声倍增器0.3,DP-SGD提供18.89%的 $ε \sim 19$ 精度，而对于噪声倍增器1.13,DP-FTRL提供19.74%的 $ε \sim 19$ 精度。我们在图4b中提供了结果</p><p>  现在，对于目标精度23%和24.5%，我们为每个技术选择最大的噪声倍率，使训练过的模型达到精度目标。为了获得精确度（23%，24.5%），我们分别为DP-SGDM和DP-FTRLM选择了噪声乘数（0.015,0.007）和（0.268,0.067）。这些数据允许我们评估这两种技术的隐私/计算权衡，假设当我们一起扩大噪声和报告目标时，精确度保持不变（在提高 $ε$ 的同时保持恒定的信噪比）。[49]引入并验证了这一假设，结果表明，保持裁剪范数界、训练轮数和添加到模型更新常数中的噪声规模，增加报告目标并不会改变最终的模型精度。在附录G.1中，我们分别在StackOverflow上证实了DP-SGD和DP-FTRL的这种效果。</p><p>  我们将结果绘制在图4c中。对于这两个精度目标，DP-FTRLM以比DP-SGDM更低的计算代价实现了任意隐私 $ε \in (0,50)$。在附录G.3中，我们为假设的更大的种群提供了类似的图，其中我们看到DP-FTRLM在大多数考虑的情况下提供了优于DP-SGDM的性能隐私制度。</p><h1>结论</h1><p>  在本文中，我们介绍了DP- ftrl算法，我们证明了该算法在DP下具有最紧密的已知遗憾保证，并对非光滑凸损失的单通道算法具有最佳的已知超额种群风险保证。对于线性和最小二乘损失，我们表明DP-FTRL实际上实现了最优的总体风险。此外，我们在基准数据集上表明，不依赖任何隐私放大的DP-FTRL在较大的 $ε$ 值下可以优于放大的DP-SGD，并且在所有 $ε$ 范围内都具有竞争力，但计算成本（批大小）略有增加。这项工作留下了两个主要的开放问题:i) DP-FTRL能否在一次通过中实现所有凸损失的最优超额种群风险?，以及ii)是否可以通过对树数据结构的梯度和的更好估计来缩小DP-SGD和DP-FTRL之间的经验差距（$ε$ 值更小）?</p><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">梯度计算在训练示例的子集上，也称为小批处理。<a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">我们也可以用泊松采样[1,49,73]创建一个小批次，只是批次大小现在是一个随机变量。为了简单起见，我们将重点放在固定批处理设置上。<a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">类似的论点也适用于通过洗牌放大[24,30]，当数据在每个时代的开始一致洗牌时。本文不考虑迭代[28]的隐私放大，因为它只适用于光滑凸函数。<a href="#fnref:3" rev="footnote"> ↩</a></span></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">4.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">尽管在文献中没有正式说明，但一个简单的论证表明，本地私有SGD[19]可以实现与[63]中相同的遗憾。<a href="#fnref:4" rev="footnote"> ↩</a></span></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">5.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">定理4.3中类似的论证可以用在损失函数是线性的情况下，$\ell (θ;d) = \left\langle θ,d \right\rangle$, $d \in \mathbb{R}^{p}$, 并且 $\| d \|_{2} ≤ L$。<a href="#fnref:5" rev="footnote"> ↩</a></span></li><li id="fn:6"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">6.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">为了完整起见，我们在附录F.2中提供了DP-SGD（无放大器）、DP-SGD（无放大器）4x和DP-SGD 4x的完整性能图。<a href="#fnref:6" rev="footnote"> ↩</a></span></li></ol></div></div>]]></content>
      
      
      <categories>
          
          <category> 科研学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 联邦学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《Learning_differentially_private_recurrent_language_models》</title>
      <link href="/2022/09/19/learning-differentially-private-recurrent-language-models/"/>
      <url>/2022/09/19/learning-differentially-private-recurrent-language-models/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h1>摘要</h1><p>  我们证明，训练具有用户级差分隐私保证的大型循环语言模型是可能的，而在预测精度方面的成本仅为可以忽略不计。我们的工作建立在深度网络在用户分区数据和随机梯度下降隐私会计方面的训练的最新进展上。特别地，我们将用户级隐私保护添加到联邦平均算法中，从而从用户级数据中进行“large step”更新。我们的研究表明，给定一个拥有足够多用户的数据集（即使是小型互联网规模的数据集也很容易满足这一要求），实现差异隐私的代价是增加计算量，而不是像大多数之前的工作那样降低效用。我们发现，当在大数据集上训练时，我们的私有LSTM语言模型在定量和定性上与无噪声模型相似。</p><h1>介绍</h1><p>  像长短期记忆（LSTM）循环神经网络（RNNs）这样的深度循环模型已经成为现代语言建模方法的标准构建模块，在语音识别、移动键盘输入解码和语言翻译方面都有应用。由于不同的问题域和数据集，语言的使用差别很大，因此用正确分布的数据训练语言模型是至关重要的。例如，一个帮助在移动键盘上打字的模型，最好是通过在移动应用程序中输入的训练数据，而不是来自扫描的书籍或转录的话语。然而，语言数据可能是唯一隐私敏感的。在手机上输入文本的情况下，这些敏感信息可能包括密码、文本消息和搜索查询。一般来说，语言数据可以明确地通过名字或隐式地（例如通过罕见或独特的短语）识别说话者，并将说话者与秘密或敏感信息联系起来。<br>  理想情况下，语言模型的参数应该对许多用户共同使用的语言使用模式进行编码，而不需要记住任何单个用户的独特输入序列。然而，我们知道卷积神经网络可以记住训练数据的任意标签（Zhang等人，2017），循环语言模型也能够记住训练数据中的唯一模式（Carlini等人，2018）。最近对神经网络的攻击，如Shokri等人（2017）的攻击，强调了隐性风险。我们工作的主要目标是提供一个强有力的保证，即训练过的模型保护个人数据的隐私，而不会在模型质量上造成不适当的牺牲。<br>  我们的动机是训练模型在移动键盘下一个单词预测的问题，并使用它作为一个运行的例子。这个问题非常适合我们介绍的技术，因为差异隐私可能允许对来自真实分布(实际移动使用)的数据进行训练，而不是对来自其他来源的代理数据进行训练，后者会产生劣质模型。然而，为了便于再现性和与非私有模型的比较，我们的实验是在公共数据集上进行的，这是差异隐私研究的标准。本文的其余部分是围绕以下贡献构建的：</p><ol><li>我们使用用户相邻数据集的概念，将差异隐私应用到模型训练中，导致用户级隐私的正式保证，而不是单个示例的隐私。</li><li>我们在§2中介绍了联邦平均算法（McMahan等人，2016）的噪声版本，该算法通过使用矩会计（Abadi等人，2016a）来满足用户相邻的差分隐私，该会计首先开发用于分析差分隐私随机梯度下降（SGD）的示例级隐私。联邦平均方法将多个SGD更新分组在一起，支持“large step”模型更新</li><li>我们演示了§3中训练的第一个高质量的LSTM语言模型，在足够大的数据集下，模型的准确性没有显著下降。例如，在763,430个用户的数据集上，基线（非私有）训练在4120轮训练中达到17.5%的准确性，其中我们在每轮训练中使用来自100个随机用户的数据。我们在4980轮中以（4.6,$10^{−9}$）的隐私差异实现了相同水平的准确性，平均每轮处理5000个用户，以大约60× <sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="<b>注释1：通过在公共数据集上进行初始化训练，而不是像我们在实验中那样从随机初始化开始，可以减少额外的计算成本。</b>">[1]</span></a></sup> 的显著计算成本保持相同水平的准确性。在拥有108个用户的更大数据集上运行相同的计算将提高隐私保障到（1.2,$10^{−9}$）。尽管lstm的内部结构很复杂（每个单词嵌入以及密集的状态转换），但我们使用联邦平均算法保证了隐私性并保持了实用性。我们证明了噪声模型的度量和定性行为（关于标题词）与非私有模型没有显著差异。据我们所知，我们的工作代表了最复杂的机器学习模型（根据模型的大小和复杂性来判断），这是有史以来在隐私保障下训练的最复杂的机器学习模型，也是第一个在用户级隐私保护下训练的这样的模型。</li><li>在§3的大量实验中，当训练具有不同隐私保证的复杂模型时，我们提供了参数调优的指导方针。我们表明，少量的实验可以缩小参数空间，使我们为隐私付出的代价不是效用的损失，而是计算成本的增加。</li></ol><p>  现在我们介绍一些初步的内容。差异隐私（DP）（Dwork等人，2006;Dwork, 2011;Dwork和Roth, 2014）为源自私人数据的信息发布提供了一种经过充分测试的形式化方法。将差分私有训练机制应用到机器学习中，可以公开发布模型参数，并提供强有力的保证：对手在分析参数的基础上对原始训练数据的了解受到严重限制，即使他们可以访问任意的侧信息。在形式上，它说：</p><p><b>定义1：差分隐私：</b>随机机制 $\mathcal{M} :\mathcal{D} \rightarrow \mathcal{R}$ 具有域 $\mathcal{D}$ （例如，可能的训练数据集）和范围 $\mathcal{R}$ （例如，所有可能的训练模型）满足 $(\epsilon, \delta)$-差分隐私如果对于任意两个相邻数据集 $d,d’ \in \mathcal{D}$ 和输出任意子集 $\mathcal{S} \subseteq \mathcal{R}$，则 $\operatorname{Pr}[\mathcal{M}(d) \in S] \leq e^{\epsilon} \operatorname{Pr}\left[\mathcal{M}\left(d^{\prime}\right) \in S\right]+\delta$。<br>  上面的定义保留了依赖于应用程序的相邻数据集的定义。大多数之前关于差分隐私机器学习的工作（例如Chaudhuri等人（2011）;Bassily等人（2014）;阿巴迪等人（2016a）;Wu等（2017）;Papernot等人（2017））处理example-level privacy：如果 $d’$ 可以通过从 $d$ 中添加或删除单个训练示例而形成，则两个数据集 $d$ 和 $d’$ 被定义为相邻的。我们指出，虽然（Papernot等人，2017）最近的PATE方法可以被调整为提供用户级隐私，但它不适合类（可能输出的单词）数量很大的语言模型。<br>  对于像语言建模这样的问题，保护单个的例子是不够的——每个键入的单词对RNN的训练目标都有自己的贡献，因此一个用户可能为训练数据贡献数千个例子。一个敏感的词或短语可能被个人用户输入多次，但它仍然应该受到保护。<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="<b>注释2：差异隐私满足一种称为组隐私的属性，该属性允许以增加 $\epsilon$ 为代价从示例级隐私转换到用户级隐私。在我们的设置中，这样的黑箱方法将产生令人难以承受的隐私成本。这迫使我们直接解决用户级隐私问题。</b>">[2]</span></a></sup> 因此，在这项工作中，我们应用差异隐私的定义来保护训练集中的整个用户历史。通过使用适当的邻接关系来确保用户级隐私:</p><p><b>定义2：用户相邻数据集：</b>设 $d$ 和 $d’$ 是两个训练示例数据集，其中每个示例与一个用户相关联。然后，如果 $d’$ 可以通过从 $d$ 中添加或删除与单个用户相关的所有示例而形成，则 $d$ 和 $d’$ 相邻。<br>  对于与用户相邻的数据集，满足不同隐私的模型训练满足了我们旨在为语言建模保护的隐私的直观概念：训练集中任何特定用户数据的存在或不存在对学习模型的参数（分布）有难以察觉的影响。由此可见，查看经过训练的模型的对手无法推断是否在训练中使用了任何特定用户的数据，无论他们可能拥有什么辅助信息。特别地，差别隐私在很强的信息理论意义上排除了对敏感信息的记忆。</p><h1>用户级差分隐私训练算法</h1><p>  我们的私有算法严重依赖于两个之前的工作：McMahan等人（2016）的federated平均（或FedAvg）算法，它在用户分区数据上训练深度网络，Abadi等人（2016a）的矩会计，它为高斯机制结合采样放大的重复应用提供了严密的组成保证。虽然我们试图使当前的工作尽可能独立，但上述参考资料提供了有用的背景。<br>  FedAvg是由McMahan等人（2016）为联邦学习引入的，其目标是训练共享模型，同时将训练数据保留在每个用户的移动设备上。相反，设备下载当前模型，并通过对数据集执行本地计算来计算更新。在每个用户的数据上执行额外的计算以最小化训练模型所需的通信轮数是值得的，因为当训练数据在移动设备上保持分散时，带宽非常有限。然而，我们观察到，即使在应用DP的数据中心，FedAvg也是有意义的：较大的更新对噪声的抵抗力更强，更少的训练轮可以意味着更少的隐私成本。最重要的是，算法自然地基于单个用户的数据形成每个用户的更新，然后对这些更新进行平均，以计算每一轮应用于共享模型的最终更新。正如我们将看到的，这种结构使扩展算法成为可能，以提供用户级的差分隐私保障。<br>  我们还评估了FederatedSGD算法，本质上是large-batch SGD，其中每个mini-batch由“microbatches（微批次）”组成，包括来自单个不同用户的数据。在某些数据中心应用程序中，FedSGD可能比FedAvg更可取，因为快速网络使运行更多迭代更加实际。然而，这些额外的迭代是以隐私为代价的。此外，联邦学习在隐私保护方面的好处与差分隐私保护方面的好处是很好的互补，FedAvg也可以应用于数据中心，因此我们将重点放在这个算法上，同时展示我们的结果也扩展到FedSGD。<br>  FedAvg和FedSGD都是迭代过程，在这两种情况下，我们都对非私有版本进行了以下修改，以实现不同的私有：</p><ol><li>（A）我们使用随机大小的batches，我们以 $q$ 的概率独立选择用户，而不是总是选择固定数量的用户</li><li>（B）我们强制裁剪每个用户的更新，因此总更新有 $L_{2}$ 范数限制</li><li>（C）我们对平均更新使用不同的估计量（接下来介绍）</li><li>（D）我们在最终的平均更新中添加高斯噪声。</li></ol><p><img src="/2022/09/19/learning-differentially-private-recurrent-language-models/1.png" alt="图1：DP-FedAvg和DP-FedSGD的主循环，唯一的区别是用户更新函数（UserUpdateFedAvg或UserUpdateFedSGD）。moments会计 $\mathcal{M}$ 的调用参考Abadi et al. (2016b)的API。"></p><p>  DP-FedAvg和DP-FedSGD的伪代码如算法1所示。在本节的其余部分，我们介绍了（C）的估计量，然后介绍了（B）的不同剪辑策略。添加来自（A）的采样过程和（D）的噪声，允许我们应用矩会计来限制算法的总隐私损失，如定理1所示。最后，我们考虑矩会计的性质，使训练大数据集特别有吸引力。</p><p><b>加权平均查询的有界灵敏度估计</b>随机抽样用户（或训练示例），以概率 $q$ 选择每个独立用户，这对于通过使用矩会计证明低隐私损失至关重要（Abadi等人，2016a）。然而，这个过程产生可变大小的样本 $\mathcal{C}$ ，当待估计的数量 $f(\mathcal{C})$ 是一个平均值而不是一个和时（如计算FedAvg中的加权平均更新或SGD中具有example-level DP的 minibatch 上的平均损失），这对查询 $f$ 的灵敏度产生了影响。</p><p>  具体来说，我们考虑加权数据库 $d$，其中每一行 $k \in d$ 都与一个特定的用户关联，并且具有关联的权重 $w_{k} \in [0,1]$。此权重捕获行 $k$ 对最终结果的预期影响。例如，我们可以认为第 $k$ 行包含 $n_{k}$ 个不同的训练示例，它们都是由用户 $k$ 生成的，权重 $w_{k}$ 与 $n_{k}$ 成正比。然后，我们对每个用户向量 $\Delta_{k}$ 的 $f(\mathcal{C})=\frac{\sum_{k \in \mathcal{C}} w_{k} \Delta_{k}}{\sum_{k \in \mathcal{C}} w_{k}}$ 的有界灵敏度估计感兴趣，例如，估计FedAvg中的加权平均用户更新。设 $W=\sum_{k} w_{k}$ 。我们考虑两个这样的估计量：<br>$$<br>\tilde{f}_{\mathrm{f}}(\mathcal{C})=\frac{\sum_{k \in \mathcal{C}} w_{k} \Delta_{k}}{q W}<br>$$</p><p>$$<br>\tilde{f}_{\mathrm{c}}(\mathcal{C})=\frac{\sum_{k \in \mathcal{C}} w_{k} \Delta_{k}}{\max \left(q W_{\min }, \sum_{k \in \mathcal{C}} w_{k}\right)}<br>$$<br>  注意 $\tilde{f}_{\mathrm{f}}$ ，因为 $\mathbb{E}\left[\sum_{k \in \mathcal{C}} w_{k}\right]=q W$ ，另一方面，只要样本中有足够的权值， $\tilde{f}_{\mathrm{c}}$ 就完全匹配 $f$。为了保护隐私，我们需要控制查询函数 $\tilde{f}$ 的灵敏度，定义为 $\mathbb{S}(\tilde{f})=\max_{\mathcal{C}, k} ||\tilde{f}(\mathcal{C} \cup { k } ) - \tilde{f}(\mathcal{C}) ||_{2}$ ，其中添加的用户 $k$ 可以有任意数据。为了控制灵敏度，需要在 $\tilde{f}_{\mathrm{c}}$ 的分母上设置下限 $qW_{min}$ 。假设每个 $w_{k} \Delta_{k}$ 有界范数，我们有：<br><b>引理1：</b>如果对于所有用户 $k$，我们有 $|w_{k} \Delta_{k} |_{2} \leq S$ ，那么两个估计器的灵敏度被限定为 $\mathbb{S}\left(\tilde{f}_{\mathrm{f}}\right) \leq \frac{S}{q W} \text { and } \mathbb{S}\left(\tilde{f}_{\mathrm{c}}\right) \leq \frac{2 S}{q W_{\min } }$。</p><p><b>多层模型的裁剪策略：</b>不幸的是，当用户向量 $\Delta_{k}$ 是来自神经网络的梯度（或梯度的和）时，我们通常没有一个先验边界<sup id="fnref:3"><a href="#fn:3" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="<b>注释3：为了控制灵敏度，引理1只要求 $\\|w_{k} \Delta_{k} \\|\_{2} \leq S$ 有界。为了简单起见，我们仅使用 $w_{k} \leq 1$ 的事实，对更新 $\Delta_{k}$ 进行裁剪，将重量感知裁剪方案的研究留作以后的工作。</b>">[3]</span></a></sup> $S$，这样 $\Delta_{k}  \leq S$。因此，我们将需要在应用 $\tilde{f}_{\mathrm{f}}$ 或  $\tilde{f}_{\mathrm{c}}$ 之前“剪辑”我们的更新来强制这样的边界。对于单个向量 $\Delta$ ，我们可以在必要时应用简单的 $L_{2}$ 投影：<br>$$<br>\pi(\Delta, S) \stackrel{\text { def }}{=} \Delta \cdot \min \left(1, \frac{S}{|\Delta|}\right)<br>$$</p><p><img src="/2022/09/19/learning-differentially-private-recurrent-language-models/2.png" alt="表1:不同的用户总数 $K$ （所有的权重相等）的隐私，每轮 $\tilde{\mathcal{C} } $ 采样的用户的预期数量，以及训练的轮数。对于每一行，我们设 $\delta = \frac{1}{K^{1.1} }$，并报告 $\epsilon $ 的值，其中 $ (\epsilon, \delta)$-微分隐私在 $1 ~ 10^{6}$ 轮后保持。对于大型数据集，额外的训练只会带来小的额外隐私损失。"><br>  然而，对于深度网络来说，更自然的做法是将每一层的参数视为一个单独的向量。对每一层的更新可能具有截然不同的 $L_{2}$ 规范，因此最好将每一层单独剪辑。<br>  形式上，假设每次更新 $\Delta_{k}$ 包含 $m$ 个向量 $\Delta_{k}=\left(\Delta_{k}(1), \ldots, \Delta_{k}(m)\right)$。我们考虑以下的裁剪策略，它们都保证了总更新最多有范数 $S$ ：</p><ol><li><b>平面裁剪</b>给定总体裁剪参数 $S$，我们将所有层的拼接裁剪为 $\Delta_{k}^{\prime}=\pi\left(\Delta_{k}, S\right)$</li><li><b>每层剪切</b>给定每层的每层裁剪参数 $S_{j}$，设 $\Delta_{k}^{\prime}(j) = \pi\left(\Delta_{k}(j), S_{j}\right)$。设 $S=\sqrt{\sum_{j=1}^{m} S_{j}^{2}}$。最简单的与模型无关的选择是对所有的 $j$ 取 $S_{j}=\frac{S}{\sqrt{m}}$，我们在实验中使用。</li></ol><p>  我们在这里指出，剪辑本身会导致额外的偏差，理想情况下，我们应该选择剪辑参数足够大，使几乎所有更新都小于剪辑值。另一方面，一个较大的 $S$ 将需要更多的噪音，以实现隐私，可能会减慢训练。我们将 $S$ 视为超参数并对其进行调优。<br><b>一个隐私保障</b>一旦所选估计量的灵敏度是有界的，我们可以将高斯噪声缩放到该灵敏度上以获得隐私保证。一个简单的方法是使用 $(\epsilon ，\delta )-DP$ 界限，并应用隐私放大引理和高级复合定理得到了总隐私代价的界限。我们使用Abadi等人(2016a)的朋友圈会计来实现更严格的隐私界限。采样高斯机构的矩会计上界为噪声 $N(0,\sigma ^{2})$ 的高斯机构T阶的总隐私成本，$\sigma=z \cdot \mathbb{S}$，其中 $z$ 为参数，$\mathbb{S}$ 为查询的灵敏度，每一行的选择概率为 $q$。给定 $\delta &gt; 0$，矩会计给出一个 $\epsilon$ 这个机制满足 $(\epsilon , \delta)-DP$。以下定理是对Abadi等人(2016a)的结果的轻微推广;见§A的证明草图。<br><b>定理1：</b>对于估计器（$\tilde{f}_{\mathrm{f}}$, $\tilde{f}<em>{\mathrm{c}}$），采样高斯机制的力矩会计，以 $z = \delta / \mathbb{E}$ 和步骤 $T$ 的噪声尺度正确计算隐私损失，其中 $\mathbb{S} = S/qW$ 对于（$\tilde{f}</em>{\mathrm{f}}$），$2S/qW_{min}$ 对于（$\tilde{f}<em>{\mathrm{c}}$）。<div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><b>注释1：通过在公共数据集上进行初始化训练，而不是像我们在实验中那样从随机初始化开始，可以减少额外的计算成本。</b><a href="#fnref:1" rev="footnote"> ↩</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><b>注释2：差异隐私满足一种称为组隐私的属性，该属性允许以增加 $\epsilon$ 为代价从示例级隐私转换到用户级隐私。在我们的设置中，这样的黑箱方法将产生令人难以承受的隐私成本。这迫使我们直接解决用户级隐私问题。</b><a href="#fnref:2" rev="footnote"> ↩</a></span></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">3.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><b>注释3：为了控制灵敏度，引理1只要求 $|w</b></span></li></ol></div></div></em>{k} \Delta_{k} |<em>{2} \leq S$ 有界。为了简单起见，我们仅使用 $w</em>{k} \leq 1$ 的事实，对更新 $\Delta_{k}$ 进行裁剪，将重量感知裁剪方案的研究留作以后的工作。<a href="#fnref:3" rev="footnote"> ↩</a></p>]]></content>
      
      
      <categories>
          
          <category> 科研学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 联邦学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《QSFL： A Two-Level Uplink Communication Optimization Framework for Federated Learning 》</title>
      <link href="/2022/09/18/qsfl-a-two-level-uplink-communication-optimization-framework-for-federated-learning/"/>
      <url>/2022/09/18/qsfl-a-two-level-uplink-communication-optimization-framework-for-federated-learning/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h1>摘要</h1><p>  在跨设备联邦学习(FL)中，由于昂贵、不可靠和低带宽的无线连接，在边缘设备和中心服务器之间传输全精度模型的通信成本是一个重要的瓶颈。作为一种解决方案，我们提出了一种新的FL框架QSFL，旨在客户端和模型层上优化FL上行（客户端到服务器）通信。在客户端层面，我们设计了一个资质判断（QJ）算法，对高资质的客户端进行抽样，以上传模型。在模型层面，我们探索了一种稀疏循环滑动段（SCSS）算法来进一步压缩传输模型。我们证明了QSFL可以在墙到墙的时间内收敛，并在理论分析的基础上开发了一种最优超参数搜索算法，使QSFL在模型精度和通信成本之间进行最佳权衡。实验结果表明，QSFL在模型精度下降的情况下获得了最先进的压缩比。</p><h1>简介</h1><p>  在传统的FL中，参与的客户端将本地全精度模型上传到服务器进行聚合，而不需要本地数据共享。当数千个客户协同训练一个GB级模型时，上行传输可以达到TB级（Sattler 等人，2019b）。然而，在数据中心中，边缘设备和服务器之间昂贵且不可靠的无线连接的速率通常低于以太网连接（Y ao等人，2019；张等人，2020）。大规模的深度网络通常有很多冗余参数（Kairouz等人，2019；陈等，2018b；Yi等人，2020），传输所有本地全精度模型导致通信开销成为主要瓶颈。此外，互联网连接的上传和下载速度是高度不对称的:前者比后者慢至少5倍（Koneˇcn´y et al.， 2016），因此压缩客户端到服务器的传输流量会产生更大的效益。<br>  现有的通信-有效FL方案主要包括：延迟通信、采样客户端、编码模型、稀疏化、量化。由于这些方法要么没有理论上的收敛保证，要么压缩比有限，因此需要设计更先进的方案来平衡通信成本和收敛性。<br>  在本文中，我们提出了一种新的通信效率框架QSFL，该框架通过以下方法优化FL的上行通信成本:a)首先采样高条件客户端上传模型更新（QJ算法）;b)将每一轮上传的模型进一步压缩成一个段（SCSS算法）。在理论分析的基础上证明了它的收敛性，并探讨了如何搜索它的最优超参数。如图1所示，在达到相同目标精度的情况下，QSFL比先进的通信效率方法实现了最先进的压缩比，促进了通信成本和模型精度之间更好的权衡。<br><b>贡献：</b>我们的主要贡献如下：</p><ol><li>我们提出了一个新的FL框架，命名为QSFL，以减少FL上行通信开销在客户端级和模型级。</li><li>我们证明了QSFL的收敛速度，并开发了一种超参数搜索算法来配置QSFL的最优超参数。</li><li>大量的实验结果验证了QSFL算法在上行通信开销优化方面的先进性。</li></ol><h1>相关工作</h1><p>目前，现有的FL通信优化方法可以归纳为以下几类：<br><b>（A）推迟沟通：</b>FedAvg（McMahan et al.,2017）通过增加局部迭代来延迟通信，从而加速整个训练过程。FedPAQ （Reisizadeh等人，2020）定期聚合局部量化模型更新，以降低通信成本。<br><b>（B）稀疏化：</b>稀疏化用适当的稀疏表示代替全精度模型。Strom（2015）；Tsuzuku et al.（2018）选择量值大于阈值的参数进行传输，但很难设置合适的阈值。梯度下降（Aji等人，2017）样本参数的 $top-p %$ 量级。Wangni等人（2018）引入了一个凸度公式，以确保压缩的梯度是真实梯度的无偏估计。Tsuzuku等人（2018）的基于方差的稀疏化只上传量级大于方差的梯度。Slim-DP （Sun et al.， 2018）以 $top-p %$ 的幅度对参数进行采样，并从其余参数中随机选择 $\varepsilon %$。HeteroFL（Diao et al.,2021）允许每个客户端将其本地模型的自适应系统资源的子网络上传到服务器，实现了对异构子网络的兼容，提高了上行通信的效率。尽管 $top-p %$ 稀疏化对非iid数据具有鲁棒性(Sattler et al.， 2019b)，但随着压缩比的提高，模型精度明显下降，且缺乏收敛的理论保证。</p>]]></content>
      
      
      <categories>
          
          <category> 科研学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 联邦学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《Practical Secure Aggregation for Federated Learning on User-Held Data》阅读笔记</title>
      <link href="/2022/09/09/practical-secure-aggregation-for-federated-learning-on-user-held-data-yue-du-bi-ji/"/>
      <url>/2022/09/09/practical-secure-aggregation-for-federated-learning-on-user-held-data-yue-du-bi-ji/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h1>介绍</h1><p>  安全聚合(Secure Aggregation)是一类安全多方计算算法，其中相互不信任的一组 $u \in \mathcal{U}$ 各自持有一个私有值 $x_u$ ，并协作计算一个聚合值，例如总和 $\sum_{u \in \mathcal{U}} x_{u}$，除了可以从聚合值本身学到的以外，彼此不透露关于其私有值的任何信息。在这项工作中，我们考虑在联邦学习模型中训练一个深度神经网络，使用分布式梯度下降在移动设备上的用户持有训练数据，使用安全聚合来保护每个用户的模型梯度的隐私。我们确定了效率和鲁棒性要求的组合，据我们所知，这是文献中现有算法无法满足的。我们继续为高维数据设计了一种新颖的、通信高效的安全聚合协议，该协议允许多达 $1/3$ 的用户无法完成协议。对于16位输入值，我们的协议为 $210$ 个用户和 $220$ 维向量提供了 $1.73 × 通信扩展$ ，为 $214$ 个用户和 $224$ 维向量提供了 $1.98 × 通信扩展$ 。</p><h1>用于联邦学习的安全聚合</h1><p>  考虑训练一个深度神经网络来预测用户在写短信时将要输入的下一个单词，以提高手机屏幕键盘[11]的打字精度。建模师可能希望针对大量用户中的所有文本消息训练这样的模型。然而，短信经常包含敏感信息；用户可能不愿意将它们的副本上传到建模者的服务器。相反，我们考虑在联邦学习设置中训练这样的模型，在这种设置中，每个用户在自己的移动设备上安全地维护一个关于她的文本消息的私有数据库，并在中央服务器的协调下训练一个共享的全局模型，该服务器基于来自用户的高处理、最小范围、短暂的更新[14,17]。</p><p>  神经网络表示函数 $f(x, \Theta)=y $，将输入 $x$ 映射到输出 $y$，其中 $f$ 由高维向量 $\Theta \in \mathbb{R}^{k}$ 参数化。对于文本消息组合建模，$x$ 可能对目前输入的单词进行编码，$y$ 可能对下一个单词进行概率分布。一个训练例子是观察这一对 $\left\langle x, y \right\rangle$，训练集是集合 $D = \{ \left\langle x_{i}, y_{i}\right\rangle ; i=1, \ldots, m \} $ 。我们定义训练集 $ \mathcal{L}_{f}(D, \Theta)=\frac{1}{|D|}  \sum_{ \left\langle x, y \right\rangle \in D} \mathcal{L}_{f} (x_{i}, y_{i}, \Theta)$ 上的损失，其中对于损失函数 $\ell$， $\mathcal{L}_{f} (x_{i}, y_{i}, \Theta) = \ell(y, f(x, \Theta))$，例如 $\ell(y, \hat{y})=(y-\hat{y})^{2}$。训练包括寻找实现小 $\mathcal{L}_{f}(D, \Theta)$ 的参数 $\Theta$ ，通常使用可变的小批量随机梯度下降[4,10]</p><p>  在联邦学习设置中，每个用户 $u \in \mathcal{U}$ 都拥有一个训练样例的私有集合 $D_{u}$，其中 $D=\bigcup_{u \in \mathcal{U}} D_{u}$。为了运行随机梯度下降，对于每次更新，我们从一个随机子集 $\mathcal{U}^{\prime} \subset \mathcal{U}$ 中选择数据，并形成一个（虚拟）小批 $B=\bigcup_{u \in \mathcal{U}^{\prime}} D_{u}$ （在实践中，我们可能会说 $ | \mathcal{U}^{\prime} |=10^{4} $ 而 $|\mathcal{U}|=10^{7}$ ；我们可能只考虑每个用户本地数据集的一个子集）。小批损耗梯度 $\nabla \mathcal{L}_{f}(B, \Theta)$ 可以改写为跨用户的加权平均值： $\nabla \mathcal{L}_{f}(B, \Theta)=\frac{1}{|B|} \sum_{u \in \mathcal{U}^{\prime}} \delta_{u}^{t}$，其中 $\delta_{u}^{t}=\left|D_{u}\right| \nabla \mathcal{L}_{f}\left(D_{u}, \Theta^{t}\right)$。因此，用户可以只与服务器共享 $\left\langle |D_{u}|, \delta_{u}^{t}\right\rangle$ ，从中可以得到一个梯度下降步骤 $\Theta^{t+1} \leftarrow \Theta^{t}-\eta \frac{\sum_{u \in \mathcal{U}^{\prime}} \delta_{u}^{t}}{\sum_{u \in \mathcal{U}^{\prime}}\left|D_{u}\right|}$ 。</p><p>  尽管每次更新 $\left\langle\left|D_{u}\right|, \delta_{u}^{t}\right\rangle$ 是短暂的，包含的信息比原始的 $D_{u}$ 少，用户可能仍然想知道还有什么信息。有证据表明，经过训练的神经网络参数有时允许重构训练示例[8,17,1]；参数更新会受到类似的攻击吗？例如，如果输入 $x$ 是编码最近键入的单词的单热词汇长度向量，那么对于每个单词 $w$，普通神经网络体系结构将在 $\Theta$ 中至少包含一个参数 $\theta_{w}$ ，使得 $\frac{\partial \mathcal{L}_{f}}{\partial \theta_{w}}$ 仅当 $x$ 编码 $w$ 时才非零。因此，通过检查 $\delta_{u}^{t}$ 的非零项，可以发现 $D_{u}$ 中最近输入的单词集。然而，服务器不需要检查任何单个用户的更新;它只需要 $\sum_{u \in \mathcal{U}}\left|D_{u}\right|$ 和 $\sum_{u \in \mathcal{U}} \delta_{u}^{t}$ 的和。使用安全聚合协议将确保服务器只知道 $\mathcal{U}$ 中的一个或多个用户写了单词 $w$ ，而不知道是哪个用户。</p><p>  联邦学习系统面临着几个实际的挑战。移动设备只有零星的电源和网络连接，因此参与每个更新步骤的集合 $U$ 是不可预测的，系统必须对退出的用户具有健壮性。因为 $\Theta$ 可能包含数百万个参数，更新 $\delta_{u}^{t}$ 可能很大，代表了用户在计量网络计划中的直接成本。移动设备通常也不能与其他移动设备建立直接通信通道（依靠服务器或服务提供商调解这种通信），也不能对其他移动设备进行本机验证。因此，联邦学习激发了对安全聚合协议的需求：</p><ol><li>对高维向量进行操作，</li><li>通信高效，即使在每个实例化上都有一组新的用户，</li><li>对用户退出具有健壮性，</li><li>在服务器中介的、未经身份验证的网络模型的约束下提供最强的可能安全性。</li></ol><h1>实用的安全聚合协议</h1><p>  在我们的协议中，有两种参与方:单个服务器 $S$ 和 $n$ 个用户 $\mathcal{U}$ 的集合。每个用户 $u \in \mathcal{U}$ 拥有一个维数为 $k$ 的私有向量 $x_{u}$ 。我们假设对于某个已知的 $R$ <a href="#1.1">点击跳转到该注释1</a>,  $x_{u}$ 和 $\sum_{u \in \mathcal{U}} x_{u}$ 的所有元素都是在 $[0, R)$ 范围内的整数。正确性要求，如果各方都诚实，$S$ 学习 $\bar{x}=\sum_{u \in \overline{ \mathcal{U} } } x_{u}$ 对于用户的某个子集 $\mathcal{U} \subseteq \mathcal{U}$ ，其中 $|\overline{\mathcal{U}}| \geq \frac{n}{2}$ 。安全性要求（1） $S$ 除了可以从 $\bar{x}$ 推断出的东西之外什么都不学习，（2）每个用户 $u \in \mathcal{U}$ 什么都不学习。我们考虑三种不同的威胁模型。在所有这些协议中，所有用户都诚实地遵守协议，但服务器可能试图通过不同的方式了解额外的信息<a href="#2.2">点击跳转到该注释2</a>：</p><ol><li>服务器是诚实的，但也很好奇，也就是说它诚实地遵循协议，但试图从它从用户那里接收到的消息中了解尽可能多的信息</li><li>服务器可以向用户谎报哪些其他用户退出了，包括不同用户之间不一致的退出报告</li><li>服务器可以谎报谁退出了（如2），也可以访问一些有限数量的用户的私有内存（这些用户自己诚实地遵守协议）。（在这种情况下，隐私要求只适用于其余用户的输入。）</li></ol><p><b>协议0：Masking with One-Time Pads：</b>我们通过一系列改进来开发我们的协议。我们首先假设所有各方都完成了协议，并拥有具有充足带宽的双向安全通信通道。每对用户首先就一对匹配的输入扰动达成一致，也就是说，用户 $u$ 为其他用户 $v$ 从 $[0,R)^{k}$ 中均匀抽样向量 $s_{u,v}$ 。用户 $u$ 和 $v$ 在他们的安全通道上交换 $s_{u,v}$ 和 $s_{v,u}$ ，并计算微扰 $p_{u,v} = s_{u,v} − s_{v,u} (mod R)$，注意到 $p_{u,v} = −p_{v,u} (mod R)$ ，当 $u = v$ 时取 $p_{u,v} = 0$。每个用户向服务器发送： $y_{u}=x_{u}+\sum_{v \in \mathcal{U}} p_{u, v}(\bmod R)$ 。服务器简单地对摄动值求和： $\bar{x}=\sum_{u \in \mathcal{U}} y_{u}(\bmod R)$ 。因为 $y_{u}$ 中的成对摄动抵消了:<br>$$<br>\bar{x}=\sum_{u \in \mathcal{U}} x_{u}+\sum_{u \in \mathcal{U}} \sum_{v \in \mathcal{U}} p_{u, v}=\sum_{u \in \mathcal{U}} x_{u}+\sum_{u \in \mathcal{U}} \sum_{v \in \mathcal{U}} s_{u, v}-\sum_{u \in \mathcal{U}} \sum_{v \in \mathcal{U}} s_{v, u}=\sum_{u \in \mathcal{U}} x_{u} \quad(\bmod R) .<br>$$<br>协议0保证用户的完美隐私;因为用户添加的 $s_{u,v}$ 因子是统一抽样的，所以 $y_{u}$ 值对服务器来说是统一随机的，受限于 $\bar{x}=\sum_{u \in \mathcal{U}} y_{u}(\bmod R)$ 。事实上，即使服务器可以访问一些用户的内存，剩余的用户仍然保留隐私 <a href="#3.3">点击跳转到该注释3</a> 。</p><div id="1.1"><b>注释1：联邦学习更新 $\delta_{u} \in \mathbb{R}^{k}$ 可以通过剪切/缩放、线性变换和（随机）量化的组合映射到 $[0,R)^{k}$ 。</b></div><div id="2.2"><b>注释2：我们不分析针对任意恶意服务器和可能串通的用户的安全性。我们将这种情况和更正式的安全性分析推迟到完整版本。</b></div><div id="3.3"><b>注释3：一个更完整和正式的论证推迟到这篇论文的完整版本。</b></div><p><b>协议1：使用秘密共享的删除用户恢复：</b>不幸的是，协议0没有达到我们的几个设计标准，包括健壮性：如果任何用户 $u$ 没有通过向服务器发送 $y_{u}$ 来完成协议，那么结果和将被 $y_{u}$ 本应取消的扰动所掩盖。为了实现鲁棒性，我们首先在协议中添加一个初始轮，其中用户 $u$ 生成一个公共/私有密钥对，并在成对通道上广播公钥。所有未来从 $u$ 到 $v$ 的消息将由服务器进行中介，但使用 $v$ 的公钥加密，并由 $u$ 签名，模拟一个安全的经过身份验证的通道。这允许服务器对哪些用户成功通过了每一轮协议保持一致的视图。（这里我们暂时假设服务器忠实地在用户之间传递所有消息。）<br>  我们还在 $s_{u,v}$ 值被选中后添加了用户之间的秘密共享轮。在这一轮中，每个用户使用 $(t,n)-$ 阈值方案 <a href="#4.4">点击跳转到该注释4</a> 计算每个摄动 $p_{u,v}$ 的 $n$ 份份额，例如Shamir的秘密共享[16]，对于某些 $t&gt;n/2$。对于每个秘密用户 $u$，她用每个用户 $v$ 的公钥加密一个共享，然后将所有这些共享发送到服务器。服务器从用户 $\mathcal{U}_{1} \subseteq \mathcal{U}$ 的子集中收集至少 $t$ 大小的共享（例如，等待一个固定的时间段），然后考虑所有其他用户被删除。服务器向每个用户 $v \in \mathcal{U}_{1}$ 提供为该用户加密的秘密共享；现在 $\mathcal{U}_{1}$ 中的所有用户都从接收到的共享集合中推断出幸存用户集 $\mathcal{U}_{1}$ 的一致视图。当用户计算 $y_{u}$ 时，她只包含那些与幸存用户相关的扰动；即 $y_{u}=x_{u}+\sum_{v \in \mathcal{U}_{1}} p_{u, v}(\bmod R)$ 。<br>  在服务器从至少 $t$ 个用户 $\mathcal{U}_{2} \subseteq \mathcal{U}_{1}$ 接收到 $y_{u}$ 之后，它将继续进行新的解掩轮，并考虑将丢弃所有其他用户。从 $\mathcal{U}_{2}$ 中剩下的用户中，服务器请求由 $\mathcal{U}_{1} \backslash \mathcal{U}_{2}$ 中被删除的用户生成的所有秘密共享。只要是 $\left|\mathcal{U}_{2}\right|&gt;t$ ，每个用户都会用这些股份来响应。一旦服务器接收到至少 $t$ 个用户的份额，它就会重建对 $\mathcal{U}_{1} \backslash \mathcal{U}_{2}$ 的扰动，并计算聚合值：$\bar{x}=\sum_{u \in \mathcal{U}_{2}} y_{u}-\sum_{u \in \mathcal{U}_{2}} \sum_{v \in \mathcal{U}_{1} \backslash \mathcal{U}_{2}} p_{u, v} (mod R)$。只要至少 $t$ 个用户完成协议，就保证了 $\overline{\mathcal{U}}=\mathcal{U}_{2}$ 的正确性。在这种情况下，总和 $x$ 包含至少 $t &gt; n/2$ 个用户的值，所有的扰动都抵消了：<br>$$<br>\bar{x}=\left(\sum_{u \in \mathcal{U}_2} x_u+\sum_{u \in \mathcal{U}_2} \sum_{v \in \mathcal{U}_1} p_{u, v}\right)-\sum_{u \in \mathcal{U}_2} \sum_{v \in \mathcal{U}_1 \backslash \mathcal{U}_2} p_{u, v}=\sum_{u \in \mathcal{U}_2} x_u+\sum_{u \in \mathcal{U}_2} \sum_{v \in \mathcal{U}_2} p_{u, v}=\sum_{u \in \mathcal{U}_2} x_u \quad(\bmod R)<br>$$<br>  然而，安全性已经丧失了：如果服务器错误地从 $\mathcal{U}_2$ 中遗漏了 $u$ ，无论是无意的（例如 $y_{u}$ 到达的稍微晚了一点）还是恶意的， $\mathcal{U}_2$ 中的诚实用户将向服务器提供所需的所有秘密共享，以消除 $y_{u}$ 中掩盖 $x_{u}$ 的所有扰动。这意味着即使面对诚实但好奇的服务器，我们也不能保证安全性（威胁模型T1）。</p><p><b>协议2：双重屏蔽阻止恶意服务器</b>为保证安全，我们引入了双重屏蔽结构，即使服务器可以重构 $u$ 的扰动，也可以保护 $x_{u}$ 。首先，在生成 $s_{u,v}$ 值的同一轮中，每个用户 $u$ 统一从 $[0,R)^k$ 中抽样一个额外的随机值 $b_{u}$。在秘密共享过程中，用户还生成 $b_{u}$ 的共享并将其分发给每个其他用户。在生成 $y_{u}$ 时，用户还添加了这个二次掩码：$y_u=x_u+b_u+\sum_{v \in \mathcal{U}_1} p_{u, v}(\bmod R)$ 。在解掩轮中，服务器必须对每个用户 $v \in \mathcal{U}_1$ 做出显式选择：从每个存活成员 $v \in \mathcal{U}_2$ 中，服务器可以请求与 $u$ 相关的 $p_{u、v}$ 摄动的份额，也可以请求与 $u$ 相关的 $b_{u}$ 的份额；诚实的用户 $v$ 只会在 $|\mathcal{U}_2| &gt; t$ 时才会响应，并且不会为同一用户显示两种类型的股份。在收集到所有 $u \in \mathcal{U}_1 / \mathcal{U}_2$ 的至少 $t$ 份 $p_{u,v}$ 和所有 $u \in \mathcal{U}_2$ 的 $b_{u}$ 的 $t$ 份 $b_{u}$ 后，服务器重新构造秘密，并计算聚合值：$\bar{x}=\sum_{u \in \mathcal{U}_2} y_u-\sum_{u \in \mathcal{U}_2} b_u-\sum_{u \in \mathcal{U}_2} \sum_{v \in \mathcal{U}_1 \backslash \mathcal{U}_2} p_{u, v}(\bmod R)$。</p><p>  我们现在可以保证 $t &gt; n/2$ 在威胁模型T1中的安全性，因为 $x_{u}$ 总是被 $p_{u,v}$ s或总线掩盖。可以看出，在威胁模型T2和T3中，阈值必须相应地提高到 $2n / 3$ 和 $4n / 5$。我们将详细的分析，以及任意恶意和串通服务器和用户的情况，推迟到完整版本 <a href="#5.5">点击跳转到该注释5</a>。</p><div id="4.4"><b>注释4：一个 $(t,n)$ 的秘密共享方案允许将一个秘密分成 $n$ 份，这样 $t$ 份中的任何子集都足以恢复秘密，但如果小于 $t$ 份的任何子集，秘密仍然完全隐藏</b></div><div id="5.5"><b>注释5：安全参数涉及限制服务器可以通过伪造退出来恢复的共享数量</b></div><p><b>协议3：有效地交换秘密：</b>虽然协议2在选择正确的 $t$ 时是健壮和安全的，但它需要 $O(kn^2)$ 通信，我们在这个协议的改进中解决了这个问题。<br>  可以观察到，通过使用单个秘密值作为加密安全伪随机生成器(PRG)的种子，可以将其扩展为一个伪随机值向量[2,9]。因此，我们可以生成标量种子 $s_{u,v}$ 和 $b_{u}$ ，并将它们展开为 $k$ 元素向量。然而，每个用户与其他用户有 $(n−1)$ 个秘密 $s_{u,v}$，并且必须发布所有这些秘密的共享。我们使用密钥协议来更有效地建立这些秘密。每个用户生成一个Diffie-Hellman密钥 $s^{SK}$ 和公钥 $s^{Pk}$。用户将自己的公钥发送给服务器（根据协议1进行认证）;然后，服务器向所有用户广播所有公钥，为自己保留一份副本。每对用户 $u,v$ 现在可以同意一个秘密 $s_{u,v} = s_{v,u} = AGREE(s_{u}^{SK}, s_{v}^{PK}) = AGREE(s_{v}^{SK}, s_{u}^{PK})$ 。为了构造摄动，我们假设 $\mathcal{U}$ 上有一个总序，当 $u&lt;v$ 时取 $p_{u,v} = PRG(s_{u,v})$，当 $u &gt; v$ 时取 $p_{u,v} = −PRG(s_{u,v})$ ，当 $u=v$ 时取 $p_{u,v} = 0$ （如前所述）。服务器现在只需要学习 $s_{u}^{SK}$ 来重构 $u$ 的所有扰动；因此你只需要在秘密分享过程中分配 $s_{u}^{SK}$ 和 $b_{u}$ 的股份。在不同的威胁模型中，协议3的安全性可以证明与协议2的安全性基本相同。</p><p><img src="/2022/09/09/practical-secure-aggregation-for-federated-learning-on-user-held-data-yue-du-bi-ji/2.png" alt="图1：协议4通信图"><br><b>协议4：实践中的最小信任值：</b>协议3不能实际部署到移动设备上，因为它们缺乏双向安全通信和身份验证。我们建议通过用服务器中介的密钥协议取代协议1中描述的公钥/私钥交换来引导通信协议，其中每个用户生成一个Diffie-Hellman密钥 $c^{SK}$ 和公钥 $c^{PK}$，并将后者与 $s^{PK}$ <a href="#7.7">点击跳转到该注释7</a>一起发布。我们立即注意到，服务器现在可能进行中间人攻击，但认为这是可以容忍的，原因有几个。首先，对于缺乏身份验证机制或预先存在的公钥基础设施的用户来说，这基本上是不可避免的。仅依靠引导轮的非恶意性也构成了信任的最小化:实现这一阶段的代码很小，可以公开审计，外包给可信的第三方，或通过提供远程认证功能的可信计算平台实现[7,6,18]。此外，该协议显著地提高了安全性（通过保护服务器不受主动恶意攻击），并提供了前向保密（在密钥交换后的任何时间损害服务器，对攻击者没有任何好处，即使所有数据和通信都已完全记录）。</p><p><img src="/2022/09/09/practical-secure-aggregation-for-federated-learning-on-user-held-data-yue-du-bi-ji/1.png" alt="表1：协议4成本总结（推导延后到全文）"></p><p>  我们在表1中总结了该协议的性能。假设密钥协议公钥和加密的秘密共享是256位，并且用户的输入都在相同的范围 <a href="#8.8">点击跳转到该注释8</a> $[0,R_{U}−1]$，每个用户传输 $\frac{256(7 n-4)+k\left\lceil\log _2\left(n\left(R_U-1\right)+1\right)\right\rceil+n}{k\left\lceil\log _2 R_U\right\rceil}$ 的数据比发送原始向量多。</p><h1>相关工作</h1><p>  在安全聚合的受限情况下，除一个用户外所有用户都有输入0，可以表示为用餐密码网络（DC-net），它通过使用输入的成对盲法提供匿名性[3,9]，允许不可追踪地学习每个用户的输入。近年来的研究对恶意用户[5]存在下的通信效率和运行进行了研究。但是，即使有一个用户过早终止，现有的协议也必须从头开始重新启动，这可能是非常昂贵的[13]。基于模相加的加密方案中的双向盲法已经被探索过，但现有方案对向量既不高效，对单个故障也不健壮[2,12]。其他方案（如基于Paillier密码系统的[15]）是非常昂贵的计算。</p><div id="6.6"><b>注释6：通过缓存拉格朗日系数，我们从 $O(t^{2} + nt)$ 中对齐的 $(t, n)-$Shamir共享中重构了 $n$ 个秘密<div id="7.7"><b>注释7：这可以看作是在每对用户之间引导一个SSL/TLS连接</b></div><div id="8.8"><b>注释8：取 $R = n(R_{U}−1)+ 1$，保证不溢出</b></div></b></div>]]></content>
      
      
      <categories>
          
          <category> 科研学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 联邦学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《Advances and Open Problems in Federated Learning》阅读笔记</title>
      <link href="/2022/09/06/advances-and-open-problems-in-federated-learning-yue-du-bi-ji/"/>
      <url>/2022/09/06/advances-and-open-problems-in-federated-learning-yue-du-bi-ji/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h1>3 efficiency_and_effectiveness</h1><p>  在本节中，我们会探索各种技术和开放性的问题，以解决使联邦学习更加高效和有效的挑战。其中包含了无数种可能的方法，包括：探索更好的优化算法；提供不同的模型给不同的客户端；让机器学习任务，例如：参数搜索、结构搜索和调试在联邦学习场景中更加容易；提高通信效率等等。<br>  解决这些目标的一个基本挑战就是non-IID（非独立同分布）数据的存在，因此，我们首先分析这个问题，并强调潜在的解决方案。</p><h2 id="3-1联邦学习中的Non-IID数据"><a class="header-anchor" href="#3-1联邦学习中的Non-IID数据">¶</a>3.1联邦学习中的Non-IID数据</h2><p>  虽然通常情况下IID的含义很清楚，但是数据non-IID则有很多种可能。在本节中，我们提供了一种对于non-IID数据的分类方法，对任何客户端分区的数据集可能出现的数据non-IID情况进行分类。最常见的独立但是非同分布的来源是每个客户端对应着一类特定的用户，一片地理区域，或者一段特定的时间。提出的分类法与数据漂移的概念有密切的关系[304, 327]，它研究训练集分布和测试集分布之间的差异。这里，我们仅考虑每个客户端上数据分布的差异。<br>  对于以下内容，我们假设一个监督任务有特征$x$和其对应的标签$y$。联邦学习的统计模型涉及到两个层面的采样： 第一层是对客户端 $i \sim Q$ 进行采样（在可用客户端上的数据分布），第二层是对客户端本地数据分布进行采样 $(x,y) \sim \mathcal{P}_i(x,y)$。<br>  当在联邦学习中提到数据non-IID时，我们通常指的是客户端$i$和客户端$j$所对应的 $\mathcal{P}_i$ 与 $\mathcal{P}_j$ 不同。然而，有一点需要我们特别注意的是：$\mathcal{Q}$ 和 $\mathcal{P_i}$ 是可能随着时间的推移而改变，从而导致另一维度上的non-IID。<br>  为了完整起见，我们注意到，即使对于单个设备上的数据，如果数据没有经过充分的随机打乱顺序，比如：按照时间排列，那么本地数据的独立性也无法保证。一个简单的例子：视频中连续的帧是高度相关的。客户端内部相关性的来源通常可以通过在本地随机打乱顺序来解决。<br><b>非同分布的客户端分布：</b> 根据Hsieh et al.[205]，我们首先研究了数据偏离同分布的一些常见方式，即对于不用的客户端 $i$ 和客户端 $j$ 的分布不同 $P_i \not= P_j$。我们将 $P_i(x,y)$ 重写为 $P_i(y|x)P_i(x)$ 和 $P_i(x|y)P_i(y)$ 让我们能够更加准确地描述他们的区别。<br><I>特征分布倾斜（协变量飘移）：</I>即使共享 $\mathcal{P}(y|x)$ ，不同客户端上的边缘分布 $\mathcal{P}_i(x)$ 也可能不同 $^4$ 。比如，在手写识别领域，用户在书写同一个单词时也可能有着不同的笔画宽度、斜度等。<br><I>标签分布倾斜（先验概率飘移）：</I>即使 $\mathcal{P}(x|y)$ 是相同的，对于不同客户端上的边缘分布 $\mathcal{P}_i(y)$ 也可能不同。比如，当客户端与特定的地理区域绑定时，标签的分布在不同的客户端上是不同的。比如：袋鼠只在澳大利亚或动物园里；一个人的脸只在出现在全球的几个地方；对于手机设备的键盘，某些特定人群使用某些表情，而其他人不使用。<br><I>标签相同，特征不同（概念飘移）：</I>即使共享 $\mathcal{P}(y)$ ，不同客户端上的条件分布 $P_i(x|y)$ 也可能是不同。由于文化差异，天气影响，生活水平等因素，对于相同的标签 $y$ ，对于不同的客户端可能对应着差异非常大的特征$x$。比如：世界各地的家庭图片千差万别，衣着也千差万别。即使在美国，冬季停放的被大雪覆盖汽车的图像只会出现在某些地区。同样的品牌在不同的时间和不同的时间尺度上看起来也会有很大的不同：白天和晚上、季节效应、自然灾害、时尚设计潮流等等。<br><I>特征相同，标签不同（概念飘移）：</I>即使 $\mathcal{P}(X)$ 是相同的，对于不同客户端上的条件分布 $P_i(y |x)$ 也可能不同。由于个人偏好，训练数据项中的相同特征向量可能具有不同的标签。例如，反映情绪或单词联想的标签有着个人和地区差异。<br><I>数量倾斜或者不平衡：</I>不同的客户可以拥有着样本数量差异很大的数据。</p><p>  在现实世界中，联邦数据集可能同时包含多个上述影响，同时如何去刻画现实世界中的不同客户端之间的数据集的分布是一个重要的开放性问题。大多数关于合成的non-IID数据集的实证工作(例如[289])都集中在标签分布倾斜上，在这种情况下，non-IID数据集是通过基于标签划分现有数据集的“平面”而形成的。为了更好地理解真实世界的non-IID数据集的性质，我们允许构建受控的但真实的non-IID数据集，用于测试算法和评估它们对不同程度的客户端异构的恢复力。<br>  此外，对于不同的non-IID分布可能需要制定不同的缓解策略。例如：在特征分布倾斜的情况下，因为 $\mathcal{P}(y|x)$ 被假设是共同的，这个问题至少在理论上是很清楚的，训练一个全局模型去学习 $\mathcal{P}(y|x)$ 将是合适的。当同一个特征在不同的客户端上被映射到不同的标签上时，某种形式的个性化（详见3.3）可能对学习真正的标签函数很重要。</p><p><b>违反独立性：</b> 在训练过程中，只要概率分布$\mathcal{Q}$发生变化，就会其导致违反独立性。举一个具有代表性的例子：在跨设备联邦学习中，设备通常需要满足特定的要求才能够参与训练（详见1.1.2）。设备通常在本地的夜间时间满足这些要求（当它们大概率在充电、使用免费wi-fi和空闲时），因此设备可用性可能存在明显的昼夜不同。更进一步的，因为当地的时间直接对应着经度，因此数据的来源就存在着非常大的地理偏见。Eichner等人[151]描述了这个问题和一些缓解策略，但是仍然有许多问题是待解决的。<br><b>数据集飘移：</b> 最后，我们注意到了分布 $\mathcal{Q}$ 和 $\mathcal{P}$ 对于时间的依赖性可能引入传统意义上的数据集偏移（训练集和测试集的分布不同）。此外，其他的条件可能会使有资格训练联合模型的客户端集合与模型被部署的客户端集合不同。例如，训练比推理可能要求设备拥有更大的内存。这些问题将在第6节被更深入的探讨。采取技术来解决数据集飘移对于联邦学习来说是另一个有趣的开放性问题。</p><h3 id="3-1-1处理Non-IID数据的策略"><a class="header-anchor" href="#3-1-1处理Non-IID数据的策略">¶</a>3.1.1处理Non-IID数据的策略</h3><p>  联邦学习的最初目标是在所有客户端数据集的并集上训练单个全局模型，而non-IID的数据则使其变得更加困难。一个自然的方法就是修改现有的算法（例如：通过不同的参数选取）或者探索一种新的方法更高效地达到这个目标。本节的3.2.2将讨论这方法。<br>  对于某些应用程序，可能可以增加数据以使不同客户端的数据更加相似。一种方法是创建一个可以全局共享的小数据集。这个数据集可能来自一个公开可用的代表性数据源，一个不涉及隐私敏感的独立于客户数据的数据集，或者可能是原始数据的蒸馏结果（参考Wang等人[404]）。<br>  客户目标函数的异构性使得如何构建目标函数的问题变得更加重要——现在已经不清楚平等地对待所有的样本是否是有意义的。替代方案包括：限制任何一个用户的数据贡献（这对隐私也很重要，见第4节），并在客户端之间引入其他公平概念；参见第6节中的讨论。<br>  但是，如果我们能够在每个设备上的本地数据上运行训练（这对于全局模型的联合学习是必要的），那么训练单个全局模型是否是正确的目标呢？在许多情况下，使用单个模型是首选地，例如：为了向没有数据的客户端提供模型，或者在为了在部署之前允许进行人工验证和质量确认。然而，由于本地训练是可能的，因此每个客户都有一个定制的模型是可行的。这种方法可以把non-IID问题从一个bug变成一个特性，几乎是字面上的意思，即因为每个客户端都有自己的模型，客户端能够独立地参数化模型，看起来有些病态但缺让non-IID变得不那么重要。例如：对每一个 $i$，$\mathcal{P}_i(y)$ 只支持一个标签，那么找到一个高精度的全局模型可能是非常具有挑战性的（特别是当 $x$ 的信息相对不足时），但是训练一个高精度的局部模型是微不足道的（只需要一个持续的预测）。这种多模型方法将在第3.3节中深入讨论。除了解决非独立的客户端分布之外，使用多个模型还可以解决由于客户端可用性变化而导致的违背独立性的问题。例如，Eichner等人[151]的方法运行单个训练，但对不同的迭代进行平均，并基于时区/经度为客户端的推断提供不同的模型。</p><h2 id="3-2-联邦学习的优化算法"><a class="header-anchor" href="#3-2-联邦学习的优化算法">¶</a>3.2 联邦学习的优化算法</h2><p>  在典型的联邦学习任务中，目标是学习单个全局模型，该模型最小化整个训练数据集上的经验风险函数，训练集数据为所有客户端数据的并集。联邦优化算法和标准分布式训练方法之间的主要区别是需要处理表格 1中的特征——对于优化需要特别关注：non-IID和不平衡的数据、有限的通信带宽、不可靠和有限的可用设备。<br>  当联邦学习在设备的总数非常庞大时（如：跨移动设备），算法每轮只需要一些客户端参与（客户端采样）。此外，每个设备都可能多次参加训练给定的模型，因此算法应当是不状态依赖的。这就排除了直接应用在数据中心上下文中非常有效的各种方法，例如：ADMM之类的有状态优化算法，以及根据前几轮遗留的压缩错误修正更新的有状态压缩策略。<br>  联邦学习算法的在现实中另一个重要考虑是与其他技术的可组合性。优化算法并非在生产部署中独立运行，而是需要与其他技术结合使用，如：第4.2.1节中的加密聚合协议、第4.2.2节的差分隐私(DP)和3.5节中的模型和更新压缩。如第1.1.2节所述，这些技术中有许多可以应用于基本类型，如“对选定的客户机求和”和“向选定的客户机广播”，以这些基本形式表达的优化算法提供了一个有价值的关注点分割，但也可能排除某些技术，例如：通过异步更新。<br>  联邦学习最常用的优化方法之一是联邦平均算法[289]，它适用于本地更新或并行的SGD。在这里，每个客户机在本地运行一些SGD步骤，然后对更新后的本地模型求平均值，以在协同服务器上形成更新的全局模型。伪代码在算法1中给出。<br>  执行本地更新并减少与中央服务器的通信频率，解决了在面对数据位置限制和移动设备客户机有限通信能力情况下的核心挑战。然而，从优化理论的角度来看，这类算法也带来了一些新的算法挑战。在第3.2节中，我们分别讨论了数据跨客户端分布IID和non-IID情况下，联邦优化算法的最新进展和面临的挑战。开发专门针对联邦学习场景特征的新算法仍然是一个重要的开放问题。<br><img src="/2022/09/06/advances-and-open-problems-in-federated-learning-yue-du-bi-ji/1.png" alt="联邦平均算法"></p><h3 id="3-2-1-优化算法和IID数据集的收敛率"><a class="header-anchor" href="#3-2-1-优化算法和IID数据集的收敛率">¶</a>3.2.1 优化算法和IID数据集的收敛率</h3><p>  虽然可以对正在优化的每个客户机函数作出各种不同的假设，但最基本的划分是假设IID和non-IID数据。在形式上，在客户端上具有IID数据意味着，用于客户端本地更新的每个mini-batch数据在统计上都与整个训练数据集（所有客户端上本地数据集的并集）的均匀抽取样本（有放回）相同。由于客户独立地收集他们自己的训练数据，这些数据在大小和分布上都有所不同，而且这些数据不与其他客户或中心节点共享，因此IID的假设在实践中几乎不可能成立。但是，这个假设极大地简化了联邦优化算法的理论收敛性分析，并给出了了一个基准线，可以用来理解non-IID数据对优化率的影响。因此，第一步自然是了解IID数据情况下的优化算法。<br>  在形式上，IID的设定让我们能够标准定义随机优化问题<br>$$<br>\min _{x \in \mathbb{R}} F(x):=\underset{z \sim P}{\mathbb{E}}[f(x ; z)]<br>$$</p><p>  我们假设一个间歇性的通信模型，如Woodworth等人[411，第4.4节]，其中$M$个无状态客户端参与每一轮T，在每一轮中，每个客户端可以计算 $K$ 个样本（如mini-batch）的梯度 $z_1,…,z_K$ 从 $\mathcal{P}$ 中IID采样得到（可能使用这些来进行连续的步骤）。在IID数据的假设中，客户端是可互换的，我们可以不失一般性地假设 $M=N$。表格4中，总结了本节中的符号。<br>  对 $f$ 的不同假设会产生不同的保证。我们将首先讨论凸设置，然后观察非凸问题的结果。<br><b>凸问题的基准线和最高水准：</b> 本节中，我们总结了 $H$-平滑的收敛结果，凸（但不一定是强凸）函数的假设下的方差随机梯度的边界为 $\sigma^2$ 。更正式地说，$H$-平滑意味着对于所有 $z,f(\cdot;z)$ 都是可微的，并且具有H-Lipschitz梯度，也就是说，对于任意 $x,y$ 的<br>$$<br>|\nabla f(x, z)-\nabla f(y, z)| \leq L|x-y|<br>$$<br>我们还假设对于所有的 $x$，随机梯度 $\bigtriangledown_x f(x;z)$ 满足：<br>$$<br>\underset{z \sim P}{\mathbb{E}}|\nabla x f(x ; z)-\nabla F(x)| \leq \sigma^{2}<br>$$<br>当求算法在 $T$ 次迭代后输出 $x_T$ 的收敛率时，我们考虑公式：<br>$$<br>\mathbb{E}\left[F\left(x_{T}\right)\right]-F\left(x^{<em>}\right)<br>$$<br>其中 $x^</em>=min_xF(x)$ 。这里讨论的所有收敛速度都是这一项的上界。 表5给出了这些函数的收敛结果的汇总。<br>  联邦平均法（又称并行SGD/本地SGD）自然地需要和两个基准线进行对比：首先，我们可以在每一轮本地更新中固定 $x$，并计算当前$x$总的 $KM$ 梯度，以加速的mini-batch数据SGD的运行。令 $\bar{x}$ 表示该算法 $T$ 次迭代的平均值。对于凸问题[256, 119, 132]，我们可以得到上界：<br>$$<br>\mathcal{O}\left(\frac{H}{T^{2}}+\frac{\sigma}{\sqrt{T K M}}\right)<br>$$<br>请注意，在训练过程中我们也需要将$z$的随机性考虑到第一项期望的计算中。<br>  第二个自然的基准线是仅考虑所有 $M$ 个活动客户端中的1个客户端，这允许(加速的)SGD连续执行 $KT$ 步。结合上述相同的一般界限，此方法提供的上界为：<br>$$<br>\mathcal{O}\left(\frac{H}{(T K)^{2}}+\frac{\sigma}{\sqrt{T K}}\right)<br>$$<br>  比较这两个结果，我们可以看到mini-batch数据SGD达到了最佳的“统计”项 $(\sigma / \sqrt{T K M})$ ，而对于单客户端SGD（忽略其他设备的更更新）获得了最佳的“优化”项 $(H/ \sqrt{(HK)^2})$ 。<br>  局部更新梯度下降方法的收敛性分析是当前研究的一个非常活跃领域[370,271,428,399,334,318,233]。本地更新梯度下降的第一个收敛结果最早可以追溯到Stich[370]对于强凸目标函数和Yu等人[428]对于非凸目标函数的有界梯度范数假设。这些分析使用次优优化项从而达到目标的 $\sigma/ \sqrt{TKM}$ 的统计项（表5总结了凸函数的中间条件的结果）。<br><img src="/2022/09/06/advances-and-open-problems-in-federated-learning-yue-du-bi-ji/2.png" alt="凸函数的中间部分总结结果"></p><p>  Wang和Joshi[399]、Stich和Karimireddy[371]去除了有界梯度的假设，进一步将优化项改进为 $HM/T$ 。结果表明，当局部步数 $K$ 小于 $T/M^3$ 时，最优统计项占主导地位。然而，对于一般的跨设备应用程序，我们可能有 $T=10^6$ 和 $M=100$ (表2)，这意味着 $K = 1$。<br>  在文献中，收敛边界经常伴随着关于可以选择多大的 $K$ 以渐近地达到与mini-batch数据SGD收敛速度相同的统计项的讨论。对于强凸函数，Khaled等人[233]改进了这个边界，Stich和Karimireddy[371]进一步改进了这个边界。<br>  对于非凸目标，Yu等[428]表明，局部更新 $K$ 小于 $T^{1/3}/M$ 时，局部梯度下降可以达到 $1/ \sqrt{TKM}$ 的渐近误差边界。Wang和Joshi [399]进一步改进了收敛性保证，他们删除了有界梯度范数假设，并表明本地更新的数量可以大到 $T/M^3$。[399]中的分析也可以应用于具有局部更新的其他算法，从而第一个为具有局部更新的去中心的SGD（或周期性分散SGD）和弹性平均SGD提供收敛保证[432]。Haddadpour等[191]改进了Wang和Joshi [399]中要求Polyak-Lojasiewicz（PL）的条件[226]的函数的界线，这是强凸性质的推广。Haddadpour等[191]研究表明，对于PL函数，每轮 $T^2/M$ 的局部更新可以达到  $\mathcal{O}(1/TKM)$ 收敛。<br>  尽管以上工作着眼于随着迭代迭代错误率的收敛情况，然而从业者最关注时间上的收敛速度。评估时必须根据通信和本地计算的相对成本，考虑参数的设计对每次迭代所花费时间的影响。从这个角度来看，在保持统计速率的同时关注于可以使用的最大$K$可能不是联邦学习中的主要关注点，在联邦学习中，人们可能会假设几乎是无限的数据集（非常大的N）。增加 $M$ 的成本（至少在时间方面）很小，因此，适当地增加$M$以匹配优化项，然后调整 $K$ 来最大化时间性能可能更自然。那么如何选择 $K$ ？增加客户端在聚合之前的本地迭代的更新次数，本地模型被平均时的差异也会随之增加。这就会导致，在训练损失方面的误差收敛相比顺序SGD的$TK$步迭代更慢。然而，执行更多的本地更新可以节省大量的通信成本并减少每次迭代所花费的时间。最优的局部更新次数在这两种现象之间取得了平衡，实现了最快的误差和时钟时间的收敛。Wang和Joshi[400]提出了一种自适应的通信策略，该策略根据训练过程中的训练损失，在一定的时间间隔内对$K$进行调整。<br>  联邦学习中的另一个重要设计参数是模型聚合方法，该方法用于使用选定客户端进行的更新来更新全局模型。在最初的联邦学习论文中，McMahan等人[289]建议对本地模型进行加权平均，与本地数据集的大小成比例。对于IID数据，假定每个客户端都有一个无限大的数据集，这可以简化为对本地模型进行简单的平均。但是，尚不清楚此聚合方法是否为最快的错误收敛方法。<br>  即使在使用IID数据的情况下，联邦优化中还有许多悬而未决的问题。<br>  Woodworth等人[411]强调了与联邦学习设置的优化上限和下限之间的一些差距，特别是对于“间隔通信图”，它包含了本地SGD方法，但是不清楚这种方法的收敛速率和对应的下限。在表5中，我们展示了对于凸设定下的收敛结果。虽然大多数方案都能够达到渐近显性统计项，但没有一个方案能够与加速mini-batch SGD的收敛速度相匹配。联邦平均算法是否能够拉近这个距离仍然是一个问题。<br>  所有 $M$ 个客户端执行相同数量的本地更新的本地更新SGD方法可能会遇到一个常见的可伸缩性问题，即如果任何一个客户端意外地速度慢或失败，则它们就可能会成为瓶颈。可以使用多种方法来解决此问题，但尚不清楚哪种方法是最佳的，尤其是在考虑到潜在的偏差时（请参见第6节）。Bonawitz等[74]建议为客户提供过多的资源（例如，向130万个客户请求更新），然后接受收到的前$M$个更新，并拒绝后续掉队者的消息。稍微复杂一点的解决方案是固定一个时间窗口，并允许客户端在此期间尽可能多地进行本地更新$K_i$轮，然后由中央服务器平均其模型。解决客户端掉队者问题的另一种方法是在 $\tau$ 处固定本地更新的数量，但允许客户端以同步或无锁方式更新全局模型。尽管一些先前的工作[432，267，143]提出了类似的方法，但是误差收敛分析是一个开放且具有挑战性的问题。但是，联邦学习环境中的一个更大挑战是，从第3.2节开始讨论起，异步方法可能会变得难以与差异性隐私或安全聚合之类的互补技术结合起来。<br>  除了本地更新的数量外，每次训练选择的客户群大小的选择与本地更新的数量存在类似的折中点。更新并平均更大数量的客户端可以让每个训练回合的模型产生更好的收敛性，但是由于与客户端进行的计算/通信中的不可预测的尾部延迟，使得训练速度容易受到下降。<br>  在non-IID设定中对本地SGD/联邦平均计算的分析更具挑战性，在下一章中将讨论与其相关的结果和未解决的问题，以及直接解决non-IID问题的专用算法。</p><h3 id="3-2-2-优化算法和IID数据集的收敛率"><a class="header-anchor" href="#3-2-2-优化算法和IID数据集的收敛率">¶</a>3.2.2 优化算法和IID数据集的收敛率</h3><p>  相比中心学习中经过充分随机而得到的独立且同分布的（IID）样本，联邦学习的样本使用来自最终用户设备的本地数据，从而产生了多种non-IID数据（第3.1节）。<br>  在这种假设下，对于 $N$ 个客户端都拥有自己的本地数据分布 $\mathcal{P}<em>i$ 和本地目标函数：<br>$$<br>f</em>{i}(x)=\underset{z \sim \mathcal{P}<em>{i}}{\mathbb{E}}[f(x ; z)]<br>$$<br>其中 $f(x;z)$ 为模型 $x$ 对于样本 $z$ 的损失。我们通常希望最小化：<br>$$<br>F(x)=\frac{1}{N} \sum</em>{i=1}^{N} f_{i}(x)<br>$$<br>请注意，当 $\mathcal{P}_i$ 是同分布的时候，这就沦落为IID的设定。我们定义 $F^$ 为 $F$ 的的最小值，此时观测值为 $x^$。类似的，我们使用 $f_i^*$ 代表 $f_i$ 的最小值。<br>  像在IID设定中一样，我们假设采用间歇性通信模型（例如Woodworth等人[411,第4.4节]），其中 $M$ 个无状态客户参与$T$轮更新，并且在每个轮次中，每个客户可以计算 $K$ 个样本（例如 mini-batches）的梯度。不同之处在于，样本 $z{i,1},…,z{i,K}$ 由第 $i$ 个客户端的本地数据分布 $\mathcal{P}_i$ 采样而出。与IID设定不同，我们不必假定 $M=N$ ，因为客户端分布并不完全相等。在下文中，如果算法假定 $M=N$ ，我们将忽略 $M$ 并地写作 $N$ 。我们注意到，尽管这样的假设可能与表1中的跨数据孤岛的联邦假设兼容，但在跨设备的假设中通常是不可行的。<br>  尽管[370,428,399,371]主要针对IID假设，但可以通过对数据差异添加假设，例如通过限制客户端梯度与全局梯度[266,261,265,401]之间或客户端之间和全局最优值的差异[264,232]，将分析技巧推广到非IID场景。在这种假设下，Yu等 [429]表明，在non-IID情况下，本地SGD的错误边界变得更糟。为了达到 $1/\sqrt{TKN}$ 的比率（在非凸目标下），本地更新数 $K$ 应该小于 $T^{1/3}/N$ ，而不是像IID情况下的 $T/N^3$ [399]。Li等[261]提出在每个局部目标函数中添加一个近似项，以使该算法对局部目标之间的异质性更加鲁棒。所提出的FedProx算法从经验上提高了联邦平均的性能。但是，目前尚不清楚是否可以证明提高收敛速度。Khaled等[232]假设所有客户都参与，并在客户上使用批量梯度下降，这可能比客户上的随机梯度更快地收敛。<br>  最近，许多工作在放宽所需的假设方面取得了进展，以便更好地应用于联邦平均的实际使用。例如，Lietal[264]研究了在更现实的环境中联邦平均的收敛性，在每一轮中仅涉及一部分客户。为了保证收敛，他们假设选择的客户是随机的，或者是与本地数据集的大小成正比的概率。但是，在实践中，服务器可能无法使用这些理想的方式对客户端进行采样，特别是在跨设备设置中，只有满足资格要求的设备（例如：充电、空闲、免费无线上网）才会被选择参与计算。在一天的不同时间，客户的特征会明显不同。 Eichner等人[151]提出了这个问题，研究了半周期SGD的收敛性，其中从多个具有不同特征的客户区域中按照规则的周期性模式（例如，昼夜）进行采样。<br><img src="/2022/09/06/advances-and-open-problems-in-federated-learning-yue-du-bi-ji/3.png" alt="Non-IID假设"><br><img src="/2022/09/06/advances-and-open-problems-in-federated-learning-yue-du-bi-ji/4.png" alt="其他假设和变量"><br><img src="/2022/09/06/advances-and-open-problems-in-federated-learning-yue-du-bi-ji/5.png" alt="收敛率"><br><b>表6：</b>非iid数据假设下联邦学习中优化方法的收敛性。我们总结了对非iid数据、每个客户机上的本地函数和其他假设的关键假设。我们还介绍了该算法与联邦平均算法的比较，以及消除常数的收敛速度。<br>  我们在表6中总结了最新的理论结果。表6中的所有方法均假定客户端的局部函数具有平滑度或Lipschitz梯度。误差范围由凸函数的最优目标（1）和非凸函数的梯度范数来衡量。对于每种方法，我们展示关键的non-IID假设，每个客户端函数 $f_i(x)$ 的假设以及其他辅助假设。我们还简要地将每种方法描述为联邦平均算法的一种变体，和显示地简化收敛速率消除常数。假设客户端功能是强凸的，则可以提高收敛速度[264,227]。 当客户使用随机局部更新[266,264,265,401,227]时，经常使用有界梯度方差，这是分析随机梯度方法的一种广泛使用的假设。Li等 [264]直接分析联邦平均算法，该算法在每轮随机抽样的 $M$ 个客户端上执行 $K$ 个步骤的本地更新，并提出了本地更新（ $K &gt; 1$）可能减慢收敛速度的速率。证明 $K &gt; 1$ 可能会损害或帮助收敛的领域是一个重要的开放问题。<br><b>与去中心优化的联系：</b>近年来，在去中心优化社区中研究了联邦优化的目标函数。如Wang和Joshi [399]所示，去中心SGD的收敛分析可以与本地SGD结合使用，也可以与网络拓扑矩阵（混合矩阵）的设定适当结合使用。为了减少通信开销，Wang和Joshi [399]提出了周期性去中心SGD（PD-SGD），它允许去中心SGD使用多次本地更新作为联邦平均。Li[265]等人将此算法进行了推广到了non-IID情况。 MATCHA [401]通过随机采样客户端进行计算和通信进一步提高了PD-SGD的性能，并提供了一种收敛分析，表明本地更新可以加速收敛。<br><b>加速和方差减少技术：</b>对于一阶优化方法，动量和方差减少是改善优化和泛化性能的有效的方法。但是，关于如何将动量或减少方差的技术应用于本地SGD和联邦平均，仍未达成共识。SCAFFOLD [227]用控制变量显式地模拟客户端更新中的差异以执行方差减少，这可以快速收敛而不会限制客户端数据分布的差异。至于动量方案，Yu等[429]建议让每个客户保持一个局部动量缓冲区，并在每个通信回合中平均这些局部缓冲区以及局部模型参数。尽管此方法从经验上提高了本地SGD的最终准确性，但它需要两倍的通信成本。Wang等[402]提出了另一种称为SlowMo的动量方案，该方案可以显着提高本地SGD的优化和泛化性能，而无需牺牲吞吐量。Hsu等[206]提出了一种类似于SlowMo的动量方案。 [429,402]均证明了局部SGD的动量变体可以以与同步mini-batch SGD以相同的速率收敛到非凸目标函数的平稳点，但要证明动量能加快联邦假设下的收敛速度是充满挑战性的。</p><h2 id="3-3-多任务学习，个性化和元学习"><a class="header-anchor" href="#3-3-多任务学习，个性化和元学习">¶</a>3.3 多任务学习，个性化和元学习</h2><p>  在本节中，我们考虑各种“多模型”方法——对于不同的客户端在推断的时候可以高效地使用不同的模型。当面对non-IID数据（第3.1节）时，这些技术尤其重要，因为它们可能优于潜在的全局共享最优模型。我们注意到，个性化已经在完全去中心的设定下也得到了一定的研究[392,54,431,22]，在这种情况下，训练个体模型尤为自然。</p><h3 id="3-3-1-通过特征个性化"><a class="header-anchor" href="#3-3-1-通过特征个性化">¶</a>3.3.1 通过特征个性化</h3><p>  本节的其余部分专门考虑了不同用户在使用不同模型参数（权重）进行运行推断时的技术需求。但是，在某些应用程序中，只需将用户和上下文功能添加到模型中，即可获得相近的收益。例如，考虑一下Hard等人[196]中用于移动键盘中下一个单词预测的语言模型。不同的客户端可能使用不同的语言，实际上，模型参数的设备上个性化已为该问题带来了显着改善[403]。但是，一种更加完善的方法可能是训练一个联邦模型，该模型不仅要输入到目前为止用户输入的单词，还要输入各种其他用户和上下文特征作为输入，例如：该用户经常使用哪些单词？ 他们当前正在使用什么应用程序？ 如果他们正在聊天，他们之前曾向此人发送过哪些消息？ 适当地加以个性化，这样的输入可以允许共享的全局模型产生更好的个性化预测。 但是，由于很大程度上很少有公共数据集包含此类辅助功能，因此探索如何有效合并不同任务上下文信息的模型结构仍然是一个重要的开放问题，有可能极大地提高联邦学习训练的模型的实用性。</p><h3 id="3-3-2-多任务学习"><a class="header-anchor" href="#3-3-2-多任务学习">¶</a>3.3.2 多任务学习</h3><p>  如果人们将每个客户的本地问题（本地数据集上的学习问题）视为一项单独的任务（而不是单个数据集的一个划分），那么多任务学习[433]的技术将立即变得有意义。值得注意的是，史密斯等[362]引入了用于多任务联合学习的MOCHA算法，直接解决了通信效率、掉队者和容错的挑战。在多任务学习中，训练过程的结果是每个任务得到一个模型。 因此，大多数多任务学习算法都假设所有客户（任务）都参与每个训练周期，并且由于每个客户都在训练一个单独的模型，因此也要求客户有自己的状态。 这使得此类技术与数据孤岛联邦学习应用相关性更高，但在跨设备方案中更难应用。<br>  另一种方法是重新考虑客户（本地数据集）和学习任务（待训练的模型）之间的关系，对于每个客户端观察单个模型和全局模型的共同点。例如，可能可以应用来自多任务学习的技术（以及其他方法，如个性化，将在接下来进行讨论），其中我们将“任务”作为客户端的子集，也许是显示选择的（例如，基于地理区域与设备或者用户的特征），或者可能基于在客户端上学习到的聚类或学习到的图的连接结构[431]。这些算法的发展是一个重要的开放问题。请参阅第4.4.4节，讨论关于稀疏的联邦学习问题（例如，在这种类型的多任务问题中自然会产生的问题）的解决方式，而不必揭示每个客户端所属的客户端子集（任务）。</p><h3 id="3-3-3-本地微调和元学习"><a class="header-anchor" href="#3-3-3-本地微调和元学习">¶</a>3.3.3 本地微调和元学习</h3><p>  本地微调，我们指的是通过联邦学习训练单个模型，然后将模型部署到所有的客户端中，并在被用于推断前使用本地的数据集通过额外的训练达到个性化的效果。 这种方法自然地融入了联邦学习模型的通常的生命周期（第1.1.1节）。仍然可以在每轮（例如，100秒）中仅使用少量客户样本进行全球模型的培训；部署模型后，仅发生一次向所有客户端（例如数百万个）广播全局模型。唯一的区别是，在使用模型对客户进行实时预测之前，会进行最终的训练，从而将模型为本地数据集进行个性化。<br>  给定一的性能优异的全局模型，对其进行个性化设置的最佳方法是什么？在非联邦学习中，研究人员经常使用微调、迁移学习、域自适应[284,115,56]或者使用本地个性化的模型进行插值。 当然，例如插值等技术，关键在于联邦学习的背景下保证其相应的学习效果。此外，这些技术通常仅假设一对域（源域和目标域），因此可能会丢失联邦学习的一些较丰富的结构。<br>  另一种研究个性化和非个性化的方法是通过元学习来进行，这是一种流行的模型适应设定。 在标准的learning-to-learn（LTL）设置中[52]，它对任务上具有一个元分布，用来学习一个学习算法的样本，例如通过发现参数空间的好的约束。 这实际上很好的对应了第3.1节中讨论的统计设定，其中我们对客户端（任务） $i\sim \mathcal{Q}$ 进行采样，然后从$\mathcal{P_i}$采样该客户端（任务）的数据。<br>  最近，已经开发了一种称为模型不可知元学习（MAML）的算法，即元学习全局模型，它可以仅使用几次局部梯度迭代作为学习适合于给定任务的良好模型的起点。 最值得注意的是，流行的Reptile算法[308]的训练阶段与联邦平均[289]密切相关，即Reptile允许服务器的学习率，并且假设所有客户端都拥有相同数量的数据，但其他都是相同的。Khodaketal等人[234]和Jiang等人[217]探索了FL和MAML之间的联系，并展示了MAML的假设是一个可以被联邦学习用于性化模型的相关框架。其他和差分隐私的关系在[260]中被研究。<br>  将FL和MAML的思想相结合的总体方向是相对较新的，存在许多未解决的问题：</p><ol><li>监督任务的MAML算法评估主要集中在合成图像分类问题上[252,331]，其中可以通过对图像类别进行下采样来构造无限的人工任务。用于模拟FL实验的现有数据集建模的FL问题（附录A）可以作为MAML算法的现实基准问题。</li><li>观察到的全局准确性与个性化准确性之间的差距[217]提出了一个很好的论据，即个性化对于FL至关重要。但是，现有的工作都没有清楚地阐明用于衡量个性化表现的综合指标。例如，对于每个客户来说，小的改进是否比对一部分客户的更大改进更好？相关讨论，请参见第6节。</li><li>Jiang等[217]强调了一个事实，即具有相同结构和性能但经过不同训练的模型可以具有非常不同的个性化能力。尤其是，以最大化全局性能为目标去训模型似乎实际上可能会损害模型的后续个性化能力。理解这个问题的根本原因和FL社区与更大的ML社区都相关。</li><li>在此多任务/LTL框架中，已经开始研究包括个性化和隐私在内的几个具有挑战性的FL命题[234,217,260]。是否还可以通过这种方式分析其他例如概念漂移的问题，比如作为终身学习中的问题[359]？</li><li>非参数传递LTL算法（例如ProtoNets [363]）是否可以用于FL？</li></ol><h3 id="3-3-4-何时进行全局FL训练更好"><a class="header-anchor" href="#3-3-4-何时进行全局FL训练更好">¶</a>3.3.4 何时进行全局FL训练更好</h3><p>  哪些是联邦学习可以为你做，而在一个设备上进行本地学习是做不了的？当本地数据集很小且数据为IID时，FL显然具有优势，实际上，联邦学习[420,196,98]的应用实际受益于跨设备训练单个模型。另一方面，给non-IID的分布的类型（例如，$\mathcal{P}<em>{i}(y|x)$ 跨客户端是完全不同的），则局部模型会更好。因此，一个自然的理论上的问题是确定在什么条件下共享全局模型比独立每设备模型更好。假设我们为每个客户机 $k$ 训练一个模型 $h_k$，使用该客户机可用的大小为 $m_k$ 的样本。我们能保证 $h</em>{FL}$ 通过联邦学习学习到的模型在用于客户 $k$ 时至少和 $h_k$ 一样准确吗?我们能量化通过联邦学习可以期望多大的改进吗?我们能否在理论上保证至少与自然基线（ $h_k$ 和 $h_{FL}$ ）的表现相匹配的个性化策略?<br>  其中一些问题与先前在多源域适应和不可知联合学习方面的工作有关[284,285,203,303]。这些问题的难易程度取决于各方之间的数据分配方式。例如，如果数据是垂直切分的，则每一方都维护有关公共实体的不同功能集的私有记录，则这些问题可能需要解决联邦学习任务中的记录链接[108]。独立于私下进行记录链接的最终技术要求[348]，该任务本身在现实世界中恰好有很大的噪声倾向[347]，只有很少的结果讨论了它对训练模型的影响[198]。可以在有监督的学习中使用损失分解技巧来缓解垂直划分假设本身，但实际的好处取决于数据的分布和参与方的数量[320]。</p><h2 id="3-4-使用于联邦学习的ML工作流"><a class="header-anchor" href="#3-4-使用于联邦学习的ML工作流">¶</a>3.4 使用于联邦学习的ML工作流</h2><p>  在将标准机器学习的工作流和流水线（包括数据扩充、功能工程、神经网络结构设计、模型选择、超参数优化和调试）适应去中心数据集和资源受限的移动设备时，会遇到许多挑战。我们在将下面讨论其中一些挑战。</p><h3 id="3-4-1-超参数调整"><a class="header-anchor" href="#3-4-1-超参数调整">¶</a>3.4.1 超参数调整</h3><p>  在资源有限的移动设备上使用不同的超参数进行多轮培训可能会受到限制。对于小型设备，这可能导致过度使用有限的通信和计算资源。但是，最近的深度神经网络在很大程度上依赖于有关神经网络的结构、正则化和优化的超参数选择。对于大型模型和大规模设备上的数据集，评估可能会很昂贵。在AutoML [339,237,241]的框架下，超参数优化（HPO）历史悠久，但它主要涉及如何提高模型的准确性[59,364,321,159]，而不是针对移动设备的通信和计算效率。因此，我们期望在联邦学习的背景下，进一步的研究应考虑研发解决方案，以实现高效地超参数优化。<br>  除了通用方法来解决超参数优化问题外，对于特殊的训练空间去针对性地去发展容易调整的优化算法也是一个主要的开放领域。中心式训练已经需要调整学习率、动量、批量大小和正则化等参数。联邦学习可能会添加更多的超参数，如：分别调整聚合/全局模型更新规则和本地客户端优化程序、每轮选择的客户端数量、每轮本地步骤的数量、更新压缩算法的配置等等。除了更高维度的搜索空间之外，联邦学习通常还需要更长的训练时间并受限于有限的计算资源。应该通过对超参数设置具有鲁棒性的优化算法（相同的超参数值适用于许多不同的现实世界数据集和网络结构）以及自适应或自调整[381,75]算法来解决这一挑战。</p><h3 id="3-4-2-神经结构设计"><a class="header-anchor" href="#3-4-2-神经结构设计">¶</a>3.4.2 神经结构设计</h3><p>  我们建议研究人员和工程师在联邦学习环境中探索神经体系结构搜索（NAS）。这是由于当前使用预定的深度学习模型的方法的缺陷引起的：当用户生成的数据对模型开发人员不可见时，深度学习模型的预定网络结构可能不是最佳的设计选择。例如，神经体系结构可能具有特定数据集的某些冗余组件，这可能导致设备上不必要的计算。对于non-IID数据分布，可能会有更好的网络体系结构设计。第3.3节中讨论的个性化方法仍在所有客户端之间共享相同的模型架构。NAS的最新进展[332,154,333,55,322,273,417,154,279]提供了解决这些缺陷的潜在方法。NAS有三种主要方法，它们利用进化算法、强化学习或梯度下降来搜索特定数据集上特定任务的最佳架构。其中，基于梯度的方法利用权重共享的高效梯度反向传播，将架构搜索过程从超过3000个GPU一天减少到只用1个GPU一天。最近发表的另一篇有趣的论文涉及权重不可知神经网络[170]，声称仅神经网络架构，无需学习任何权重参数，就可以为给定任务提供编码解决方案。如果该技术进一步发展并得到广泛使用，则可以将其应用于联邦学习而无需在设备之间进行协作训练。尽管尚未针对分布式设定（例如联帮学习）开发这些方法，但将它们全部转换为联邦设定都是可行的。因此，我们认为在联邦学习环境中针对全局或个性化模型的神经体系结构搜索（NAS）是有希望的研究方向。</p><h3 id="3-4-3-联邦学习的调试和可解释性"><a class="header-anchor" href="#3-4-3-联邦学习的调试和可解释性">¶</a>3.4.3 联邦学习的调试和可解释性</h3><p>  尽管联邦模型联邦训练取得了实质性进展，但这完全是ML工作流的一部分。经验丰富的建模人员可以直接检查子数据集的任务，包括基本的健全性检查、调试错误分类\发现异常值\手动标记样本或检测训练集中的偏差。开发隐私保护技术来解决此类去中心的问题是主要的开放性问题上。最近，Augensteinetal[32]提出了使用经过联帮学习训练的差分生成模型（包括GAN）的使用，以回答此类问题。但是，仍然存在许多悬而未决的问题（请参见[32]中的讨论），特别是改进FL DP生成模型的精确度的算法的开发。</p><h2 id="3-5-通信和压缩"><a class="header-anchor" href="#3-5-通信和压缩">¶</a>3.5 通信和压缩</h2><p>  现在，众所周知，通信可能是联邦学习的主要瓶颈，因为无线连接和其他最终用户互联网连接的运行速率通常低于数据中心内或数据中心间连接的速率，并且可能昂贵且不可靠。这引起了最近对减少联邦学习的通信带宽的极大兴趣。联邦平均和模型更新的量化和量化到少量比特的方法已经证明，通信成本显着降低，并且对训练精度的影响最小[245]。但是，尚不清楚是否可以进一步降低通信成本，以及这些方法中的任何一种或其组合是否可以接近在联邦学习中的通信和准确性之间提供最佳的折衷。描述这种精确性和通信量之间的基本平衡是理论统计学最近的研究热点[434,81,195,11,47,380]。这些工作描述了在通信约束下用于分布式统计估计和学习的最佳最小极大速率。然而，由于这些理论工作通常忽略了优化算法的影响，因此很难在实践中从这些理论工作中得出具体的结论来减少通信带宽。利用这种统计方法来指导实际的训练方法仍然是一个开放的方向。<br><b>压缩目标：</b> 由于当前设备中计算机、内存和通信资源的限制，有几个不同的具有实用价值的压缩目标如下：</p><ol><li>梯度压缩，减少从客户端到服务器通信的对象的大小，该对象用于更新全局模型；</li><li>模型广播压缩，减小从服务器向客户端广播的模型的大小，客户端从该模型开始本地训练；</li><li>减少本地计算，修改整体训练算法，使本地训练过程在计算上更加高效。</li></ol><p>  这些目标在大多数情况下是互补的。其中，(1)在总运行时间方面具有最大的实际影响潜力。这是因为客户端连接的上传速度通常比下载速度慢，因此与(2)相比，可以获得更多的带宽，也因为在许多客户端上平均的效果可以实现更积极的有损压缩方案。通常，(3)与(1)和(2)通过特定的方法共同实现。<br>  许多现有的文献适用于目标(1)[245, 376, 244, 20, 204]。(2)对一般收敛性的影响直到最近才得到研究，在[231]中有一个有限的分析。Caldas等人[87]提出了一种联合处理所有(1)、(2)和(3)的方法，该方法通过约束所需的模型更新，使得只有模型变量特定的元素需要在客户端被使用。<br>  在跨设备FL中，算法通常不能假设客户端上保留了任何状态（表1）。但是，在跨数据孤岛FL设置中通常不存在这种约束，在跨设备FL设置中，相同的客户端重复参与。因此，一些更广泛的关于错误修正的思想，如[272,346,396,380,228,371]在这种情况下是相关的，其中许多可以同时处理(1)和(2)。<br>  另一个目标是修改训练程序，以使最终模型更紧凑或更有效地进行推理。这个主题在更大的ML社区中得到了很多关注[194,120,436,270,312]，但是这些方法或者没有直接对应到联邦学习，或者使训练过程更加复杂，这使得它变得很难采纳。同时产生一个紧凑的最终模型的研究，也同时解决了上述三个目标，具有产生实际影响的巨大潜力。<br>  对于梯度压缩，依据最小的最大感知量出现了一些现有的工作[376]，以表征最坏的情况。然而，通常在信息论中，压缩保证是特定于实例的，并取决于基础分布的熵[122]。换句话说，如果数据易于压缩，则可以证明它们被大量压缩。 有趣的是，是否可以为梯度压缩获得类似的实例特定结果。同样，最近的工作表明，以数据相关的方式学习压缩方案可以显着提高数据压缩[412]和梯度压缩的压缩率。因此，值得在联邦设定中评估这些与数据相关的压缩方案[171]。<br><b>差分隐私和安全聚合的兼容：</b> 联邦学习中使用的许多算法，例如安全聚合[72]和使噪声变淡以实现差分隐私[7，290]的机制，都没有设计用于压缩或量化通信。例如，Bonawitz等人的Secure Aggregation协议的直接应用。 [73]要求每个标量有一个额外的 $O(logM)$ 通信位，其中 $M$ 是要累加的客户端数，这可能会使$M$大时更新的主动量化无效（尽管更有效的方法请参见[75]）。现有的噪声添加机制假定在每个客户端上添加实值高斯或拉普拉斯噪声，这与用于减少通信的标准量化方法不兼容。我们注意到，最近的一些工作允许有偏估计，并且可以很好地与Laplacian噪声[371]一起使用，但是无论如何都不会放弃差分隐私，因为它们在两轮之间具有独立性。在增加离散噪声方面有一些工作[13]，但目前还不清楚这些方法是否最佳。 因此，联邦设定下具有兼容性和安全性的压缩方法是一个有价值的开放问题。<br><b>无线联邦学习协同设计：</b> 联邦学习中的现有文献通常忽略了模型训练期间无线通道动态的影响，这有可能破坏训练周期，从而破坏整个生产系统的可靠性。特别是，无线干扰，嘈杂的信道和信道波动会严重阻碍服务器与客户端之间的信息交换（或直接在单个客户端之间进行信息交换，如在完全分散的情况下，请参阅第2.1节）。对于任务关键应用程序而言，这是一项主要挑战，其根源在于减少延迟和增强可靠性。应对这一挑战的潜在解决方案包括联邦蒸馏（FD），其中工人交换它们的模型输出参数（logit）而不是模型参数（梯度和/或权重），并通过适当的通信和计算资源来优化工人的调度策略[ 215、316、344]。另一种解决方案是利用无线信道的独特特性（例如广播和叠加）作为自然的数据聚合器，其中，不同工作人员同时传输的模拟波会叠加在服务器上，并由无线信道进行系数权衡[8]。这样可以在服务器上更快地进行模型聚合，并且可以将训练速度加速因子提高到参与者的数量。这与传统的正交频分复用（OFDM）范式形成鲜明对比，在传统的正交频分复用（OFDM）范式中，工人在正交频率上上传其模型，而正交频率的性能会随着参与数量的增加而降低。</p><h2 id="3-6-应用到更多类型的机器学习问题和模型"><a class="header-anchor" href="#3-6-应用到更多类型的机器学习问题和模型">¶</a>3.6 应用到更多类型的机器学习问题和模型</h2><p>  迄今为止，联邦学习主要考虑了监督学习任务，其中每个客户都自然可以获得标签。 将FL扩展到其他ML范式，包括强化学习、半监督和无监督学习、主动学习和在线学习[200，435]都提出了有趣且开放的挑战。<br>  与FL高度相关的另一类重要模型是可以表征预测不确定性的模型。大多数现代深度学习模型无法表示其不确定性，也无法对参数学习进行概率解释。这推动了贝叶斯模型与深度学习相结合的工具和技术的最新发展。从概率论的角度来看，使用单点估计进行分类是不合理的。贝叶斯神经网络[358]已经被提出并显示出对过度拟合更为健壮，并且可以轻松地从小型数据集中学习。贝叶斯方法通过其参数以概率分布的形式进一步提供不确定性估计，从而防止过度拟合。此外，借助概率推理，人们可以预测不确定性如何减小，从而使网络做出的决策随着数据大小的增长变得更加准确。<br>  由于贝叶斯方法相比深度模型在置信度上拥有丰富的经验，并且在许多任务上也能达到最先进的性能，因此人们希望贝叶斯方法能够为经典的联邦学习提供理论上的改进。实际上，Lalitha等人的初步工作[254]表明，合并贝叶斯方法可用于跨non-IID数据和异构平台的模型聚合。但是，必须解决有关可伸缩性和计算可行性的诸多问题。</p><h1>4 privacy</h1><p>  机器学习过程由许多功能不同的角色参与运作。例如，用户可以通过与设备交互来生成训练数据，在机器学习训练过程中，其从这些数据中提取人机交互模式（例如，以训练后模型参数的形式），之后机器学习工程师或分析师可以评估该训练模型的质量，最后可能将该模型部署在最终用户主机上，以支持定制的用户体验（见下图1）。<br>  在一个理想的世界里，系统中的每一个参与者只会学到扮演他们角色所需的信息。例如，如果分析师只需要确定某个特定的质量度量是否超过了所需的阈值，以便授权将模型部署到最终用户，那么在理想化的世界中，该度量值是分析师可以获得的唯一信息；该分析师既不需要访问训练数据，也不需要访问模型参数。类似地，最终用户体验到的可能只需要由经过训练的模型提供的预测，而不需要其他任何内容。<br>  此外，在理想的世界中，系统中的每个参与者都能够轻松、准确地推断出自己和他人的哪些个人信息可能通过参与系统而泄露，参与者将能够利用这一推理结果，就是否参与以及如何参与做出明智的选择。<br>  创建一个具有上述所有理想隐私属性的系统本身将是一项令人望而生畏的壮举，若其还可以实现其他令人满意的属性，则更是难上加难，比如所有参与者的易用性、最终用户体验的质量和公平性（以及影响体验结果的模型），智能地使用通信和计算资源、抵御攻击和失败的能力等。<br>  与其追求无法企及的完美，我们不如另辟蹊径——整个系统由模块化单元组成，这些单元可以相对独立地进行学习和改进，同时我们也要注意，我们最终必须根据上述我们的理想隐私目标，测量整个系统的隐私属性。本节中，我们将提出目前无论是通过单个模块实现方法还是通过整个系统实现方法，还不了解如何同时实现所有目标的领域，作为开放性研究问题。<br><img src="/2022/09/06/advances-and-open-problems-in-federated-learning-yue-du-bi-ji/6.png" alt="图1：fl训练的模型的生命周期和联邦学习系统中的各种参与者。(重复第7页)"><br>  联邦学习提供了一个有吸引力的结构，可以将整个机器学习工作流程分解成我们想要的可实现的模块单元。联邦学习模型的一个主要优点是它可以通过数据最小化为参与的用户提供一定程度的隐私：设备从不发送原始用户数据，只将对模型的更新（例如梯度更新）发送到中央服务器。这些模型更新更侧重于要完成的学习任务而非关注原始数据（即，与原始数据相比，它们严格不包含关于用户的附加信息，而且通常不包含其他意义），并且单个更新只需要由服务器暂时保存。<br>  虽然这些特性可以在集中所有训练数据的基础上提供显著的实用性隐私改进，但是在这个基线联邦学习模型中仍然没有隐私的正式保证。例如，可以构造这样的场景，在该场景中，原始数据的信息从客户端泄漏到服务器。比如，知道以前的模型和用户的梯度更新将允许某一方推断该用户持有的训练示例。因此，本节调查现有的结果，并概述了设计可以提供严格的隐私保障的联邦学习系统的开放性的挑战。我们更专注于联邦学习和分析设置中特定的问题，而不考虑了在一般的机器学习设置中也会出现的问题。<br>  除了针对用户隐私的攻击之外，还有其他种类的对联邦学习的攻击；例如，对方可能会试图阻止模型被学习，或者他们可能会试图使模型产生偏向对方的训练结果。我们稍后在第5章讨论这些类型的攻击的讨论。<br>  本章的其余小节梗概如下。第4.1节讨论了我们希望提供抵御的各种威胁模型。第4.2节列出了一套核心工具和技术，可用于针对第4.1节中讨论的威胁模型提供严格的保护。第4.3节假定可信服务器的存在，并讨论在对抗对手和/或分析师提供保护方面的公开问题和挑战。第4.4节讨论了在没有完全可信服务器的情况下的开放性问题和挑战。最后，第4.5节讨论了关于用户感知的开放性问题。</p><h2 id="4-1-参与者，威胁模型与深层隐私"><a class="header-anchor" href="#4-1-参与者，威胁模型与深层隐私">¶</a>4.1 参与者，威胁模型与深层隐私</h2><p>  在联邦学习中，对隐私风险的规范处理需要一种整体的、跨学科的方法。对于一些风险类型，可以通过将现有技术扩展到指定场景中从而保护隐私和减轻风险，而其他更复杂的风险类型则需要跨学科的协同努力。<br>  隐私不是二进制量，甚至不是标量。这种规范处理的第一步是准确描述不同的参与者（见第1节图1，为方便起见，在第35页重复）及其在模型中扮演的角色，最终确定相关的威胁模型（见表7）。例如，我们希望将服务器管理员的视图与使用所学模型的分析师的视图区分开来，因为可以想象，设计用于针对恶意分析师提供强大隐私保证的系统可能不会提供任何恶意攻击的机会。这些参与者行为模式很好映射到其他文献中讨论的威胁模型上；例如，在Bittau等人中。[67，第3.1节]，“编码器”对应于客户机，“洗牌者”通常对应于服务器，“分析器”可能对应于服务器或分析师完成的后处理。<br>  例如，一个特定的系统可能提供差异性的隐私保证（差异性隐私将在4.2.2节规范化介绍，这里知道更小的ε表示更强的隐私性即可），比如向服务器管理员提供的特定参数为ε，而分析师观察到的参数结果可能具有更高的保护性ε’&lt;ε。<br>  此外，这一保证可能仅适用于能力受到特定限制的对手，例如可以观察服务器上发生的所有事情（但不能影响服务器的行为）的对手，这类对手同时完全控制占客户端总数比例为γ的客户端（“完全控制”即可以观察他们可获知的所有数据并以任意方式影响其行为）；对手也可能被认为无法破解在特定安全级别σ下，实例化的加密机制。为了对抗实力突破这些限制的对手，在服务器管理员的看来可能仍然需要差异性隐私保证，但认为分析师观察的结果在较弱的隐私保护级别ε’&gt;ε。<br>  正如我们在本例中看到的，精确地指定系统的假设和隐私目标，以及诸如差异性隐私保证、老实但好奇行为等安全性概念，可以很容易地通过到几个参数（ε、ε’、ε0、γ、σ等）进行具体实例化。<br>  实现联邦学习所需的所有隐私属性通常需要将下述的许多工具和技术组合到端到端系统中，包括以下两种：多种分层策略都是为了保护系统的同一部分（例如，在可信执行环境（TEE）中运行安全多方计算（MPC）协议的一部分，使对手更难对该组件产生足够大的损害）以及使用不同的策略来保护系统中不同的部分（例如，使用MPC保护模型更新的聚合，然后在服务器之外共享聚合更新之前使用隐私披露技术）。<br>  因此，在两种技术都无法提供其预期隐私保护能力的情况下，我们提倡构建这样一种优美的联邦系统，即尽可能降低隐私性。例如，在TEE中运行MPC协议的服务器组件可能允许维护隐私，即使TEE安全性或MPC安全性假设中的一个（但不是两个）在实践中不成立。另一个例子是，要求客户端向服务器端TEE发送渐变更新，而要求客户端将原始训练示例发送到服务器端TEE将被强烈不推荐。因为一旦TEE的安全性失效，前者的隐私性期望将更优美地降级。我们将这种优美降级的原则称为“深度隐私”，类似于成熟的深度防御网络安全原则[311]。</p><h2 id="4-2-工具与技术"><a class="header-anchor" href="#4-2-工具与技术">¶</a>4.2 工具与技术</h2><p>  一般来说，联邦学习计算的目的是让分析师或工程师通过计算请求获得结果，这可以看作是对分布式客户机数据集上的函数f的评估（通常是机器学习模型训练算法，但可能更简单，例如基本的数据统计）。有三个隐私方面需要解决。<br>  首先，我们需要考虑f是如何计算的，以及在这个过程中中间结果的信息流是什么，它主要影响对恶意客户端、服务器和管理参与者的敏感性。除了设计系统中的信息流（例如提前数据最小化）外，包括安全多方计算（MPC）和可信执行环境（TEEs）等安全计算相关技术对于解决这些问题特别重要。这些技术将在第4.2.1节中详细讨论。<br>  其次，我们必须考虑该计算哪些内容。换言之，f本身的计算结果会向分析师和域内参与者透露了参与客户的多少信息。这与隐私保护披露技术，特别是差异隐私（DP）是高度相关的，将在第4.2.2节中详细讨论。<br>  最后，可验证性也是需要考虑的问题，即客户机或服务器能够向系统中的其他人证明他们已忠实地运行了所需的指令，而不泄露他们运行过程中的潜在隐私数据。验证技术，包括远程认证和零知识证明，将在第4.2.3节中讨论。</p><h3 id="4-2-1-安全计算"><a class="header-anchor" href="#4-2-1-安全计算">¶</a>4.2.1 安全计算</h3><p>  安全计算的目标是评估计算分散输入的函数，通过判断其是否仅向预期各方显示计算结果，而不显示任何附加信息（例如各方的输入或任何中间结果）。<br><b>安全多方计算：</b>安全多方计算（MPC）是密码学的一个子领域，与这样一个问题有关：一组参与方计算其隐私输入通过共识函数得到输出，从而可以只向每个参与方显示期望的输出。这一领域在20世纪80年代由姚[422]开创。由于理论和工程上的突破，该领域已经从单纯的理论研究转向工业上的部署技术[71、70、257、29、169、209、210]。值得注意的是，MPC定义了一组技术，应该更多地被视为安全计算中的领域或安全性的一般概念，而不是技术本身。MPC的一些最新进展可以归因于低级原语的突破，例如不经意传输协议[211]和具有同态性质的加密方案（如下所述）。</p><table border="1"><caption align="top"><b>表4-1 不同的敌对参与者的多种威胁模型<b></b></b></caption><tr><td><b>数据/访问节点</b></td><td><b>参与者</b></td><td><b>危胁模型</b></td></tr><tr><td>客户端</td><td>通过系统设计或破坏设备获得客户端设备的最高访问权限者</td><td>恶意客户端可以检查所参与轮次从服务器接收的全部消息（包括模型迭代），并可以篡改训练过程。老实但好奇的客户端可以检查从服务器接收的所有消息，但不能篡改培训过程。在某些情况下，安全包围/TEEs等技术可能会限制此类攻击者的影响和信息可见性，从而削弱该模型威胁程度。</td></tr><tr><td>服务器</td><td>通过系统设计或破坏设备获得服务器设备的最高访问权限者</td><td>恶意服务器可以检查所有轮次发送到服务器的全部消息（包括梯度更新），并可以篡改训练过程。老实但好奇的客户端可以检查发送到服务器的所有消息，但不能篡改培训过程。在某些情况下，安全包围/TEEs等技术可能会限制此类攻击者的影响和信息可见性，从而削弱该模型威胁程度。</td></tr><tr><td>输出模型</td><td>工程师与分析师</td><td>恶意分析师或模型工程师可以访问系统的多组输出，例如，使用不同超参数的多个训练运行的模型迭代序列。该向这类参与者发布什么信息是一个重要的系统设计问题。</td></tr><tr><td>部署模型</td><td>其他设备</td><td>在跨设备联邦学习场景下，最终模型可能部署到数亿个设备上。访问部分受损的设备可能仍满足黑盒模型，而访问完全受损的设备可以认为是白盒模型。</td></tr></table><p>  密码学解决方案的共同点是，操作通常在一个有限的字段上完成（即，素数p都是整数），这在表示实数时会带来困难。一种常见的方法是调整机器学习模型及其训练程序，即通过标准量化操作并依赖精心设计的量化模式，以确保下（上）溢量在可控范围[172、14、182、77]。<br>  即使在恶意对手面前，任何函数都可以安全计算[183]这一点在几十年间达成了共识。虽然通用解决方案存在，但它们的性能特征常常使它们在实际设置中不适用。因此，研究显著趋势是线性和逻辑斯蒂回归[309，172，302]和神经网络训练和推理[302，14，46]等应用设计定制协议。这些协议通常在孤井互通的设置中进行，或是将计算委托给一组不相互协作的计算服务器的变体模型。将这些协议移植到跨设备设置并不简单，因为它们需要大量的通信。<br>  同态加密 同态加密（Homomorphic encryption）方案允许在密文上直接执行某些数学运算，而无需事先解密。同态加密通过使参与者计算函数值，同时保持值隐藏，是一个使MPC成为可能的强大工具。<br>  从一般的全同态加密（FHE）〔176〕到更高水平的变体[79, 160, 80，112 ]，同态加密存在多种实现[3, 350, 4 ]。一些称为部分同态的方案同样具有实际意义，例如包括ElGamal和Paillier，允许同态加法或乘法。加性同态加密称为孤井互通设置中MPC协议的一种成分[309 198]。文献[345]调研了一些同态加密软件库，并简要说明了选择库时应考虑的标准/特性。</p><table border="1"><caption align="top"><b>表4-2 不同技术及其特性描述<b></b></b></caption><tr><td><b>技术</b></td><td><b>特性描述</b></td></tr><tr><td>差异隐私（本地、中心、混编、聚合、混合模型）</td><td>从包含用户的数据集的输出分析中可以了解到的个人信息量。具有差分隐私的算法必然包含一定数量的随机性或噪声，可以对其进行调整以掩盖用户对输出的影响。</td></tr><tr><td>安全多方计算</td><td>两个或多个参与者协作，通过密码学模拟完全可信的第三方，第三方满足：   •计算所有参与者提供的输入的函数；   •向选定的部分参与者显示计算结果，同时任一方没有进一步学习。</td></tr><tr><td>同态加密</td><td>允许一方在不具有纯文本访问权限的情况下，不解密密文下对密文执行数学运算，从而计算出它们的数据的函数。尽管计算成本更高，任意复杂度的数据函数都可以通过这种方式计算（“完全同态加密”）。</td></tr><tr><td>可信执行环境（安全环境）</td><td>可信执行环境提供了在远程计算机上可靠地运行代码的能力，即使不信任计算机的所有者/管理员。这是通过限制任何一方（包括管理员）的能力来实现的。尤其是，可信执行环境具有以下性质373]：   •一致性：除非程序显式发布消息，否则程序的执行状态始终不可见；   •完整性：除非程序显式接收输入，否则程序的执行不会受到影响；   •可测量/认证性：可信执行环境可以向远程方证明什么程序（二进制）正在执行，以及它的起始状态是什么，定义了一致性和完整性的初始条件。</td></tr></table><p>  考虑在联邦学习设置中使用同态加密，会遇到谁该持有该模式的密钥这一问题。虽然每个客户机加密其数据并将其发送到服务器端进行同态计算的想法很有吸引力，但服务器不应该能够解密单个客户机的提交数据。克服这一问题的一个简单方法是依赖一个持有密钥并解密计算结果的外部非合谋方。然而，大多数同态加密方案要求密钥经常更新（例如，由于易受选择密文攻击[102]）。此外，使用信任的非共谋方不在标准的联邦学习设置中。<br>  解决此问题的另一种方法是依赖于分布式（或阈值）加密方案，其中密钥在各方之间分发。Reyzin等人。[336]和Roth等人。[341]提出在跨设备设置中计算总和的这种解决方案。他们的协议使用了加性同态方案（分别是基于ElGamal和基于格的方案的变体）。<br><b>可信执行环境：</b>可信执行环境（TEEs，也称为安全环境）可以将联邦学习过程的一部分转移到云中的可信环境中，而该环境的代码可以被证明和验证。可信执行环境拥有几个关键性质，使他人相信，一段程序已被忠实而保密地执行[373]：</p><ol><li><b>一致性：</b>除非程序明确发布消息，否则程序的执行状态仍是保密的。</li><li><b>完整性：</b>除非程序显式地接收输入，否则程序的执行不会受到影响。</li><li><b>可测量/认证性：</b>可信执行环境可以向远程方证明什么程序（二进制）正在执行，以及它的起始状态是什么，定义了一致性和完整性的初始条件。</li></ol><p>  可信执行环境已经在实现不同体系结构上被，包括英特尔的SGX处理器[208，116]、ARM的TrustZone[2，1]和Sanctumon RISC-V[117]，它们在上述关键性质上的性能各不相同。<br>  当前的安全环境在内存方面受到限制，只提供对CPU资源的访问，即它们不允许在GPU或机器学习处理器上进行处理（Tram`er和Boneh[382]探索如何将可信执行环境与GPU结合起来进行机器学习推断）。此外，对于可信执行环境（特别是那些在共享微处理器上操作的可信执行环境）来说，完全排除所有类型的侧信道攻击也是一项挑战[391]。<br>  虽然安全环境为运行在其中的所有程序提供保护，但在实践中还必须解决其他问题。例如，通常有必要将运行在该环境中的代码构造为一个不受数据影响的过程，这样它的运行时和内存访问模式就不会显示它正在计算的数据的信息（参见示例[67]）。此外，可测量/证明性通常只证明某个特定的二进制文件正在运行；系统架构师需要提供一种方法来证明该二进制文件具有所需的隐私属性，这可能需要使用来自开源代码的复现过程来重构该二进制文件。<br>  如何在安全环境、云计算资源和客户端设备之间划分联邦学习功能仍然是一个悬而未决的问题。例如，安全环境可以执行安全聚合或混编等关键功能，以限制服务器对原始客户端上传内容的访问，同时此可信计算基础不涉及大多数联合学习逻辑。<br><b>受关注的安全计算问题：</b>虽然安全多方计算和可信执行环境为分布式隐私数据上的任何函数的保密计算问题提供了一般解决方案，但许多优化可以应用到某些特定功能中。下面描述的任务就是这种情况。<br>  <b>安全聚合</b> 安全聚合是指在n个客户端和一个服务器的场景中，允许每个客户端提交一个值（通常是联邦学习设置中的向量或张量），这样服务器只学习客户端值的一个聚合函数，通常是这些值的和。<br>  大量文献对单服务器设置（通过成对加法遮蔽[12，188，73]、通过阈值同态加密[356，193，92]、通过一般安全多方计算[86]）以及在多个非合谋服务器设置[71，29，113]中的安全聚合进行了探讨。也可以使用可信执行环境（如上所述）来实现安全聚合，如[269]。<br>  <b>安全混编</b> 安全混编是指在n个客户端和服务器的场景中，允许每个客户机提交一条或多条消息，这样服务器只从所有客户端学习一个无序的消息集合（multiset），而不需要更多。具体来说，除了消息本身包含的信息之外，服务器无法判断任一条消息的发送者。安全混编可以被视为安全聚合的一个实例，其中值是多集单例，聚合操作为多集求和，尽管通常情况下，为达到安全混编和安全聚合的最佳性能，典型操作制度提供了非常不同的实现<br>  在安全多方计算的背景下，安全混编通常是在混合网络的标题下进行研究[95 251]，也有在可信计算的背景下进行的研究[67]。混合网络中已经存在了以Tor网络[138]的形式进行的大规模部署。<br>  <b>隐私信息检索</b> 隐私信息检索（PIR）是服务器为客户端提供的功能。它使客户机能够从服务器托管的数据库中下载条目，这样服务器就不会获得客户机请求的条目的任何信息。<br>  MPC方法将PIR分为两大类：基于计算的PIR（cPIR），其中一方可以执行协议的整个服务器端[249]；基于信息论的PIR（itPIR），需要其中多个非共谋方执行协议的服务器端[106]。<br>  PIR适用性的主要障碍如下：cPIR具有非常高的计算成本[361]，而非共谋方设置难以在工业场景中有说服力地实现。最近关于PIR的研究结果表明，通过使用基于点阵的密码系统，计算成本显著降低[16，313，17，25，175]。[218]已经证明了在单个服务器上如何利用用户可用的边带信息构建高效通信PIR。最近的相关工作建议利用客户机本地状态来加速PIR。帕特尔等人[319]展示了如何获取后者边带信息，并在单个服务器上实现和验证了一个实用的混合（基于计算和信息论）PIR方案。Corrigan-Gibbs和Kogan[114]在一个离线/在线模型上提出了一种压线性的在线PIR协议，在该模型的离线阶段，客户端从服务器获取信息，这些请求独立于将来要执行的查询。<br>  过去的工作[410]，进一步探索了PIR和隐私共享之间的联系，最近[139]将编码数据联系到PIR中，并且已经建立了高效通信PIR[66]。PIR也在开-关隐私的背景下进行了研究，在这种背景下，客户可以关闭他们的隐私保护以换取更好的实用性或性能[306, 423]。</p><h3 id="4-2-2-隐私保护披露"><a class="header-anchor" href="#4-2-2-隐私保护披露">¶</a>4.2.2 隐私保护披露</h3><p>  量化和限制个人信息披露的最新模型是差异隐私（DP）[147，144，145]，其目的是在发布的模型中引入一定程度的不确定性，以充分掩盖任何个人用户的贡献。差异隐私由隐私损失参数 $(\varepsilon,\delta)$ 量化，其中较小的 $(\varepsilon,\delta)$ 对应于隐私性增强。更正式地说，对于所有 $S \subseteq \operatorname{Range}(A)$，以及所有相邻数据集 $D$ 和 $D’$，如果满足下式，则称随机化算法A是 $(\varepsilon,\delta)$-差异隐私的：<br>$$<br>P(A(D) \in S) \leq \operatorname{e\varepsilon } P\left(A\left(D^{\prime}\right) \in S\right)+\delta .(3)<br>$$<br>  在联邦学习的情景中， $D$ 和 $D’$ 对应于分散的数据集，如果 $D’$ 可以通过加上或减去单个客户机（用户）的所有记录而从 $D$ 获得，则这些数据集是相邻的[290]。这种差异隐私的概念被称为用户级差异隐私。它比通常使用的相邻概念强，其中$D$ 和 $D’$ 只相差一条记录[145]，因为通常一个用户可以向数据集贡献多条记录（例如训练集）。<br>  在过去的十年中，用于差异性私有数据分析的广泛技术已经得到发展，特别是在假设集中设置的情况下，在应用实现隐私所需的扰动之前，原始数据由可信方收集。在联邦学习中，通常编排服务器将充当DP机制的可信实现者，确保只将私有化的输出发布给模型工程师或分析师。<br>  然而，在可能的情况下，我们通常希望减少对可信方的需求。近年来，人们考虑了几种减少对数据管理员信任需求的方法。<br><b>本地差异隐私：</b> 通过让每个客户机在与服务器共享数据之前对其数据进行差异隐私转换，可以在不需要可信集中服务器的情况下实现差异隐私。也就是说，我们将公式（3）应用于处理单个用户的本地数据集D的机制A，并且保证对任何可能的其他本地数据集D’保持相同的性质。该模型被称为本地差异隐私模型（LDP）[406，229]。LDP已经被谷歌、苹果和微软有效地用于收集大型用户群中热门项目的统计数据[156，135，136]。它还被Snap[325]用于垃圾邮件分类训练的联邦设置中。这些LDP部署都涉及大量的客户机和表项，在Snap中甚至高达10亿，这与DP的集中实例化形成鲜明对比，而后者可以从更小的数据集中提供高实用性。不幸的是，正如我们将在第4.4.2节中讨论的那样，在保持效用的同时实现LDP是很困难的[229，388]。因此，需要一个介于完全中心和完全本地DP之间的差分隐私模型。这可以通过分布式差异隐私或混合模型来实现，如下所述。<br><b>分布式差异隐私：</b> 为了在不依赖可信的中心服务器的情况下恢复中心DP的一些实用性，可以使用分布式差分隐私模型[146、356、67、105]。在此模型下，客户机首先计算并编码一个最小（特定应用程序）的报告，然后将编码后的报告通过安全计算函数，该功能的输出可供中央服务器访问，从而在服务器能够检查时，此输出已经满足了不同的隐私要求。编码是为了帮助维护客户端的隐私，可以包括如LDP等隐私项。安全计算功能可以有多种体现。它可以是一个MPC协议，一个在TEE上完成的标准计算，甚至是两者的结合。每种选择都有不同的假设和威胁模型。<br>  必须指出的是，分布式差异隐私和本地差异隐私从多个角度得到了不同的保证：虽然分布式DP框架可以为与LDP相同级别的差异隐私生成更准确的统计数据，但它依赖于不同的设置，并且通常会做出更有力的假设，例如作为对MPC协议的访问。下面，我们概述了两种可能的分布式差异隐私方法，依赖于安全聚合和安全混编，尽管我们强调还有许多其他方法可以使用。<br>  通过安全聚合实现分布式DP。在第4.2.1节中讨论过，在FL中，安全聚合是实现分布式DP的一种的工具。安全聚合可用于确保中心服务器获得聚合结果，同时确保单个设备和参与者的中间参数不会透露给中心服务器。为了进一步确保聚合结果不会向服务器显示附加信息，我们可以使用本地差异隐私（例如，中等ε级别）。例如，每个设备可以在安全聚合之前扰动其自身的模型参数，以实现本地差异隐私。通过正确设计噪声，我们可以确保聚合结果中的噪声与可信服务器（例如，低ε/高隐私级别）集中添加的噪声匹配[12，330，181，356，188]。<br>  通过安全混编实现分布DP。另一个分布式差异隐私模型是混编模型，它由最近引入的混编分析编码（ESA）框架[67]启动（如图3所示）。在这个框架的最简单版本中，每个客户端在其数据上运行一个LDP协议（例如，具有中等级的ε），并将其输出提供给一个安全的混编器。混编器随机排列报告表项，并将混编后报告的集合（没有任何标识信息）发送到服务器进行最终分析。直观地说，此安全计算功能的介入使得服务器更难了解参与者的任何信息，并支持差异隐私分析（例如，低ε/高隐私级别）。在更一般的多消息混编框架中，每个用户可以向混编器发送多个消息。混编器独立于服务器并专门用于混编，可以直接作为一个受信任的实体实现，也可以通过上面讨论的更复杂的加密原语来实现。<br>  Bittau等人 [67]提议采用Prochlo系统作为实施ESA框架的一种方式。该系统采用整体隐私方法，考虑到安全计算方面（使用TEEs解决）、隐私披露方面（通过差异隐私解决）和验证方面（使用安全环境弱化认证功能）。<br>  更普遍地说，差异隐私的混编模型可以使用更广泛的局部随机者类，甚至可以自适应地选择这些局部随机者[157]。这可以使差异私有协议的错误远远小于本地模型中可能的错误，同时依赖于弱于中心模型的信任假设[105、157、43、179、178]。<br><b>混合差分隐私：</b> 另一个可行的方法是混合差分隐私[39]，它通过根据用户的信任模型偏好（例如对管理员信任与否）来划分用户，从而组合多个信任模型。在混合模型之前，有两种选择。第一种是使用最不可信的模型，它通常提供最低的效用，并且保守地将其统一应用于整个用户群。第二种方法是使用最信任的模型，它通常提供最高的实用程序，但只应用于最信任管理者的用户。通过允许多个模型共存，混合模型机制可以从给定的用户基础获得更多的效用，与纯本地或中心DP机制相比。例如，[39]描述了一个系统，其中大多数用户在本地隐私模型中贡献他们的数据，而一小部分用户选择在可信的管理员模型中贡献他们的数据。这使得能够设计一种机制，在某些情况下，该机制的性能优于应用于所有用户的保守的本地机制以及仅应用于小部分选择参加用户的可信管理员机制。这种结构可以直接应用于联邦学习环境中；然而，组合信任模型或计算模型的一般概念也可能激发类似新的联邦学习方法。<br><img src="/2022/09/06/advances-and-open-problems-in-federated-learning-yue-du-bi-ji/7.png" alt="图3 包含四个参与者的混编分析编码（ESA）框架"></p><h3 id="4-2-3-可验证性"><a class="header-anchor" href="#4-2-3-可验证性">¶</a>4.2.3 可验证性</h3><p>  与上述隐私技术正交的一个重要概念是验证性。一般来说，可验证的计算将使一方能够向另一方证明其已忠实地对其数据执行了所需的行为，而不会损害数据的潜在保密性。可验证计算的概念可追溯到Babai等人 [40]，并且已经在文献中的不同术语下进行了研究：检验[40]、认证计算[295]、委托计算[185]以及可验证性计算[173]。<br>  在FL的背景下，验证能力可用于两个目的。首先，它将使服务器能够向客户机证明它忠实地执行了预期的行为（例如，聚合输入、显示输入消息或添加用于差异隐私的噪声）。其次，它将使客户端能够向服务器证明其输入和行为遵循协议规范（例如，输入属于某个范围，或者数据是正确生成的密文）。<br>  多种技术可用于提供验证：零知识证明（ZKPs）、可信执行环境（TEEs）或远程认证。其中ZKPs提供了基于数学硬度的形式化密码安全保证，而其他技术则依赖于可信硬件的安全性假设。<br><b>零知识证明（ZKPs）：</b> 零知识证明是一种密码原语，它使一方（称为证明者）能够向另一方（称为验证者）声明证明，而验证者依赖于见证者已知的秘密信息，而不向验证者泄露这些信息。20世纪80年代末，Goldwasser等人引入了零知识（ZK）的概念[184]。它为隐私数据的可验证性问题提供了解决方案。虽然ZK构建方面存在大量的工作，但第一个将ZKP和通用功能的可验证计算引入实用领域的是Parnoetal的工作 [317]，它介绍了第一个针对简洁ZK的优化构建和实现。现在，ZKP协议可以实现100字节的证明大小和毫秒级的验证，无论被证明语句的大小。<br>  ZKP有三个显著的属性：完整性（如果陈述是真的，证明者和验证者遵循协议，验证者将接受证明）、可靠性（如果陈述是假的，验证者遵循协议，验证者将拒绝证明）和零知识（如果陈述是真的，证明者遵循协议协议中，验证者只会了解到声明是真实的，不会从交互中了解到任何隐私信息）。<br>  除了这些共性之外，在证明支持的语言、设置要求、证明和验证计算效率、交互性、简洁性和潜在的硬度假设方面，还有不同类型的零知识结构。有许多ZK结构支持特定的语句类，Schnorr证明[349]和Sigma协议[128]就是这样广泛使用的协议的例子。虽然此类协议在特定设置中有许多用途，但是能够支持任何功能的通用ZK系统提供了一个更广泛适用的工具（包括在FL的上下文中），因此我们将在接下来的讨论中重点讨论此类构造。<br>  不同构造之间的一个主要区别是需要可信设置。一些ZKP依赖于一个公共引用字符串（common reference string，CRS），该字符串使用应该保持y隐私性来计算，以保证证明的可靠性。这种CRS的计算被称为可信设置。虽然这种要求对于这样的系统是不利的，但是现有的能够实现最简洁的证明和验证者有效性的ZKP，都需要可信的设置。<br>  影响不同情况下适用性的另一个重要特性是，生成证明是否需要证明者和验证者之间的交互，这里我们区分非交互零知识证明（NIZK），该证明使验证者能够向验证者发送一条消息，且无需进一步通信。通常，我们可以将交互式的证明转换为非交互式的证明，从而对理想哈希函数的功能做出更有力的假设（即哈希函数生成结果完全随机）。<br>  此外，对ZKP系统的有效性有不同的测量，如证明的长度和证明者、验证者的计算复杂度。从评估执行时间来说，理想的证明者的复杂性应该是线性的，但许多现有的ZKPS引入额外的（有时产生重大影响）证明者，产生更多开销。最有效的验证者复杂度要求在估计的功能的输入的大小上至少是线性的，并且在FL服务器的工作场景下的设置中，这个输入规模将是巨大的。<br>  简洁的非交互式零知识证明（SNARKs）[65]是一种ZKP类型，它提供恒定的证明和验证大小，与输入大小成线性关系。这些有吸引力的性能是以更强的假设为代价的，而这一假设在大多数现有的方案中都是固有而可信的。大多数现有SNARK结构利用二次算术程序〔174, 317, 118〕，现在可在开源库中使用，如LiSnAgAg[[ 5 ] ]，并部署在加密货币场景中，如Zcash（57）。值得注意的是，SNARK系统通常需要证明者部分的开销；特别是，证明者的计算需要与被证明语句的大小上满足超线性关系。最近，谢等人〔418〕提出了一种ZKP系统，该系统能够实现线性证明者的复杂度，但增加了证明空间开销和验证时间。<br>  如果我们放松对构造的简洁性或非交互性的要求，就会有大量广泛的效率权衡的构造实现，避免可信设置要求，并使用更标准的加密假设[84、397、23、58]。<br>  近年来，越来越多的实际应用使用非交互式零知识证明，主要是由区块链驱动的。在FL中有效地使用交互式ZKP系统和NIZKs仍然是一个具有挑战性的开放性问题。在这种设置中，NIZKs可以向服务器证明客户机的输入。在验证者是客户端的情况下，创建一个可信的语句来验证将是一个挑战，因为它涉及到来自其他客户的输入。在这种情况下，最近的工作[76]使我们能够处理多个验证者共享陈述语句的情况。<br><b>可信执行环境和远程认证：</b> 我们在第4.2.1节中讨论了TEEs，但这部分的重点是TEEs提供可验证计算的可能。实际上，TEEs能够证明和验证在其环境中运行的代码（二进制）。特别是，当验证者知道（或可以复制）哪个二进制文件应该在安全环境中运行时，TEEs将能够提供完整性（除了输入之外，代码执行不会受到影响）和可证明性（TEE可以证明特定二进制文件正在执行，并且什么是启动状态）[373,385]。一般来说，远程认证允许验证者安全地测量远程硬件平台的内部状态，并可用于建立静态或动态信任源。虽然TEEs支持基于硬件的远程认证，但文献中提出了基于软件的远程认证[351]和混合远程认证设计[152,238]，并能够权衡硬件要求与可验证性。<br>  在联邦学习环境中，TEEs和远程认证对于客户端能够有效地验证服务器上运行的关键功能可能特别有用。例如，安全聚合或混编可以在TEEs中运行，并为它们的输出提供不同的隐私保证。因此，服务器随后对差异隐私数据应用的后处理逻辑可以在服务器上运行，并且对客户端保持不敏感。注意，这样的系统设计要求客户端知道并信任要在安全环境中应用的关键函数的确切代码（二进制）。此外，远程证明可以使服务器证明FL计算中涉及的客户端的特定要求，例如无泄漏、不变性和不可中断性（关于远程证明的最低要求的详细列表，请参阅[166]。</p><h2 id="4-3-防范外部恶意参与者"><a class="header-anchor" href="#4-3-防范外部恶意参与者">¶</a>4.3 防范外部恶意参与者</h2><p>  在本节中，我们假设可信服务器的存在，并讨论实现对外部恶意参与者（例如敌对客户、敌对分析者、消耗学习模型的敌对设备或其任意组合）的严格隐私保证的各种挑战和公开问题。<br>  如表7所述，恶意客户端可以在其参与的轮次中检查从服务器接收的所有消息（包括模型迭代），恶意分析师可以使用不同的超参数检查来自多个训练运行的模型迭代序列，在跨设备FL中，恶意设备可以通过白盒或黑盒访问最终模型。因此，要提供严格的保护以防范外部对手，首先必须考虑从中间迭代和最终模型中可以学到什么。</p><h3 id="4-3-1-评估迭代轮次和最终模型"><a class="header-anchor" href="#4-3-1-评估迭代轮次和最终模型">¶</a>4.3.1 评估迭代轮次和最终模型</h3><p>  为了更好地理解从中间迭代或最终模型中可以学到什么，我们建议量化联邦学习模型对特定攻击的易感性。在联邦学习环境中，这是一个特别有趣的问题。一方面，敌方从服务器接收到对模型的直接访问，拓宽了攻击面。另一方面，服务器确定对手将在训练过程的哪个特定阶段获得对模型的访问，并在每个阶段控制对手对模型的影响。<br>  对于经典的（非联邦的）计算模型，了解模型对攻击的敏感性是一个活跃而富有挑战性的研究领域[167,357,91,293]。最常用的易于攻击量化模型方法是使用代理（评估）数据集模拟对模型的攻击，该数据集与实际中预期的数据集类似。如果代理数据集确实与最终用户数据相似，那么这就说明了模型的预期攻击敏感性。更安全的方法是确定模型攻击敏感性的最坏情况上限。 [425]在理论上给出了可接近的界限，尽管这一上界在实际模型中通常松散、空洞而无法达到。经验方法也许能够提供更严格的界限，但对于许多类型的攻击和模型，这一努力可能是棘手的。这一领域中一个有趣的新兴研究领域考察了理论条件（关于评估模型和攻击），即在何种条件下，通过模拟失败隐私侵犯的失败攻击，就意味着没有更强的攻击能够成功完成这样的任务[134]。然而，这一领域仍处于初级阶段，需要做更多的工作来更好地理解评估（通过模拟攻击）的基本要求。<br>  联邦学习框架不仅为攻击提供了独特的设置，而且也为攻击数量和防御提供了独特的设置。具体来说，由于服务器可以控制每个用户在培训过程中何时可以访问和影响模型，因此可以设计新的可处理方法来量化模型的平均情况或最坏情况下的攻击敏感性。这样的方法将使得能够开发新的自适应防御，它可以应用于即时运行过程中，从而在最大化效用的前提下，面对不可抗拒的敌方时取得优先权。</p><h3 id="4-3-2-考虑中心差异隐私的模型训练"><a class="header-anchor" href="#4-3-2-考虑中心差异隐私的模型训练">¶</a>4.3.2 考虑中心差异隐私的模型训练</h3><p>  为了限制或消除从迭代（和/或最终模型）中可以了解到的关于用户的信息，可以在FL的迭代训练过程中使用用户级差异隐私[72,90,288,62]。使用这种技术，服务器通过剪裁单次更新的l2范数结果，聚合被剪裁的更新，然后将高斯噪声添加到聚合结果中。这样可以确保迭代不会过拟合任何单个用户的更新。为了跨回合跟踪总体隐私预测，可以使用更高级的组合理论[148,221]或在[72, 97,299,405]中提出的分析动量统计方法。动量统计方法特别适用于均匀亚采样高斯机制。<br>  在跨设备FL中，训练示例的数量在不同设备之间可能有很大的差异。因此，与中心模型[24]中关于用户级DP的最新工作类似，研究如何自适应地限制用户的贡献并剪裁模型参数仍然是一个有趣的研究方向[381,324]。更广泛地说，与记录级DP不同，在记录级DP中，各种规范学习和估计任务的准确性和隐私性之间的基本权衡关系很好理解，用户级DP从根本上来说就不那么容易理解（尤其是当贡献的数量在用户之间变化很大，并且没有先验的严格限制时）。因此，需要做更多的工作，以更好地理解在这种新出现的DP环境中的基本权衡。<br>  除上述内容外，还必须区分在训练期间可能看到（某些）中间迭代的恶意客户端和只能看到最终模型的恶意分析师（或部署设备）。尽管中心DP提供了对两种威胁模型的保护，但仔细的理论分析可以发现，对于上述高斯机制（或任何其他差异隐私机制）的具体实现，我们可能会得到这两种威胁模型的不同隐私参数。当然，对于恶意分析师，我们应该获得比恶意客户更强的差异隐私保证（因为恶意客户可能比恶意分析师获得更多的信息）。费尔德曼等人最近对这种“通过迭代进行隐私放大”的设置进行了凸优化问题研究[163]。然而，还不清楚[163]中的结果是否可以被迁移到非凸设置。<br>  非均匀设备采样隐私放大程序 跨设备FL的基本更新步骤在可用客户端的子集上进行。当所选择的用户子集从用户的基本群体中均匀抽样时，可以使用在[405]中开发的动量统计方法来分析该抽样过程的隐私增益。然而，这样的抽样程序在实践中几乎是不可能的。这是因为（a）服务提供商可能不知道设备的总体情况，并且（b）可用设备的特定子集可能会随时间而显著变化。因此，量化跨设备FL中的隐私放大是一个有趣的开放问题。<br>  随机源（改编自[288]） 大多数计算设备只能访问很少的熵源，而且它们的速率往往很低（硬件中断、车载传感器）。使用熵为加密安全的伪随机数生成器（PRNG）种子并根据需要使用PRNG的输出是标准要求，这在理论上也是合理的。基于标准密码原语的鲁棒高效PRNG存在，在现代CPU上具有每秒千兆字节的输出速率，并且要求种子短于128比特[343 ]。<br>  只要识别器在计算上有界，访问PRNG的随机算法A的输出分布与访问真实熵源的随机算法A的输出分布是不可区分的。相比之下，无论对手有多强大，差异隐私的保障对任何对手都是有效的。因此，几乎所有差异隐私的实现都只满足[298]引入的计算差异隐私的（变体）。从积极的方面来说，一个计算能力有限的对手无法分辨出两者的区别，这使得我们避免在这一点上过于局限。<br>  一个训练过程可能有多个非确定性来源（例如，退出层或生成模型的输入），但只有那些反映在隐私目次中的来源必须来自加密安全的PRNG。特别是，设备采样过程和加高斯噪声必须从加密安全的PRNG中提取，以满足计算差异隐私。<br>  评估差异隐私实现 众所周知，隐私和安全协议很难正确实现（例如，[296, 192]用于区分隐私）。什么技术可以用来测试FL实现的正确性？由于这些技术通常由那些可能选择不使用开源代码的组织部署，黑盒测试的可能性有多大？一些著作[137，275]开始在差异隐私的背景下探索这一领域，但仍有许多悬而未决的问题。</p><h3 id="4-3-3-迭代隐蔽"><a class="header-anchor" href="#4-3-3-迭代隐蔽">¶</a>4.3.3 迭代隐蔽</h3><p>  在典型的联邦学习系统中，模型迭代（即每轮训练后模型的更新版本）被假定为对系统中的多个参与者可见，包括选择参与该轮的服务器和客户端。但是，可以使用第4.2节中的工具来对这些参与者隐藏迭代模型。<br>  为了向客户端隐藏迭代，每个客户端都可以在提供保密特性的TEE中运行其联邦学习的本地部分（参见第4.2.1节）。服务器将验证预期的联邦学习代码是否在TEE中运行（依赖于TEE的认证和完整性功能），然后将加密的模型迭代结果传输到设备，以便它只能在TEE中解密。最后，模型更新将在返回到服务器之前在TEE内部加密，使用仅在安全环境内部和服务器上已知的密钥。不幸的是，TEE通常不适用于客户端，尤其是当这些客户端是智能手机等终端用户设备时。此外，即使存在TEE，它们也可能不够强大，无法支持训练计算，为了保护模型迭代，必须在TEE内部进行训练计算，并且可能需要高计算成本和/或大量的RAM。尽管TEE的能力可能会随着时间的推移而提高，以及诸如在[382]中所述的技术可以通过将计算的部分转移到TEE之外而减少对TEE的要求，同时保持计算的整体的证明、完整性和机密性需求。<br>  在MPC模型下也可以实现类似的保护[302，14]。例如，服务器可以在将迭代器的模型参数发送到客户端之前，使用只有服务器知道的密钥在同态加密方案下对其进行加密。然后，客户端可以使用密码系统的同态属性计算加密的模型更新，而无需解密模型参数。然后，可以将加密的模型更新返回到服务器进行聚合。这里的一个关键挑战是在解密之前在服务器上强制聚合，否则服务器可能会学习到客户端的模型更新。另一个具有挑战性的开放性问题是提高性能，因为即使是最先进的系统也需要相当可观的计算资源才能在深层神经网络中完成一轮训练。这方面的进展既可以通过算法的进步，也可以通过为MPC开发更高效的硬件加速器来实现[337]。<br>  向服务器隐藏模型迭代也会产生额外的挑战。在TEE模型下，联邦学习的服务器部分可以在TEE中运行，所有各方（即客户端和分析师）都验证服务器TEE仅在满足适当的培训标准后才发布最终模型。在MPC模型下，加密密钥可以保护模型迭代，密钥由分析师持有，在客户端之间共享，或由可信的第三方持有；在这种设置中，密钥持有者将被要求参与模型参数的解密，从而可以确保此过程只发生一次。</p><h3 id="4-3-4-对不断变化数据的重复分析"><a class="header-anchor" href="#4-3-4-对不断变化数据的重复分析">¶</a>4.3.4 对不断变化数据的重复分析</h3><p>  对于联邦学习的许多应用，分析师希望分析流式数据，并且还必须提供动态更新的学习模型，这些模型（1）对目前接受到的数据结果正确，以及（2）准确预测未来将要到达的数据。在没有隐私问题的情况下，分析师可以在新数据到达后简单地重新训练所学模型，以确保在任何时候都能达到最大的精度。然而，隐私保证等级随着关于相同数据的附加信息的发布而降低[147, 148]，这些附加信息引起更新的频率必须降低以保持整体分析的隐私性和准确性。<br>  动态数据库和时间序列数据的差异隐私[125, 124, 89]研究的最新进展都假设存在可信的管理员，他们可以在上线时看到原始数据，并发布动态更新的统计数据。一个悬而未决的问题是，如何将这些算法技术扩展到联邦设置，以实现对时间序列数据或其他动态演变数据库的私有联邦学习。<br>  具体开放性问题包括：</p><ol><li>分析师应如何在有新数据的情况下更新私有FL模型？或者，在数据集 $D$ 上使用FL私下学习的模型扩展到数据集 $D'$ （在给定的相似性度量中保证与D相似）的程度如何？由于FL已经出现在在线到达的样本上，并且没有过拟合它所用来训练的数据，因此这种模型很可能仍然会在新的数据集 $D'$ 上表现良好。这也与第5节探讨的鲁棒性问题有关。</li><li>解决隐私构成问题的一种方法是生成可以独立使用，而不会造成额外隐私损失的合成数据[145，9]。这来自于差异隐私的后处理保证[147]。奥根斯坦等人在[32]中探索证明了以联邦方式生成合成数据。在动态数据设置中，合成数据可以重复使用，直到它相对于新数据变得“过时”，并且必须更新。即使在以联邦方式生成数据之后，它也必须以私有和联邦方式进行更新。</li><li>之前关于动态数据库差异隐私的工作[124]或不公开地检测时间序列数据变化[125, 89]中的具体方法是否可以扩展到联邦设置？</li><li>如何在联邦模型中首先查询时间序列数据？在设计上，同一个用户不会被多次定期查询更新的数据点，因此很难在评估过程中内收集到真实的个人数据随时间变化的估计值。在这里，可以使用时间序列数据统计抽样的常用工具，但必须与隐私工具和联合工具一起使用。其他方法包括重新格式化查询，以便评估查询中的每个子查询都可以在设备上完全应答。</li></ol><h3 id="4-3-5-防止模型被盗或误用"><a class="header-anchor" href="#4-3-5-防止模型被盗或误用">¶</a>4.3.5 防止模型被盗或误用</h3><p>  在某些情况下，开发ML模型的参与者或组织可能有动机来限制检查、误用或窃取模型的能力。例如，限制对模型参数的访问可能会使对手更难搜索漏洞，例如产生意外模型输出的输入。<br>  如第4.3.3节所述，在推断下保护已部署的模型与在训练期间向客户端隐藏模型迭代的挑战密切相关。同样，可以使用TEEs和MPC方法。在TEE模型下，模型参数只能由设备上的TEE访问，如第4.3.3节所述；主要区别在于，所需的计算现在是推断而不是训练。<br>  如果不放弃设备推理所提供的优点，就很难使MPC策略适应这个用例：如果用户数据、模型参数和推理结果原来都是在设备上进行的，则不清楚还有哪些其他方参与了多方计算。例如，朴素地试图使用同态加密将要求解密密钥位于要使用推断功能的设备上，从而首先破坏加密的价值。要求分析师参与的解决方案（例如，持有加密密钥或模型参数本身）意味着对最终用户的额外推理延迟、带宽成本和连接要求（例如，对于处于飞行模式的设备，推理将不再可用）。<br>  必须注意的是，即使模型参数本身被成功隐藏；研究表明，在许多情况下，它们可以由对手重建，而对手只需访问基于这些参数的推理/预测API[384]。对于驻留在数百万或数十亿最终用户设备上的模型，需要采取哪些附加保护措施来防止此类问题，这是一个悬而未决的问题。</p><h2 id="4-4-针对敌意服务器的保护"><a class="header-anchor" href="#4-4-针对敌意服务器的保护">¶</a>4.4 针对敌意服务器的保护</h2><p>  在前面的章节中，我们假设存在一个可以编排整个训练过程的受信服务器。本节，我们讨论一种针对一个有敌意服务器的更合适的场景。特别是，我们首先调查这种环境和现有工作的挑战，然后继续描述未解决的问题以及如何使用第4.2节中讨论的技术来应对这些挑战。</p><h3 id="4-4-1-信道，女巫攻击，选取"><a class="header-anchor" href="#4-4-1-信道，女巫攻击，选取">¶</a>4.4.1 信道，女巫攻击，选取</h3><p>  在跨设备FL设置中，我们拥有一台具有大量计算资源的服务器和大量客户端，这些客户端（i）仅能与该服务器通信（如在星形网络拓扑中），并且（ii）连通性和带宽可能受到限制。 在执行给定的信任模型时，这提出了非常具体的要求。 特别是，客户端没有独立于服务器的清晰方法来在客户端之间建立安全通道。 如Reyzin等人所示。 [336]对于实际设置，在需要客户端之间专用通道的情况下，需要假设服务器在密钥分发阶段（如[73]中所做的）是诚实的（或至少是半诚实的）行为。 这包括基于MPC技术的密码解决方案。 对此假设的替代方法是将额外的参与方或公共公告板（例如，参见[341]）合并到客户端已知且可以信任的不与服务器串通的模型中。<br>  除了信任服务器以促进专用通信渠道之外，跨设备FL的参与者还必须信任服务器以公平，诚实的方式形成客户群。控制服务器的主动恶意攻击者可能会模拟大量伪造的客户端设备（“ Sybil攻击” [140]），或者可能会从可用设备池中优先选择以前受到破坏的设备。无论采用哪种方式，对手都可以在联邦学习的一轮中控制更多的参与者，这要比简单地从总体中的对手设备的基本速率所预期的要多。这将使打破MPC中至少有一部分设备是诚实的普遍假设变得容易得多，从而破坏了协议的安全性。即使协议本身的安全性保持不变（例如，如果其安全性植根于不同的信任源，例如安全的隔离区），也可能存在以下风险：如果已知大量敌对客户端的模型更新，或由对手控制该更新，则可能会破坏其余客户的更新的隐私。注意，这些顾虑也可以适用于TEE。例如，基于TEE的混洗器也可能遭受Sybil攻击；如果将单个诚实用户的输入与来自假用户的已知输入进行混洗，那么对手将很容易在混洗输出中识别出诚实用户的价值。<br>  请注意，在某些情况下，有可能在一轮客户端之间建立证明它们都在执行正确的协议，例如，如果客户端设备上有安全的隔离区，并且客户端之间可以进行远程证明。 在这些情况下，有可能为该回合中所有诚实的参与者建立隐私（例如，通过证明已正确遵循安全的多方计算协议，秘密且正确地添加了分布式差异隐私贡献等），即使 模型更新本身是对手已知的或由对手控制的。</p><h3 id="4-4-2-现有方案的缺陷"><a class="header-anchor" href="#4-4-2-现有方案的缺陷">¶</a>4.4.2 现有方案的缺陷</h3><p>  鉴于FL的目标是服务器在客户数据中构建人口级别模式的模型，自然而然的隐私目标是量化并可证明地限制服务器重建单个客户输入数据的能力。这涉及形式上的定义（a）作为FL执行结果显示给服务器的客户数据的视图是什么，以及（b）这种视图的隐私泄漏是什么。在FL中，我们特别希望保证服务器可以汇总来自客户端的报告，同时以某种方式掩盖每个单独客户端的贡献。如第4.2.2节所述，这可以通过多种方式完成，通常使用一些差异性隐私的概念。这种方法有很多种，每种方法都有其自身的弱点，尤其是在FL中。例如，如已经讨论的，中央DP遭受对可信任中央服务器的访问的需求。这引出了第4.2.2节中讨论的其他有前途的私人披露方法。在这里，我们概述了这些方法的一些缺点。<br><b>本地差分隐私（LDP）</b><br>  如前所述，LDP通过让每个客户端在将报告发送到中央服务器之前对其报告执行不同的私有转换，从而消除了对受信任的中央服务器的需求。 LDP假定用户的隐私完全来自该用户自己的随机性；因此，用户的隐私保证独立于所有其他用户添加的额外随机性。尽管LDP协议有效地加强了隐私并具有理论上的依据[156，135，136]，但许多结果表明，在保持实用性的同时实现局部差分隐私尤其具有挑战性，特别是在高维数据设置中[229，388， 219，51，220，424，142，111]。造成这种困难的部分原因是，引入的随机噪声的幅度必须与数据中信号的幅度相当，这可能需要合并客户端之间的报告。因此，要获得与中央设置相当的LDP效用，就需要相对较大的用户群或较大的参数选择。<br><b>混合差分隐私（HDP）</b><br>  差异隐私的混合模型可以通过根据用户的信任首选项对用户进行划分来帮助减少所需用户群的大小，但是它不能为用户本地添加的噪声提供隐私放大功能。 而且，尚不清楚哪个应用领域和算法可以最佳地利用混合信任模型数据[39]。 混合模型的当前工作通常假设，无论用户信任偏好如何，其数据都来自相同的分布[39、141、53]。 放宽此假设对于FL尤其重要，因为信任首选项和实际用户数据之间的关系可能并不重要。<br><b>随机混合模型</b><br>  随机混合模型可从用户的本地噪声中放大隐私，尽管它有两个缺点。首先是可信中介的要求；如果用户已经不信任管理员，那么他们不太可能会信任由管理员批准或创建的中介人（尽管TEE可能有助于弥合这一差距）。 Prochlo框架[67]（据我们所知）是唯一现有的实例。第二个缺点是随机混合模型的差异性隐私保证与参与计算的对手用户数量成比例地降低[43]。由于用户或管理员不知道该数字，因此将不确定性引入了用户所接收的真实隐私级别。在联合学习的情况下，这种风险尤其重要，因为用户（可能是对抗性的）是计算管道中的关键组成部分。当用户在本地添加自己的噪声时，安全的多方计算除了会给每个用户增加大量的计算和通信开销外，也无法解决此风险。<br><b>安全集聚协议</b><br>  [73]中的安全聚合协议在聚合客户报告时具有强大的隐私保证。 此外，该协议是针对联合学习的设置而量身定制的。 例如，它对于客户端在执行过程中退出（跨设备FL的一个共同特征）是健壮的，并且可以扩展到大量参与者和更长的的向量。 但是，这种方法有几个局限性：（a）假定服务器为半诚实的服务器（仅在私钥基础结构阶段），（b）允许服务器查看全方位的聚合（可能仍会泄漏信息）， （c）稀疏向量聚合效率不高；（d）缺乏强制客户输入格式正确的能力。 如何构建一个有效，强大的安全聚合协议来解决所有这些挑战是一个悬而未决的问题。</p><h3 id="4-4-3-分布式差分隐私训练"><a class="header-anchor" href="#4-4-3-分布式差分隐私训练">¶</a>4.4.3 分布式差分隐私训练</h3><p>  如果没有受信任的服务器，则可以使用分布式差异隐私（在第4.2.2节中介绍）来保护参与者的隐私。<br><b>分布式差分隐私下的通信，隐私和准确性权衡</b><br>  我们指出，在分布式差异隐私中，三个性能指标是普遍关注的：准确性，隐私和通信，并且一个重要的目标是确定这些参数之间可能的折衷。 我们注意到，在没有隐私要求的情况下，关于分布估计（例如[376]）和通信复杂性的文献已经很好地研究了通信和准确性之间的权衡（有关教科书的参考，请参见[248]） 。 另一方面，在假设所有用户数据都由一个实体保存并且因此不需要进行通信的集中式设置中，从[147，146]的基础性工作开始，就已经在中央DP中广泛研究了准确性和隐私之间的权衡取舍。<br><b>安全混合的折衷选择</b><br>  最近在混合模型中研究了这些折衷，以解决聚合的两个基本任务（目标是计算用户输入的总和）和频率估算（输入属于离散集，目标是 估算拥有给定元素的用户数）。 有关这两个问题的最新发展情况，请参见表9和表10。 两个值得注意的开放问题是（i）在改组模型中研究纯差异隐私，以及（ii）在多消息设置中确定变量选择的最佳的私密性，准确性和通信折衷情况（在最近[178]提到的单消息情况下紧紧贴合的下限）<br><b>安全集聚协议的折衷选择</b><br>  研究以下类似问题以进行安全聚合将非常有意思。假设一轮有 $n$ 个用户参与的联邦学习，同时假设每个用户 $i$ 持有数据 $x_i$。用户 $i$ 对 $x_i$ 采用算法 $A(.)$ 来获得 $y_i = A(x_i)$；在这里，$A(.)$ 可以被视为压缩方案和私有化方案。通过使用安全集聚协议作为黑盒，服务提供商可以观察 $\overline y = \Sigma_i A(x_i)$ 同时可以通过计算 $\hat{\overline x} = g(\overline y)$ 来使用 $\overline{y}$ 估计 $x_i$ 的真实总值 $\overline{x}$。理想情况下，我们希望以最小化x估计误差的方式设计 $A(.),g(.)$；形式上，我们想解决最优化问题 $min{g,A} ||g(\Sigma_i A(x_i)) - \Sigma_i x_i||$，此处 $||.||$ 可以是 $l_1$ 范数或者 $l_2$ 范数。当然，在不对 $g(.)$ 和 $A(.)$ 施加任何约束的情况下，我们始终可以选择它们作为恒等函数，并获得0错误。然而，$A(.)$ 需要满足两个约束：（1） $A(.)$ 应该输出B位（可以认为是每个用户的通信成本）,同时（2） $\overline y = \Sigma_i A(x_i)$ 应当是 $\overline x = \Sigma_i x_i$ 的 $(\varepsilon, \delta)-DP$ 版本。因此，关注的基本问题是确定在聚合时也能实现DP且同时满足固定通信预算的最优算法 $A$。换个角度看问题，对于固定的 $n,B,\varepsilon,\delta$，我们希望实现的最小的 $l_1$ 或 $l_2 $ 误差是多少？我们注意到Agarwal等人的最新工作[13]提供了一种基于均匀量化和二项式噪声相加的候选算法A。但是，尚不清楚所提出的方法是否是在这种情况下的最佳方法。因此，在上述约束下得出 $l_1$ 或 $l_2$ 误差的下限是非常重要的。<br><b>隐私账户？隐私登录？</b><br>  在DP的中心模型中，经常使用欠采样的高斯机制来实现DP，并且使用==矩值会计==方法在FL的各轮之间紧密跟踪隐私预算（请参见第4.3节中的讨论）。 但是，在DP的分布式设置中，由于与安全混合和安全聚合的实际实现相关的有限精度问题，因此无法使用高斯机制。 因此，该空间中的现有工作已恢复为具有离散性质的噪声分布（例如，添加伯努利噪声或二项式噪声）。 尽管这样的分布有助于解决由安全混合/聚合的基础实现所施加的有限精度约束，但它们自然不会从==矩数会计==方法中受益。 因此，一个重要的开放问题是推导针对这些分布式DP考虑的离散（和有限支持）噪声分布量身定制的==隐私权计费==技术。<br><b>处理用户掉线问题</b><br>  上面的分布式DP模型假设参与的客户端在一轮中保持与服务器的连接。 但是，当大规模运行时，某些客户端会由于网络连接断开或暂时不可用而退出。 这要求分布式噪声生成机制要针对此类遗漏具有鲁棒性，并且还要影响扩张联邦学习的规模和对大量参与客户端的分析。<br>就健壮的分布式噪声而言，客户端退出可能会导致添加的噪声太少，无法满足差分隐私epsilon目标。 保守的方法是增加每个客户端的噪音，以便即使使用最少数量的客户端才能满足差分隐私 $\varepsilon$ 目标，以使服务器完成安全聚合并计算总和。 但是，当更多的客户报告时，这会导致过多的噪音，这引发了一个问题，即是否有可能提供更有效的解决方案。<br>  在扩展方面，增加参与安全聚合回合的客户端数量时，丢失的客户端数量将成为瓶颈。 同时收集足够的客户也可能是一个挑战。 为此，可以对协议进行结构化，以便客户端可以在长时间运行的聚合回合过程中多次连接，以完成其任务。 更普遍的，还有在文献中尚未系统地解决当客户可能间歇性可用时的大规模操作的问题。<br><b>新的可信赖的模型</b><br>  联合学习框架利用联合学习的独特计算模型，并可能在对抗性用户的能力上做出现实的假设，从而推动了比以前使用的模型更加精细的新信任模型的开发。例如，假设多少比例的客户可能会受到对手的损害？攻击者是否有可能同时攻击服务器和大量设备，或者通常假设攻击者只能攻击一个或另一个设备就足够了吗？在联合学习中，服务器通常由众所周知的实体（例如长期存在的组织）操作。可否利用它来建立一个信任模型，在该模型中，服务器的行为是可信的但经过验证的，即，其中不会阻止服务器偏离所需协议，但是如果服务器确实偏离了协议，则很有可能被检测到（从而破坏信任，声誉，以及托管组织的潜在财务或法律地位）？</p><h3 id="4-4-3-在训练子模型时保护隐私"><a class="header-anchor" href="#4-4-3-在训练子模型时保护隐私">¶</a>4.4.3 在训练子模型时保护隐私</h3><p>  有许多这种情况出现，就是其中每个客户端可能具有仅与正在训练的完整模型的相对较小部分有关的本地数据。 例如，对大型清单进行操作的模型，包括自然语言模型（对单词的清单进行操作）或内容排名模型（对内容的清单进行操作）），经常使用嵌入查找表作为神经网络的第一层。 通常，客户仅与极少量的清单项进行交互，并且在许多训练策略下，客户数据支持更新的唯一嵌入向量是与客户交互的物料相对应的嵌入向量。<br>  再举一个例子，多任务学习策略可能是实现个性化的有效方法，但可能会产生复合模型，其中任何特定的客户端仅使用与该客户端的用户群相关联的子模型，如3.3.2节所述。<br>  如果不关心沟通效率，那么子模型训练就像标准的联合学习：客户将在参与时下载完整模型，使用与他们相关的子模型，然后提交涵盖整个模型参数的集合的模型更新（即，除了与相关子模型相对应的条目中，其余所有地方都为零）。 但是，在部署联合学习时，通信效率通常是一个重要问题，这引发了我们是否可以实现通信效率高的子模型训练的问题。<br>  如果没有隐私敏感信息进入客户将更新哪个特定子模型的选择，那么可能会有直接的方法来适应联合学习以实现有效的通信子模型训练。 例如，一个人可以运行多个联合学习过程的副本，每个子模型一个,或者并行（例如，客户端基于他们希望更新的子模型，选择合适的联合学习实例参与），或者按顺序（例如， 对于每轮FL，服务器都会发布要更新哪个子模型的信息），或者两者混合。 但是，尽管此方法具有较高的通信效率，但服务器可以观察到客户端选择的子模型。<br>  能否在保持客户的子模型选择私密性的同时，实现沟通效率高的子模型联合学习？ 一种有前景的方法是将PIR用于私有子模型下载，同时使用针对稀疏向量优化的安全聚合变体来聚合模型更新[94，216，310]。<br>  该领域中的开放问题包括表征与实际感兴趣的子模型训练问题相关联的稀疏机制，以及开发在这些稀疏机制中通信效率高的稀疏安全聚合技术。 与仅简单地使每种技术独立运行（例如，通过在两个功能的实现之间分担一些消耗）相比，是否可以共同优化私有信息检索（PIR）和安全聚合以实现更好的通信效率，这也是一个悬而未决的问题。<br>  某些形式的局部和分布式差分隐私在这里也带来了挑战，因为通常会将噪声添加到向量的所有元素中，甚至是零。 结果，在每个客户端上添加此噪声将把原本稀疏的模型更新（即仅子模型上的非零）转换为密集的私有化模型更新（几乎在任何地方几乎都为非零）。 是否可以解决这种紧张关系是一个悬而未决的问题，即是否存在有意义的分布式差分隐私实例化，该实例化也保持了模型更新的稀疏性。<br><img src="/2022/09/06/advances-and-open-problems-in-federated-learning-yue-du-bi-ji/8.png" alt="图九：具有 $(\varepsilon,\delta)$-差分隐私的的多消息混合模型中的差分私有聚合协议的比较。参与方数目为n，$l$ 为整数参数。消息大小以位为单位。为便于阅读，我们假设 $\varepsilon \le O(1)$，并且渐近符号被抑制。"><br><img src="/2022/09/06/advances-and-open-problems-in-federated-learning-yue-du-bi-ji/9.png" alt="图十：在不同模型的DP中，对大小为B的域以及n个以上的用户进行频率估计时，预期最大误差的上限和下限。 边界固定的，正的隐私参数 $\varepsilon$ 和 $\delta$ 表示界限，并且渐近符号抑制因子 $\overline\Theta / \overline{O}/\overline\Omega$ 在B和n中为多对数。==每个用户的通信以发送的位数为单位。 在所有的上限中，该协议相对于用户是对称的，并且不需要公共随机性，引用的是我们所知道的第一个结果，它暗示了规定的界限。"></p><h2 id="4-5-用户感知（users’-perception）"><a class="header-anchor" href="#4-5-用户感知（users’-perception）">¶</a>4.5 用户感知（users’ perception）</h2><p>  联邦学习蕴含了数据收集和最小化的原则（focused data collection and minimization），并且可以减小许多由系统本身带来的隐私风险。然而，正如上文所说，搞清楚联邦学习本身提供或者不提供哪些保护措施、哪些技术可以用来抵御4.1节中的威胁模型是十分重要的。前几节重点关注在抵御精度威胁模型时，隐私的严格量化（rigorous quantification of privacy against precise threat models），这节重点关注用户体验（users’ perception）和需求的相关挑战。<br>  以下是几个具有重要实用价值的待解决的问题。是否有办法可以让普通用户直观地了解联邦学习的优缺点。联邦学习的基础结构的哪些参数和功能可能足以（或不足）满足隐私和数据最小化要求？联邦学习可能会让用户误以为他的隐私没有问题了吗（Might federated learning give users a false sense of privacy?）？当用户更多地了解联邦学习在他的数据上的操作时，如何能使用户对他的数据隐私感到安全且保障隐私数据确实是安全的？不同的用户对隐私的评估是否一致呢？人们想要保护的非隐私的内容（facts）呢？了解这些可以让我们设计出更好的机制吗？有什么方法可以很好地模拟人们的隐私偏好，从而决定如何设置这些参数？如果不同技术的实用程序/隐私/安全属性不同，谁来决定使用哪种技术？只是服务提供商？还是用户？还是他们的操作系统？他们的政治管辖权？（ Their political jurisdiction?）是否有像“受保护的隐私（仅）” [230]这样的机制可以为大多数用户提供隐私保障，同时允许对社会优先事项（例如反恐）进行有针对性的监视？有没有一种方法可以让用户选择所需的隐私级别？<br>  对于解决这些问题，似乎有两个重要的方向特别重要。</p><h3 id="4-5-1-了解特定分析任务的隐私需求"><a class="header-anchor" href="#4-5-1-了解特定分析任务的隐私需求">¶</a>4.5.1 了解特定分析任务的隐私需求</h3><p>  FL的许多潜在应用涉及复杂的学习任务和来自用户的高维数据，这两者都可能导致需要大量噪声来保护差分隐私。但是，如果用户不太介意受保护的数据不受各种干扰，则可以放宽隐私约束，允许添加少量的噪声。例如，考虑由智能家居恒温器生成的数据，使得设备在房屋空置时关闭，而在居民返回家园时打开。根据此数据，观察者将推断出居民晚上什么时候回家，这可能是高度敏感的。但是，较粗略的信息结构只能显示居民是否在凌晨2-4点之间处于睡眠状态，这可以说不那么敏感。<br>  这种方法在Pufferfish隐私框架中规范化地提出[235]，该框架允许分析人员指定一类受保护的推断（predicates ），必须在保证差异性隐私的前提下进行学习，而所有其他推断（predicates ）也可以在没有差异性隐私的情况下进行学习。为了使这种方法在实践中提供令人满意的隐私保证，分析人员必须了解用户对其特定分析任务和数据收集程序的隐私需求。可以修改联邦学习框架，以允许各个用户指定他们允许和不允许的推断（inferences） 。这些数据限制可以在设备上进行处理，在联邦学习模型更新步骤中仅与服务器共享“允许”信息，也可以在收集数据后将其作为聚合步骤的一部分。应该做进一步的工作来开发将这种用户偏好纳入联邦学习模型的技术工具，并开发对于用户有意义地偏好的技术（meaningful preference elicitation from users.）。</p><h3 id="4-5-2-行为研究以激发隐私首选项"><a class="header-anchor" href="#4-5-2-行为研究以激发隐私首选项">¶</a>4.5.2 行为研究以激发隐私首选项</h3><p>  任何要求个人用户指定自己的隐私标准的隐私保护方法也应包括行为或现场研究，以确保用户可以表达知情的偏好。任何可以获得隐私的方法都需要用户自己来指定隐私保护标准，而且这些方法需要包括行为和领域内的研究，这样就可以保证用户充分地表达自己的偏好（ informed preferences）。这应同时包括教育成分（educational component）和偏好测量（preference measurement）。教育部分应衡量并提高用户对所使用的隐私技术（例如第4.2节）和数据使用细节的理解（译者注：这里的说的“教育”估计是指用户使用引导）。对于涉及联邦学习的应用程序，还应包括联邦学习的说明以及将要发送到服务器的数据是什么。一旦研究的过程说明（educational component）证实了典型用户可以很好地理解学习过程所提供的隐私保护，那么研究者就可以开始偏好激发了（preference elicitation）。这可以在行为实验室，大规模现场实验或专题研究小组中发生。这里应当谨慎，以确保提供有关其偏好数据的用户足够了解情况，以提供高质量的数据，并能够代表目标人群。尽管行为经济学和实验经济学的丰富领域早已表明，人们在公共和私人条件下的行为有所不同（也就是说，其他人是否观察到他们的选择），但在引起人们对差异性隐私的偏好方面所做的行为工作却很少。[ 126，10]。扩展这一工作范围将是迈向未来广泛实施隐私联邦学习的关键一步。在这里，教育部分（educational component)的结果将对确保研究参与者充分了解情况并理解他们面临的决定很有用。这是这些实验的重要参与者，这些实验应遵循道德原则，并且不涉及任何欺骗行为。</p>]]></content>
      
      
      <categories>
          
          <category> 科研学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 联邦学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2022/09/06/hello-world/"/>
      <url>/2022/09/06/hello-world/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a class="header-anchor" href="#Quick-Start">¶</a>Quick Start</h2><h3 id="Create-a-new-post"><a class="header-anchor" href="#Create-a-new-post">¶</a>Create a new post</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo new &quot;My New Post&quot;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a class="header-anchor" href="#Run-server">¶</a>Run server</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo server<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a class="header-anchor" href="#Generate-static-files">¶</a>Generate static files</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo generate<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a class="header-anchor" href="#Deploy-to-remote-sites">¶</a>Deploy to remote sites</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo deploy<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      <categories>
          
          <category> hexo </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo blog </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>差分隐私实现机制</title>
      <link href="/2022/03/17/chai-fen-yin-si-shi-xian-ji-zhi/"/>
      <url>/2022/03/17/chai-fen-yin-si-shi-xian-ji-zhi/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>  在实践中为了使一个算法满足差分隐私保护的要求，对不同的问题有不同的实现方法，这些实现方法称为“机制”。$ Laplace $ 机制 $ Laplace \; Mechanism $ 与指数机制 $ Exponential \; Mechanism $ 是两种最基础的差分隐私保护实现机制。其中，$ Laplace $ 机制适用于对数值型结果的保护，指数机制则适用于非数值型结果。</p><h2 id="Laplace-机制"><a class="header-anchor" href="#Laplace-机制">¶</a>$ Laplace $ 机制</h2><p>  $ Laplace $ 机制通过向确切的查询结果中加入服从 $ Laplace $ 分布的随机噪声来实现 $ \varepsilon -$ 差分隐私保护。记位置参数为 $ 0 $、尺度参数为 $ b $ 的 $ Laplace $ 分布为 $ Lap(b) $，那么其概率密度函数为<br>$$<br>p(x) = \frac{1}{ {2b} }\exp ( - \frac{ {|x|} }{b})<br>$$<br>  <b>定义1. $ Laplace $ 机制.</b>给定数据集 $ D $，设有函数 $ f:D \to {R^d} $，其敏感度为 $ \Delta f $，那么随机算法 $ M(D) = f(D) + Y $ 提供 $ \varepsilon -$ 差分隐私保护，其中 $ Y \sim Lap(\Delta f/\varepsilon ) $ 为随机噪声，服从尺度参数为 $ \Delta f/\varepsilon $ 的 $ Laplace $ 分布。<br><img src="/2022/03/17/chai-fen-yin-si-shi-xian-ji-zhi/1.png" alt="Laplace概率密度函数"></p><h2 id="指数机制"><a class="header-anchor" href="#指数机制">¶</a>指数机制</h2><p>  由于 $ Laplace $ 机制仅适用于数值型查询结果，而在许多实际应用中，查询结果为实体对象（例如一种方案或一种选择）。对此，$ McSherry $ 等人提出了指数机制。<br>  设查询函数的输出域为 $ Range $，域中的每个值 $ r \in Range $ 为一实体对象。在指数机制下，函数 $ q(D,r) \to R $ 称为输出值 $ r $ 的可用性函数，用来评估输出值 $ r $ 的优劣程度。<br>  <b>定义2. 指数机制.</b>设随机算法 $ M $ 输入为数据集 $ D $，输出为一实体对象 $ r \in Range $ ， $ q(D,r) $ 为可用性函数，$ \Delta q $ 为函数 $ q(D,r) $ 的敏感度。若算法 $ M $ 以正比于 $ \exp (\frac{ {\varepsilon q(D,r)} }{ {2\Delta q} }) $ 的概率从 $ Range $ 中选择并输出 $ r $ ，那么算法 $ M $ 提供 $ \varepsilon -$ 差分隐私保护。<br>  以下是一个指数机制的应用实例。假如拟举办一场体育比赛，可供选择的项目来自集合｛足球，排球，篮球，网球｝，参与者们为此进行了投票，现要从中确定一个项目，并保证整个决策过程满足 $ \varepsilon -$ 差分隐私保护要求。以得票数量为可用性函数，显然 $ \Delta q = 1 $ 。那么按照指数机制，在给定的隐私保护预算  $ \varepsilon $ 下，可以计算出各种项目的输出概率，如下图所示：<br><img src="/2022/03/17/chai-fen-yin-si-shi-xian-ji-zhi/2.png" alt="指数机制应用示例"><br>  可以看出，在ε较大时（如 $ \varepsilon ＝１ $），可用性最好的选项被输出的概率被放大。当 $ \varepsilon $ 较小时，各选项在可用性上的差异则被平抑，其被输出的概率也随着ε的减小而趋于相等。</p>]]></content>
      
      
      <categories>
          
          <category> 科研学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 差分隐私 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>差分隐私性质</title>
      <link href="/2022/03/15/chai-fen-yin-si-xing-zhi/"/>
      <url>/2022/03/15/chai-fen-yin-si-xing-zhi/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h1>差分隐私保护算法的组合性质</h1><p>  一个复杂的隐私保护问题通常需要多次应用差分隐私保护算法才能得以解决．在这种情况下，为了保证整个过程的隐私保护水平控制在给定的预算 $\varepsilon$ 之内，需要合理地将全部预算分配到整个算法的各个步骤中．这时可以利用隐私保护算法的两个组合性质，如下图所示：<br><img src="/2022/03/15/chai-fen-yin-si-xing-zhi/1.png" alt="差分隐私保护算法的组合性质"><br>  <b>性质1.序列组合性.</b>设有算法 $ {M_1},{M_2}, \cdots ,{M_n} $，其隐私保护预算分别为 $ {\varepsilon _1},{\varepsilon _2}, \cdots ,{\varepsilon _n} $， 那么对于同一数据集 $ D $，由这些算法构成的组合算法 $ M({M_1}(D)),{M_2}(D), \cdots ,{M_n}(D)) $ 提供 $ (\sum\limits_{i = 1}^n { {\varepsilon _i} } )- $ 差分隐私保护。<br>  该性质表明，一个差分隐私保护算法序列构成的组合算法，其提供的隐私保护水平为全部预算的总和。该性质也称为“序列组合性”。<br>  <b>性质2.序列组合性.</b>设有算法 $ {M_1},{M_2}, \cdots ,{M_n} $，其隐私保护预算分别为 $ {\varepsilon _1},{\varepsilon _2}, \cdots ,{\varepsilon _n} $，那对于不相交的数据集 $ {D_1},{D_2}, \cdots ,{D_n} $， 这些算法构成的组合算法 $ M({M_1}({D_1})),{M_2}({D_2}), \cdots ,{M_n}({D_n})) $ 提供 $ (\max {\varepsilon _i}) - $ 差分隐私保护。<br>  该性质表明，如果一个差分隐私保护算法序列中所有算法处理的数据集彼此不相交，那么该算法序列构成的组合算法提供的隐私保护水平取决于算法序列中的保护水平最差者，即预算最大者。该性质也称为“并行组合性”。</p><h1>差分隐私保护算法的变换不变性</h1><p>  给定任意一个算法 $ A_1 $ 满足 $ \varepsilon - $ 差分隐私。对于任意算法 $ A_2 $ ​（$ A_2 $ 不一定满足差分隐私的算法），则有 $ A(\cdot)=A_2(A_1(\cdot)) $ 满足 $ \varepsilon - $ 差分隐私。<br>  说明了差分隐私对于后处理算法具有免疫性, 如果一个算法的结果满足 $ \varepsilon - $ 差分隐私, 那么在这个结果上进行的任何处理都不会对隐私保护有所影响。</p><h1>差分隐私保护算法的中凸性</h1><p>  给定2个算法 $ A_1 $ 和 $ A_2​ $ 满足 $ \varepsilon - $ 差分隐私。对于任意的概率 $ p \in [0,1] $ ，用符号 $ A_p $ 表示为一种机制，它以 $ p $ 的概率使用 $ A_1 $ 算法，以 $ 1 − p $ 的概率使用 $ A_2 $​ 算法则 $ A_p $ 机制满足 $ \varepsilon - $ 差分隐私。<br>  说明了如果有 $ 2 $ 个不同的差分隐私算法, 都提供了足够的不确定性来保护隐私, 那么可以通过选择任意的算法来应用到数据上实现对数据的隐私保护, 只要选择的算法和数据是独立的。</p>]]></content>
      
      
      <categories>
          
          <category> 科研学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 差分隐私 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>玻尔兹曼机</title>
      <link href="/2022/03/07/bo-er-zi-man-ji/"/>
      <url>/2022/03/07/bo-er-zi-man-ji/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>  在本文开始前，为大家强推一位大佬，因为玻尔兹曼机（BM）、受限玻尔兹曼机（RBM）、深度信念网络（DBN）相关内容是我毕设的主题，恰好在网上看到了这位大佬的相关研究，特别赞，以下附上该为大佬的<a href="https://www.zhihu.com/people/qing-kuang-shu-sheng-18-44">知乎主页</a>和大佬的<a href="https://github.com/2019ChenGong/Machine-Learning-Notes">gthub主页</a>。里面有大佬对于这些模型的公式推导。下面写一些我学习后的个人总结，里面大部分是一些大佬的推导过程，大家想深入研究，直接看大佬的专栏就好，这里仅用来自我总结。</p><h1>介绍</h1><p>  玻尔兹曼机（Boltzmann Machine，BM）可以看作是一种随机动力系统，每个变量的状态都以一定的概率受到其他变量的影响。下面是任何讲到BM都会涉及到的经典模型图：<br><img src="/2022/03/07/bo-er-zi-man-ji/1.png" alt="玻尔兹曼模型图"><br>  其中BM模型有如下的三个特性：<br>  （1）每个随机变量都可以用一个二值的随机变量表示，即0和1。<br>  （2）所有节点之间都是全连接的。<br>  （3）每两个变量之间的相互影响是对称的，即权重对称。<br>  我们假设 $ {v_{D \times 1} } \in { \{0,1\}^D } $，$ {h_{P \times 1} } \in { \{0,1\}^P} $。根据“受限玻尔兹曼”那节的知识，可以得出，概率图的联合概率分布为：<br>$$<br>\begin{array}{l}<br>P(v,h) = \frac{1}{2}\exp {  - E(v,h)} \\<br>E(v,h) =  - ({v^T} \cdot W \cdot h + \frac{1}{2}{v^T} \cdot Lv + \frac{1}{2}{h^T} \cdot J \cdot h)<br>\end{array}<br>$$<br>其中，$ L = { [ { {L_{ij} } } ]_{D \times D} } $ 表示可见层神经元内部的权重，$ J = { [ { {J_{ij} } } ]_{P \times P} } $ 表示隐藏层神经元内部的权重，$ W = { [ { {w_{ij} } } ]_{D \times P} } $ 表示可见层和隐藏层神经元之间的权重。在此问题中，要学习的参数集合为 $ \theta  = \{ W,L,J\} $。</p><h1>基于极大似然的梯度上升</h1><p>  基于极大似然的梯度上升包括极大似然函数和梯度两个部分。<b>极大似然估计的主要思路是，使极大似然函数最大时的参数</b>。首先明确一下，要求的参数为 $ \theta  = \{ W,L,J\} $。假设样本集合为 $v$，$ |v| = D $，那么似然函数为：<br>$$ \sum\limits_v {P(v) = } \sum\limits_v {\sum\limits_h {P(v,h)} } $$<br>那么对数似然函数为：<br>$$ \frac{1}{D}\sum\limits_v {\log P(v)} $$</p><h2 id="似然导数求解"><a class="header-anchor" href="#似然导数求解">¶</a>似然导数求解</h2><p>  是对对数似然函数求导，即为：<br>$$<br>\frac{\partial }{ {\partial \theta } }\frac{1}{D}\sum \limits_v {\log p(v) } = \frac{1}{D}\sum\limits_v {\frac{ {\partial \log p(v)} }{ {\partial \theta } } }<br>$$<br>而Boltzmann Distribution 的 log 似然梯度为：<br>$$<br>\frac{1}{D}\frac{ \partial }{ { \partial \theta } }\log P(v) = \frac{1}{D}\left( {\sum \limits_h {\sum \limits_v {P(v,h)} } \frac{ \partial }{ { \partial \theta } }E(h,v) - \sum \limits_h {P(h|v)\frac{ \partial }{ { \partial \theta } }E(h,v)} } \right)<br>$$<br>我们对 $ w $ 求导，可以得到：<br>$$<br>\frac{1}{D}\sum \limits_v {\frac{ {\partial \log P(v)} }{ {\partial W} } } =  \frac{1}{D}\sum \limits_v {\sum \limits_h {P(v|h)} }  \cdot v{h^T} - \sum \limits_v {\sum \limits_h {P(v,h)} }  \cdot v{h^T}<br>$$<br>可以简写为：<br>$$<br>\frac{1}{D}\sum \limits_v {\frac{ {\partial \log P(v)} }{ {\partial W} } } = \mathbb{E}_{P_{data} }[v{h^T}] - \mathbb{E}_{P_{model} }[v{h^T}]<br>$$<br>其中：<br>$$<br>\begin{array}{l}<br>{P_{data}} = {P_{data}}(v) \cdot {P_{model}}(h|v) \\<br>{P_{model}} = {P_{model}}(v,h)<br>\end{array}<br>$$<br>在 $ \sum\limits_v {\sum\limits_h {P(v,h)} } $ 中，$ P(v,h) $ 是生成模型，本身就是我们建立的模型，所以被称为 $P_{model}$。而在 $ \sum\limits_v {\sum\limits_h {P(v|h)} } $ 首先从经验分布 $P(v)$ 从采样得到 $v$，然后利用模型分布来求解 $P(h|v)$，所以 $P_{data} = P_{data}(v) \cdot P_{model}(h|v)$。采样出 $P_{model}(h|v)$ 和 $P_{model}(v)$ 就可以求解出 $P_{model}(h,v)$ 了。按照同样的方法可以求得对 ${L, J}$ 的导数。<br>  同理通过计算可以得到每个参数矩阵的似然梯度为：<br>$$<br>\begin{array}{l}<br>\Delta W = \alpha ({\mathbb{E}_{ {P_{data} } } }[v{h^T}] - {\mathbb{E}_{ {P_{model} } } }[v{h^T}]) \\<br>\Delta L = \alpha ({\mathbb{E}_{ {P_{data} } } }[v{v^T}] - {\mathbb{E}_{ {P_{model} } } }[v{v^T}]) \\<br>\Delta J = \alpha ({\mathbb{E}_{ {P_{data} } } }[h{h^T}] - {\mathbb{E}_{ {P_{model} } } }[h{h^T}])<br>\end{array}<br>$$</p><h1>基于MCMC的似然梯度下降</h1><h2 id="MCMC似然梯度求解总述"><a class="header-anchor" href="#MCMC似然梯度求解总述">¶</a>MCMC似然梯度求解总述</h2><p>  在上面我们使用梯度上升法来使 log 似然函数达到最大，从而求解对应的最优参数。参数更新公式为：<br>$$<br>{\theta ^{(t + 1)} } = {\theta ^{(t)} } + \Delta \theta<br>$$<br>其中，$ \Delta \theta  = \{ {\Delta W,\Delta L,\Delta J} \} $ 。以 $ \Delta W $ 为例，$ \Delta W $ 是一个矩阵 $ \Delta W = \left[ {\Delta {w_{ij} } } \right] $。其中：<br>$$<br>\Delta {w_{ij} } = \alpha \left[ {\underbrace { {\mathbb{E}_{ {P_{data} } } }\left[ { {v_i}{h_j} } \right]}_{Postive \; phase} - \underbrace { {\mathbb{E}_{ {P_{\bmod el} } } }\left[ { {v_i}{h_j} } \right]}_{Negative \; phase} } \right]<br>$$<br>  在 Boltzmann Machines 中，Postive phase 和 Negative phase 都是 Intractable。所以，Hinton 提出了用 MCMC 来对 $ P(h|v) $ 进行采样。<b>在求解 $ \Delta W $ 中，主要是解决三个部分，$ P_{data}(v) $, $ P_{model}(h|v) $, $ P_{model}(v, h) $，其中 $ P_{model}(v, h) = P_{model}(h|v) · P_{model}(v) $。所以，而 $ P_{data}(v) $ 和 $ P_{model}(v) $ 相对比较简单，所以难点在于 $ P_{model}(h|v) $ 的求解。</b>但在 Boltzmann Machines 中，由于关系过于复杂，没有办法分解，甚至最大团分解都没有用，因为最大团就是自己，那么连 $ P_{model}(h|v) $ 都求不出来，那么 Postive phase 和 Negative phase 都是 Intractable。<br>  但是通过推导可以得出：<br>$$<br>\begin{array}{l}<br>P({v_i} = 1|h,{v_{ - i} }) = \sigma (\sum\nolimits_{j = 1}^P { {w_{ij} }{h_j} }  + \sum\nolimits_{k = 1\backslash i}^D { {L_{ik} }{v_k} } )\<br>P({h_j} = 1|v,{h_{ - j} }) = \sigma (\sum\nolimits_{j = 1}^P { {w_{ij} }{v_i} }  + \sum\nolimits_{m = 1\backslash j}^D { {J_{im} }{h_n} } )<br>\end{array}<br>$$<br>  公式表达的是，在已知一个节点以外的所有的点的条件下，这个节点的条件概率是可求的。其中 $ 1 \backslash i $ 表达的意思是 $ 1 ∼ D $ 但不包括 $ i $ 的所有节点。<br>  具体的推导过程在大佬的博客和github中有，这里不再搬运。<br>  <b>其他具体的介绍以及推导过程，点击该链接可以下载查看。</b><a href="White_2020_04_08_Boltzmann_Machine.pdf" title="相关资料下载">点击下载</a></p>]]></content>
      
      
      <categories>
          
          <category> 科研学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工智能 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度信念网络</title>
      <link href="/2022/03/04/shen-du-xin-nian-wang-luo/"/>
      <url>/2022/03/04/shen-du-xin-nian-wang-luo/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h1>简介</h1><p>  $ Deep \; Belief \; Network $ 是 $ Hinton $ 在 2006 年提出的方法，应用在分类问题上的效果明显好过 $ SVM $。首先，来看看 $ Deep \; Belief \; Network $ 这个名字的含义，$ Belief \; Network $ 实际上就是 $ Bayes \; Network $（有向图模型），而 $ Deep $ 的含义就很简单了，代表有很多层。所以，从字面上理解，$ Deep \; Belief \; Network $ 可以认为是有很多层的有向图模型。$ Deep \; Belief \; Network $ 的概率图模型如下所示：<br><img src="/2022/03/04/shen-du-xin-nian-wang-luo/1.png" alt="Deep Belief Network 的概率图模型"><br>  从上述图中，可以看出 $ DBN $ 是一个混合模型，上面是 $ Restricted \; Boltzmann \; Model $ $ (RBM) $ 模型，下面是一个 $ Sigmoid \; Belief \; Network \; (SBN) $。而每个节点都服从 $ 0/1 $的伯努利分布，实际上就是一个分层模型。深层的含义，我们将会在下文中描述。注意，这里的 $ w $ 是用来描述节点直接连接权重的矩阵。</p><h2 id="SBN-简介"><a class="header-anchor" href="#SBN-简介">¶</a>$ SBN $ 简介</h2><p>  $ SBN $ 的概率图模型如下图所示：<br><img src="/2022/03/04/shen-du-xin-nian-wang-luo/2.png" alt="Sigmoid Belief Network 的概率图模型"><br>其中：<br>$$<br>P({S_i} = 1) = \frac{1}{ {1 + \exp \left\{ { {b_i} + {w_1}{s_1} + {w_2}{s_2} + {w_3}{s_3} } \right\} } }<br>$$</p><h2 id="DBN-的概率联合分布"><a class="header-anchor" href="#DBN-的概率联合分布">¶</a>$ DBN $ 的概率联合分布</h2><p>  在使用极大似然估计法中基本离不开求联合概率分布。所以，这里需要求 $ DBN $ 的联合概率分布，而求联合概率分布最终的就是因子分解。那么，我们首先要理顺一下各层之间的依赖关系。显然，$ v $ 层只和 $ h^{(1)} $ 有关， $ h^{(1)} $ 层只和  $ h^{(2)} $ ，那么有：<br>$$<br>\begin{array}{ccccc}<br>P(v,{h^{(1)} },{h^{(2)} },{h^{(3)} }) = &amp; P(v|{h^{(1)} },{h^{(2)} },{h^{(3)} })P({h^{(1)} },{h^{(2)} },{h^{(3)} }) &amp; \\<br>= &amp; P(v|{h^{(1)} })P({h^{(1)} },{h^{(2)} },{h^{(3)} }) &amp; \\<br>= &amp; P(v|{h^{(1)} })P({h^{(1)} }|{h^{(2)} },{h^{(3)} })P({h^{(2)} },{h^{(3)} }) &amp; \\<br>= &amp; \prod\limits_i { P({v_{i} }|{h^{(1)} }) } \prod\limits_j { P(h_{j}^{(1)} | {h^{(2)} },{h^{(3)} }) P({h^{(2)} },{h^{(3)} }) }<br>\end{array}<br>$$<br>将这三个部分分布表示。根据 $ SBN $ 公式的结论，可以类比的得出：<br>$$<br>\begin{array}{cc}<br>P({v_i}|{h^{(1)} }) = sigmoid({(W_{: , i}^{(1)})}^{T} \cdot {h^{(1)} } + b_{i}^{(0)}) \\<br>P(h_{j}^{(1)}|{h^{(2)} }) = sigmoid({(W_{:,j}^{(2)})^T} \cdot {h^{(2)} } + b_j^{(1)})<br>\end{array}<br>$$<br>而 $ h^{(2)} $ 和 $ h^{(3)}  $ 之间是 $ RBM $ 模型，沿用在 $ RBM $ 那一章讲的 $ Boltzmann \; Distribution $ 可以得到：<br>$$<br>P({h^{(2)} },{h^{(3)} }) = \frac{1}{Z} \exp \{ {({h^{(3)} })^T} {w^{(3)} }{h^{(2)} } + {({h^{(2)} })^T}{b^{(2)} } + {({h^{(3)} })^T}{b^{(3)} } \}<br>$$<br>其中参数为：<br>$$<br>\theta  = \{ {W^{(1)} },{W^{(2)} },{W^{(3)} },{b^{(0)} },{b^{(1)} },{b^{(2)} },{b^{(2)} }\}<br>$$</p><h1>$ DBN $ 的叠加</h1><p>  原始的 $ RBM $ 模型的表达方式是使用对比散度的方法求解。概率图模型如下图所示：<br><img src="/2022/03/04/shen-du-xin-nian-wang-luo/3.png" alt="Restricted Boltzmann Distribution 的概率图模型"><br>$ RBM $ 的对数似然梯度的表达形式如下所示：<br>$$<br>\frac{\partial }{ {\partial {\omega _{ij} } } }\log P(v) = \sum\limits_h {P(h|v){h_i}{v_j} }  - \sum\limits_h {\sum\limits_v {P(h,v){h_i}{v_j} } }<br>$$<br>但是该公式的计算过于复杂，基本是 $ intractable $。所以提出了用结合梯度上升法的 $ Gibbs $ 采样来求梯度，从而使得 $ P(v) $ 的 $ Log ; Likelihood $ 达到最大，公式如下所示：<br>$$<br>\begin{array}{ccccc}<br>\Delta {\omega _{ij}} \leftarrow  - \Delta {\omega _{ij} } + \frac{\partial }{ {\partial {\omega _{ij} } } }\log P(v) \<br>\frac{\partial }{ {\partial {\omega _{ij} } } }\log P(v) = P({h_i} = 1|{v^{(0)} })v_j^{(0)} - P({h_i} = 1|{v^{(k)} })v_j^{(k)}<br>\end{array}<br>$$<br>引入 $ RBM $ 是为了探究观测变量的数据结构的关系，其中未观察变量被看作观察变量发生的原因。所以，模型的关注重点实际上是 $ v $，而 $ h $ 不过是我们为了探究 $ v $ 的数据结构和发生的原因，所做的模型假设而已。$ v $ 是没有标签的数据，可以认为是 $ RBM $ 方法生成的，我们可以通过改进 $ RBM $，让生成的数据更接近真实分布。</p><h2 id="RBM-的改进"><a class="header-anchor" href="#RBM-的改进">¶</a>$ RBM $ 的改进</h2><p>  根据 $ Restricted ; Boltzmann ; Distribution $ 的概率图模型可知<br>$$<br>P(v) = \sum\limits_{ {h^{(1)} } } {P(v,{h^{(1)} })}  = \sum\limits_{ {h^{(1)}}} {P({h^{(1)} })P(v|{h^{(1)} })}<br>$$<br>通常 $ P(h^{(1)}) $ 看成是 $ prior $ 先验，$ P(v|h^{(1)}) $ 则看成是一个生成过程。$ RBM $ 在无向图中并没有箭头，无向图可以看成是一个双向的有向图，如下所示：<br><img src="/2022/03/04/shen-du-xin-nian-wang-luo/4.png" alt="Restricted Boltzmann Distribution 的有向图概率图模型"><br>  这样，将无向图改写成有向图，可以把概率图分成 $ h \rightarrow v $ 和 $ v \rightarrow h $ 两个过程。两个过程的权重都是一样的。那么假定在 $ RBM $ 已经学习出来的情况下（ $ w^{(1)} $ 的参数是确定的），$ P(v|h^{(1)}) $ 可以看成是 $ h \rightarrow v $ ，显示是不变得，而$ P(h^{(1)}) $ 显然也是确定的，和 $ v \rightarrow h $ 过程相关，$ w^{(1)} $ 和 $ v $ 都是确定的。那么，我们可以猜想，可不可以不用 $ w $ 来表示 $ P(h^{(1)}) $，给 $ P(h^{(1)}) $ 重新赋一组参数，构建一个新的模型来对 $ P(h^{(1)}) $ 重新建模，用另一个 $ RBM $ 来表示 $ P(h^{(1)}) $，从而通过提高 $ P(h^{(1)}) $ 的办法来提高 $ P(v) $<br>  我们可以添加一层 $ RBM $，这样就可以给 $ P(h^{(1)}) $ 重新赋一组参数，然后通过新的 $ RBM $ 参数进行优化的方式来提高 $ P(h^{(1)}) $ 。如下图所示：<br><img src="/2022/03/04/shen-du-xin-nian-wang-luo/5.png" alt="Restricted Boltzmann Machine 的改进示意图"><br>  由此，可以认为这样一个模型，比原来的 $ RBM $ 更好，因为假设 $ P(v|b^{(1)}) $ 是固定的，实际可以优化 $ P(h^{(1)}) $ 来进一步提高模型的性能。同样的思路，可以用同样的办法来优化 $ P(h^{(2)}) $ ，所以就达到了不停的往上加的效果。<br>  我们希望在训练 $ h(1) $ 层的时候，希望 $ v $ 不会对 $ h(1) $ 造成影响，否则计算复杂度就太高了。所以，就假设在训练 $ h(1) $ 的过程中和 $ v $ 无关，所以概率图模型就变为：<br><img src="/2022/03/04/shen-du-xin-nian-wang-luo/6.png" alt="服从假设后的 RBM 改进示意图"><br>  <b>其他具体的介绍以及推导过程、训练过程，点击该链接可以下载查看。</b><a href="White_2020_04_01_Deep_Belief_Network.pdf" title="相关资料下载">点击下载</a></p>]]></content>
      
      
      <categories>
          
          <category> 科研学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工智能 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>受限玻尔兹曼机</title>
      <link href="/2022/03/03/shou-xian-bo-er-zi-man-ji/"/>
      <url>/2022/03/03/shou-xian-bo-er-zi-man-ji/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h1>简介</h1><p>  受限玻尔兹曼机（英语：$ restricted \; Boltzmann \; machine \; , RBM $ ）是一种可通过输入数据集学习概率分布的随机生成神经网络。受限玻兹曼机在降维、分类、协同过滤、特征学习和主题建模中得到了应用。根据任务的不同，受限玻兹曼机可以使用监督学习或无监督学习的方法进行训练。受限玻尔兹曼机是玻尔兹曼机（$ Boltzman \; machine，BM $）的一种特殊拓扑结构。$ BM $ 的原理起源于统计物理学，是一种基于能量函数的建模方法，能够描述变量之间的高阶相互作用，$ BM $ 的学习算法较复杂，但所建模型和学习算法有比较完备的物理解释和严格的数理统计理论作基础。$ BM $ 是一种对称耦合的随机反馈型二值单元神经网络，由可见层和多个隐层组成，网络节点为可见单元（$ visible \;unit $）和隐单元（$ hidden \; unit $），用可见单元和隐单元来表达随机网络与随机环境的学习模型，通过权值表达单元之间的相关性。正如名字所提示的那样，受限玻兹曼机是一种玻兹曼机的变体，但限定模型必须为二分图。模型中包含对应输入参数的输入（可见）单元和对应训练结果的隐单元，每条边必须连接一个可见单元和一个隐单元。（与此相对，“无限制”玻兹曼机包含隐单元间的边，使之成为递归神经网络。）这一限定使得相比一般玻兹曼机更高效的训练算法成为可能，特别是基于梯度的对比分歧（$ contrastive \; divergence $）算法。受限玻兹曼机也可被用于深度学习网络。具体地，深度信念网络可使用多个RBM堆叠而成，并可使用梯度下降法和反向传播算法进行调优。</p><h1>结构</h1><p>  $ RBM $ 可以说是所有神经网络中最简单的架构之一，属于两层神经网络，并且这些浅层神经网络是 $ DBN $（深度信念网络）的构建块。如下图，一个受限玻尔兹曼机包含一个输入层 $ visible \; layer $（第一层）和隐藏层 $ hidden \; layer $（第二层）。<br><img src="/2022/03/03/shou-xian-bo-er-zi-man-ji/1.png" alt="在这幅图片中"><br>  因为所有可见（或输入）节点的输入都被传递到所有的隐藏节点了，所以 $ RBM $ 可以被定义为对称二分图。对称意味着每个可见节点都与一个隐藏节点相连。二分则意味着它具有两部分，或者两层。图是一个数学术语，指的是由节点和边组成的网络。<br>  在每一个隐藏节点，每个输入 $ x $ 都与对应的权重 $ w $ 相乘。也就是说，一个输入 $ x $ 会拥有 12 个权重（4 个输入节点 × 3 个输出节点）。两层之间的权重总会形成一个矩阵，矩阵的行数等于输入节点的个数，列数等于输出节点的个数。每个隐藏节点会接收 4 个与对应权重相乘的输入。这些乘积的和再一次与偏置相加，并将结果馈送到激活函数中以作为隐藏单元的输出。<br>  如果这两层是更深网络的一部分，那么第一个隐藏层的输出会被传递到第二个隐藏层作为输入，从这里开始就可以有很多隐藏层，直到它们增加到最终的分类层。对于简单的前馈网络，$ RBM $ 节点起着自编码器的作用，除此之外，别无其它。<br><img src="/2022/03/03/shou-xian-bo-er-zi-man-ji/2.png" alt="在这幅图片中"></p><h1>能量模型</h1><p>  有些深度学习架构使用能量来衡量模型的质量，深度学习模型的目的之一是编码变量间的依赖关系。给变量的每种配置分配一个标量作为能量，可以描述这一依赖关系。能量较高意味着变量配置的兼容性不好。能量模型总是尝试最小化一个预先定义的能量函数。<br>  $ RBM $ 的能量函数定义为：<br>$$ E(v,h) =  - \sum\limits_i { {a_i}{v_i} - \sum\limits_j { {b_j}{h_j} - \sum\limits_{i,j} { {v_i}{h_j}{w_{ij} } } } } $$<br>  由定义可知，能量函数的值取决于变量/输入状态、隐藏状态、权重和偏置的配置。RBM的训练包括为给定的输入值寻找使能量达到最小值的参数。</p><h1>能量模型</h1><p>  受限玻尔兹曼机是一个概率模型。这一模型并不分配离散值，而是分配概率。在每一时刻 $ RBM $ 位于一个特定的状态。该状态指输入层 $v$ 和隐藏层 $h$ 的神经元值。观察到 $v$ 和 $h$ 的特定状态的概率由以下联合分布给出：<br>$$<br>\begin{array}{l}<br>p(v,h) = \frac{1}{Z}{e^{ - E(v,h)} } \\<br>Z = \sum\limits_{v,h} { {e^{ - E(v,h)} } }<br>\end{array}<br>$$<br>  这里 $Z$ 被称为<b>配分函数</b>（partition function），该函数累加所有输入向量和隐藏向量的可能组合。<br>  在物理学中，这一联合分布称为玻尔兹曼分布，它给出一个微粒能够在能量 $E$ 的状态下被观测到的概率。就像在物理中一样，我们分配一个观测到状态 $v$ 和 $h$ 的概率，这一概率取决于整个模型的能量。不幸的是，由于配分函数 $Z$ 中 $v$ 和 $h$ 所有可能的组合数目十分巨大，计算这一联合分布十分困难。而给定状态 $v$ 计算状态 $h$ 的条件概率，以及给定状态 $h$ 计算状态 $v$ 的条件概率则要容易得多：<br>$$<br>\begin{array}{l}<br>p(h|v) = \prod\limits_i {p({h_i}|v)} \\<br>p(h|v) = \prod\limits_i {p({v_i}|h)}<br>\end{array}<br>$$<br>  $ RBM $ 中的每个神经元只可能是二元状态0或1中的一种。我们最关心的因子是隐藏层或输入层地神经元位于状态1（激活）的概率。给定一个输入向量 $v$ ，单个隐藏神经元 $j$ 激活的概率为：<br>$$ p({h_j} = 1|v) = \frac{1}{ {1 + {e^{( - ({b_j} + {W_j}{v_i}))} } } } = \sigma ({b_j} + \sum\limits_i { {v_i}{w_{ij} } } ) $$<br>  其中，$\sigma$ 为 $ sigmoid $ 函数。以上等式可由对之前的条件概率等式应用贝叶斯定理推导得出，类似地，单个输入神经元 $i$ 为 1 的概率为：<br>$$<br>p({v_j} = 1|h) = \frac{1}{ {1 + {e^{( - ({a_i} + {W_i}{h_j}))} } } } = \sigma ({a_i} + \sum\limits_j { {h_j}{w_{ij} } } )<br>$$<br>  <b>其他具体的介绍以及推导过程，点击该链接可以下载查看。</b><a href="White_2020_02_28_Restricted_Boltzmann_Machine.pdf" title="相关资料下载">点击下载</a></p>]]></content>
      
      
      <categories>
          
          <category> 科研学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工智能 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>差分隐私个人理解（一）</title>
      <link href="/2022/01/24/chai-fen-yin-si-ge-ren-li-jie-yi/"/>
      <url>/2022/01/24/chai-fen-yin-si-ge-ren-li-jie-yi/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h1>差分隐私简介</h1><p>  差分隐私（differential privacy）是密码学中的一种手段，旨在提供一种当从统计数据库查询时，通过混淆数据库查询结果，来实现数据在个人层面的隐私性，最大化数据查询的准确性。<br>  举一个简单的例子：例如，下表显示了一个医疗数据集 $ D $ 。<br><img src="/2022/01/24/chai-fen-yin-si-ge-ren-li-jie-yi/1.png" alt="医疗数据集示例"><br>其中的每个记录表示某个人是否患有癌症（１表示是，０表示否）。数据集为用户提供统计查询服务（例如计数查询），但不能泄露具体记录的值．设用户输入参数 $ S_{i} $，调用查询函数 $ f(i)=count(i) $ 来得到数据集前 $ i $ 行中满足“诊断结果” $ = 1 $ 的记录数量，并将函数值反馈给用户。假设攻击者欲推测 $ Alice $ 是否患有癌症，并且知道 $ Alice $在数据集的第5行，那么可以用 $ count(5) - count(4) $ 来推出正确的结果。<br>  但是，如果 $ f $ 是一个提供 $\varepsilon$-差分隐私保护的查询函数，例如$ f(i)=count(i) + noise $，其中 $ noise $ 是服从某种随机分布的噪声．假设 $ f(5) $ 可能的输出来自集合 $ \{2,2.5,3 \} $，那么 $ f(4) $ 也将以几乎完全相同的概率输出 $ \{2,2.5,3 \} $ 中的任一可能的值，因此攻击者无法通过 $f(5) - f(4) $ 来得到想要的结果．这种针对统计输出的随机化方式使得攻击者无法得到查询结果间的差异，从而能保证数据集中每个个体的安全。</p><h1>差分隐私的定义</h1><p>  对于一个有限域 $ Z $，$ z \in Z $ 为 $ Z $ 中的元素，从 $ Z $ 中抽样所得 $ z $ 的集合组成数据集 $ D $，其样本量为 $ n $，属性的个数为维度 $ d $ 。<br>  对数据集 $ D $ 的各种映射函数被定义为查询 $ (Query) $ ，用 $ F = \{ f_{1},f_{2},\cdots \} $ 来表示一组查询，算法 $ M $ 对查询 $ F $ 的结果进行处理，使之满足隐私保护的条件，此过程称为隐私保护机制。<br>  设数据集 $ D $ 和 $ {D’} $，具有相同的属性结构，两者的对称差记作 $ D \Delta D’ $，$ |D \Delta D’| $ 表示 $ D \Delta D’ $ 中记录的数量．若$ |D \Delta D’| = 1 $，则称 $ D $ 和 $ {D’} $ 为邻近数据集 $(Adjacent Dataset )$。<br>  <b>定义1.差分隐私.</b>设有随机算法 $ M $，$ P_{M} $ 为 $ M $ 所有可能的输出构成的集合．对于任意两个邻近数据集（什么是临近数据集在下面相关概念中会介绍） $ D $ 和 $ {D’} $ 以及 $ P_{M} $ 的任何子集 $ S_{M} $，若算法 $ M $ 满足：<br>$$<br>\Pr [M(D) \in {S_M}] \le \exp (\varepsilon ) \times \Pr [M(D’) \in {S_M}]<br>$$<br>则称算法 $ M $ 提供 $\varepsilon$-差分隐私保护，其中参数 $ \varepsilon $ 称为隐私保护预算。<br>  我们来看一个极端的设定，当 $ \varepsilon = 0 $ ，得到<br>$$<br>\Pr [M(D) \in {S_M}] = \Pr [M(D’) \in {S_M}]<br>$$<br>也就是说，通过算法 $ M $ 对两个相邻的数据集进行查询后，得到的查询集合是相同的，即不会揭露数据集中的信息，因此随机算法 $ M $ 具有极好的隐私性，但是这样会导致查询不会提供关于这个数据库的任何信息。所以，一般让 $ \varepsilon $ 取一个比较小的数，比如 $ 0.05 $，让 $ M $ 满足这种条件就可以了，又能通过 $ M $ 获得一些有用的信息。直白地说，就是只有一行不同的两个数据库，进行随机算法 $ M $ 后，这两个数据库分别得到的结果有相似的概率分布。</p><h1>相关概念介绍</h1><h2 id="数据集（数据库）的距离"><a class="header-anchor" href="#数据集（数据库）的距离">¶</a>数据集（数据库）的距离</h2><p>  对于数据库 $ x $，它的第一范数为：<br>$$<br>{\left| { {x} } \right|_{1} } = \sum\limits_{i = 1}^{|\chi |} { {x_i} }<br>$$<br>两个数据库 $ x $ 和 $ y $ 的 $ l_{1} $ 距离是 $ {\left| {x - y} \right|_1} $。对于 $ {\left| { {x} } \right|_1} $ 表示数据库x的大小。 $ {\left| {x - y} \right|_1} $ 表示数据 $ x $ 和 $ y $ 不同元素的个数。</p><h2 id="隐私保护预算-varepsilon"><a class="header-anchor" href="#隐私保护预算-varepsilon">¶</a>隐私保护预算 $ \varepsilon $</h2><p>  从定义１可以看出，隐私保护预算 $ \varepsilon $ 用来控制算法 $ M $ 在两个邻近数据集上获得相同输出的概率比值，它事实上体现了 $ M $ 所能够提供的隐私保护水平。在实际应用中， $ \varepsilon $ 通常取很小的值，例 如 $ 0.01 $，$ 0.1 $，或者，$ \ln 2 $，$ \ln 3 $ 等。 $ \varepsilon $ 越小，表示隐私保护水平越高．当 $ \varepsilon $ 等于 $ 0 $ 时，保护水平达到最高，此时对于任意邻近数据集，算法都将输出两个概率分布完全相同的结果，这些结果也不能反映任何关于数据集的有用的信息．因此， $ \varepsilon $ 的取值要结合具体需求来达到输出结果的安全性与可用性的平衡。</p><h2 id="敏感度"><a class="header-anchor" href="#敏感度">¶</a>敏感度</h2><p>  差分隐私保护可以通过在查询函数的返回值中加入适量的干扰噪声来实现。加入噪声过多会影响结果的可用性，过少则无法提供足够的安全保障。敏感度是决定加入噪声量大小的关键参数，它指删除数据集中任一记录对查询结果造成的最大改变。在差分隐私保护方法中定义了两种敏感度，即全局敏感度（Global Sensitivity）和局部敏感度（Local Sensitivity）。</p><h3 id="全局敏感度"><a class="header-anchor" href="#全局敏感度">¶</a>全局敏感度</h3><p>  <b>定义2.全局敏感度.</b>设有函数 $ f:D \to {R^d} $ ，输入为一数据集，输出为一 $ d $ 维实数向量。对于任意的邻近数据集 $ D $ 和 $ {D’} $ ，其实<br>$$ G{S_f} = \max\limits_{D, D’} { \left| {f(D) - f({D’})} \right|_1} $$<br>称为函数 $ f $ 的全局敏感度。<br>  其中，$ { \left| {f(D) - f({D’})} \right|_1} $ 是 $ f(D) $ 和 $ f(D’) $ 之间的 $1$-阶范数距离。<br>  函数的全局敏感度由函数本身决定，不同的函数会有不同的全局敏感度．一些函数具有较小的全局敏感度（例如计数函数，其全局敏感度为 $１$），因此只需加入少量噪声即可掩盖因一个记录被删除对查询结果所产生的影响，实现差分隐私保护．但对于某<br>些函数而言，例如求平均值、求中位数等函数，则往往具有较大的全局敏感度．以求中位数函数为例，设函数为 $ f(D) = median({x_1},{x_2}, \cdots ,{x_n}) $，其中 $ x_{i} (i=１, \cdots ,n) $ 是区间 $ [a, b] $ 中的一个实数，不妨设 $ n $ 为奇数，且数据已被排序，那么函数的返回值即为第 $ m=(n + 1) / 2 $ 个数．在某种极端的情况下，设 $ x_1 = x_2 = \cdots = $ $ x_m = a $ 且 $ x_{m + 1} = x_{m + 2} = \cdots = x_{n} = b $，那么从中删除一个数就可能使函数的返回值由 $ a $ 变为 $ b $，因此函数的全局敏感度为 $ b - a $，这可能是一个很大的值。<br>  当全局敏感度较大时，必须在函数输出中添加足够大的噪声才能保证隐私安全，导致数据可用性较差。</p><h3 id="局部敏感度"><a class="header-anchor" href="#局部敏感度">¶</a>局部敏感度</h3><p>  <b>定义2.局部敏感度.</b>设有函数 $ f:D \to {R^d} $ ，输入为数据集 $ D $ ，输出为一 $ d $ 维实数向量。对于给定数据集 $ D $ 和它的任意邻近数据集 $ {D’} $ ，则<br>$$<br>LS_{f}(D) = \max\limits_{D} { \left| {f(D) - f({D’})} \right|_1}<br>$$<br>称为函数 $ f $ 在 $ D $ 上的局部敏感度。<br>  局部敏感度由函数 $ f $ 及给定数据集 $ D $ 中的具体数据共同决定。由于利用了数据集的数据分布特征，局部敏感度通常要比全局敏感度小得多。以前文的求<br>中位数函数为例，其局部敏感度为 $ \max ( x_{m} - x_{m - 1}, x_{m + 1} - x_{m}) $。另外，局部敏感度与全局敏感度之间的关系可以表示为<br>$$<br>GS_{f} = \max\limits_{D}(LS_{f}(D))<br>$$<br>  但是，由于局部敏感度在一定程度上体现了数据集的数据分布特征，如果直接应用局部敏感度来计算噪声量则会泄露数据集中的敏感信息。因此，局部敏感度的平滑上界（Smooth Upper Bound）被用来与局部敏感度一起确定噪声量的大小。</p>]]></content>
      
      
      <categories>
          
          <category> 科研学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据安全 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>内存管理（一）</title>
      <link href="/2021/04/12/nei-cun-guan-li-yi/"/>
      <url>/2021/04/12/nei-cun-guan-li-yi/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h1>内存管理概念</h1><h2 id="内存管理的基本原理和要求"><a class="header-anchor" href="#内存管理的基本原理和要求">¶</a>内存管理的基本原理和要求</h2><p>  内存管理就是操作系统对内存的管理和分配。<br>  内存管理的功能有：<br>  （1）<strong>内存空间的分配与回收</strong>：由操作系统完成主存储器空间的分配和管理。尤其是在编程时，可以提高编程效率。<br>  （2）<strong>地址转换</strong>：在多道程序环境下，程序中的逻辑地址与内存中的物理地址不可能一致，因此存储管理必须提供地址变换功能，把逻辑地址转换成相应的物理地址。<br>  （3）<strong>内存空间的扩充</strong>：利用虚拟存储技术或自动覆盖技术，从逻辑上扩充内存。<br>  （4）<strong>存储保护</strong>：保证各道作业在各自的存储空间内运行，互不干扰。</p><h3 id="程序装入和链接"><a class="header-anchor" href="#程序装入和链接">¶</a>程序装入和链接</h3><p>  创建进程首先要将程序和数据装入内存。将用户源程序变为可在内存中执行的程序，其主要经过以下几个步骤：<br>  （1）<strong>编译</strong>：由编译程序将用户源代码编译成若干目标模块<br>  （2）<strong>链接</strong>：由链接程序将编译后形成的一组目标模块及所需的库函数链接在一起，形成一个完整的装入模块。<br>  （3）<strong>装入</strong>：由装入程序将装入模块装入内存运行。<br><img src="/2021/04/12/nei-cun-guan-li-yi/1.png" alt="将用户程序变为可在内存中执行的程序的步骤"><br>  程序的链接有以下三种方式：<br>  （1）<strong>静态链接</strong>：在程序运行之前，先将各目标模块及它们所需的库函数链接成一个完整的可执行程序，以后不再拆开。<br>  （2）<strong>装入时动态链接</strong>：将用户源程序编译后所得到的一组目标模块，在装入内存时，采用边装入边链接的方式。<br>  （3）<strong>运行时动态链接</strong>：对某些目标模块的链接，是在程序执行中需要该目标模块时才进行的，其优点是便于修改和更新，便于实现对目标模块的共享。<br>  内存的装入模块在装入内存时，同样有以下三种方式：<br>  （1）<strong>绝对装入</strong>：在编译时，如果知道程序将驻留在内存的某个位置，编译程序将产生绝对地址的目标代码。绝对装入程序按照装入模块中的地址，将程序和数据装入内存。由于程序中的逻辑地址与实际内存地址完全相同，故不需对程序和数据的地址进行修改。<strong>绝对装入方式只适用于单道程序环境</strong>。另外，程序中所使用的绝对地址,可在编译或汇编时给出，也可由程序员直接赋予。而通常情况下在程序中采用的是符号地址，编译或汇编时再转换为绝对地址。<br><img src="/2021/04/12/nei-cun-guan-li-yi/2.png" alt="绝对装入方式"><br>  （2）<strong>可重定位装入</strong>：在多道程序环境下，多个目标模块的起始地址通常都是从$0$开始，程序中的其他地址都是相对于起始地址的,此时应采用可重定位装入方式。根据内存的当前情况，将装入模块装入到内存的适当位置。<strong>装入时对目标程序中指令和数据的修改过程称为重定位</strong>，地址变换通常是在装入时一次完成的，所以又称为静态重定位。静态重定位的特点是在一个作业装入内存时，必须分配其要求的全部内存空间，如果没有足够的内存，就不能装入该作业。此外，作业一旦进入内存后，在整个运行期间不能在内存中移动，也不能再申请内存空间。<br><img src="/2021/04/12/nei-cun-guan-li-yi/3.png" alt="可重装定位装入方式"><br>  （3）<strong>动态运行时装入</strong>：又称<strong>动态重定位</strong>，装入程序把装入模块装入内存后，并不立即把装入模块中的相对地址转化为绝对地址，而是推迟到程序要真正执行时才进行。因此，装入内存后的所有地址均为相对地址。这种方法需要一个<strong>重定位寄存器</strong>的支持。动态重定位有如下特点：**可以将程序分配到不连续的存储区中；在程序运行之前可以只装入它的部分代码即可投入运行，然后在程序运行期间，根据需要动态申请分配内；便于程序段的共享，可以向用户提供一个比存储空间大得多的地址空间。**如下图所示：<br><img src="/2021/04/12/nei-cun-guan-li-yi/4.png" alt="动态重定位"></p><h3 id="逻辑地址空间与物理地址空间"><a class="header-anchor" href="#逻辑地址空间与物理地址空间">¶</a>逻辑地址空间与物理地址空间</h3><p>  程序经过编译后，每个目标模块都是从$0$号单元开始编址，称为该目标模块的<strong>相对地址（或逻辑地址）</strong>。当链接程序将各个模块链接成一个完整的<font color="red"><strong>可执行目标程序</strong></font>时，链接程序顺序依次按各个模块的相对地址构成统一的从$0$号单元开始编址的<strong>逻辑地址空间</strong>。并且用户程序和程序员<font color="red"><strong>只需要知道逻辑地址而内存管理的具体机制则是完全透明的</strong></font>。<strong>不同进程可以有相同的逻辑地址，因为这些相同的逻辑地址可以映射到主存的不同位置</strong>。<br>  <font color="blue"><strong>物理地址空间</strong></font>是指内存中<strong>物理单元</strong>的集合，它是地址转换的最终地址，进程在运行时<strong>执行指令和访问数据</strong>都要通过物理地址从主存中存取。当装入程序将可执行代码装入内存时，<strong>必须通过地址转换将逻辑地址转换成物理地址</strong>，这个过程称为<font color="blue"><strong>地址重定位</strong></font>。</p><h3 id="内存保护"><a class="header-anchor" href="#内存保护">¶</a>内存保护</h3><p>  内存分配前，需要保护操作系统不受用户进程的影响，同时保护用户进程不受其他用户进程的影响。可以采取的两种内存保护的方法：<br>  （1）在$CPU$中设置一对<font color="blue"><strong>上、下限寄存器</strong></font>，存放用户在主存中的下限和上限地址，每当CPU要访问一个地址时，分别和两个寄存器的值相比，判断有无越界。<br>  （2）通过采用<font color="blue"><strong>重定位寄存器（基址寄存器）</strong></font>和<font color="blue"><strong>界地址寄存器（限长寄存器）</strong></font>来实现这种保护。重定位寄存器含最小的物理地址值，界地址寄存器含逻辑地址的最大值。每个逻辑地址值必须小于界地址寄存器，内存管理机构动态的将逻辑地址与界地址寄存器进行比较，如果未发生地址越界，则加上重定位寄存器的值后映射成物理地址，再送交内存单元。其中，<strong>界地址寄存器是用来与逻辑地址进行比较的，如果界地址寄存器中存储的逻辑地址的最大值小于逻辑地址，则未发生越界。而重定位寄存器是用来“加”的，逻辑地址加上重定位寄存器中的值就能得到物理地址</strong>。<br><img src="/2021/04/12/nei-cun-guan-li-yi/5.png" alt="重定位和界地址寄存器的硬件支持"></p><h2 id="覆盖与交换"><a class="header-anchor" href="#覆盖与交换">¶</a>覆盖与交换</h2><p>  在多道程序环境下，计算机的内存往往并不充裕，操作系统需要对内存进行管理，进行合理的划分和有效的动态分配，并且需要<strong>进行内存的扩充</strong>，而<font color="red"><strong>覆盖</strong></font>与<font color="red"><strong>交换</strong></font>这两种技术就是用来扩充内存的方法。</p><h3 id="覆盖"><a class="header-anchor" href="#覆盖">¶</a>覆盖</h3><p>  早期的计算机内存很小，经常会出现内存大小不够的情况。后来人们引入了覆盖技术，用来解决<strong>程序大小超过物理内存总和</strong>的问题。<br>  <strong>覆盖的基本思想</strong>：因为程序并非任何时候都要访问程序及数据各个部分，因此，将程序分为多个段（多个模块）。常用的段常驻内存，不常用的段在需要时调入内存。内存中分为一个<strong>固定区</strong>和若干个<strong>覆盖区</strong>。需要常驻内存的段放在<strong>固定区</strong>中，其余部分按调用关系分段，首先将那些即将要访问的段放入覆盖区，其他段放在外存中，在需要调用前，系统再将其调入覆盖区，替换覆盖区中原有的段。例如在下图中：该程序正文段所需要的内存空间是：<br>$$A(20KB) + B(50KB) + F(30KB) + C(30KB) + D(20KB) + E(40KB) = 190KB$$<br>但在采用了覆盖技术后，只需要空间：<br>$$A(20KB) + B(50KB) + E(40KB) = 110KB$$<br><img src="/2021/04/12/nei-cun-guan-li-yi/6.png" alt="覆盖技术示例图"><br>  覆盖技术的特点：（1）打破了必须将一个程序的全部信息装入主存后才能运行的限制。（2）当同时运行程序的代码量大于主存时仍不能运行。（3）内存中能够更新的地方只有覆盖区的段，不在覆盖区中的段会常驻内存。</p><h3 id="交换"><a class="header-anchor" href="#交换">¶</a>交换</h3><p>  <strong>交换（对换）技术的设计思想</strong>：内存空间紧张时，系统将内存中某些处于等待状态（或在$CPU$调度原则下被剥夺运行权利）的程序从内存移到外存，这一过程称为<strong>换出</strong>。把外存中某些已具备运行条件的进程换入内存（进程在内存与磁盘间动态调度）这一过程称为<strong>换入</strong>。那些被换出至外存的进程称为挂起状态，再根据换出之前进程所处的状态又可以分为<font color="blue"><strong>就绪挂起</strong></font>以及<font color="blue"><strong>阻塞挂起</strong></font>两种状态。在之前中提到的<strong>中级调度采用的就是交换技术</strong>。下面是进程各种状态之间的转化关系：<br><img src="/2021/04/12/nei-cun-guan-li-yi/7.png" alt="进程各状态之间的转换关系"><br>  交换需要注意的问题：（1）交换需要备份存储。（2）为了有效使用$CPU$，需要使每个进程的执行时间比交换时间长。（3）若换出进程，则必须保证该进程完全处于空闲状态。（4）交换空间通常作为磁盘的一整块，且独立于系统文件。（5）交换通常在有许多进程运行且内存空间吃紧时开始启动，而在系统负荷降低时暂停。<br>  <font color="red"><strong>交换技术主要在不同进程（或作业）中进行，而覆盖则用于同一个程序或进程中</strong></font>。现代操作系统是通过虚拟内存技术来解决覆盖技术对于用户和程序员不透明，主存无法存放用户程序的矛盾的。</p><h2 id="连续分配管理方式"><a class="header-anchor" href="#连续分配管理方式">¶</a>连续分配管理方式</h2><p>  连续分配方式是指为一个用户程序分配一个连续的内存空间，连续分配的方式主要包括<strong>单一连续分配</strong>、<strong>固定分区分配</strong>和<strong>动态分区分配</strong>。</p><h3 id="单一连续分配"><a class="header-anchor" href="#单一连续分配">¶</a>单一连续分配</h3><p>  </p>]]></content>
      
      
      <categories>
          
          <category> 课本学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 操作系统 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>进程管理（四）</title>
      <link href="/2021/04/07/jin-cheng-guan-li-si/"/>
      <url>/2021/04/07/jin-cheng-guan-li-si/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h1>死锁</h1><h2 id="死锁的概念"><a class="header-anchor" href="#死锁的概念">¶</a>死锁的概念</h2><h3 id="死锁的定义"><a class="header-anchor" href="#死锁的定义">¶</a>死锁的定义</h3><p>  在多道程序系统中，多个进程会并发执行，提高了处理器的吞吐量和资源的利用率。但是在多个进程的并发执行过程中，经常会出现死锁的问题，死锁就是：<strong>多个进程因竞争资源而造成的一种僵局（互相等待），若无外力作用，这些进程都将无法向前进行</strong>。</p><h3 id="死锁产生的原因"><a class="header-anchor" href="#死锁产生的原因">¶</a>死锁产生的原因</h3><h4 id="系统资源的竞争"><a class="header-anchor" href="#系统资源的竞争">¶</a>系统资源的竞争</h4><p>  通常系统拥有的不可剥夺的资源在数量上不足以满足多个进程运行的需求，使得进程在运行过程中，会因资源争夺而陷入僵局，例如：磁带、打印机等。只有对不可剥夺资源的竞争才可能产生死锁。</p><h4 id="进程推进顺序非法"><a class="header-anchor" href="#进程推进顺序非法">¶</a>进程推进顺序非法</h4><p>  （1）进程在运行过程中，请求和释放资源顺序不当，也会导致死锁。例如：并发进程$P_{1}, P_{2}$分别保持了资源$R_{1}, R_{2}$，而进程$P_{1}$申请资源$R_{2}$、进程$P_{2}$申请资源$R_{1}$时，两者都会因为所需资源被占用而阻塞。<br>  （2）信号量使用不当。<br>  （3）进程间彼此相互等待对方发来的消息，也会使得这些进程间无法继续向前推进。例如：进程$A$等待进程$B$发的消息，进程$B$也在等待进程$A$发送的消息，所以可以看出进程$A$和进程$B$不是因为竞争同一资源，而是等待对方的资源导致死锁。</p><h4 id="死锁产生的必要条件"><a class="header-anchor" href="#死锁产生的必要条件">¶</a>死锁产生的必要条件</h4><p>  死锁必须同时满足以下的$4$个条件，只要其中任意一个条件不成立，死锁就不会发生。<br>  （1）<strong>互斥条件</strong>：指进程对所分配到的资源进行排它性使用，即在一段时间内某资源只由一个进程占用。如果此时还有其它进程请求资源，则请求者只能等待，直至占有资源的进程用毕释放。<br>  （2）<strong>不剥夺条件</strong>：指进程已获得的资源，在未使用完之前，不能被剥夺，只能在使用完时由自己释放（主动释放）。<br>  （3）<strong>请求并保持条件</strong>：指进程已经保持至少一个资源，但又提出了新的资源请求，而该资源已被其它进程占有，此时请求进程阻塞，但又对自己已获得的其它资源保持不放。<br>  （4）<strong>循环等待条件</strong>：即存在一个等待队列：$P_{1}$占有$P_{2}$的资源，$P_{2}$占有$P_{3}$的资源，$P_{3}$占有$P_{1}$的资源。这样就形成了一个等待环路。若干进程之间形成一种头尾相接的循环等待资源关系。如下图所示：<br><img src="/2021/04/07/jin-cheng-guan-li-si/1.png" alt="循环等待示例图"></p><h2 id="死锁的处理策略"><a class="header-anchor" href="#死锁的处理策略">¶</a>死锁的处理策略</h2><p>  设法破坏产生死锁的$4$个必要条件之一，或者允许产生死锁，但是当死锁发生时有能力检测出死锁，并有能力实现恢复。</p><h3 id="死锁预防"><a class="header-anchor" href="#死锁预防">¶</a>死锁预防</h3><p>  <strong>通过设置一些限制条件，破坏死锁的四个必要条件中的一个或几个，让死锁无法发生</strong>。例如，将资源分层，得到上一层资源后才能够申请下一层资源，这样就破坏了循环等待条件。用户申请资源时，要求一次性申请所需要的全部资源，这就破坏了占有并等待条件。当一个已经占有某些不可剥夺资源的进程，请求新的资源而得不到满足时，它必须释放已经占有的所有资源，待以后需要时再重新申请，这就破坏了不剥夺条件。<br>  这些预防死锁的方法破坏了系统的并行性和并发性，通常会降低系统的效率。</p><h3 id="避免死锁"><a class="header-anchor" href="#避免死锁">¶</a>避免死锁</h3><p>  在资源的动态分配过程中，用某种方法防止系统进入不安全状态，从而避免死锁。<br>  预防死锁的几种策略，会严重地损害系统性能。因此在避免死锁时，要施加较弱的限制，从而获得较满意的系统性能。由于在避免死锁的策略中，允许进程动态地申请资源。因而，系统在进行资源分配之前预先计算资源分配的安全性。若此次分配不会导致系统进入不安全状态，则将资源分配给进程；否则，进程等待。其中最具有代表性的避免死锁算法是银行家算法。</p><h3 id="死锁的检测及解除"><a class="header-anchor" href="#死锁的检测及解除">¶</a>死锁的检测及解除</h3><p>  无须采取任何限制性措施，允许进程在运行过程中发生死锁。通过系统的检测机构及时地检测出死锁的发生，然后采取某种措施解除死锁。<br>  三种思索策略的比较如下表所示：<br><img src="/2021/04/07/jin-cheng-guan-li-si/2.png" alt="死锁处理策略的比较"></p><h2 id="死锁预防-v2"><a class="header-anchor" href="#死锁预防-v2">¶</a>死锁预防</h2><p>  只需要破坏死锁产生的$4$个必要条件之一即可。<br>  （1）<strong>破坏互斥条件</strong>：例如允许系统资源都能共享使用。但是这种方法不可行。<br>  （2）<strong>破坏不剥夺条件</strong>：当一个已保持了某些不可剥夺的进程请求新的资源而得不到满足时，它必须释放已经保持的所有资源，待以后需要时重新申请，但是这样会导致已获得的资源可能造成前一阶段工作的失效，同时反复的申请和释放资源会增加系统开销，降低系统吞吐量。<br>  （3）<strong>破坏请求并保持条件</strong>：采取预先静态分配方法，即进程在运行前一次申请完它所需要的全部资源，在他的资源未满足前，不投入运行，一旦投入运行，不再提出其他资源请求。但这样会导致系统资源被严重浪费。<br>  （4）<strong>破坏循环等待条件</strong>：将系统中的所有资源统一编号，进程可在任何时刻提出资源申请，但所有申请必须按照资源的编号顺序（升序）提出。这样做就能保证系统不出现死锁。这种方法存在的问题是，编号必须稳定，这就限制了新类型设备的增加，而且经常会发生作业使用资源的顺序与系统规定顺序不同，造成资源浪费。</p><h2 id="死锁避免"><a class="header-anchor" href="#死锁避免">¶</a>死锁避免</h2><p>  死锁避免同样属于事先预防策略，但是并不是事先采取某种限制措施破坏死锁的必要条件，而是在资源动态分配的过程中，防止系统进入不安全状态，以避免发生死锁。并且这种方法施加的限制条件比较弱，可以获得较好的系统性能。</p><h3 id="系统安全状态"><a class="header-anchor" href="#系统安全状态">¶</a>系统安全状态</h3><p>  安全状态是指系统按某种进程推进顺序$(P_{1},P_{2},P_{3},…,P_{n})$为每个进程$P_{i}$分配其所需的资源，直至满足每个进程对资源的最大需求，使每个进程都可顺序完成。此时称$(P_{1},P_{2},P_{3},…,P_{n})$为安全序列。若系统无法找到一个安全序列，则称系统处于不安全状态。<br>  例如系统中有三个进程$P_{1}$、$P_{2}$和$P_{3}$,共有$12$台磁带机。进程$P_{1}$总共需要$10$台磁带机，$P_{2}$和$P_{3}$ 分别需要$4$台和$9$台。假设在$T_{0}$时刻，进程$P_{1}$、$P_{2}$和$P_{3}$已分别获得$5$合、$2$台和$2$台，尚有$3$台未分配，见下表：<br><img src="/2021/04/07/jin-cheng-guan-li-si/3.png" alt="资源分配表"><br>  如果要满足各进程的最大进程，$p_{1}$差$5$台，$p_{2}$差$2$台，$p_{3}$差$7$台，当前可用资源只有$3$台。<br>  将所有进程推进顺序的情况一一列举：<br>  （1）$(P_{1}, P_{2}, P_{3})$，因为$3 &lt; 5$，$P_{1}$没法完成，不符合。<br>  （2）$(P_{1}, P_{3}, P_{2})$，因为$3 &lt; 5$，$P_{1}$没法完成，不符合。<br>  （3）$(P_{2}, P_{1}, P_{3})$，因为$3 &gt; 2$，$P_{2}$可以完成，$P_{2}$完成后释放资源，可用资源为$5$台，$5 = 5$，$P_{1}$可以完成；$P_{1}$完成后释放资源，可用资源为$10$台，$10 &gt; 7$，$P_{3}$可以完成；所有进程都可以完成，符合。<br>  （4）$(P_{2}, P_{3}, P_{1})$，因为$3 &gt; 2$，$P_{2}$可以完成；$P_{2}$完成后释放资源，可用资源为$5$台，$5 &lt; 7$，$P_{3}$没法完成，不符合。<br>  （5）$(P_{3}, P_{1}, P_{2})$，$3 &lt; 7$，$P_{3}$没法完成，不符合。<br>  （5）$(P_{3}, P_{2}, P_{1})$，$3 &lt; 7$，$P_{3}$没法完成，不符合。<br>  综上，存在一个安全序列$(P_{2}, P_{1}, P_{3})$，即只要系统按此进程序列分配资源，则每个进程都能顺利完成,此时系统便进入安全状态,否则进入不安全状态。</p><h3 id="银行家算法"><a class="header-anchor" href="#银行家算法">¶</a>银行家算法</h3><p>  在银行中，客户申请贷款的数量是有限的，每个客户在第一次申请贷款时要声明完成该项目所需的最大资金量，在满足所有贷款要求时，客户应及时归还。银行家在客户申请的贷款数量不超过自己拥有的最大值时，都应尽量满足客户的需要。<br>  因此银行家算法的思想是：把操作系统视为银行家，操作系统管理的资源相当于银行家管理的资金，进程向操作系统请求分配资源相当于用户向银行家贷款。操作系统按照银行家制定的规则为进程分配资源。进程运行之前先声明对各种资源的最大需求量，当进程在执行中继续申请资源时，先测试该进程已占用的资源数与本次申请的资源数之和是否超过该进程声明的最大需求量。若超过则拒绝分配资源，若未超过则再测试系统现存的资源能否满足该进程尚需的最大资源量，若能满足则按当前的申请量分配资源，否则也要推迟分配。</p><h4 id="银行家算法数据结构"><a class="header-anchor" href="#银行家算法数据结构">¶</a>银行家算法数据结构</h4><p>  为了实现银行家算法，在系统中必须设置这样四个数据结构，分别用来描述系统中可利用的资源、所有进程对资源的最大需求、系统中的资源分配，以及所有进程还需要多少资源的情况。<br>  （1）可利用资源向量$Available$。这是一个含有$m$个元素的数组，其中的每一个元素代表一类可利用的资源数目，其初始值是系统中所配置的该类全部可用资源的数目，其数值随该类资源的分配和回收而动态地改变。如果$Available[j] = K$，则表示系统中现$R_{j}$类资源$K$个。<br>  （2）最大需求矩阵$Max$。这是一个$n * m$的矩阵，它定义了系统中$n$个进程中的每个进程对$m$类资源的最大需求。如果$Max[i, j] = K$，则表示进程$i$需要$R_{j}$ 类资源的最大数目为$K$。<br>  （3）分配矩阵$Allocation$。这也是一个$n * m$的矩阵，它定义了系统中每一类资源当前已分配给每一进程的资源数。如果$Allocation[i, j] = K$，则表示进程$i$当前己分得$R_{j}$类资源的数目为$K$。<br>  （4）需求矩阵$Need$.这也是一个$n * m$的矩阵，用以表示每一个进程尚需的各类资源数。如果$Need[i, j] = K$，则表示进程$i$还需要$R_{j}$类资源$K$个方能完成其任务。<br>  上述三个矩阵间存在下述关系:<br>$$Need[i, j] = Max[i, j] - allocation[i, j]$$</p><h4 id="银行家算法描述"><a class="header-anchor" href="#银行家算法描述">¶</a>银行家算法描述</h4><p>  设$Request_{i}$是进程$P_{i}$的请求向量，如果$Requesti_{i}[j] = K$，表示进程$P_{i}$需要$K$个$R_{j}$类型的资源。当$P_{i}$发出资源请求后，系统按下述步骤进行检査:<br>  （1）如果$Request_{i}[j] ≤ Need[i, j]$便转向步骤$(2)$；否则认为出错，因为它所需要的资源数已超过它所宣布的最大值。<br>  （2）如果$Request_{i}[j] ≤ Available[j]$，便转向步骤$(3)$；否则，表示尚无足够资源，$P_{i}$须等待。<br>  （3）系统试探着把资源分配给进程$P_{i}$，并修改下面数据结构中的数值：<br>$$<br>　　Available[j] = Available[j] - Request_{i}[j];\\<br>　　Allocation[i, j] = Allocation[i, j] + Request_{i}[j]; \\<br>　　Need[i, j] = Need[i,j] - Request_{i}[j];<br>$$<br>  （4）系统执行安全性算法，检查此次资源分配后系统是否处于安全状态。若安全，才正式将资源分配给进程$P_{i}$，以完成本次分配；否则，将本次的试探分配作废，恢复原来的资源分配状态，让进程$P_{i}$等待。</p><h4 id="安全性算法"><a class="header-anchor" href="#安全性算法">¶</a>安全性算法</h4><p>  设置工作向量$Work$，它表示系统中的剩余可用资源数目，它含有$m$个元素，在执行安全算法开始时，$Work = Available$；<br>  （1）初始时安全序列为空。<br>  （2）从$Need$矩阵中找出符合下面条件的行：改行对应的进程不再安全序列中，而且该行小于等于$Work$向量，找到后，把对应的进程加入安全序列；若找不到，执行步骤$(4)$<br>  （3）进程$P_{i}$进入安全序列后，可顺利执行，直至完成，并释放分配给它的资源，因此应该执行$Work = Work + Allocation[i]$，其中$Allocatin[i]$表示进程$P_{i}$代表的在$Allocation$矩阵中对应的行，返回步骤$(2)$。<br>  （4）若此时安全序列中已有所有进程，则系统处于安全状态，否则系统处于不安全状态。</p><h2 id="死锁检测和解除"><a class="header-anchor" href="#死锁检测和解除">¶</a>死锁检测和解除</h2><p>  当系统为进程分配资源时，不采取任何措施，则应该提供死锁检测和解除手段。</p><h3 id="资源分配图"><a class="header-anchor" href="#资源分配图">¶</a>资源分配图</h3><p>  系统思索可利用资源分配图来描述，用圆圈代表一个进程，用框代表一类资源，因为一种类型的资源可能有多个，因此用框中的一个圆代表一类资源中的一个资源。从进程到资源的有向边称为<strong>请求边</strong>，表示该进程申请一个单位的该类资源；从资源到进程的边称为<strong>分配边</strong>，表示该类资源已有一个资源分配给了该进程。资源分配图如下图所示：<br><img src="/2021/04/07/jin-cheng-guan-li-si/4.png" alt="资源分配图实例"></p><h3 id="死锁定理"><a class="header-anchor" href="#死锁定理">¶</a>死锁定理</h3><p>  $S$为死锁的条件是当且仅当$S$状态的资源分配图是不可完全简化的，该条件为<strong>死锁定理</strong>。</p><h3 id="死锁解除"><a class="header-anchor" href="#死锁解除">¶</a>死锁解除</h3><p>  （1）<strong>资源剥夺法</strong>：挂起某些死锁进程，并抢占它的资源，将这些资源分配给其他的死锁进程。<br>  （2）<strong>撤销进程法</strong>：强制撤销部分甚至全部死锁进程，并剥夺这些进程的资源。撤销的原则可以按进程优先级和撤销进程代价的高低进行。<br>  （3）<strong>进程回退法</strong>：让一个或（多个）进程回退到足以回避死锁的地步，进程回退时自愿释放资源而非被剥夺。要求系统保持进程的历史信息，设置还原点。</p>]]></content>
      
      
      <categories>
          
          <category> 课本学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 操作系统 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>经典同步问题</title>
      <link href="/2021/04/06/jing-dian-tong-bu-wen-ti/"/>
      <url>/2021/04/06/jing-dian-tong-bu-wen-ti/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h1>经典同步动作</h1><h2 id="生产者-消费者问题"><a class="header-anchor" href="#生产者-消费者问题">¶</a>生产者-消费者问题</h2><p>  <strong>问题描述</strong>：一组生产者进程和一组消费者进程共享一个初始为空、大小为$n$的缓冲区，只有缓冲区没满时，生产者才能把消息放入缓冲区，否则必须等待；只有缓冲区不空时，消费者才能从中取出消息，否则必须等待。由于缓冲区是临界资源，它只允许一个生产者放入消息，或一个消费者从中取出消息。<br>  <strong>问题分析</strong>：<br>  （1）<strong>关系分析</strong>：生产者和消费者对缓冲区互斥访问是互斥关系，因为只有生产者生产之后，消费者才能消费，所以也存在同步关系。<br>  （2）<strong>整理思路</strong>：只有生产者和消费者，<strong>同时存在互斥关系和同步关系</strong>，所以相当于解决互斥和同步的$P/V$操作。<br>  （3）<strong>信号量设置</strong>：信号量$mutex$作为互斥信号量，用于控制互斥访问缓冲池，互斥信号量初值为$1$；信号量$full$用于记录当前缓冲池中的“满”缓冲区数，初值为$0$。信号量$empty$用于记录当前缓冲池中的“空”缓冲区数，初值为$n$。</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">semaphore mutex &#x3D; 1;                  &#x2F;&#x2F; 临界区互斥信号量semaphore empty &#x3D; n;                  &#x2F;&#x2F; 空闲缓冲区semaphore full &#x3D; 0;                   &#x2F;&#x2F; 缓冲区初始化为空producer() &#123;                          &#x2F;&#x2F; 生产者进程    while(1) &#123;        produce an item in nextp;     &#x2F;&#x2F; 生产数据        P(empty);                     &#x2F;&#x2F; 获取空缓冲区单元        P(mutex);                     &#x2F;&#x2F; 进入临界区        add nextp to buffer;          &#x2F;&#x2F; 将数据放入缓冲区        V(mutex);                     &#x2F;&#x2F; 离开临界区，释放互斥信号量        V(full);                      &#x2F;&#x2F; 满缓冲区数加1    &#125;&#125;consumer() &#123;                          &#x2F;&#x2F; 消费者进程    while(1) &#123;        P(full);                      &#x2F;&#x2F; 获取满缓冲区单元        P(mutex);                     &#x2F;&#x2F; 进入临界区        remove an item from buffer;   &#x2F;&#x2F; 从缓冲区中取出数据        V(mutex);                     &#x2F;&#x2F; 离开临界区，释放互斥信号量        V(empty);                     &#x2F;&#x2F; 空缓冲区数加1        consume the item;             &#x2F;&#x2F; 消费数据    &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>  需要注意的是：（1）<strong>当缓冲区中有空时，便可对empty变量执行$P$操作，一旦取走一个产品便要执行$V$操作以释放空闲区</strong>。（2）<strong>对$empty$和$full$变量的操作必须放在对$mutex$的$P$操作之前</strong>。</p><h2 id="生产者-消费者问题（扩展）"><a class="header-anchor" href="#生产者-消费者问题（扩展）">¶</a>生产者-消费者问题（扩展）</h2><p>  <strong>问题描述</strong>：桌子上有一个盘子，每次只能向其中放一个水果。爸爸专门向盘子中放入苹果，妈妈专向盘子中放橘子，儿子专等吃盘子中的橘子，女儿专等吃盘子中的苹果。只有当盘子为空时，爸爸或妈妈才可向盘子中放一个水果；仅当盘子中有自己需要的水果时，儿子或女儿可以从盘子中取出。<br>  <strong>问题分析</strong>：<br>  <strong>关系分析</strong>：因为每次只能向其中放入一只水果可知，爸爸和妈妈是互斥关系。爸爸和女儿、妈妈和儿子是同步关系，而且这两对进程必须连起来，儿子和女儿之间没有互斥和同步关系，因此他们是选择条件执行，不可能并发。<br><img src="/2021/04/06/jing-dian-tong-bu-wen-ti/1.png" alt="进程之间的关系"><br>  <strong>整理思路</strong>：在这个问题中有四个进程，但是可抽象为两个生产者和消费者被连接到大小为$1$的缓冲区上。<br>  <strong>信号量设置</strong>：首先设置互斥信号量$plate$，表示是否允许向盘子中放入水果，初值为$1$表示允许放入且只允许放入一个。信号量$apple$表示盘子中是否有苹果，初值为$0$表示盘子为空且不允许取，为$1$则相反。信号量$orange$表示盘子中是否有橘子，初值为$0$表示盘子为空，不允许取，为$1$则相反。</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">semaphore plate&#x3D;l, apple&#x3D;0， orange&#x3D;0;dad() &#123;                               &#x2F;&#x2F; 父亲进程    while (1) &#123;        prepare an apple;        P(plate);                     &#x2F;&#x2F; 互斥向盘中取、放水果        put the apple on the plate;   &#x2F;&#x2F; 向盘中放苹果        V(apple);                     &#x2F;&#x2F; 允许取苹果    &#125;mom() &#123;                                &#x2F;&#x2F; 母亲进程    while (1) &#123;        prepare an orange;            &#x2F;&#x2F; 互斥向盘中取、放水果        P(plate);                     &#x2F;&#x2F; 向盘中放橘子        put the orange on the plate;  &#x2F;&#x2F; 允许取橘子        V(orange);    &#125;&#125;son() &#123;                               &#x2F;&#x2F; 儿子进程    while(1) &#123;        P(orange);                    &#x2F;&#x2F; 互斥向盘中取橘子        take an orange from the plate;&#x2F;&#x2F; 允许向盘中取、放水果        V(plate);        eat the orange;    &#125;&#125;daughter() &#123;                          &#x2F;&#x2F; 女儿进程    while(1) &#123;        P(apple);                     &#x2F;&#x2F; 互斥向盘中取苹果        take an apple from the plate;        V(plate);                     &#x2F;&#x2F; 运行向盘中取、放水果        eat the apple;    &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>  在上面的进程中，$dad()$和$daughter()$、$mon()$和$son()$必须连续执行，儿子和女儿也必须在拿走水果后执行$V(plate)$操作。</p><h2 id="读者-写作问题"><a class="header-anchor" href="#读者-写作问题">¶</a>读者-写作问题</h2><p>  <strong>问题描述</strong>：有读者和写者两组并发进程，共享一个文件，当两个或两个以上的进程同时访问共享数据时不会产生副作用，但若某个写进程和其他进程（读进程和写进程）同时访问共享数据时，则可能导致数据不一致的错误。因此要求：（1）<strong>允许多个读者可以同时对文件执行读操作</strong>；（2）<strong>只允许一个写者往文件中写信息</strong>；（3）<strong>任一写者在写完成操作之前不允许其他读者或写者工作</strong>；（4）<strong>写者执行写操作，应让已有的读者和写者全部退出</strong>。<br><img src="/2021/04/06/jing-dian-tong-bu-wen-ti/2.png" alt="读写进程对文件的操作"><br>  <strong>问题分析</strong>：<br>  <strong>关系分析</strong>：在题目分析读者和写者是互斥的，写者和写者也是互斥的，读者和读者不存在互斥问题。<br>  <strong>整理思路</strong>：在这个问题中，有两个进程，读者和写者，写者比较简单，它和任何进程互斥，用互斥信号量的$P$操作、$V$操作即可解决。读者的问题比较复杂，它必须在实现与写者互斥的同时，实现与其他读者的同步，因此一对简单的$P$、$V$操作无法解决问题。这里就需要用到计数器，用来判断当前是否有读者读文件，当有读者时，写者是无法写文件的，此时读者会一只占用文件，当没有读者时，写者才可以写文件，同时，<strong>不同读者对计数器的访问也应该是互斥的</strong>。<br>  <strong>信号量设置</strong>：首先设置信号量$count$为计数器，用于记录当前读者的数量，初值为$0$，设置$mutex$为互斥信号量，用于保护更新$count$变量时的互斥，设置互斥信号量$rw$，用于保证读者和写者的互斥访问。</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">int count&#x3D;0;                         &#x2F;&#x2F; 用于记录当前的读者数量semaphore mutex&#x3D;1;                   &#x2F;&#x2F; 用于保护更新count变量时的互斥semaphore rw&#x3D;1;                      &#x2F;&#x2F; 用于保证读者和写者互斥地访问文件writer() &#123;                           &#x2F;&#x2F; 写者进程    while(1) &#123;        P(rw);                       &#x2F;&#x2F; 互斥访问共享文件        writing;                     &#x2F;&#x2F; 写入        V(rw);                       &#x2F;&#x2F; 释放共享文件    &#125;&#125;reader() &#123;    while(1) &#123;                       &#x2F;&#x2F; 读者进程        P(mutex);                    &#x2F;&#x2F; 互斥访问count变量        if(count &#x3D;&#x3D; 0) &#123;             &#x2F;&#x2F; 当第一一个读进程读共享文件时            P(rW);                   &#x2F;&#x2F; 阻止写进程写        &#125;        count++;                     &#x2F;&#x2F; 读者计数器加1        V(mutex);                    &#x2F;&#x2F; 释放互斥变量count        reading;                     &#x2F;&#x2F; 读取        P(mutex);                    &#x2F;&#x2F; 互斥访问count变量        count--;                     &#x2F;&#x2F; 读者计数器减1        if(count &#x3D;&#x3D; 0) &#123;             &#x2F;&#x2F; 当最后一个读进程读完共享文件            V(rw);                   &#x2F;&#x2F; 允许写进程写        &#125;        V(mutex);                    &#x2F;&#x2F; 释放互斥变量count    &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>  上面的方法时读进程优先，只要存在一个读进程，写进程都需要等待，这种方式会导致写进程长时间等待，出现饿死现象。为了避免这种现象的产生，当有读进程正在读取共享文件时，有些进程请求访问，这时应禁止后续读进程的请求，等到已在共享文件的读进程执行完毕，立即让写进程执行，只有在无写进程执行的情况下，才允许读进程再次运行。</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">int count&#x3D;0;                              &#x2F;&#x2F; 用于记录当前的读者数量semaphore mutex&#x3D;1;                        &#x2F;&#x2F; 用于保护更新count变量时的互斥semaphore rw&#x3D;1 ;                          &#x2F;&#x2F; 用于保证读者和写者互斥地访问文件semaphore w&#x3D;1;                            &#x2F;&#x2F; 用于实现“写优先”writer() &#123;                                &#x2F;&#x2F; 写者进程    while(l) &#123;        P(w);                             &#x2F;&#x2F; 在无写进程请求时进入        P(rw);                            &#x2F;&#x2F; 互斥访问共享文件        writing;                          &#x2F;&#x2F; 写入        V(rw);                            &#x2F;&#x2F; 释放共享文件        V(w);                             &#x2F;&#x2F; 恢复对共享文件的访问    &#125;&#125;reader() &#123;                                &#x2F;&#x2F; 读者进程    while(1) &#123;        P(w);                             &#x2F;&#x2F; 在无写进程请求时进入        P(mutex);                         &#x2F;&#x2F; 互斥访问count变量        if(count &#x3D;&#x3D; 0) &#123;                  &#x2F;&#x2F; 当第一个读进程读共享文件时            P(rw);                        &#x2F;&#x2F; 阻止写进程写        &#125;        count++;                          &#x2F;&#x2F; 读者计数器加1        V(mutex);                         &#x2F;&#x2F; 释放互斥变量count        V(w);                             &#x2F;&#x2F; 恢复对共享文件的访问，如果在这里不即使回复对共享文件的访问，则会出现后续读进程无法同时与该进程读取文件，形成互斥，与实际情况不符        reading;                          &#x2F;&#x2F; 读取        P(mutex);                         &#x2F;&#x2F; 互斥访问count变量        count--;                          &#x2F;&#x2F; 读者计数器减1        if(count &#x3D;&#x3D; 0) &#123;                  &#x2F;&#x2F; 当最后一个读进程读完共享文件            V(rw);                        &#x2F;&#x2F; 允许写进程写        &#125;        V(mutex);                         &#x2F;&#x2F; 释放互斥变量count    &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>  这个并不是一个完全的写操作优先的方式，因为当一个写进程访问文件时，若有一些读进程要求访问文件，后又另一个写进程请求访问文件，则当前访问文件的写进程结束对文件的写操作时，会是一个读进程而不是写进程占用文件。</p><h2 id="哲学家进餐问题"><a class="header-anchor" href="#哲学家进餐问题">¶</a>哲学家进餐问题</h2><p>  <strong>问题描述</strong>：一张圆桌边上坐着5名哲学家，每两秒哲学家之间的桌子上摆一根筷子，两根筷子中间是一碗米饭，哲学家们倾注毕生经历用于思考和进餐，哲学家在思考时，并不影响他人，只有当哲学家饥饿时，才试图拿起左、右两根筷子（一根一根的拿起）。若筷子已在他人手上，则需要等待。饥饿的哲学家只有同时拿到了两根筷子才可以开始进餐，进餐完毕后，放下筷子继续思考。具体样例如下图所示。<br><img src="/2021/04/06/jing-dian-tong-bu-wen-ti/3.png" alt="5名哲学家进餐"><br>  <strong>问题分析</strong>：<br>  <strong>关系分析</strong>：$5$名哲学家与左右邻居对其中间筷子的访问是互斥关系。<br>  <strong>整理思路</strong>：在这个问题中，存在$5$个进程，最关键的地方在于如何让一名哲学家拿到左右两根筷子而不造成死锁或饥饿现象。解决方法有两个：（1）让他们同时拿两根筷子。（2）对每个哲学家的动作制定规则，避免饥饿或死锁现象发生。<br>  <strong>信号量设置</strong>：定义互斥信号量数组$chopstick[5] = {1, 1, 1, 1, 1}$，用于对5个筷子的互斥访问，哲学家按照顺序编号为$0\sim4$，哲学家$i$左边筷子的编号为$i$，哲学家右边筷子的编号为$(i+1)$ % $5$。</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">semaphore chopstick[5] &#x3D; &#123;1, 1, 1, 1, 1&#125;; &#x2F;&#x2F; 定义信号量数组chopstick[5]，并初始化Pi() &#123;                                    &#x2F;&#x2F; i号哲学家的进程    do&#123;        P(chopstick[i]);                  &#x2F;&#x2F; 取左边筷子        P(chopstick[(i + 1) % 5]);        &#x2F;&#x2F; 取右边筷子        eat;                              &#x2F;&#x2F; 进餐        V(chopstick[i]);                  &#x2F;&#x2F; 放回左边筷子        V(chopstick[(i + 1) % 5]);        &#x2F;&#x2F; 放回右边筷子        think;                            &#x2F;&#x2F; 思考    &#125; while(1);&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>  该算法存在一下问题：当$5$名哲学家都想要进餐并分别拿起左边的筷子时（都恰好执行完$chopstick[i]$；），筷子已经被拿光，等到他们再想拿右边的筷子时（执行$wait(chopstick[i + 1] \% 5)$；）就全被阻塞，出现死锁。为了防止死锁发生，可对哲学家进行一些限制条件。比如：（1）至多允许4名哲学家同时进餐；（2）仅当一名哲学家左右两边的筷子都可用时，才允许他抓取筷子；（3）对哲学家顺序编号，要求奇数号哲学家先拿左边的筷子，然后拿右边的筷子，而偶数号哲学家刚好相反。</p><h3 id="采用第一种改进方法"><a class="header-anchor" href="#采用第一种改进方法">¶</a>采用第一种改进方法</h3><p>  设置一个初值为$4$的信号量$r$，只允许$4$个哲学家同时去拿左筷子，这样就能保证至少有一个哲学家可以就餐，不会出现饿死和死锁的现象。具体代码如下所示：</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">semaphore chopstick[5]&#x3D;&#123;1, 1, 1, 1, 1&#125;;semaphore r&#x3D;4;void pi(int i) &#123;    while(true) &#123;        P(r);                        &#x2F;&#x2F; 请求进餐        P(chopstick[i]);             &#x2F;&#x2F; 请求左手边的筷子        P(chopstick[(i+ 1) % 5]);    &#x2F;&#x2F; 请求右手边的筷子        eat();        V(chopstick[(i+1) mod 5]);   &#x2F;&#x2F; 释放右手边的筷子        V(chopstick[i);              &#x2F;&#x2F; 释放左手边的筷子        V(r);                        &#x2F;&#x2F; 释放信号量r        think();    &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="采用第二种改进方法"><a class="header-anchor" href="#采用第二种改进方法">¶</a>采用第二种改进方法</h3><p>  利用信号量的保护机制实现。通过互斥信号量$mutex$对$eat()$之前取左侧和右侧筷子的操作进行保护，可以防止死锁的出现。</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">semaphore mutex &#x3D; 1;semaphore chopstick[5] &#x3D; &#123;1, 1, 1, 1, 1&#125;;void Pi(int i) &#123;    while(true) &#123;        P(mutex);        P(chopstick[i]);        P(chopstick[(i+ 1) % 5]);        V(mutex);        eat();        V(chopstick[(i+ 1) % 5]);        V(chopstick[i]);        think();    &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="采用第三种改进方法"><a class="header-anchor" href="#采用第三种改进方法">¶</a>采用第三种改进方法</h3><p>  规定奇数号哲学家先拿左筷子再拿右筷子，而偶数号哲学家相反。按照下图，将是$2$，$3$号哲学家竞争$3$号筷子，$4$，$5$号哲学家竞争$5$号筷子。$1$号哲学家不需要竞争。最后总会有一个哲学家能获得两支筷子而进餐。<br><img src="/2021/04/06/jing-dian-tong-bu-wen-ti/4.png" alt="哲学家进餐编号示意图"></p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">semaphore chopstick[5] &#x3D; &#123;1, 1, 1, 1, 1&#125;;void philosopher(int i) &#123;    while(true) &#123;        if(i % 2 &#x3D;&#x3D; 0) &#123;       &#x2F;&#x2F; 偶数哲学家,先右后左            P(chopstick[(i + 1) % 5]);            P(chopstick[i]) ;            eat();            V(chopstick[i]);            V(chopstick[(i+ 1) % 5]);        &#125;        else &#123;                   &#x2F;&#x2F; 奇数哲学家，先左后右            P(chopstick[i]);            P(chopstick[(i + 1) % 5]);            eat();            V(chopstick[(i + 1) % 5]);            V(chopstick[i]);        &#125;    &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="吸烟者问题"><a class="header-anchor" href="#吸烟者问题">¶</a>吸烟者问题</h2><p>  <strong>问题描述</strong>：假设一个系统有三个抽烟者进程和一个供应者进程，每个抽烟者不停地卷烟并抽调它，但是卷起来并抽调一支烟，抽烟者需要有：烟草、纸和胶水。三个抽烟者中，第一个拥有烟草，第二个拥有纸，第三个拥有胶水。供应者进程无限地提供三种材料。供应者每次将两种材料放在桌子上，拥有剩下那种材料的抽烟者卷一根烟并抽掉它，并给供应者一个信号告诉已完成，此时供应者就会将另外两种材料放到桌上，如此重复（让三个抽烟者轮流地抽烟）。<br>  <strong>问题分析</strong>：<br>  <strong>关系分析</strong>：在上述问题中，供应者与三个抽烟者是同步关系，抽烟者与抽烟者之间是互斥关系。<br>  <strong>整理思路</strong>：整体有$4$个进程。<br>  <strong>信号量设置</strong>：信号量$offer1$、$offer2$、$offer3$分别表示烟草和纸的组合，烟草和胶水的组合、纸和胶水组合。信号量$finish$用于互斥进行抽烟的动作。</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">int num&#x3D;0;               &#x2F;&#x2F; 存储随机数semaphore offer1 &#x3D; 0;      &#x2F;&#x2F; 定义信号量对应烟草和纸组合的资源semaphore offer2 &#x3D; 0;      &#x2F;&#x2F; 定义信号量对应烟草和胶水组合的资源semaphore offer3 &#x3D; 0;      &#x2F;&#x2F; 定义信号量对应纸和胶水组合的资源semaphore finish &#x3D; 0;      &#x2F;&#x2F; 定义信号量表示抽烟是否完成process P1() &#123;           &#x2F;&#x2F; 供应者    while(1) &#123;        num++;        num &#x3D; num % 3;        if(num &#x3D;&#x3D; 0) &#123;            V(offer1);   &#x2F;&#x2F; 提供烟草和纸        &#125;        else if (num &#x3D;&#x3D; 1) &#123;            V(offer2) ;  &#x2F;&#x2F; 提供烟草和胶水        &#125;        else &#123;            V(offer3);   &#x2F;&#x2F; 提供纸和胶水        &#125;        任意两种材料放在桌子上;        P(finish);    &#125;&#125;process P2 () &#123;          &#x2F;&#x2F; 拥有烟草者    while(1) &#123;        P(offer3);        拿纸和胶水，卷成烟，抽掉;        V(finish);    &#125;&#125;process P3() &#123;           &#x2F;&#x2F; 拥有纸者    while(1) &#123;        P(offer2);        拿烟草和胶水，卷成烟，抽掉;        V(finish);    &#125;&#125;process P4() &#123;           &#x2F;&#x2F; 拥有胶水者    while(1) &#123;        P(offerl);        拿烟草和纸，卷成烟，抽掉;        V(finish);    &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="个人总结"><a class="header-anchor" href="#个人总结">¶</a>个人总结</h2><p>  （1）在解决进程同步问题中，首先是要理解所有进程之间的同步、互斥等关系，了解哪些进程之间是可以同时执行的，这样有利于在整体上把控解决方案。<br>  （2）信号量个数的设置往往与互斥进程数和进程执行条件有关。<br>  （3）了解$P/V$操作的具体含义，对解题会更加有益。<br>  （4）所写的代码并不是串行执行的，而是要具有并行的思想，多考虑如果这些进程同时发生会有什么样的影响。</p>]]></content>
      
      
      <categories>
          
          <category> 课本学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 操作系统 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>进程管理（三）</title>
      <link href="/2021/03/31/jin-cheng-guan-li-san/"/>
      <url>/2021/03/31/jin-cheng-guan-li-san/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h1>进程同步</h1><h2 id="进程同步的基本概念"><a class="header-anchor" href="#进程同步的基本概念">¶</a>进程同步的基本概念</h2><p>  并发执行的进程之间存在相互制约的关系，为了协调进程之间的相互制约关系，进入了进程同步的概念。例如在系统计算算式：$1+2*3$，其中乘法进程一定要在加法进程之前进行运算（在这里把加、乘运算比作进程），但是因为操作系统具有异步性，如果不加以制约，会出现运行顺序相反的情况。同时熟悉以下概念</p><h3 id="临界资源"><a class="header-anchor" href="#临界资源">¶</a>临界资源</h3><p>  在操作系统中，虽然多个进程可以共享系统中的各种资源，但其中许多资源一次只能为一个进程所用，将这些一次仅允许一个进程使用的资源称为<strong>临界资源</strong>，许多物理资源如打印机等都属于临界资源，一些变量、数据等都可以被若干进程共享，也属于临界资源。在每个进程中，访问临界资源的那段代码称为<strong>临界区</strong>。为了保证临界资源的正确使用，把其访问过程分为4个部分：<br>  （1）进入区：在进入区要检查可否进入临界区，若能进入临界区，则应设置正在访问临界区的标志，以阻止其他进程同时进入临界区。<br>  （2）临界区：访问临界资源的那段代码称。<br>  （3）退出区：将正在访问临界区的标志清除。<br>  （4）剩余区：代码中的剩余部分。</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">do &#123;    entry section     &#x2F;&#x2F; 进入区    critical section  &#x2F;&#x2F; 临界区    exit section      &#x2F;&#x2F; 退出区    remainder section &#x2F;&#x2F; 剩余区 &#125; while(true)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="同步"><a class="header-anchor" href="#同步">¶</a>同步</h3><p>  同步即直接制约关系，是指为完成某种任务而建立的两个或多个进程，这些进程因为需要在某些位置上协调它们的工作次序而等待、传递信息所产生的制约关系。</p><h3 id="互斥"><a class="header-anchor" href="#互斥">¶</a>互斥</h3><p>  互斥即间接制约关系，当一个进程进入临界区使用临界资源时，另一个进程必须等待，当占用临界资源的进程退出临界区后，另一进程才允许去访问此临界资源。为了禁止两个进程同时进入临界区，同步机制应遵循以下准则：<br>  （1）<strong>空闲让进</strong><br>  （2）<strong>忙则等待</strong><br>  （3）<strong>有限等待</strong>：对请求访问的进程，应保证能在有限时间内进入临界区。<br>  （4）<strong>让权等待</strong>：当进程不能进入临界区时，应立即释放处理器，防止进程忙等待。</p><h2 id="实现临界区互斥的基本方法"><a class="header-anchor" href="#实现临界区互斥的基本方法">¶</a>实现临界区互斥的基本方法</h2><h3 id="软件实现法"><a class="header-anchor" href="#软件实现法">¶</a>软件实现法</h3><p>  在进入区设置并检查一些标志来标明是否有进程在临界区中，若已有进程在临界区，则在进入临界区通过循环检查进行等待，进程离开临界区后则在退出区修改标志。</p><h4 id="单标志法"><a class="header-anchor" href="#单标志法">¶</a>单标志法</h4><p>  设置一个公用型整数变量turn，用于指示被允许进入临界区的进程编号，即若$turn = 0$，则允许$P_{0}$进程进入临界区。该算法可确保每次只允许一个进程进入临界区。但两个进程必须交替进入临界区。若某个进程不再进入临界区，则另一个进程也无法进入临界区，这样很容易造成资源利用不充分。该情况如下所示：<br>  $P_{0}$进程：</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">while(turn !&#x3D; 0);  &#x2F;&#x2F; 进入区critical section;  &#x2F;&#x2F; 临界区turn &#x3D; 1;          &#x2F;&#x2F; 退出区remainder section; &#x2F;&#x2F; 剩余区 <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>  $P_{1}$进程：</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">while(turn !&#x3D; 1);  &#x2F;&#x2F; 进入区critical section;  &#x2F;&#x2F; 临界区turn &#x3D; 0;          &#x2F;&#x2F; 退出区remainder section; &#x2F;&#x2F; 剩余区 <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>  若$P_{0}$顺利进入临界区并从临界区离开，则此时临界区是空闲的，但$P_{1}$并没有进入临界区的打算，$turn = 1$一直成立，$P_{0}$就无法再次进入临界区（一直被while循环困住）。</p><h4 id="双标志法先检查"><a class="header-anchor" href="#双标志法先检查">¶</a>双标志法先检查</h4><p>  基本思想是在每个进程访问临界区资源之前，先查看临界资源是否正被访问，若正在被访问，该进程需等待；否则，进程才进入自己的临界区。因此，设置一个数据$flag[i]$,如果第$i$个元素值为$FALSE$，表示$P_{i}$进程未进入临界区，值为$TRUE$，表示$P_{i}$进程进入临界区。例如下面的代码表示：<br>  $P_{i}$进程：</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">while(flag[j]);      &#x2F;&#x2F; 进入区  （1）flag[i] &#x3D; TRUE;      &#x2F;&#x2F; 进入区  （3）critical section;    &#x2F;&#x2F; 临界区flag[i] &#x3D; FALSE;     &#x2F;&#x2F; 退出区remainder section;   &#x2F;&#x2F; 剩余区 <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>  $P_{j}$进程：</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">while(flag[i]);      &#x2F;&#x2F; 进入区  （2）flag[j] &#x3D; TRUE;      &#x2F;&#x2F; 进入区  （3）critical section;    &#x2F;&#x2F; 临界区flag[j] &#x3D; FALSE;     &#x2F;&#x2F; 退出区remainder section;   &#x2F;&#x2F; 剩余区 <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>  这样做的优点是：<strong>不用交替进入，可连续使用</strong>。<br>  缺点是：$P_{i}$和$P_{j}$可能同时进入临界区。当两个进程都没有进入时，$flag[i]$和$flag[j]$全部为$FALSE$，上述算法按照（1）（2）（3）（4）执行时，会同时进入临界区（违背了“忙则等待”）。即在检查对方的$flag$后和切换自己的$flag$前有一段时间，结果都检查通过。这里的问题出在检查和修改操作不能一次进行。</p><h4 id="双标志法后检查"><a class="header-anchor" href="#双标志法后检查">¶</a>双标志法后检查</h4><p>  与<strong>双标志法先检查</strong>不同的是，该算法先将自己的标志设置为$TRUE$，再检测对方的状态标志，若对方标志为$TRUE$，则进程等待；否则进入临界区。<br>  $P_{i}$进程：</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">flag[i] &#x3D; TRUE;      &#x2F;&#x2F; 进入区  while(flag[j]);      &#x2F;&#x2F; 进入区  critical section;    &#x2F;&#x2F; 临界区flag[i] &#x3D; FALSE;     &#x2F;&#x2F; 退出区remainder section;   &#x2F;&#x2F; 剩余区 <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>  $P_{j}$进程：</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">flag[j] &#x3D; TRUE;      &#x2F;&#x2F; 进入区  while(flag[i]);      &#x2F;&#x2F; 进入区  critical section;    &#x2F;&#x2F; 临界区flag[j] &#x3D; FALSE;     &#x2F;&#x2F; 退出区remainder section;   &#x2F;&#x2F; 剩余区 <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>  在这种情况下，可能会发生两个进程同时都想进入临界区时，它们分别将自己的标志值$flag$设置为$TRUE$，并且同时检测对方的状态（执行$while$语句），发现对方也要进入临界区时，双方互相谦让，结果两个进程都无法进入临界区，从而导致“饥饿”现象。</p><h4 id="Peterson’s-Algorithm"><a class="header-anchor" href="#Peterson’s-Algorithm">¶</a>Peterson’s Algorithm</h4><p>  为了防止两个进程为进入临界区而无限期等待，又设置了变量$turn$，每个进程在先设置自己的标志后再设置$turn$标志。这时，再同时检测另一个进程状态标志和不允许进入标志，以便保证两个进程同时要求进入临界区时，只允许一个进程进入临界区。<br>  $P_{i}$进程：</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">flag[i] &#x3D; TRUE; turn &#x3D; j;      &#x2F;&#x2F; 进入区  while(flag[j] &amp;&amp; turn &#x3D;&#x3D; j);   &#x2F;&#x2F; 进入区  critical section;              &#x2F;&#x2F; 临界区flag[i] &#x3D; FALSE;               &#x2F;&#x2F; 退出区remainder section;             &#x2F;&#x2F; 剩余区 <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>  $P_{j}$进程：</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">flag[j] &#x3D; TRUE; turn &#x3D; i;      &#x2F;&#x2F; 进入区  while(flag[i] &amp;&amp; turn &#x3D;&#x3D; i);   &#x2F;&#x2F; 进入区  critical section;              &#x2F;&#x2F; 临界区flag[j] &#x3D; FALSE;               &#x2F;&#x2F; 退出区remainder section;             &#x2F;&#x2F; 剩余区 <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>  当进程$P_{i}$设置$flag[i] = true$时，就表示该进程想要进入临界区，同时$turn = j$，此时进程$P_{j}$已在临界区中，符合进程$P_{i}$中的循环条件，所以$P_{i}$不能进入临界区。若$P_{j}$不想要进入临界区，即$flag[i] = false$,循环条件不符合，则$P_{i}$可以顺利进入，反之亦然。</p><h3 id="硬件实现方法"><a class="header-anchor" href="#硬件实现方法">¶</a>硬件实现方法</h3><p>  计算机提供了特殊的硬件指令，允许对一个字中的内容进行检测和修正，或对两个字的内容进行交换。通过硬件支持实现临界段问题的方法称为<strong>低级方法</strong>，或者<strong>元方法</strong>。</p><h4 id="中断屏蔽方法"><a class="header-anchor" href="#中断屏蔽方法">¶</a>中断屏蔽方法</h4><p>  当一个进程正在使用处理机执行它的临界区代码时，防止其他进程进入其临界区进行访问的最简方法是，禁止一切中断发生，或称之为屏蔽中断、关中断。因为CPU只发生在中断时引起进程切换，因此屏蔽中断能够保证当前允许的进程让临界区代码顺利地执行完，进而保证互斥的正确实现，然后执行开中断。其典型模式为：</p><pre class="line-numbers language-none"><code class="language-none">...关中断临界区开中断...<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>  但是<strong>这种方法限制了处理机交替执行程序的能力，因此执行的效率会明显降低</strong>。对内核来说，在它执行更新变量或列表的几条指令期间，关中断是很方便的，但关中断的权力交给用户则很不明智，若一个进程关中断后不再开中断，则系统可能会因此终止。</p><h4 id="硬件指令方法"><a class="header-anchor" href="#硬件指令方法">¶</a>硬件指令方法</h4><p>  $TestAndSet$指令：这条指令为原子操作，即执行代码时不允许被中断，其功能是读出指定标志后把该标志设置为真。具体指令功能如下：</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">boolean TestAndSet(boolean *lock) &#123;    boolean old;    old &#x3D; *lock;    *lock &#x3D; true;    return old;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>  为每个临界资源设置一个共享变量$lock$，表示资源的两种状态：$ture$表示正在被占用，其初值为$false$。在进程访问临界资源之前，利用$TsetAndSet$检查和修改标志$lock$；若有进程在临界区，则重复检查，直到退出进程，利用该指令实现进程互斥算法描述如下：</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">while TestAndSet(&amp;lock);&#x2F;&#x2F; 进程的临界区代码段lock &#x3D; false;&#x2F;&#x2F; 进程的其他代码<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>  $Swap$指令：功能是交换两个字（字节）的内容，具体功能描述如下:</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">Swap(boolean *a, boolean *b) &#123;    boolean temp;    temp &#x3D; *a;    a* &#x3D; *b;    *b &#x3D; temp;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>  <strong>上述两个指令的描述仅是功能实现，而并非软件实现的定义，真正是由硬件逻辑直接实现的，不会被中断</strong>。利用$Swap$指令实现进程互斥的算法描述如下：</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">key &#x3D; true;while(key !&#x3D; false) &#123;    Swap(&amp;lock, &amp;key);&#125;&#x2F;&#x2F; 进程的临界区代码段lock &#x3D; false;&#x2F;&#x2F; 进程的其他代码<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>  为每个临界资源设置一个变量$lock$，初值为$false$；在每个进程中再设置一个局部布尔变量$key$，用于与$lock$交换信息。在进入临界区前，先利用$Swap$指令交换$lock$与$key$的内容，然后检查$key$的状态；有进程在临界区时，重复交换和检查过程，直到进程退出。</p><p>  硬件方法的优点是：（1）适用于任意数目的进程，不管是单处理机还是多处理机.（2）简单，容易验证其正确性。（3）支持进程内有多个临界区，只需要为每个临界区设立一个布尔值变量。<br>  但是其任然存在缺点：（1）进程等待进入临界区时要耗费处理机时间，不能实现让权等待。（2）从等待进程中随机选择一个进入临界区，有的进程可能一直选不上，从而导致“饥饿”现象。</p><h2 id="信号量"><a class="header-anchor" href="#信号量">¶</a>信号量</h2><p>  信号量机制是一种功能较强的机制，可以用来解决互斥与同步问题，只能被两个原语“$P$操作”和“$V$操作”访问。<strong>原语是指完成某种功能且不被分割，不被中断执行的操作序列</strong>，通常由硬件实现。该特性在单处理机上可由软件通过屏蔽中断方法实现。<strong>原语之所以不能被中断执行，是因为原语对变量的操作过程若被打断，可能会去允许另一个对同一变量的操作过程。从而出现临界段问题</strong>。</p><h3 id="整型信号量"><a class="header-anchor" href="#整型信号量">¶</a>整型信号量</h3><p>  整型信号量被定义为一个用于表示资源数目的整型量$S$，$wait$和$signal$操作可描述为</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">wait(S) &#123;    while(S &lt;&#x3D; 0);    S &#x3D; S - 1;&#125;signal(S) &#123;    S &#x3D; S + 1;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>  $wait$操作中，只要信号量$S &lt;= 0$，就会不断测试，因此一直使进程处于“忙等状态”。</p><h3 id="记录型信号量"><a class="header-anchor" href="#记录型信号量">¶</a>记录型信号量</h3><p>  记录型信号量是不存在“忙等”现象的进程同步机制。除需要一个用于代表资源数目的整型变量$value$外，再增加一个进程链表$L$，用于链接所有等待该资源的进程。其结构体如下：</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">typedef struct &#123;    int value;    struct process *L;&#125; semaphore;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>  相应的$wait$和$signal$操作如下：</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">void wait(semaphore S) &#123;     &#x2F;&#x2F; 相当于申请资源    S.value--;    if(S.value &lt; 0) &#123;        add this process to S.L;        block(S.L);    &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>  在$wait$操作中，$S.value–$表示进程请求一个该类资源，当$S.value &lt; 0$时，表示该资源已分配完毕，因此进程应调用$block$原语，进行自我阻塞，放弃处理机，并插入该类资源的等待队列$S.L$。</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">void signal(semaphore S) &#123;   &#x2F;&#x2F;相当于释放资源    S.value++;    if(S.value &lt;&#x3D; 0) &#123;        remove a process P from S.L;        wakeup(P);    &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>  $signal$操作，表示释放一个资源，使系统中可供分配的该类资源数增$1$，因此有$S.value++$。若加$1$后仍是$S.value &lt;= 0$，则表示在$S.L$中仍有等待该资源的进程被阻塞，因此还应调用$wakeup$原语，将$S.L$中的第一个等待进程唤醒。</p><h3 id="利用信号量实现同步"><a class="header-anchor" href="#利用信号量实现同步">¶</a>利用信号量实现同步</h3><p>  信号量机制能用于解决进程间的各种同步问题。设$S$为实现进程$P_{1}$和$P_{2}$同步的公共信号量，初值为$0$。<strong>进程$P_{2}$中的语句$y$要使用进程$P_{1}$中语句$x$的允许结果，所以只有当语句$x$执行完成之后，语句$y$才可以执行</strong>。</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">semaphore S &#x3D; 0;     &#x2F;&#x2F; 初始化信号量P1() &#123;    sqrt(10000);    x;               &#x2F;&#x2F; 语句x    V(s)             &#x2F;&#x2F; 告诉进程P2，语句 x 已经完成    ...&#125;P2() &#123;    ...    P(S);            &#x2F;&#x2F; 检查语句 x 是否运行完成    y;               &#x2F;&#x2F; 检查无误， 运行 y 语句    ...&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>  若$P_{2}$先执行到$P(S)$时，$S$为$0$，执行$P$操作会把进程$P_{2}$阻塞，并放入阻塞队列，当进程$P_{1}$中的$x$执行完后，执行$V$操作，把$P_{2}$从阻塞队列中放回就绪队列，当$P_{2}$得到处理机时，就继续执行。</p><h3 id="利用信号量实现进程互斥"><a class="header-anchor" href="#利用信号量实现进程互斥">¶</a>利用信号量实现进程互斥</h3><p>  设$S$为实现进程$P_{1}$，$P_{2}$互斥的信号量，由于每次只允许一个进程进入临界区，所以$S$的初值应为$1$（即资源可用数为$1$）。只需要把临界区置于$P(S)$和$V(S)$之间，即可实现两个进程对临界区的互斥访问。</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">semaphore S &#x3D; 1;          &#x2F;&#x2F; 初始化信号量P1() &#123;    ...    P(S);                 &#x2F;&#x2F; 准备开始访问临界资源，加锁    进程 P1 的临界区       V(S);                 &#x2F;&#x2F; 访问结束，解锁    ...&#125;P2() &#123;    ...    P(S);                 &#x2F;&#x2F; 准备开始访问临界资源，加锁    进程 P2 的临界区    V(S);                 &#x2F;&#x2F; 访问结束，解锁&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>  当没有进程在临界区时，任意一个进程要进入临界区，就要执行$P$操作，把$S$的值减为$0$，然后进入临界区；当有进程存在于临界区时，$S$的值为$0$，在有进程要进入临界区，执行$P$操作时将会被阻塞，直至在临界区中的进程退出，这样便实现了临界区的互斥。</p><h3 id="信号量实现前驱关系"><a class="header-anchor" href="#信号量实现前驱关系">¶</a>信号量实现前驱关系</h3><h2 id="管程"><a class="header-anchor" href="#管程">¶</a>管程</h2><p>  在信号量机制中，每个要访问临界资源的进程都必须自备同步的$PV$操作，大量分散的同步操作给系统管理带来了麻烦，且容易因同步操作不当而导致系统死锁。于是产生了一种新的进程同步工具——管程，降低了思索发生的可能性。</p><h3 id="管程的定义"><a class="header-anchor" href="#管程的定义">¶</a>管程的定义</h3><p>  系统中的各种硬件资源和软件资源，均可用数据接结构抽象地描述其资源特性。利用共享数据结构抽象地表示系统中的共享资源，而把对数据结构实施的操作定义为一组过程。进程对共享资源的申请、释放等操作，都通过这组过程来实现，这组过程还可以根据资源情况，或接受或阻塞进程的访问，实现进程互斥。<strong>这个代表共享数据结构，以及由对该共享数据结构实施操作的一组过程所组成的资源管理程序，称为管程</strong>。因此管程由以下$4$部分组成：<br>  （1）管程的名称<br>  （2）局部于管程内部的共享结构数据说明。<br>  （3）对该数据结构进行操作的一组过程（或函数）<br>  （4）对局部于管程内部的共享数据设置初始值的语句。</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">monitor Demo &#123; &#x2F;&#x2F; 定义一个名称为“Demo”的管程    &#x2F;&#x2F; 定义共享数据结构，对应系统中的某种共享资源    &#x2F;&#x2F; 共享数据结构 S；    &#x2F;&#x2F; 对共享数据结构初始化的语句    init_code() &#123;        S &#x3D; 5;     &#x2F;&#x2F; 初始资源数等于 5    &#125;    &#x2F;&#x2F; 过程 1：申请一个资源    take_away() &#123;        &#x2F;&#x2F; 对共享数据结构的 x 的一系列处理        S--;       &#x2F;&#x2F; 可用资源数 -1        ...    &#125;    &#x2F;&#x2F; 过程 2：归还一个资源    give_back() &#123;        &#x2F;&#x2F; 对共享数据结构 x 的一系列处理；        S++;       &#x2F;&#x2F; 可用资源数 +1        ...    &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>  管程很像面向对象程序设计中的一个类，<strong>会把所有的共享资源的操作封装起来</strong>，<strong>并且每次仅允许一个进程进入管程</strong>。</p><h3 id="条件变量"><a class="header-anchor" href="#条件变量">¶</a>条件变量</h3><p>  当一个进程进入管程后被阻塞，直到阻塞的原因被解除时，在此期间，如果该进程不释放管程，则其他进程无法进入管程。因此将阻塞原因定义为<strong>条件变量$condition$</strong>,因为一个进程被阻塞的原因有多个，因此在管程中设置了多个条件变量，每个条件变量保存了一个等待队列，用于记录因该条件变量而阻塞的所有进程，对条件变量只能进行两种操作，$wait$和$signal$。<br>  <strong>$x.wait$</strong>：当$x$对应的条件不满足时，正在调用管程的进程调用$x.wait$将自己插入$x$条件的等待队列，并释放管程。此时其他进程可以使用该管程。<br>  <strong>$x.signal$</strong>：$x$对应的条件发生了变化，则调用$x.signal$，唤醒一个因$x$条件而阻塞的进程。</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">monitor Demo &#123;    共享数据结构 S;    condition x;                         &#x2F;&#x2F; 定义一个条件变量    init_code() &#123;...&#125;    take_away() &#123;        if(S &lt;&#x3D; 0) x.wait();            &#x2F;&#x2F; 资源不够，在条件变量x上阻塞等待        &#x2F;&#x2F; 资源足够，分配资源，做一系列相应处理    &#125;    give_back() &#123;        &#x2F;&#x2F; 归还资源，做一系列相应处理；        if(&#x2F;*有进程在等待*&#x2F;) x.xignal;    &#x2F;&#x2F; 唤醒一个阻塞进程    &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>  条件变量和信号量的比较：<br>  相似点：条件变量的$wait/signal$操作类似于信号量的$P/V$操作，可以实现进程的阻塞/唤醒。<br>  不同点：条件变量是“没有值的”，仅实现了“排队等待”功能；而信号量是“有值”的，信号量的值反映了剩余资源数，而在管程中，剩余资源数用共享数据结构记录。</p><h2 id="经典的同步问题"><a class="header-anchor" href="#经典的同步问题">¶</a>经典的同步问题</h2><p>  这部分内容单独用一章博客进行讲述。</p>]]></content>
      
      
      <categories>
          
          <category> 课本学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 操作系统 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2021华为软件精英挑战赛初赛代码及思路</title>
      <link href="/2021/03/30/2021-hua-wei-ruan-jian-jing-ying-tiao-zhan-sai-chu-sai-dai-ma-ji-si-lu/"/>
      <url>/2021/03/30/2021-hua-wei-ruan-jian-jing-ying-tiao-zhan-sai-chu-sai-dai-ma-ji-si-lu/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="2021华为软件精英挑战赛训练赛、正式赛思路分享"><a class="header-anchor" href="#2021华为软件精英挑战赛训练赛、正式赛思路分享">¶</a>2021华为软件精英挑战赛训练赛、正式赛思路分享</h2><p>    有幸再次参加了华为软件精英挑战赛（去年由于不知道数据集有坑，导致没能进入复赛，今年决定再来一次弥补去年的遗憾）<br>    今年的赛题相比去年个人感觉还是好了一些的，从任务指导书所给的评分规则来看，确实要比去年单一的按程序运行时间来评判要好一些。并且题目属于开放性赛题，没有唯一的标准答案，所以在逻辑思维上可以很好的区分参赛选手（至少我是这么认为的）。具体的赛题可以到比赛官网中进行下载，或者点击这个链接<a href="https://developer.huaweicloud.com/hero/forum.php?mod=viewthread&amp;tid=112802">初赛赛题下载</a><br>    说说我们队伍的情况吧，我们是武长赛区正式赛排行榜中的第七名。<br><img src="/2021/03/30/2021-hua-wei-ruan-jian-jing-ying-tiao-zhan-sai-chu-sai-dai-ma-ji-si-lu/2.png" alt="队伍最终最好成绩"></p><p>但是因为队友题目出来当晚，官方没有禁止开源代码的情况下，开源了一份baseline（只开源了一次），那份baseline没有加任何迁移算法，只是一个很基础的供选手参赛的代码，在训练赛中也只是50e的分数，（参赛的同学应该知道50e的分数是一个什么水平，就是垫底的分数）在大赛继续进行到12号的时候，因为开源代码的选手零零星星出现了不少，官方禁止了开源，我们也没有对外透漏过任何代码，在正式赛结束之后，很不幸，被查重了，我很呵呵，我加了迁移的算法怎么与其他人去冲突？？？？查重不应该是在逻辑上查重吗？难道有人用了我的代码，自动的在我们的思路带领下，和我们写了一样的调度算法？？？？很迷惑，申诉之后，官方的回信也的内容只有大赛的四条比赛规则（感觉只是走了一个流程），而且我们知道的其他一些开源的队伍，竟然没有被查到，更加迷惑了。我大三下了，为了这个比赛放弃了一段时间的考研复习，就是希望在这个赛事中找回去年的不爽，结果再次寒心……。吃一堑长一智吧。<br>    说点开心的吧，在正式赛中团队整体奋斗的是我和另外一位老哥，老哥实力很强，在开始冲的还是很猛的，后期因为一些其他的事情，没有把太多精力放在这个上面，因为进复赛很稳了，但是没想到，呵呵呵。当然在这次比赛中，还认识了其他不少的大佬，比赛嘛，最开心的是认识很多朋友了。</p><h3 id="赛题分析"><a class="header-anchor" href="#赛题分析">¶</a>赛题分析</h3><p>    在分析赛提前提醒大家，想要尽快写baseline、出结果、上分，一定要先读题，认认真真读题，好好分析赛题。<br>    题目大致是给你一些服务器和虚拟机的类型，服务器分为两个节点A、B，<strong>服务器拥有的资源（CPU 和内存）均匀分布在这两个节点上</strong>。这句话是重点！！！这句话是重点！！！这句话是重点！！！也就意味着，如果服务器的类型为NV603 ，其CPU为92C，内存为324G，那么其 A、B 两个节点分别包含的资源为：CPU核数：46C 和，内存大小162G 的资源。并且保证服务器的CPU 核数和内存大小均为偶数。并且在题目中还给出了服务器的硬件成本以及每日耗能成本。<br>    题目中所给的虚拟机有两种部署方式，分别为单双节点部署，单节点部署指的是一台虚拟机所需的资源（CPU和内存）完全由主机上的一个节点提供；<strong>双节点部署指的是一台虚拟机所需的资源（CPU 和内存）必须由一台服务器的两个节点同时提供，并且每个节点提供总需求资源的一半</strong>。<br>    赛题要求根据所给的请求序列，创建服务器，部署虚拟机，或者按照用户请求在对应的服务器上删除相应的虚拟机。但要注意，服务器上的任意一个节点(A和 B)上的资源负载(CPU 和内存)均不能超过其容量上限。在完成每一天的服务器的扩容之后，在处理每一天的新请求之前，你还可以对当前存量虚拟机进行一次迁移，即把虚拟机从一台服务器迁移至另一台服务器。对于单节点部署的虚拟机，将其从一台服务器的 A 节点迁移至 B 节点(或反之)也是允许的。但迁移的虚拟机总量不超过当前存量虚拟机数量的千分之五。<br>    通读赛题所有要求之后，其实我们可以发现这是一个类似于装箱的问题，把服务器比作箱子，把虚拟机比作需要放进去的货物。在一般的装箱问题中，我们首先要考虑的就是如何选择箱子，即选择用什么样的箱子来装什么样的货物是最合理的。这就需要涉及到对所有的箱子进行特征上的分析。接下来就是我们队伍对于整个赛题的思路（谨代表团队思路，如果讲解有误，勿喷！）</p><h3 id="购买服务器思路"><a class="header-anchor" href="#购买服务器思路">¶</a>购买服务器思路</h3><p>    首先最开始我们想到的当然就是如何购买服务器，买容量最大的？买价格最便宜的？买CPU和内存大小最接近的？……上面的这几种思路并不没有一个标准的正确，首先在这种调度问题中，是没有一个正确的答案的，通过这些思路最终得到的结果是好是坏，很大一部分是取决于数据集的，但是我们也不能因为没有正确的答案，而且五花八门的胡乱猜测。首先，我们继续回到题目背景中去，背景中有如下一句话：<br><img src="/2021/03/30/2021-hua-wei-ruan-jian-jing-ying-tiao-zhan-sai-chu-sai-dai-ma-ji-si-lu/1.png" alt="赛题中的背景信息"><br>众所周知，这种比赛的目的一方面是为了选拔优秀选手，另一方面也是为了给自己公司当前所存在的问题，在民间寻找解决方法，可能你的思路在这个比赛中节约了一点点成本，放在真正的市场中，可以让华为的成本节约<strong>亿点点</strong>。<br>    我们队伍在题目中所给数据中的服务器的规格进行了一个粗略的分析，服务器的种类比较丰富，每种服务器上CPU和内存的大小也不同，我们按照以下的几种方式对服务器进行了排序，并在每种排序后面写了我们的思考依据：</p><ol><li>CPU+内存：这样为了保证可以选择资源足够大的服务器</li><li>每日的耗能：如果请求天数足够多，服务器开启时间足够长，那么每日耗能的费用要远远大于其硬件成本</li><li>CPU * 0.75 + 内存 * 0.25：魔法数字，后面还有部分也会用到，对于我们版本的代码来说，很重要！！！。</li><li>服务器的硬件成本：在请求天数较小，每天请求数量较小的情况下， 硬件成本所占的费用比重较大</li><li>CPU：仅作为参考</li><li>内存：仅作为参考</li><li>abs(CPU - 内存)：大规模虚拟机的部署情况下，所有虚拟机所需的内存和CPU总和是接近的或者会趋近于某一个比值，为了让服务器可以部署更多的虚拟机，我们就尽量使服务器的CPU和内存大小接近，</li><li>服务器的硬件成本 / (CPU + 内存)：考虑服务器的性价比。</li></ol><h3 id="处理请求思路"><a class="header-anchor" href="#处理请求思路">¶</a>处理请求思路</h3><p>    本题中的数据输入是一次性输入的，我们可以先把所有的数据保存下来，然后去对这些信息集中处理，这样我们就可以用一个上帝视角来解决这个问题，也可以按照每一条请求进行中规中距的进行处理（因为我们队伍依次对请求进行处理的分数还是比较客观的，又因为是初赛，也没有去花太多功夫去更改已经写好的baseline，所以没有采用上面看似比较良好的上帝视角去处理）在得到每一次的请求之后，判断其为“add”，还是“del”，依次进行请求处理。</p><h3 id="选择服务器"><a class="header-anchor" href="#选择服务器">¶</a>选择服务器</h3><p>    在刚开始遇到“add”请求，或者在当前已有服务器无法满足请求中虚拟机所需要的资源时，我们需要重新开启新的服务器，在这时我们就会遇到服务器的选择问题。我们队伍根据上面的选择思路以及实验中得到结果进行对比后，采用的是第二种排序方式，然后对排好序的服务器从前到后依次遍历，选择刚好符合该虚拟机的服务器，具体代码如下：</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">int ii;for (ii &#x3D; 0; ii &lt; model_pair_size; ii++)&#123; &#x2F;&#x2F;找到一个最合适虚拟机的服务器，vv代表的是虚拟机信息的结构体    if (vv.cpu &lt; model_pair[ii].first.cpu &#x2F; 2 &amp;&amp; vv.Memory &lt; model_pair[ii].first.Memory &#x2F; 2 &amp;&amp; !vv.Is_Double_node)    &#123;        break;    &#125;    if (vv.cpu &lt; model_pair[ii].first.cpu &amp;&amp; vv.Memory &lt; model_pair[ii].first.Memory &amp;&amp; vv.Is_Double_node)    &#123;        break;    &#125;&#125;server ss &#x3D; model_pair[ii].first;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>    在我们初赛的代码中，其他排序得到的结果没有这种排序的方式费用低，并且在线下数据集和正式赛中，（3）、（4）两种排序方式最终得到的分数是相同的，并且在线下数据集中，根据这两种方式排序，得到的服务器的顺序大致是相同的。（很可能是我们猜测的系数刚好符合简化后数据集的系数）。</p><h3 id="添加虚拟机"><a class="header-anchor" href="#添加虚拟机">¶</a>添加虚拟机</h3><p>    如果在当前服务器有满足需要部署的虚拟机所需要资源的情况下，我们部署的策略是，首先遍历所有服务器，找到当前虚拟机部署在第一次出现的合适的服务器，对于双节点虚拟机，计算其部署后剩余资源的大小，即<strong>剩余CPU * 0.75 + 剩余内存 * 0.25</strong>，如果为单节点虚拟机，<br>只计算其一个节点的剩余资源，计算公式还是如上所示。<strong>在单节点中，选取A、B节点也要考虑两个节点之间的负载均衡，因为后面可能会出现双节点虚拟机不会因为负载不均衡而无法部署</strong>。最终找到剩余资源最小的那个服务器进行部署，具体代码如下所示：</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">for (int i &#x3D; 0; i &lt; sizes; i++)&#123;    if (vv.Is_Double_node)    &#123;        if (server_myselfs[i].A_cpu &gt;&#x3D; vv.cpu &#x2F; 2 &amp;&amp; server_myselfs[i].A_Memory &gt;&#x3D; vv.Memory &#x2F; 2 &amp;&amp; server_myselfs[i].B_cpu &gt;&#x3D; vv.cpu &#x2F; 2 &amp;&amp; server_myselfs[i].B_Memory &gt;&#x3D; vv.Memory &#x2F; 2)        &#123;            if (((server_myselfs[i].A_cpu + server_myselfs[i].B_cpu - vv.cpu) * Q3 + (server_myselfs[i].A_Memory + server_myselfs[i].B_Memory - vv.Memory) * Q4) &lt; myself_max)            &#123;            myself_max &#x3D; (server_myselfs[i].A_cpu + server_myselfs[i].B_cpu - vv.cpu) * Q3 + (server_myselfs[i].A_Memory + server_myselfs[i].B_Memory - vv.Memory) * Q4;                myself_idnex &#x3D; i;                flag &#x3D; 1;            &#125;        &#125;    &#125;    else    &#123;    int flag1 &#x3D; 0, flag2 &#x3D; 0;        if (server_myselfs[i].A_cpu &gt;&#x3D; vv.cpu &amp;&amp; server_myselfs[i].A_Memory &gt;&#x3D; vv.Memory)        &#123;        if (((server_myselfs[i].A_cpu - vv.cpu) * Q3 + (server_myselfs[i].A_Memory - vv.Memory) * Q4) &lt; myself_max)            &#123;            myself_max &#x3D; (server_myselfs[i].A_cpu - vv.cpu) * Q3 + (server_myselfs[i].A_Memory - vv.Memory) * Q4;                myself_idnex &#x3D; i;                A_or_B &#x3D; 1;                flag &#x3D; 1;                flag1 &#x3D; 1;            &#125;        &#125;        int myself_max_tmp &#x3D; myself_max;        if (server_myselfs[i].B_cpu &gt;&#x3D; vv.cpu &amp;&amp; server_myselfs[i].B_Memory &gt;&#x3D; vv.Memory)        &#123;        if (((server_myselfs[i].B_cpu - vv.cpu) * Q3 + (server_myselfs[i].B_Memory - vv.Memory) * Q4) &lt; myself_max)            &#123;            myself_max &#x3D; (server_myselfs[i].B_cpu - vv.cpu) * Q3 + (server_myselfs[i].B_Memory - vv.Memory) * Q4;                myself_idnex &#x3D; i;                A_or_B &#x3D; 0;                flag &#x3D; 1;                flag2 &#x3D; 1;            &#125;        &#125;        if (myself_max_tmp &lt;&#x3D; myself_max &amp;&amp; flag1 &#x3D;&#x3D; 1 &amp;&amp; flag2 &#x3D;&#x3D; 1)        &#123;        myself_max &#x3D; myself_max_tmp;            A_or_B &#x3D; 1;            flag &#x3D; 1;        &#125;        if (myself_max_tmp &gt;&#x3D; myself_max &amp;&amp; flag1 &#x3D;&#x3D; 1 &amp;&amp; flag2 &#x3D;&#x3D; 1)        &#123;        A_or_B &#x3D; 0;            flag &#x3D; 1;        &#125;    &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="迁移调度"><a class="header-anchor" href="#迁移调度">¶</a>迁移调度</h3><p>    在迁移过程中，我们使用的是一层虚拟机循环，一层服务器循环，我们对上一天中的所有虚拟机进行遍历，在循环中将这个虚拟机部署到其他合适的服务器上，首先去计算当前虚拟机在当前服务器CPU和内存的使用率，**当前CPU和内存的剩余资源所占总资源的百分比小于0.07，那么我们直接跳过，**这样做的理由如下：</p><ol><li>首先可以加速，在正式赛中，数据量比线下的数据量要大很多，我们可以在牺牲一部分迁移的情况下来避免超时，正式赛的超时现象很严重。</li><li>其次，在服务器所剩余资源较小的情况下，我们可以认为当前服务器已经达到了负载均衡，满足理想中的条件，如果再加迁移，可能会影响均衡。</li></ol><p>    在正式赛的提交中，我们的代码在取该值为0.07时，得到的分数是最好的。由于迁移代码较长，就不在此处进行放置，大家可以在我们公布的源码中进行理解。</p><h3 id="可以优化的思路"><a class="header-anchor" href="#可以优化的思路">¶</a>可以优化的思路</h3><p>    因为我们的当前版本的分数可以进入复赛，加上队友后期有事，就没有花太多精力去进行其他方面的优化（也是本人比较菜，写了几个bug，懒得改了）现在分享一些其我们想到的他方面的一些没有实现的优化吧：</p><ol><li>可以将每天的请求进行提前保存，对需要部署的虚拟机进行排序选择，或者可以根据当天的虚拟机的CPU和内存进行拟合，对服务器重新排序，选择合适的服务器。</li><li>可以在每次迁移前，对所有的服务器进行排序，将利用率较小的服务器迁移到利用率较高的服务器上去。</li><li>可以在整个添加服务器上进行单双节点分治，即单节点部署的虚拟机可以迁移到有双节点部署的服务器上去，但是双节点部署的虚拟机不可以迁移到只有单节点部署的服务器上面，这点在我们中间版本的代码中是有一定提升的，但是因为后面的版本有了一些改动，所以就没有继续采用，大佬们可以自己实验以下。</li><li>服务器的选择，线上和线下的服务器中了以及特征是差不多的，因为上面有两种排序得到结果是相同的，线下的数据也是相同的。我们的服务器选择不算差，但是与前排大佬比起来还是逊色一些的，所以服务器的选择可以使用一些更高级的拟合来进行。</li></ol><h3 id="队伍正式赛分数最优代码"><a class="header-anchor" href="#队伍正式赛分数最优代码">¶</a>队伍正式赛分数最优代码</h3><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">#include &lt;iostream&gt;#include &lt;string&gt;#include &lt;vector&gt;#include &lt;unordered_map&gt;#include &lt;fstream&gt;#include &lt;algorithm&gt;#include &lt;ctime&gt;using namespace std;&#x2F;&#x2F;clock_t startTime, endTime;&#x2F;&#x2F; 服务器typedef struct A&#123;    &#x2F;&#x2F;型号, CPU核数, 内存大小, 硬件成本, 能耗成本    string model;              &#x2F;&#x2F;型号    double cpu;                &#x2F;&#x2F;CPU核数    double Memory;             &#x2F;&#x2F;内存大小    long long Hardware_cost;   &#x2F;&#x2F;硬件成本    long long Energy_cost_day; &#x2F;&#x2F;每日能耗成本&#125; server;&#x2F;&#x2F; 虚拟机typedef struct B&#123;    &#x2F;&#x2F;型号, CPU核数, 内存大小, 是否双节点部署    string model;       &#x2F;&#x2F;型号    double cpu;         &#x2F;&#x2F;CPU核数    double Memory;      &#x2F;&#x2F;内存大小    int Is_Double_node; &#x2F;&#x2F;是否双节点部署&#125; VM;&#x2F;&#x2F;用来存放当前所有服务器的信息的typedef struct C&#123;    string model; &#x2F;&#x2F;型号    double A_cpu; &#x2F;&#x2F;已经当前还未使用数量    double A_Memory;    double B_cpu;    double B_Memory;    long long Energy_cost_day;    int flag;    vector&lt;pair&lt;int, int&gt;&gt; VM_ids;&#125; server_myself;&#x2F;&#x2F; 每一天中增加的虚拟机信息typedef struct D&#123;    string model;    int index;    int server_id;      &#x2F;&#x2F;存放在哪一个server中    int Is_Double_node; &#x2F;&#x2F;是否使用双节点部署    int A_or_B;         &#x2F;&#x2F;如果使用的是单节点部署，那么部署在哪个节点,1代表A&#125; add_VM;typedef struct E&#123;    string op;    string model;    int id;&#125; operators;int cost;                                 &#x2F;&#x2F;这个用来记录消费，用来评估算法水平unordered_map&lt;string, server&gt; server_map; &#x2F;&#x2F;存储所有服务器的信息server server_buf;vector&lt;pair&lt;server, double&gt;&gt; model_pair;unordered_map&lt;string, VM&gt; VM_map;VM VM_buf;vector&lt;pair&lt;int, int&gt;&gt; ids_pair;unordered_map&lt;int, add_VM&gt; adds;vector&lt;server_myself&gt; server_myselfs;int day &#x3D; 0;int migrations_index &#x3D; 0;int sum_vm &#x3D; 0;vector&lt;int&gt; cin_size;vector&lt;operators&gt; cin_buf;string buf;double Q1 &#x3D; 0.75, Q2 &#x3D; 0.25;double Q3 &#x3D; 0.75, Q4 &#x3D; 0.25;void InitServer(string buf)&#123;    int j &#x3D; 1;    server_buf.model.clear();    &#x2F;&#x2F; 服务器型号    while (buf[j] !&#x3D; &#39;,&#39;)    &#123;        server_buf.model.push_back(buf[j]);        j++;    &#125;    j++;    while (buf[j] &#x3D;&#x3D; &#39; &#39;)    &#123;        j++;    &#125;    &#x2F;&#x2F; 服务CPU大小    server_buf.cpu &#x3D; 0;    while (buf[j] !&#x3D; &#39;,&#39;)    &#123;        server_buf.cpu &#x3D; server_buf.cpu * 10 + buf[j] - &#39;0&#39;;        j++;    &#125;    j++;    while (buf[j] &#x3D;&#x3D; &#39; &#39;)    &#123;        j++;    &#125;    &#x2F;&#x2F; 服务内存大小    server_buf.Memory &#x3D; 0;    while (buf[j] !&#x3D; &#39;,&#39;)    &#123;        server_buf.Memory &#x3D; server_buf.Memory * 10 + buf[j] - &#39;0&#39;;        j++;    &#125;    j++;    while (buf[j] &#x3D;&#x3D; &#39; &#39;)    &#123;        j++;    &#125;    &#x2F;&#x2F; 服务硬件成本    server_buf.Hardware_cost &#x3D; 0;    while (buf[j] !&#x3D; &#39;,&#39;)    &#123;        server_buf.Hardware_cost &#x3D; server_buf.Hardware_cost * 10 + buf[j] - &#39;0&#39;;        j++;    &#125;    j++;    while (buf[j] &#x3D;&#x3D; &#39; &#39;)    &#123;        j++;    &#125;    &#x2F;&#x2F; 服务耗能成本    server_buf.Energy_cost_day &#x3D; 0;    while (buf[j] !&#x3D; &#39;)&#39;)    &#123;        server_buf.Energy_cost_day &#x3D; server_buf.Energy_cost_day * 10 + buf[j] - &#39;0&#39;;        j++;    &#125;    server_map[server_buf.model] &#x3D; server_buf;    &#x2F;&#x2F;double weight &#x3D; server_buf.cpu * 0.75 + server_buf.Memory * 0.25;    double weight &#x3D; server_buf.Energy_cost_day;    model_pair.push_back(make_pair(server_buf, weight));&#125;void InitVM(string buf)&#123;    int j &#x3D; 1;    VM_buf.model.clear();    while (buf[j] !&#x3D; &#39;,&#39;)    &#123;        VM_buf.model.push_back(buf[j]);        j++;    &#125;    j++;    while (buf[j] &#x3D;&#x3D; &#39; &#39;)    &#123;        j++;    &#125;    VM_buf.cpu &#x3D; 0;    while (buf[j] !&#x3D; &#39;,&#39;)    &#123;        VM_buf.cpu &#x3D; VM_buf.cpu * 10 + buf[j] - &#39;0&#39;;        j++;    &#125;    j++;    while (buf[j] &#x3D;&#x3D; &#39; &#39;)    &#123;        j++;    &#125;    VM_buf.Memory &#x3D; 0;    while (buf[j] !&#x3D; &#39;,&#39;)    &#123;        VM_buf.Memory &#x3D; VM_buf.Memory * 10 + buf[j] - &#39;0&#39;;        j++;    &#125;    j++;    while (buf[j] &#x3D;&#x3D; &#39; &#39;)    &#123;        j++;    &#125;    VM_buf.Is_Double_node &#x3D; buf[j] - &#39;0&#39;;    VM_map[VM_buf.model] &#x3D; VM_buf;&#125;string Operation(string buf, int &amp;index)&#123;    string op;    while (buf[index] !&#x3D; &#39;,&#39;)    &#123;        op.push_back(buf[index]);        index++;    &#125;    index++;    while (buf[index] &#x3D;&#x3D; &#39; &#39;)    &#123;        index++;    &#125;    return op;&#125;bool static cmp(pair&lt;server, double&gt; &amp;A, pair&lt;server, double&gt; &amp;B)&#123;    return A.second &lt; B.second;&#125;bool static cmp2(server_myself &amp;A, server_myself &amp;B)&#123;    return A.A_cpu + A.B_cpu &lt; B.A_cpu + B.B_cpu;&#125;&#x2F;*bool static cmp3(server_myself &amp;A, server_myself &amp;B)&#123;    return A.id &lt; B.id;&#125;*&#x2F;&#x2F;&#x2F;ofstream outfile(&quot;out2.txt&quot;, ios::trunc);&#x2F;&#x2F;ofstream outfile2(&quot;out.txt&quot;, ios::trunc);void Select(int &amp;sum, vector&lt;string&gt; &amp;migrations)&#123;    int j &#x3D; migrations_index;    int server_myselfs_len &#x3D; (int)server_myselfs.size();    int ids_pair_len &#x3D; (int)ids_pair.size();    int find_sum &#x3D; 0;    &#x2F;&#x2F; sort(server_myselfs.begin(), server_myselfs.end(), cmp2);    while (1)    &#123;        j++;        find_sum++;        j &#x3D; j % ids_pair_len;                if (j &#x3D;&#x3D; migrations_index)        &#123;            break;        &#125;        &#x2F;&#x2F; if(find_sum&gt;ids_pair_len&#x2F;2)&#123;        &#x2F;&#x2F;     break;        &#x2F;&#x2F; &#125;        &#x2F;&#x2F;调度不能超过总数的5&#x2F;1000;        if (sum &gt;&#x3D; (sum_vm &#x2F; 1000 * 5 - 1))        &#123;            break;        &#125;        if (ids_pair[j].second)        &#123; &#x2F;&#x2F;表示当前虚拟机还存在的            add_VM temp &#x3D; adds[ids_pair[j].first];            int max_index &#x3D; temp.server_id;            int max_index_or &#x3D; max_index;            &#x2F;*            if (max_index !&#x3D; server_myselfs[max_index].id)            &#123;                cout &lt;&lt; &quot;Error&quot; &lt;&lt; endl;            &#125;            *&#x2F;            double max, max1, max2;            if (temp.Is_Double_node)            &#123;                max &#x3D; (server_myselfs[max_index].A_cpu + server_myselfs[max_index].B_cpu) * Q1 + (server_myselfs[max_index].A_Memory + server_myselfs[max_index].B_Memory) * Q2;                max1 &#x3D; (server_myselfs[max_index].A_cpu + server_myselfs[max_index].B_cpu) * 1.0 &#x2F; (server_map[server_myselfs[max_index].model].cpu);                max2 &#x3D; (server_myselfs[max_index].A_Memory + server_myselfs[max_index].B_Memory) * 1.0 &#x2F; (server_map[server_myselfs[max_index].model].Memory);            &#125;            else            &#123;                if (temp.A_or_B)                &#123;                    max &#x3D; server_myselfs[max_index].A_cpu * Q1 + server_myselfs[max_index].A_Memory * Q2;                    max1 &#x3D; server_myselfs[max_index].A_cpu * 1.0 &#x2F; (server_map[server_myselfs[max_index].model].cpu &#x2F; 2);                    max2 &#x3D; server_myselfs[max_index].A_Memory * 1.0 &#x2F; (server_map[server_myselfs[max_index].model].Memory &#x2F; 2);                &#125;                else                &#123;                    max &#x3D; server_myselfs[max_index].B_cpu * Q1 + server_myselfs[max_index].B_Memory * Q2;                    max1 &#x3D; server_myselfs[max_index].B_cpu * 1.0 &#x2F; (server_map[server_myselfs[max_index].model].cpu &#x2F; 2);                    max2 &#x3D; server_myselfs[max_index].B_Memory * 1.0 &#x2F; (server_map[server_myselfs[max_index].model].Memory &#x2F; 2);                &#125;            &#125;                        if (max1 &lt; 0.07 &amp;&amp; max2 &lt; 0.07)            &#123;                continue;            &#125;                        int flag &#x3D; 0;            int A_or_B &#x3D; 0;            VM vv &#x3D; VM_map[temp.model];            for (int i &#x3D; 0; i &lt; server_myselfs_len; i++)            &#123;                if (i &#x3D;&#x3D; max_index_or)                &#123;                    continue;                &#125;                if (vv.Is_Double_node)                &#123;                    if (server_myselfs[i].A_cpu &gt;&#x3D; vv.cpu &#x2F; 2 &amp;&amp; server_myselfs[i].A_Memory &gt;&#x3D; vv.Memory &#x2F; 2 &amp;&amp; server_myselfs[i].B_cpu &gt;&#x3D; vv.cpu &#x2F; 2 &amp;&amp; server_myselfs[i].B_Memory &gt;&#x3D; vv.Memory &#x2F; 2)                    &#123;                        if (((server_myselfs[i].A_cpu + server_myselfs[i].B_cpu - vv.cpu) * Q1 + (server_myselfs[i].A_Memory + server_myselfs[i].B_Memory - vv.Memory) * Q2) &lt; max)                        &#123;                            max &#x3D; (server_myselfs[i].A_cpu + server_myselfs[i].B_cpu - vv.cpu) * Q1 + (server_myselfs[i].A_Memory + server_myselfs[i].B_Memory - vv.Memory) * Q2;                            max_index &#x3D; i;                            flag &#x3D; 1;                        &#125;                    &#125;                &#125;                else                &#123;                    if (server_myselfs[i].A_cpu &gt;&#x3D; vv.cpu &amp;&amp; server_myselfs[i].A_Memory &gt;&#x3D; vv.Memory)                    &#123;                        if (((server_myselfs[i].A_cpu - vv.cpu) * Q1 + (server_myselfs[i].A_Memory - vv.Memory) * Q2) &lt; max)                        &#123;                            max &#x3D; (server_myselfs[i].A_cpu - vv.cpu) * Q1 + (server_myselfs[i].A_Memory - vv.Memory) * Q2;                            max_index &#x3D; i;                            A_or_B &#x3D; 1;                            flag &#x3D; 1;                        &#125;                    &#125;                    if (server_myselfs[i].B_cpu &gt;&#x3D; vv.cpu &amp;&amp; server_myselfs[i].B_Memory &gt;&#x3D; vv.Memory)                    &#123;                        if (((server_myselfs[i].B_cpu - vv.cpu) * Q1 + (server_myselfs[i].B_Memory - vv.Memory) * Q2) &lt; max)                        &#123;                            max &#x3D; (server_myselfs[i].B_cpu - vv.cpu) * Q1 + (server_myselfs[i].B_Memory - vv.Memory) * Q2;                            max_index &#x3D; i;                            A_or_B &#x3D; 0;                            flag &#x3D; 1;                        &#125;                    &#125;                &#125;            &#125;            if (flag)            &#123;                &#x2F;&#x2F;outfile2 &lt;&lt; &quot;day: &quot; &lt;&lt; day &lt;&lt; &quot;j: &quot; &lt;&lt; j &lt;&lt; &quot; ,&quot; &lt;&lt; ids_pair_len &lt;&lt; endl;                add_VM tt &#x3D; adds[ids_pair[j].first];                sum++;                if (vv.Is_Double_node)                &#123;                    &#x2F;&#x2F;还原                    server_myselfs[max_index_or].A_cpu +&#x3D; vv.cpu &#x2F; 2;                    server_myselfs[max_index_or].A_Memory +&#x3D; vv.Memory &#x2F; 2;                    server_myselfs[max_index_or].B_cpu +&#x3D; vv.cpu &#x2F; 2;                    server_myselfs[max_index_or].B_Memory +&#x3D; vv.Memory &#x2F; 2;                    server_myselfs[max_index].A_cpu -&#x3D; vv.cpu &#x2F; 2;                    server_myselfs[max_index].A_Memory -&#x3D; vv.Memory &#x2F; 2;                    server_myselfs[max_index].B_cpu -&#x3D; vv.cpu &#x2F; 2;                    server_myselfs[max_index].B_Memory -&#x3D; vv.Memory &#x2F; 2;                    tt.server_id &#x3D; max_index;                    tt.Is_Double_node &#x3D; 1;                    migrations.push_back(&quot;(&quot; + to_string(ids_pair[j].first) + &quot;,&quot; + to_string(max_index) + &quot;)&quot;);                &#125;                else                &#123;                    if (tt.A_or_B)                    &#123;                        server_myselfs[max_index_or].A_cpu +&#x3D; vv.cpu;                        server_myselfs[max_index_or].A_Memory +&#x3D; vv.Memory;                    &#125;                    else                    &#123;                        server_myselfs[max_index_or].B_cpu +&#x3D; vv.cpu;                        server_myselfs[max_index_or].B_Memory +&#x3D; vv.Memory;                    &#125;                    if (A_or_B)                    &#123;                        tt.A_or_B &#x3D; 1;                        tt.server_id &#x3D; max_index;                        server_myselfs[max_index].A_cpu -&#x3D; vv.cpu;                        server_myselfs[max_index].A_Memory -&#x3D; vv.Memory;                        migrations.push_back(&quot;(&quot; + to_string(ids_pair[j].first) + &quot;,&quot; + to_string(max_index) + &quot;,A&quot; + &quot;)&quot;);                    &#125;                    else                    &#123;                        tt.A_or_B &#x3D; 0;                        tt.server_id &#x3D; max_index;                        server_myselfs[max_index].B_cpu -&#x3D; vv.cpu;                        server_myselfs[max_index].B_Memory -&#x3D; vv.Memory;                        migrations.push_back(&quot;(&quot; + to_string(ids_pair[j].first) + &quot;,&quot; + to_string(max_index) + &quot;,B&quot; + &quot;)&quot;);                    &#125;                &#125;                if ((server_myselfs[max_index_or].A_cpu + server_myselfs[max_index_or].B_cpu) &#x3D;&#x3D; server_map[server_myselfs[max_index_or].model].cpu)                &#123;                    if ((server_myselfs[max_index_or].A_Memory + server_myselfs[max_index_or].B_Memory) &#x3D;&#x3D; server_map[server_myselfs[max_index_or].model].Memory)                    &#123;                        server_myselfs[max_index_or].flag &#x3D; 0;                    &#125;                &#125;                adds[ids_pair[j].first] &#x3D; tt;            &#125;        &#125;    &#125;    migrations_index &#x3D; j;&#125;void ReadallData(int T)&#123;    for (int k &#x3D; 0; k &lt; T; k++)    &#123;        int R;        cin &gt;&gt; R;        getchar();        cin_size.push_back(R);        for (int j &#x3D; 0; j &lt; R; j++)        &#123;            getline(cin, buf);            int index &#x3D; 1;            operators ops;            ops.op &#x3D; Operation(buf, index);            if (ops.op &#x3D;&#x3D; &quot;add&quot;)            &#123;                while (buf[index] !&#x3D; &#39;,&#39;)                &#123;                    ops.model.push_back(buf[index]);                    index++;                &#125;                index++;                while (buf[index] &#x3D;&#x3D; &#39; &#39;)                &#123;                    index++;                &#125;                ops.id &#x3D; 0; &#x2F;&#x2F; 虚拟机ID                while (buf[index] !&#x3D; &#39;)&#39;)                &#123;                    ops.id &#x3D; ops.id * 10 + buf[index] - &#39;0&#39;;                    index++;                &#125;            &#125;            else            &#123;                ops.id &#x3D; 0;                while (buf[index] !&#x3D; &#39;)&#39;)                &#123;                    ops.id &#x3D; ops.id * 10 + buf[index] - &#39;0&#39;;                    index++;                &#125;            &#125;            cin_buf.push_back(ops);        &#125;    &#125;&#125;int main()&#123;    &#x2F;&#x2F;startTime &#x3D; clock();    int N; &#x2F;&#x2F;服务器种类    cin &gt;&gt; N;    getchar();    for (int i &#x3D; 0; i &lt; N; i++)    &#123;        getline(cin, buf);        InitServer(buf);    &#125;    server server_tem;    int max &#x3D; 10000000, server_tem_index &#x3D; 0;    server_tem_index &#x3D; 11;    sort(model_pair.begin(), model_pair.end(), cmp);    &#x2F;&#x2F;M    int M;    cin &gt;&gt; M;    getchar();    for (int i &#x3D; 0; i &lt; M; i++)    &#123;        getline(cin, buf);        InitVM(buf);    &#125;    &#x2F;&#x2F;T    int T;    cin &gt;&gt; T;    ReadallData(T);    int model_pair_size &#x3D; model_pair.size();    long long cost_min &#x3D; 5000000000;    long long cost &#x3D; 0;    vector&lt;string&gt; res_min;    int i1_index &#x3D; 0;    cost &#x3D; 0;    vector&lt;string&gt; res;    unordered_map&lt;string, int&gt; purchase;    &#x2F;&#x2F;用来存放服务器当天使用种类    vector&lt;pair&lt;string, int&gt;&gt; purchase_num; &#x2F;&#x2F;用来存放服务器当天使用种类个数    for (int k &#x3D; 0; k &lt; T; k++)    &#123;        day++;        vector&lt;string&gt; migrations;        int sum &#x3D; 0;        if (day % 1 &#x3D;&#x3D; 0 &amp;&amp; day !&#x3D; 1)        &#123;            Select(sum, migrations);        &#125;        purchase.clear();        purchase_num.clear();        purchase_num.push_back(make_pair(&quot;000&quot;, 0)); &#x2F;&#x2F;purchase_num第0个是没有用的        vector&lt;string&gt; dis;        int len &#x3D; server_myselfs.size();        vector&lt;vector&lt;int&gt;&gt; ids;        vector&lt;int&gt; dis_id;        vector&lt;operators&gt; day_ops;                for (int j &#x3D; 0; j &lt; cin_size[k]; j++)        &#123;            operators ops &#x3D; cin_buf[i1_index++];            if (ops.op &#x3D;&#x3D; &quot;add&quot;)            &#123;                sum_vm++;                int flag &#x3D; 0;                VM vv &#x3D; VM_map[ops.model];                add_VM add_tt;                add_tt.model &#x3D; ops.model;                add_tt.Is_Double_node &#x3D; 0;                dis_id.push_back(ops.id);                int myself_idnex &#x3D; 0, A_or_B &#x3D; 0;                double myself_max &#x3D; 10000000;                int sizes &#x3D; server_myselfs.size();                for (int i &#x3D; 0; i &lt; sizes; i++)                &#123;                    if (vv.Is_Double_node)                    &#123;                        if (server_myselfs[i].A_cpu &gt;&#x3D; vv.cpu &#x2F; 2 &amp;&amp; server_myselfs[i].A_Memory &gt;&#x3D; vv.Memory &#x2F; 2 &amp;&amp; server_myselfs[i].B_cpu &gt;&#x3D; vv.cpu &#x2F; 2 &amp;&amp; server_myselfs[i].B_Memory &gt;&#x3D; vv.Memory &#x2F; 2)                        &#123;                            if (((server_myselfs[i].A_cpu + server_myselfs[i].B_cpu - vv.cpu) * Q3 + (server_myselfs[i].A_Memory + server_myselfs[i].B_Memory - vv.Memory) * Q4) &lt; myself_max)                            &#123;                                myself_max &#x3D; (server_myselfs[i].A_cpu + server_myselfs[i].B_cpu - vv.cpu) * Q3 + (server_myselfs[i].A_Memory + server_myselfs[i].B_Memory - vv.Memory) * Q4;                                myself_idnex &#x3D; i;                                flag &#x3D; 1;                            &#125;                        &#125;                    &#125;                    else                    &#123;                        int flag1 &#x3D; 0, flag2 &#x3D; 0;                        if (server_myselfs[i].A_cpu &gt;&#x3D; vv.cpu &amp;&amp; server_myselfs[i].A_Memory &gt;&#x3D; vv.Memory)                        &#123;                            if (((server_myselfs[i].A_cpu - vv.cpu) * Q3 + (server_myselfs[i].A_Memory - vv.Memory) * Q4) &lt; myself_max)                            &#123;                                myself_max &#x3D; (server_myselfs[i].A_cpu - vv.cpu) * Q3 + (server_myselfs[i].A_Memory - vv.Memory) * Q4;                                myself_idnex &#x3D; i;                                A_or_B &#x3D; 1;                                flag &#x3D; 1;                                flag1 &#x3D; 1;                            &#125;                        &#125;                        int myself_max_tmp &#x3D; myself_max;                        if (server_myselfs[i].B_cpu &gt;&#x3D; vv.cpu &amp;&amp; server_myselfs[i].B_Memory &gt;&#x3D; vv.Memory)                        &#123;                            if (((server_myselfs[i].B_cpu - vv.cpu) * Q3 + (server_myselfs[i].B_Memory - vv.Memory) * Q4) &lt; myself_max)                            &#123;                                myself_max &#x3D; (server_myselfs[i].B_cpu - vv.cpu) * Q3 + (server_myselfs[i].B_Memory - vv.Memory) * Q4;                                myself_idnex &#x3D; i;                                A_or_B &#x3D; 0;                                flag &#x3D; 1;                                flag2 &#x3D; 1;                            &#125;                        &#125;                        if (myself_max_tmp &lt;&#x3D; myself_max &amp;&amp; flag1 &#x3D;&#x3D; 1 &amp;&amp; flag2 &#x3D;&#x3D; 1)                        &#123;                            myself_max &#x3D; myself_max_tmp;                            A_or_B &#x3D; 1;                            flag &#x3D; 1;                        &#125;                        if (myself_max_tmp &gt;&#x3D; myself_max &amp;&amp; flag1 &#x3D;&#x3D; 1 &amp;&amp; flag2 &#x3D;&#x3D; 1)                        &#123;                            A_or_B &#x3D; 0;                            flag &#x3D; 1;                        &#125;                    &#125;                &#125;                if (flag)                &#123;                    server_myselfs[myself_idnex].VM_ids.push_back(make_pair(ops.id, 1));                    server_myselfs[myself_idnex].flag &#x3D; 1;                    if (vv.Is_Double_node)                    &#123;                        add_tt.Is_Double_node &#x3D; 1;                        server_myselfs[myself_idnex].A_cpu -&#x3D; vv.cpu &#x2F; 2;                        server_myselfs[myself_idnex].A_Memory -&#x3D; vv.Memory &#x2F; 2;                        server_myselfs[myself_idnex].B_cpu -&#x3D; vv.cpu &#x2F; 2;                        server_myselfs[myself_idnex].B_Memory -&#x3D; vv.Memory &#x2F; 2;                        add_tt.server_id &#x3D; myself_idnex;                        if (myself_idnex &gt;&#x3D; len)                        &#123;                            ids[myself_idnex - len].push_back(ops.id);                        &#125;                    &#125;                    else                    &#123;                        if (A_or_B)                        &#123;                            add_tt.A_or_B &#x3D; 1;                            server_myselfs[myself_idnex].A_cpu -&#x3D; vv.cpu;                            server_myselfs[myself_idnex].A_Memory -&#x3D; vv.Memory;                            add_tt.server_id &#x3D; myself_idnex;                            if (myself_idnex &gt;&#x3D; len)                            &#123;                                ids[myself_idnex - len].push_back(ops.id);                            &#125;                        &#125;                        else                        &#123;                            add_tt.A_or_B &#x3D; 0;                            server_myselfs[myself_idnex].B_cpu -&#x3D; vv.cpu;                            server_myselfs[myself_idnex].B_Memory -&#x3D; vv.Memory;                            add_tt.server_id &#x3D; myself_idnex;                            if (myself_idnex &gt;&#x3D; len)                            &#123;                                ids[myself_idnex - len].push_back(ops.id);                            &#125;                        &#125;                    &#125;                &#125;                else                &#123;                    int ii;                                        for (ii &#x3D; 0; ii &lt; model_pair_size; ii++)                    &#123; &#x2F;&#x2F;找到一个最合适服务器的                        if (vv.cpu &lt; model_pair[ii].first.cpu &#x2F; 2 &amp;&amp; vv.Memory &lt; model_pair[ii].first.Memory &#x2F; 2 &amp;&amp; !vv.Is_Double_node)                        &#123;                            break;                        &#125;                        if (vv.cpu &lt; model_pair[ii].first.cpu &amp;&amp; vv.Memory &lt; model_pair[ii].first.Memory &amp;&amp; vv.Is_Double_node)                        &#123;                            break;                        &#125;                    &#125;                                        &#x2F;&#x2F; if((vv.cpu&lt;model_pair[server_tem_index].first.cpu&#x2F;2&amp;&amp;vv.Memory&lt;model_pair[server_tem_index].first.Memory&#x2F;2&amp;&amp;!vv.Is_Double_node)||(vv.cpu&lt;model_pair[server_tem_index].first.cpu&amp;&amp;vv.Memory&lt;model_pair[server_tem_index].first.Memory&amp;&amp;vv.Is_Double_node))&#123;                    &#x2F;&#x2F;     ii &#x3D; server_tem_index;                    &#x2F;&#x2F; &#125;                    &#x2F;&#x2F; else&#123;                    &#x2F;*                    double tmp1 &#x3D; vv.cpu * 1.0 &#x2F; vv.Memory * 1000;                    double maxn &#x3D; 999999;                    for (int ii &#x3D; 0; ii &lt; model_pair_size; ii++)                    &#123; &#x2F;&#x2F;找到一个最合适服务器的                                                double tmp2 &#x3D; model_pair[ii].first.cpu * 1.0 &#x2F; model_pair[ii].first.Memory * 1000;                                                                        if (vv.cpu &lt; model_pair[ii].first.cpu &#x2F; 2 &amp;&amp; vv.Memory &lt; model_pair[ii].first.Memory &#x2F; 2 &amp;&amp; !vv.Is_Double_node)                        &#123;                            if (abs(tmp1 - tmp2) &lt; maxn)                             &#123;                                iii &#x3D; ii;                                maxn &#x3D; abs(tmp1 - tmp2);                            &#125;                        &#125;                        if (vv.cpu &lt; model_pair[ii].first.cpu &amp;&amp; vv.Memory &lt; model_pair[ii].first.Memory &amp;&amp; vv.Is_Double_node)                        &#123;                            if (abs(tmp1 - tmp2) &lt; maxn)                             &#123;                                iii &#x3D; ii;                                maxn &#x3D; abs(tmp1 - tmp2);                            &#125;                        &#125;                    &#125;                    *&#x2F;                    &#x2F;&#x2F; &#125;                    server ss &#x3D; model_pair[ii].first;                    cost +&#x3D; ss.Hardware_cost;                    server_myself tt;                    tt.A_cpu &#x3D; ss.cpu &#x2F; 2;                    tt.A_Memory &#x3D; ss.Memory &#x2F; 2;                    tt.B_cpu &#x3D; ss.cpu &#x2F; 2;                    tt.B_Memory &#x3D; ss.Memory &#x2F; 2;                    tt.model &#x3D; ss.model;                    tt.Energy_cost_day &#x3D; ss.Energy_cost_day;                    tt.flag &#x3D; 1;                    int tt_id &#x3D; 0;                    int tt_index &#x3D; 0;                    if (purchase[ss.model] &#x3D;&#x3D; 0)                    &#123;                        purchase[ss.model] &#x3D; purchase_num.size();                        purchase_num.push_back(make_pair(ss.model, 1));                        tt_id &#x3D; server_myselfs.size();                        tt_index &#x3D; ids.size();                        server_myselfs.push_back(tt);                        ids.push_back(&#123;ops.id&#125;);                    &#125;                    else                    &#123;                        if (purchase[ss.model] &#x3D;&#x3D; (int)purchase_num.size())                        &#123;                            tt_id &#x3D; server_myselfs.size();                            tt_index &#x3D; ids.size();                            server_myselfs.push_back(tt);                            ids.push_back(&#123;ops.id&#125;);                        &#125;                        else                        &#123;                            int sum &#x3D; 0;                            for (int i &#x3D; 1; i &lt;&#x3D; purchase[ss.model]; i++)                            &#123;                                sum +&#x3D; purchase_num[i].second;                            &#125;                            tt_id &#x3D; len + sum;                            tt_index &#x3D; sum;                            server_myselfs.insert(server_myselfs.begin() + tt_id, tt);                            vector&lt;int&gt; temp &#x3D; &#123;ops.id&#125;;                            ids.insert(ids.begin() + sum, temp);                            for (int i &#x3D; sum + 1; i &lt; (int)ids.size(); i++)                            &#123;                                for (int j &#x3D; 0; j &lt; (int)ids[i].size(); j++)                                &#123;                                    if (adds.find(ids[i][j]) !&#x3D; adds.end())                                    &#123;                                        adds[ids[i][j]].server_id++;                                    &#125;                                &#125;                            &#125;                        &#125;                        purchase_num[purchase[ss.model]].second++;                    &#125;                    server_myselfs[tt_id].flag &#x3D; 1;                    server_myselfs[tt_id].VM_ids.push_back(make_pair(ops.id, 1));                    if (vv.Is_Double_node)                    &#123;                        add_tt.Is_Double_node &#x3D; 1;                        if (vv.cpu &#x2F; 2 &lt;&#x3D; server_myselfs[tt_id].A_cpu &amp;&amp; vv.Memory &#x2F; 2 &lt;&#x3D; server_myselfs[tt_id].A_Memory)                        &#123;                            server_myselfs[tt_id].A_cpu -&#x3D; vv.cpu &#x2F; 2;                            server_myselfs[tt_id].A_Memory -&#x3D; vv.Memory &#x2F; 2;                            server_myselfs[tt_id].B_cpu -&#x3D; vv.cpu &#x2F; 2;                            server_myselfs[tt_id].B_Memory -&#x3D; vv.Memory &#x2F; 2;                            add_tt.server_id &#x3D; tt_id;                        &#125;                        else                        &#123;                            cout &lt;&lt; &quot;----------CPU OR Memory 有问题的&quot; &lt;&lt; endl;                            return 0;                        &#125;                    &#125;                    else                    &#123;                        add_tt.A_or_B &#x3D; 1;                        if (vv.cpu &lt;&#x3D; server_myselfs[tt_id].A_cpu &amp;&amp; vv.Memory &lt;&#x3D; server_myselfs[tt_id].A_Memory)                        &#123;                            server_myselfs[tt_id].A_cpu -&#x3D; vv.cpu;                            server_myselfs[tt_id].A_Memory -&#x3D; vv.Memory;                            add_tt.server_id &#x3D; tt_id;                        &#125;                        else                        &#123;                            cout &lt;&lt; &quot;----------CPU OR Memory 有问题的&quot; &lt;&lt; endl;                            return 0;                        &#125;                    &#125;                &#125;                add_tt.index &#x3D; (int)ids_pair.size();                ids_pair.push_back(make_pair(ops.id, 1));                adds[ops.id] &#x3D; add_tt;            &#125;            else            &#123;                &#x2F;&#x2F;回收部分                add_VM del &#x3D; adds[ops.id];                ids_pair[del.index].second &#x3D; 0;                sum_vm--;                if (del.Is_Double_node)                &#123;                    server_myselfs[del.server_id].A_cpu +&#x3D; VM_map[del.model].cpu &#x2F; 2;                    server_myselfs[del.server_id].A_Memory +&#x3D; VM_map[del.model].Memory &#x2F; 2;                    server_myselfs[del.server_id].B_cpu +&#x3D; VM_map[del.model].cpu &#x2F; 2;                    server_myselfs[del.server_id].B_Memory +&#x3D; VM_map[del.model].Memory &#x2F; 2;                &#125;                else                &#123;                    if (del.A_or_B)                    &#123;                        server_myselfs[del.server_id].A_cpu +&#x3D; VM_map[del.model].cpu;                        server_myselfs[del.server_id].A_Memory +&#x3D; VM_map[del.model].Memory;                    &#125;                    else                    &#123;                        server_myselfs[del.server_id].B_cpu +&#x3D; VM_map[del.model].cpu;                        server_myselfs[del.server_id].B_Memory +&#x3D; VM_map[del.model].Memory;                    &#125;                &#125;                for (int i &#x3D; 0; i &lt; (int)server_myselfs[del.server_id].VM_ids.size(); i++)                &#123;                    if (server_myselfs[del.server_id].VM_ids[i].first &#x3D;&#x3D; ops.id)                    &#123;                        server_myselfs[del.server_id].VM_ids[i].second &#x3D; 0;                    &#125;                &#125;                if ((server_myselfs[del.server_id].A_cpu + server_myselfs[del.server_id].B_cpu) &#x3D;&#x3D; server_map[server_myselfs[del.server_id].model].cpu)                &#123;                    if ((server_myselfs[del.server_id].A_Memory + server_myselfs[del.server_id].B_Memory) &#x3D;&#x3D; server_map[server_myselfs[del.server_id].model].Memory)                    &#123;                        server_myselfs[del.server_id].flag &#x3D; 0;                    &#125;                &#125;                &#x2F;&#x2F;adds.erase(id);            &#125;        &#125;        &#x2F;*        for (int i &#x3D; 0; i &lt; (int)server_myselfs.size(); i++)        &#123;            if (server_myselfs[i].flag)            &#123;                cost +&#x3D; server_myselfs[i].Energy_cost_day;            &#125;        &#125;        *&#x2F;        for (int i &#x3D; 0; i &lt; (int)dis_id.size(); i++)        &#123;            if (adds[dis_id[i]].Is_Double_node)            &#123;                dis.push_back(&quot;(&quot; + to_string(adds[dis_id[i]].server_id) + &quot;)&quot;);            &#125;            else            &#123;                if (adds[dis_id[i]].A_or_B)                &#123;                    dis.push_back(&quot;(&quot; + to_string(adds[dis_id[i]].server_id) + &quot;,&quot; + &quot;A)&quot;);                &#125;                else                &#123;                    dis.push_back(&quot;(&quot; + to_string(adds[dis_id[i]].server_id) + &quot;,&quot; + &quot;B)&quot;);                &#125;            &#125;        &#125;        res.push_back(&quot;(purchase,&quot; + to_string(purchase_num.size() - 1) + &quot;)&quot;);        for (int i &#x3D; 1; i &lt; (int)purchase_num.size(); i++)        &#123;            res.push_back(&quot;(&quot; + purchase_num[i].first + &quot;,&quot; + to_string(purchase_num[i].second) + &quot;)&quot;);        &#125;        res.push_back(&quot;(migration,&quot; + to_string(migrations.size()) + &quot;)&quot;); &#x2F;&#x2F;一个简单的调度算法的        for (int i &#x3D; 0; i &lt; (int)migrations.size(); i++)        &#123;            res.push_back(migrations[i]);        &#125;        for (int i &#x3D; 0; i &lt; (int)dis.size(); i++)        &#123;            res.push_back(dis[i]);        &#125;    &#125;    for (int i &#x3D; 0; i &lt; (int)res.size(); i++)    &#123;        cout &lt;&lt; res[i] &lt;&lt; endl;        &#x2F;&#x2F;outfile &lt;&lt; res[i] &lt;&lt; endl;    &#125;    &#x2F;&#x2F; cout &lt;&lt; cost_min &lt;&lt; endl;    &#x2F;&#x2F;cout &lt;&lt; cost &lt;&lt; endl;    &#x2F;&#x2F;endTime &#x3D; clock();    &#x2F;&#x2F; cout &lt;&lt; &quot;The run time is:&quot; &lt;&lt;(double)(endTime - startTime) &#x2F; CLOCKS_PER_SEC &lt;&lt; &quot;s&quot; &lt;&lt; endl;    return 0;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>    复赛的题目应该是在初赛上加一些约束或者其他的数据，这里面的优化思路还是有一定的指引性的，仅供参考。如果在我的博客中有什么写的不妥的，或者错误的地方，欢迎大家留言批评指正。<br><strong>凡不能摧毁我者，必将使我更强大！！！</strong></p>]]></content>
      
      
      <categories>
          
          <category> 比赛总结 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 比赛算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>进程管理（二）</title>
      <link href="/2021/03/27/jin-cheng-guan-li-er/"/>
      <url>/2021/03/27/jin-cheng-guan-li-er/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h1>处理机调度</h1><h2 id="调度概念"><a class="header-anchor" href="#调度概念">¶</a>调度概念</h2><h3 id="调度的基本概念"><a class="header-anchor" href="#调度的基本概念">¶</a>调度的基本概念</h3><p>  因为在我们平时使用的多道程序系统中，进程的数量会多于处理机的个数，因此会经常出现进程争用处理机的情况。而处理机的调度就是<strong>对处理机进行分配，从就绪队列中按照一定的算法（公平、高效）选择一个进程并将处理机分配给它允许，以实现进程并发地执行。</strong></p><h3 id="调度层次"><a class="header-anchor" href="#调度层次">¶</a>调度层次</h3><p>  一个作业从提交开始直到完成，往往要经历以下的三级调度：<br>  （1）<strong>作业调度</strong>：主要任务是按一定的原则，从外存上处于后备状态的作业中挑选一个（或多个）作业，给它（们）分配内存、输入/输出设备等必要的资源，并建立相应的进程，以使它（们）获得竞争处理机的权利。<strong>多道批处理系统中大多数配有作业调度，而其他系统中通常不需要配置作业调度，并且作业调度的执行频率较低，通常为几分钟一次</strong>。<br>  （2）<strong>中级调度</strong>：为了提高内存利用率和系统吞吐量，应将那些暂时不能运行的进程调至外存等待，把此时的进程状态称为<strong>挂起态</strong>。当这些进程已经具备运行条件时且内存有空闲时，就由中级调度来决定把外存上的那些已经具备运行条件的就绪进程重新调入内存，并且修改其状态为就绪态，挂在就绪队列上等待。<br>  （3）<strong>进程调度</strong>：也被称为低级调度，按照某种方法和策略从就绪队列中选取一个进程，将处理机分配给它。<strong>进程调度是操作系统中最基本的一种调度</strong>，在一般的操作系统中都必须配置进程调度。<br><img src="/2021/03/27/jin-cheng-guan-li-er/1.png" alt="处理机的三级调度"><br>  因为在这里涉及到了作业和进程的调度，所以我在这里按照我的理解以及我参考其他的人博客中的理解来进行一定的解释（有误请大家私聊告诉我）。<br>  <strong>作业与进程的区别</strong>：一个进程是一个程序对某个数据集的执行过程，是资源的基本单位。作业是用户需要计算机完成的某项任务，是要求计算机所做工作的集合。一个作业的完成要经过作业提交、作业收容、作业执行和作业完成4个阶段。而进程是对已提交完毕的程序所执行过程的描述，是资源分配的基本单位。主要区别如下：<br>  （1）作业是用户向计算机提交任务的任务实体，在用户向计算机提交作业后，系统将它放入外存中的作业等待队列中等待执行。而进程则是完成用户任务的执行实体，是向系统申请分配资源的基本单位，任一进程，只要它被创建，总有相应的部分存在于内存中。<br>  （2）一个作业可由多个进程组成，且必须至少由一个进程组成，反过来不成立。<br>  （3）<strong>作业的概念主要用在批处理系统中</strong>，像分时系统中就没有作业的概念，而<strong>进程的概念则用在几乎所有的多道程序系统中</strong>。</p><h3 id="三级调度的联系"><a class="header-anchor" href="#三级调度的联系">¶</a>三级调度的联系</h3><p>  （1）作业调度为进程活动做准备，进程调度使进程正常活动起来，中级调度将暂时不能运行的进程挂起，中级调度处于作业调度和进程调度之间。<br>  （2）作业调度次数较少，中级调度次数略多，进程调度频率最高。<br>  （3）进程调度是最基本的，不可或缺的。</p><h2 id="调度的时机、切换与过程"><a class="header-anchor" href="#调度的时机、切换与过程">¶</a>调度的时机、切换与过程</h2><p>  进程调度和切换程序是操作系统内核程序。一般情况下，在请求调度的事件发生后，才可能运行进程调度程序，调度了新的就绪进程后，才会进行进程间的切换。但是在实际中可能会发送以下一些情况导致不能进行进程的调度与切换。<br>  （1）<strong>在处理中断的过程中</strong><br>  （2）<strong>进程在操作系统内核程序临界区中</strong>：进入临界区后，需要独占式的访问共享数据，理论上要加锁来防止其他程序进入，在解锁前不应切换到其他进程，以加快共享数据的释放。<br>  （3）<strong>其他需要完全屏蔽中断的原子操作过程中</strong>：如加锁、解锁、中断现场保护，即使中断也要进行相应的屏蔽。<br>  进行进程调度切换的情况如下:<br>  （1）发送引起调度条件且当前进程无法继续马上运行下去时，进行调度切换（非剥夺式调度）<br>  （2）中断处理结束或自陷处理结束后，返回被中断进程的用户态程序执行现场前，若置上请求调度标志，马上进行进程调度与切换（剥夺式调度）</p><h2 id="进程调度方式"><a class="header-anchor" href="#进程调度方式">¶</a>进程调度方式</h2><p>  （1）非剥夺调度方式：当一个进程在处理机上执行时，即使有更重要的进程进入就绪队列，处理机上的进程依然可以执行直到变为阻塞状态后，让出处理机。优点是<strong>实现简单，系统开销小适用于大多数的批处理系统，但是不适用于分时系统和大多数的实时系统</strong>。<br>  （2）剥夺调度方式：当一个在处理机上运行的进程在遇到另一个更为重要的进程进入就绪队列后，立即暂停，将处理机让出给更为重要的进程。<strong>对提高系统吞吐率和相应效率都有明显的好处</strong>。</p><h2 id="调度的基本准则"><a class="header-anchor" href="#调度的基本准则">¶</a>调度的基本准则</h2><p>  （1）<strong>CPU利用率</strong><br>  （2）<strong>系统吞吐量</strong>：表示单位时间内CPU完成作业的数量。<br>  （3）<strong>周转时间</strong>：从作业提交到作业完成所经历的时间，是作业等待、在就绪队列中排队、在处理机上运行以及进行输入/输出操作所花费时间的总和。<br>  作业的周转时间可用公式表示如下：<br>$$周转时间 = 作业完成时间 - 作业提交时间$$<br>  平均周转时间是指多个作业周转时间的平均值：<br>$$平均周转时间 = (作业1的周转时间 + … + 作业 n 的周转时间) / n$$<br>  带权周转时间是指作业周转时间与作业实际运行时间的比值。<br>  平均带权周转时间是指多个作业带权周转时间的平均值：<br>$$平均带权周转时间 = (作业1的带权周转时间 + … + 作业 n 的带权周转时间) / n$$<br>  （4）<strong>等待时间</strong>：指进程处于等处理机状态时间之和。因为处理机调度算法在实际上并不影响作业执行或输入/输出操作，只影响作业在就绪队列中等待所花的时间，所以衡量一个调度算法优劣常常只需简单地考察等待时间。<br>  （5）<strong>响应时间</strong>：是指从用户提交请求到系统首次产生响应所用的时间。</p><h2 id="典型的调度算法"><a class="header-anchor" href="#典型的调度算法">¶</a>典型的调度算法</h2><h3 id="先来先服务（FCFS）调度算法"><a class="header-anchor" href="#先来先服务（FCFS）调度算法">¶</a>先来先服务（FCFS）调度算法</h3><p>  在作业调度中，每次从后备队列中选择最先进入该队列的一个或几个作业，将它们调入内存，分配必要的资源，创建进程并放入就绪队列。进程调度类似。算法实例如下图所示：<br><img src="/2021/03/27/jin-cheng-guan-li-er/2.png" alt="FCFS调度算法性能"><br>  从上图中可以看出，该算法的特点是：算法简单，但是效率低；对长作业比较有利，对短作业不利；有利于CPU繁忙型作业，而不利于I/O繁忙型作业。</p><h3 id="短作业优先（SJF）调度算法"><a class="header-anchor" href="#短作业优先（SJF）调度算法">¶</a>短作业优先（SJF）调度算法</h3><p>  从队列中选出一个估计运行时间最短的作业优先调度，既可用于作业调度，也可用于进程调度。算法实例如下图所示：<br><img src="/2021/03/27/jin-cheng-guan-li-er/3.png" alt="SJF调度算法性能"><br>  SJF调度算法也存在不容忽视的缺点<br>  （1）<strong>对长作业不利</strong>。严重的是，若一长作业（进程）进入系统的后备队列（就绪队列），由于调度程序总是优先调度那些（即使是后进来的）短作业（进程），将导致长作业（进程）长期不被调度——饥饿<br>  （2）<strong>完全未考虑作业（进程）的紧迫程度，因而不能保证紧迫性作业（进程）会被及时处理</strong>。<br>  （3）由于作业（进程）的长短只是根据用户所提供的估计执行时间而定的，而用户又可能会有意或无意地缩短其作业的估计运行时间，致使该算法不一定能真正做到短作业优先调度。<br>  <strong>SJF调度算法的平均等待时间、平均周转时间最少</strong>。</p><h3 id="优先级调度算法"><a class="header-anchor" href="#优先级调度算法">¶</a>优先级调度算法</h3><p>  既可用于作业调度，又可用于进程调度。每次从后备作业队列中选择优先级最高的一个或几个作业（进程）进行执行。根据新的更高优先级进程能否抢占正在执行的进程，将调度算法分为两种（1）<strong>非剥夺式优先级调度算法</strong>、（2）<strong>剥夺式优先级调度算法</strong>。（在本文上面以及有讲解，此处省略。）<br>  根据进程创建后其优先级是否可以改变，可以将进程优先级分为以下两种。<br>  （1）<strong>静态优先权</strong>：静态优先权在创建进程时确定，且在进程的整个运行期间保持不变。确定进程优先权的依据有进程类型、进程对资源的需求、用户要求。<br>  （2）<strong>动态优先权</strong>：在创建进程时赋予的优先权是随进程的推进或随其等待时间的增加而改变，以获得更好的调度性能。<br>  在一般情况下，进程优先级的设置可以参照以下的原则：<br>  （1）<strong>系统进程 &gt; 用户进程</strong><br>  （2）交互型进程 &gt; 非交互型进程<br>  （3）I/O型进程 &gt; 计算型进程，因为I/O设备的处理速度要比CPU慢很多，所以需要让I/O设备尽早开始工作。</p><h3 id="高响应比优先调度算法"><a class="header-anchor" href="#高响应比优先调度算法">¶</a>高响应比优先调度算法</h3><p>  主要用于作业调度，既考虑作业估计的运行时间也考虑作业的等待时间，综合了先来先服务和最短作业优先两种算法的特点。该算法中的响应比是指作业等待时间与运行比值，响应比公式定义如下：<br>$$响应比R_{p} =（等待时间+要求服务时间）/ 要求服务时间$$<br>  优点有：（1）等待时间相同的作业，则要求服务的时间愈短，其优先权愈高，——对短作业有利；（2）要求服务的时间相同的作业，则等待时间愈长，其优先权愈高，——是先来先服务；（3）对于长作业，优先权随等待时间的增加而提高，其等待时间足够长时，其优先权便可升到很高， 从而也可获得处理机——对长作业有利。<br>  缺点：要进行响应比计算，增加了系统开销。</p><h3 id="时间片轮转调度算法"><a class="header-anchor" href="#时间片轮转调度算法">¶</a>时间片轮转调度算法</h3><p>  系统将所有的就绪进程按先来先服务的原则排成一个队列，每次调度时，把CPU分配给队首进程，并令其执行一个时间片；当执行的时间片用完时，由一个计时器发出时钟中断请求，调度程序便停止该进程的执行，并将其放就绪队列尾；然后，再把处理机分配给就绪队列中新的队首。<br>  <strong>时间片大小的选取很重要</strong>，若时间片足够大，以至于所有进程都能在一个时间片内执行完毕，则算法退化为先来先服务调度算法。若时间片很小，则处理机将在进程间过于频繁地切换，使得处理机的开销增大，而真正用于进程处理的时间很少。<br>  时间片的长短通常由：系统的响应时间、就绪队列中的进程数目和系统的处理能力来决定。</p><h3 id="多级反馈队列调度算法"><a class="header-anchor" href="#多级反馈队列调度算法">¶</a>多级反馈队列调度算法</h3><p>  是时间片轮转调度算法和优先级调度算法的综合。其具体思路如下：<br>  （1）先将它放入第一个队列的末尾，按FCFS原则排队等待调度。<br>  （2）如果时间片内完成，便可准备撤离系统。<br>  （3）如果时间片内未能完成，调度程序便将该进程转入第二队列的末尾等待再次被调度执行。<br>  （4）当第一队列中的进程都执行完，系统再按FCFS原则调度第二队列。在第二队列的稍放长些的时间片内仍未完成，再依次将它放入第三队列。<br>  （5）依次降到第n队列后，在第n队列中便采取按时间片轮转的方式运行。<br><img src="/2021/03/27/jin-cheng-guan-li-er/4.png" alt="多级反馈队列调度算法"><br>  需要注意的是以下几个方面：<br>  （1）设置多个就绪队列，各队列有不同的优先级,优先级从第一个队列依次降低。<br>  （2）赋予各队列进程执行时间片大小不同, 优先权越高，时间片越短。<br>  （3）仅当优先权高的队列（如第一队列）空闲时，调度程序才调度第二队列中的进程运行。<br>  （4）高优先级抢占时，被抢占的进程放回原就绪队列末尾。<br>  优点：（1）终端型作业用户：短作业优先。（2）短批处理作业用户：周转时间较短。（3）长批处理作业用户：经过前面几个队列得到部分执行，不会长期得不到处理。</p>]]></content>
      
      
      <categories>
          
          <category> 课本学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 操作系统 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>进程管理（一）</title>
      <link href="/2021/03/23/jin-cheng-guan-li-yi/"/>
      <url>/2021/03/23/jin-cheng-guan-li-yi/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h1>进程与线程</h1><h2 id="进程的概念与特征"><a class="header-anchor" href="#进程的概念与特征">¶</a>进程的概念与特征</h2><h3 id="进程的概念"><a class="header-anchor" href="#进程的概念">¶</a>进程的概念</h3><p>  <strong>进程概念</strong>：<strong>进程是进程实体的运行过程</strong>，<strong>是系统进行资源分配和调度的一个独立单位</strong>。<br>  在上述定义中，我们应该如何去理解进程实体？在最初的单道程序环境下，当我们的程序被载入到内存之后，它会被划分为程序段和数据段。如下图所示：<br><img src="/2021/03/23/jin-cheng-guan-li-yi/1.png" alt="程序在内存存储图"><br>  但是在早期，由于计算机内存只支持一道应用程序，所以我们把该进程的程序段和数据段放在固定的位置。但是随之计算机的发展，计算机可以支持多道程序并发运行，操作系统为了去记录这些进程的程序段和数据段的位置，构建了一个叫<strong>程序控制块</strong>（<strong>PBC</strong>）的数据结构来存放这些信息。如下图所示：<br><img src="/2021/03/23/jin-cheng-guan-li-yi/2.png" alt="PBC存储信息"><br>  至此，可以使参与并发执行的程序（包括数据）能够独立地运行，因此<strong>进程实体</strong>=<strong>PCB</strong>+<strong>程序段</strong>+<strong>数据段</strong>。（进程实体也可以称为进程映像）所以<strong>进程创建的实质就是创建进程映像中的PCB</strong>，反之如此。在这里可以看出<strong>PCB是进程存在的唯一标志</strong>。在一般情况下，我们把进程实体就称为进程，但是严格来说，<strong>进程实体和进程并不一样，进程实体是静态的，而进程是动态的</strong>。<br>  同时在进程的定义中，要准确的理解其系统资源，在定义中的系统资源实际上是指处理机、存储器和其他设备服务于某个进程的“时间”。因为进程在多道程序中并发执行时，通常由处理机的时间片为其分配运行时间，这也决定了进程一定是一个动态的概念。</p><h3 id="进程的特征"><a class="header-anchor" href="#进程的特征">¶</a>进程的特征</h3><p>  因为在多道程序环境下，允许多个程序并发执行，在这个过程中，这些程序就失去了封闭性，因此引出进程，所以进程的基本特征也是对进程管理提出的基本要求。<br>（1）<strong>动态性</strong>：进程是程序的一次执行，从创建到活动、暂停、终止等过程，具有自己的生命周期，是动态地产生、变化和消亡的。该特性是进程最基本的特性。<br>（2）<strong>并发性</strong>：顾名思义，多个进程实体可以同时存在于内存中，进程能够在同一时间段内同时运行，该特性是操作系统的重要特征，使程序与其他进程的程序并发执行，提高了资源利用率。<br>（3）<strong>独立性</strong>：指进程实体是一个能独立运行、独立获得资源和独立接受调度的基本单位。<strong>注意</strong>：必须是进程实体，也就是说，必须要创建PCB！！！。<br>（4）<strong>异步性</strong>：由于进程的相互制约，使得进程具有执行的间断性，即进程按各自独立的、不可预知的速度向前推进，比如，当正在执行的进程提出某种资源请求时，如打印请求，而此时打印机正在为其他某进程打印，由于打印机属于临界资源，因此正在执行的进程必须等待，且放弃处理机，直到打印机空闲，并再次把处理机分配给该进程时，该进程方能继续执行。可见，由于资源等因素的限制，进程的执行通常都不是“一气呵成”，而是以“停停走走”的方式运行。异步性就是描述进程这种以不可预知的速度走走停停、何时开始何时暂停何时结束不可预知的性质。异步性会导致执行结果的不可再现性，为此操作系统中必须配置响应的进程同步机制。<br>（5）<strong>结构性</strong>：每个进程都配置一个PCB对其进行描述，其结构为<strong>PCB</strong>+<strong>程序段</strong>+<strong>数据段</strong>。</p><h2 id="进程的状态与转换"><a class="header-anchor" href="#进程的状态与转换">¶</a>进程的状态与转换</h2><p>  进程在其生命周期内，由于进程之间的相互制约以及系统的运行环境的变化，导致其状态也在不断地发生变化。在一般情况下存在以下五种状态：<br>  （1）<strong>运行态</strong>：进程在处理及上运行，并且在单机环境下，每个时刻最多只有一个进程在运行。<br>  （2）<strong>就绪态</strong>：进程获得了除处理机外的一切所需资源，一旦得到处理机，便可立即运行。系统中处于就绪状态的进程可能有多个，通常将它们排成一个队列，称为就绪队列。<br>  （3）<strong>阻塞态</strong>：又称<strong>等待态</strong>，进程正在等待某一事件而暂停运行，如等待某资源为可用（<strong>不包括处理机，这点与就绪态有明显的差别</strong>）或等待输入/输出完成，即使处理机空闲，该程序也不能运行。<br>  （4）<strong>创建态</strong>：进程正在被创建，尚未转到就绪态。创建进程通常需要多个步骤：首先申请一个空白的PCB，并向PCB中填写一些控制和管理进程的信息；然后由系统为该该进程分配运行时所必须的资源；最后把该进程转入就绪态。<br>  （5）<strong>结束态</strong>：进程正从系统中消失，可能是进程正常结束或其他原因中断推出运行。进程需要结束运行时，系统首先必须将该进程置为结束态，然后进一步处理资源释放和回收等工作。<br>  <strong>注意</strong>，就绪态和等待态是不同的，因为就绪态是指进程仅缺少处理机，只要获得处理机资源就立即运行；而等待态是指进程需要其他资源（除了处理机）或等待某一事件。五种进程状态的转换情况如下图所示：<br><img src="/2021/03/23/jin-cheng-guan-li-yi/3.png" alt="5种进程状态的转换"><br>  （1）<strong>就绪态</strong>$\rightarrow$<strong>运行态</strong>：处于就绪态的进程被调度后，获得处理机资源（分派处理机时间片），于是进程由就绪态转换为运行态。<br>  （2）<strong>运行态</strong>$\rightarrow$<strong>就绪态</strong>：当处于运行态的进程在时间片用完之后，会让出处理机，转化为就绪态，或者在可剥夺的操作系统种，当有更高级的进程就绪时，调度程序将正在执行的进程转化为就绪态，让更高级的进程执行。<br>  （2）<strong>运行态</strong>$\rightarrow$<strong>阻塞态</strong>：进程请求某一资源的使用和分配或等待某一事件的发生（例如I/O操作的完成）时，就会从运行态转换为阻塞态。<strong>进程以系统调用的形式请求操作系统提供服务，这是一种特殊的、由运行用户态程序调用操作系统内核过程的形式</strong>。<br>  需要注意的是，一个进程从运行态变成阻塞态是主动行为，而从阻塞态变成就绪态是被动的行为，需要其他相关进程协助。</p><h2 id="进程控制"><a class="header-anchor" href="#进程控制">¶</a>进程控制</h2><p>  进程控制主要是对系统中的所有进程实施有效的管理，其具有创建新进程、撤销已有进程、实现进程状态转换等功能。通常，我们把进程控制用的程序段称为<strong>原语</strong>。它是一个不可分割的基本单位。</p><h3 id="进程的创建"><a class="header-anchor" href="#进程的创建">¶</a>进程的创建</h3><p>  一个进程可以创建另一个进程，创建者为父进程，被创建的进程称为子进程。子进程可以继承父进程所拥有的资源。并且在其被撤销时，要归还所有从父进程那里得到的资源。当父进程被撤销时，必须撤销其所有的子进程。创建进程的过程如下：<br>  （1）为新进程分配一个唯一的标识号，并且申请一个空白的PCB，<strong>若PCB申请失败，则创建失败</strong>，这与在上文中（进程的概念）我解释的进程创建的实质相对应。<br>  （2）为进程分配资源，例如为程序和数据以及用户栈分配必要的内存空间。当资源不足时，进程会进入阻塞态。<br>  （3）初始化PCB，设置进程优先级等。<br>  （4）当进程就绪队列可以接纳新进程后，就将新进程插入队列中，等待被调度运行。</p><h3 id="进程的终止"><a class="header-anchor" href="#进程的终止">¶</a>进程的终止</h3><p>  在进程被执行的过程中，通常会因为一些事件而引起进程终止。主要由以下的几种情况：<br>  （1）正常结束。<br>  （2）异常结束，如存储区越界，非法指令、I/O故障等使程序无法继续运行。<br>  （3）外界干预：程应外界的请求而终止运行，例如：操作员和操作系统的干预，父进程请求和父进程终止。<br>  操作系统终止进程的过程如下：<br>  （1）根据被终止进程的标识符，检索PCB，从中读出该进程的状态。<br>  （2）若被终止进程处于执行状态，立即终止该进程的执行，将处理机资源分配给其他进程。<br>  （3）若该进程还有子孙进程，则应将其所有的子孙进程终止。<br>  （4）将该进程所有的资源全部归还与父进程或者操作系统。<br>  （5）将PCB从所在队列（链表中）删除。</p><h3 id="进程的阻塞和唤醒"><a class="header-anchor" href="#进程的阻塞和唤醒">¶</a>进程的阻塞和唤醒</h3><p>  正在执行的进程由于期待的某些事件未发生，由系统自动执行阻塞原语（Block，也就是执行阻塞程序），使自己由运行态变为阻塞态。对此可以看出，<strong>进程的阻塞是进程自身的一种主动行为，同时，也只有处于阻塞态的进程（获得了CPU）才可能将其转为阻塞态</strong>。阻塞原语的执行过程如下：<br>  （1）找到将要被阻塞进程的标识号对应的PCB。<br>  （2）若该进程为运行态，则保护其现场，将其状态转为阻塞态，停止运行。<br>  （3）把该PCB插入相应时间的等待队列，将处理机资源调度给其他就绪进程。<br>  当阻塞进程所期待的事件出现时，由有关进程调用唤醒原语，将该进程唤醒。唤醒原语（Wakeup）的执行过程如下：<br>  （1）把该事件的等待队列种找到相应进程的PCB。<br>  （2）将其从等待队列中移除，并置其状态为就绪态。<br>  （3）把该PCB插入就绪队列，等待调度程序调度。<br>  需要注意的是，这一对原语作用恰好相反，必须成对使用，<strong>其中阻塞原语是被阻塞进程自我调用实现的，而唤醒原语则是有一个被唤醒进程合作或被其他相关进程调用实现的。</strong></p><h3 id="进程切换"><a class="header-anchor" href="#进程切换">¶</a>进程切换</h3><p>  通常情况下，进程的创建、撤销等操作都是利用系统调用进入内核，在内核中由相应处理程序完成的。而进程的切换是指处理机从一个进程的运行转到另一个进程上的运行，同样需要内核的支持，所以<strong>任何进程都是在操作系统内核的支持下运行的</strong>，其切换过程如下：<br>  （1）保存处理机上下文，包括程序计数器和其他寄存器。<br>  （2）更新PCB信息<br>  （3）把进程的PCB移入到相应的队列，如就绪、在某事件阻塞等队列。<br>  （4）选择另一个进程执行，并更新其PCB。<br>  （5）更新内存管理的数据结构。<br>  （6）恢复处理机上下文。<br>  需要注意一点的是，<strong>调度和切换是两种不同的情况，调度是指决定资源分配给哪个进程的行为，是一种决策行为，而切换则是指实际分配的行为，是执行行为。</strong></p><h2 id="进程的组织"><a class="header-anchor" href="#进程的组织">¶</a>进程的组织</h2><p>  **进程是一个独立的运行单位，也是操作系统进程资源分配和调度的基本单位。**由PCB + 数据段 + 程序段组成。其中最为核心的部分是PCB。</p><h3 id="进程控制块（PCB）"><a class="header-anchor" href="#进程控制块（PCB）">¶</a>进程控制块（PCB）</h3><p>  操作系统通过PCB表来管理和控制进程。在进程创建时，操作系统为进程新建一个PCB，该结构之后可以存储在内存中，并且在任意时刻都可以存取，在进程的执行过程中，系统可以通过PCB来了解进程的先行状态信息，以便对其进行管理和控制，在进程结束时，系统收回其PCB，该进程也随之消亡。并且在PCB中还保存了进程状态及优先级，处理机状态信息，数据和程序的内存初始地址。发生断点的处理机环境。<br>  为了方便进程的调度和管理，需要将各进程的PCB用适当的方法组织起来，目前常用的方法有链式法和索引法。链式方式就是将同一状态的PCB链接成队列的。索引方式是将同一状态的进程组织在一个索引表中，索引表的表项指向相应的PCB。</p><h3 id="程序段"><a class="header-anchor" href="#程序段">¶</a>程序段</h3><p>  程序段就是能被进程调度程序调度到CPU执行的程序代码段。<strong>程序可被多个进程共享，即多个进程可有运行同一个程序</strong>。</p><h3 id="数据段"><a class="header-anchor" href="#数据段">¶</a>数据段</h3><p>  一个进程的数据段，可以是进程对应的程序加工处理的原始数据，也可以是程序执行时产生的中间或最终结果。</p><h2 id="进程的通信"><a class="header-anchor" href="#进程的通信">¶</a>进程的通信</h2><p>  进程通信是指进程之间的信息交换。其中PV操作属于低级通信方式，而高级通信是指以较高的效率传输大量数据的通信方式。高级通信主要有以下三种方式：</p><h3 id="共享存储"><a class="header-anchor" href="#共享存储">¶</a>共享存储</h3><p>  在通信的进程之间存在一块可直接访问的共享空间，通过对这片共享空间进行读/写操作，实现进程之间的信息交换。操作系统只负责为通信进程提供可共享使用的存储空间和同步互斥工具（P/V原语），而其中的数据交换则由用户自己安排读/写指令完成。共享存储又分为两种：（1）低级方式的基于数据结构的共享。（2）高级方式的基于存储区的共享。<br>  <strong>用户进程空间一般都是独立的，进程运行期间一般不能访问其他进程空间，必须通过特殊的系统调用才能实现。进程内的线程是自然共享进程空间的</strong>。<br><img src="/2021/03/23/jin-cheng-guan-li-yi/4.png" alt="共享存储"></p><h3 id="消息传递"><a class="header-anchor" href="#消息传递">¶</a>消息传递</h3><p>  在消息传递系统中，进程间的数据交换是以格式化的消息为单位的。<strong>如果通信的进程之间不存在可直接访问的共享空间，则必须利用操作系统提供的消息传递方法实现进程通信</strong>。操作系统为进程提供了发送消息和接收消息两个原语来进行数据交换。消息传递的通信方式分为两种。<br>  （1）直接通信方式：发送进程把消息发送给接收进程，把消息挂在接收进程的消息缓冲队列上，接收进程从消息缓冲队列中得到消息。该过程可以看作两个人之间写信进行交流的过程。<br><img src="/2021/03/23/jin-cheng-guan-li-yi/5.png" alt="直接通信方式"><br>  （2）间接通信方式：即发送在某个实体，接收进程从该实体中获取消息。该实体类似于一个邮差的功能。</p><h3 id="管道通信"><a class="header-anchor" href="#管道通信">¶</a>管道通信</h3><p>  管道通信时消息传递的一种特殊方式。这里的“管道”是指用于连接一个读进程和一个写进程，以实现它们之间的通信的一个共享文件。其中向管道输入和接收数据都是由进程实现的。为了协调读写进程的通信，管道机制必须提供<strong>互斥</strong>、<strong>同步和确定对方的存在</strong>的三方面能力。<br>  <strong>从管道读数据是一次性操作，数据一旦被读取，它就从管道中被抛弃，释放空间以便写入更多的数据。并且管道在某一时刻只能单向传输，要实现父子进程双方互动通信，需要定义两个管道</strong>。</p><h2 id="线程的概念和多线程模型"><a class="header-anchor" href="#线程的概念和多线程模型">¶</a>线程的概念和多线程模型</h2><h3 id="线程的基本概念"><a class="header-anchor" href="#线程的基本概念">¶</a>线程的基本概念</h3><p>  <strong>引入进程的目的是更好地使多道程序并发执行，提高资源利用率和系统吞吐量；引入线程的目的则是减小程序在并发执行时所付出的时空开销，提高操作系统的并发性能。</strong><br>  <strong>线程是一个基本的CPU执行单元，也是程序执行流的最小单元。线程是进程中的一个实体，是被系统独立调度和分配的基本单位（与进程要进行区分，进程是系统进行资源分配和调度的一个独立单位）</strong><br>  <strong>线程自己不拥有系统资源，只拥有一点在运行中必不可少的资源。但是它可与同属一个进程的其他线程共享进程所拥有的全部资源。</strong><br>  <strong>一个线程可以创建和撤销另一个线程，同一个进程中的多个线程之间可以并发执行。</strong><br>  由于线程之间的相互制约，致使线程在运行中呈现出间断性。并且线程也有就绪、阻塞和运行三种状态。</p><h3 id="进程与线程的比较（理解重点）"><a class="header-anchor" href="#进程与线程的比较（理解重点）">¶</a>进程与线程的比较（理解重点）</h3><p>  （1）调度：<strong>线程是独立调度的基本单位，进程是拥有资源的基本单位</strong>，在同一进程中，线程的切换不会引起进程的切换，。在不同进程中进行线程切换，会引起进程切换。<br>  （2）拥有资源：进程是拥有资源的基本单位，而线程不拥有系统资源。但线程可以访问其隶属进程的系统资源。<br>  （3）并发性：不仅进程之间可以并发执行，而且多个线程之间也可以并发执行，从而使操作系统具有更好的并发性，提高系统的吞吐量。<br>  （4）系统开销：由于创建或撤销进程时，系统都要为之分配或回收资源，因此操作系统所付出的开销远大于创建或撤销线程时的开销。因为同一进程内的多个线程共享进程的地址空间，所以这些线程之间的通信非常容易实现，甚至无需操作系统的干预。<br>  （5）地址空间和其他资源：进程的地址空间之间相互独立，同一进程的各线程间共享进程的资源，某进程内的线程对于其他进程不可见。<br>  （6）通信方面：进程间通信需要进程同步和互斥手段的辅助，以保证数据的一致性，而线程间可以直接读/写进程数据段（如全局变量）来进行通信。</p><h3 id="线程的属性"><a class="header-anchor" href="#线程的属性">¶</a>线程的属性</h3><p>  在多线程操作系统把线程作为独立运行（或调度）的基本单位之后，进程就已经不再是一个基本的可执行实体。因此所谓进程处于“执行状态”，实际上是指该进程中的某线程正在执行。线程的主要属性如下：<br>  （1）线程不拥有系统资源，但每个线程有一个唯一的标识符和一个线程控制块，其记录了线程执行的寄存器和栈等现场状态。<br>  （2）不同的线程可以执行相同的程序。<br>  （3）同一个进程中的各个线程共享该进程所拥有的资源。<br>  （4）线程是处理机的地理调度单位，多个线程是可以并发执行的。<br>  （5）一个线程被创建后，便开始了它的生命周期，直至终止。</p><h3 id="线程的实现方式"><a class="header-anchor" href="#线程的实现方式">¶</a>线程的实现方式</h3><p>  分为用户级线程和内核级线程。</p><h3 id="多线程模型"><a class="header-anchor" href="#多线程模型">¶</a>多线程模型</h3><p>  有些系统支持用户线程和内核线程，由此产生了不同的多线程模型，即实现用户级线程和内核级线程的连接方式。主要有（1）多对一模型、（2）一对一模型、（3）多对多模型。</p>]]></content>
      
      
      <categories>
          
          <category> 课本学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 操作系统 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>计算机系统概述（三）</title>
      <link href="/2021/03/22/ji-suan-ji-xi-tong-gai-shu-san/"/>
      <url>/2021/03/22/ji-suan-ji-xi-tong-gai-shu-san/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h1>操作系统的运行环境</h1><h2 id="操作系统的运行机制"><a class="header-anchor" href="#操作系统的运行机制">¶</a>操作系统的运行机制</h2><p>  在计算机系统中，CPU通常会执行两种不同性质的程序，（1）<strong>操作系统的内核程序即管理程序</strong>，（2）<strong>用户自编程序即应用程序</strong>。<br>  <strong>管理程序</strong>：执行一些特权指令，这些指令不允许用户直接使用，例如I/O指令、置中断指令、存取用于内存保护的寄存器等。<br>  <strong>用户自编程序</strong>：出于安全考虑不能执行这些指令。<br>  用户自编程序运行在用户态，操作系统内核程序运行在核心态。并且现在操作系统几乎都是层次式的结构。操作系统的各项功能分别被设置在不同的层次上。一些与硬件关联较紧密的模块，如时钟管理、中断处理、设备驱动等处于最底层；其次是一些运行频率较高的程序，如进程管理、存储器管理和设备管理等。并且上面的这两部分构成了操作系统的<strong>内核</strong>，内核的指令操作工作在<strong>核心态</strong>。内核主要包括以下四部分内容。</p><h3 id="时钟管理"><a class="header-anchor" href="#时钟管理">¶</a>时钟管理</h3><p>  时钟管理的主要作用如下几点：<br>  （1）计时，即向用户提供标准的系统时间。<br>  （2）<strong>通过时钟中断的管理，切换进程</strong>。在分时操作系统中就有很好的体现（采用时间片轮转调度）。<br>  （3）衡量作业的运行程度（在批处理系统中，无论是多道批处理系统还是单道批处理系统，在微观上都是属于串行，因此可以用作业的执行时间来衡量作业的运行程度）。</p><h3 id="中断机制"><a class="header-anchor" href="#中断机制">¶</a>中断机制</h3><p>  在中断机制中，只有一小部分功能属于内核，它们保护和恢复中断现场的信息，转移控制权到相关的处理程序。提高系统的并行处理能力。并且可以提高多道程序运行环境中CPU的利用率。例如：<strong>进程的管理和调度</strong>，<strong>系统功能的调用</strong>，<strong>设备驱动</strong>……</p><h3 id="原语"><a class="header-anchor" href="#原语">¶</a>原语</h3><p>  按照上述所说的，操作系统是层次式的结构，那么在其底层一定是一些可被调用的具有特定功能的公用程序，例如：CPU切换、设备驱动等，因此将这些称为<strong>原语</strong>。其具有以下的特征：<br>  （1）处于操作系统的最底层，最接近硬件的部分。<br>  （2）其运行具有原子性，其操作必须一次性完成。<br>  （3）被频繁调用，且运行时间较短。</p><h3 id="系统控制的数据结构及处理"><a class="header-anchor" href="#系统控制的数据结构及处理">¶</a>系统控制的数据结构及处理</h3><p>  用来等级状态信息的数据结构，如作业控制块、进程控制块、消息队列、链表、内存分配表等。其常见操作有以下3种：<br>  （1）<strong>进程管理</strong>：进程状态管理、进程调度和分派、创建与撤销进程控制块等。<br>  （2）<strong>存储器管理</strong>存储器的空间分配和回收、内存信息保护程序、代码对换程序等。<br>  （3）<strong>设备管理</strong>：缓冲区管理、设备分配和回收。</p><h2 id="中断和异常"><a class="header-anchor" href="#中断和异常">¶</a>中断和异常</h2><h3 id="定义"><a class="header-anchor" href="#定义">¶</a>定义</h3><p>  <strong>CPU运行上层程序时，唯一可以实现从用户态进入核心态的方式就是中断或异常</strong>。并且中断可以在程序并未使用某种资源时，把它对那种资源的占有权释放，提高资源利用率。<br>  <strong>中断</strong>：指计算机运行过程中，出现某些意外情况需主机干预时，机器能自动停止正在运行的程序并转入处理新情况的程序，处理完毕后又返回原被暂停的程序继续运行。<br>  <strong>异常</strong>：指的是在程序运行过程中发生的异常事件，通常是由外部问题（如硬件错误、输入错误）所导致的。</p><h3 id="中断处理过程"><a class="header-anchor" href="#中断处理过程">¶</a>中断处理过程</h3><p>  中断处理流程图如下图所示：<br><img src="/2021/03/22/ji-suan-ji-xi-tong-gai-shu-san/1.png" alt="中断处理的流程"><br>  （1）<strong>关中断</strong>：CPU响应中断后，首先要保护程序的现场状态，不应响应更高级中断源的中断请求。否则现场会保存不完整。<br>  （2）<strong>保存断点</strong>：将原来的程序断点保存起来，在中断服务程序执行完毕后能正确地返回到原来的程序。<br>  （3）<strong>中断服务程序寻址</strong>：取出中断服务程序的入口地址送入程序计数器PC。<br>  （4）<strong>保存现场和屏蔽字</strong>：现场信息一般是指程序状态字寄存器PSWR和某些通用寄存器的内容。<br>  （5）<strong>开中断</strong>：允许更高级中断请求得到响应。<br>  （6）<strong>关中断</strong>：保证在恢复现场和屏蔽字时不被中断。<br>  （7）<strong>开中断、中断返回</strong>：中断服务程序的最后一条指令通常是一条中断返回指令，使其返回到原程序的断点处，以便继续执行原程序。</p><h3 id="系统调用"><a class="header-anchor" href="#系统调用">¶</a>系统调用</h3><p>  系统中的各种共享资源都由操作系统统一掌管，因此在用户程序种，凡是与资源有关的操作（如存储分配、进行I/O传输及管理文件等），都必须通过系统调用方式向操作系统提出服务请求，并由操作系统代为完成。系统调用功能大致可分为如下几类：<br>  （1）<strong>设备管理</strong>：完成设备的请求或释放，以及设备启动等功能。<br>  （2）<strong>文件管理</strong>：完成文件的读、写、创建即删除等功能。<br>  （3）<strong>进程控制</strong>：完成进程的创建、撤销、阻塞及唤醒等功能。<br>  （4）<strong>进程通信</strong>：完成进程之间的消息传递或信号传递等功能。<br>  （5）<strong>内存管理</strong>：完成内存的分配、回收以及获取作业占用内存区大小及实址等功能。<br>  这样做的好处是<strong>保证系统的稳定性和安全性，防止用户程序随意更改或访问重要的资源系统，影响其他进程的运行</strong>。</p><p>  操作系统的运行环境可以理解为：用户通过操作系统运行上层程序（如用户自编程序），而上层程序的运行需要操作系统的底层管程序提供服务支持。当需要管理程序服务时，系统通过硬件中断机制进入核心态，运行管理程序，或者在出现异常时，被动提供管理程序服务。实现了从用户态转入核心态，在管理服务程序结束后，保存程序现场退出中断处理程序或异常处理程序，返回断点处继续执行用户自编程序。具体流程图如下所示：<br><img src="/2021/03/22/ji-suan-ji-xi-tong-gai-shu-san/2.png" alt="系统调用执行过程"></p>]]></content>
      
      
      <categories>
          
          <category> 课本学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 操作系统 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>计算机系统概述（二）</title>
      <link href="/2021/03/22/ji-suan-ji-xi-tong-gai-shu-er/"/>
      <url>/2021/03/22/ji-suan-ji-xi-tong-gai-shu-er/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h1>操作系统的发展与分类</h1><h2 id="手工操作阶段"><a class="header-anchor" href="#手工操作阶段">¶</a>手工操作阶段</h2><p>  所有的工作都需要人工干预，缺点如下：<br>  （1）用户独占全机，虽然不会出现因资源已被其他用户占用而等待的现象，但资源利用低。<br>  （2）CPU等待手工操作，CPU的利用不充分。</p><h2 id="批处理阶段（操作系统开始出现）"><a class="header-anchor" href="#批处理阶段（操作系统开始出现）">¶</a>批处理阶段（操作系统开始出现）</h2><p>  解决人机矛盾及CPU和I/O设备之间速度不匹配的矛盾，出现了单道批处理系统和多道批处理系统。</p><h3 id="单道批处理系统"><a class="header-anchor" href="#单道批处理系统">¶</a>单道批处理系统</h3><p>  主要特征：<br>  （1）<strong>自动性</strong>：在顺利的情况下，磁带上的一批作业能自动地逐个运行，而且无须人工干预。<br>  （2）<strong>顺序性</strong>：磁带上的各道作业顺序地进入内存，先调入内存的作业先完成。<br>  （3）<strong>单道性</strong>：内存中仅有一道程序运行，当该程序完成或发生异常情况时，才换入其后继程序进入内存运行。<br>  缺点：内存中作业运行期间发出输入/输出请求后，高速的CPU要等待低速的I/O完成，资源利用率和系统的吞吐量较低。</p><h3 id="多道批处理系统"><a class="header-anchor" href="#多道批处理系统">¶</a>多道批处理系统</h3><p>  多道批处理系统允许多个程序同时进入内存并允许它们在CPU中交替地运行，这些程序共享系统中的各种硬/软件资源，当一道程序因I/O请求而暂停运行时，CPU便立即转去运行另一道程序。<br>  主要特征如下：<br>  （1）<strong>多道</strong>：计算机内存中同时存放多道相互独立的程序。<br>  （2）<strong>宏观上并行</strong>：同时进入系统的多道程序都处于运行过程中，即各自运行，但都未运行完毕。<br>  （3）<strong>微观上串行</strong>：内存中的多道程序轮流占有CPU，交替执行。<br>  多道程序设计技术的实现需要解决以下问题：<br>  （1）如何分配处理器。<br>  （2）多道程序的内存分配问题。<br>  （3）I/O设备如何分配。<br>  （4）如何组织和存放大量的程序和数据，以方便用户使用并保证其安全性与一致性。<br>  <strong>优点</strong>：（1）资源利用率高。（2）系统吞吐量大，CPU和其他资源保持“忙碌”状态。<br>  <strong>缺点</strong>：（1）用户响应的时间较长。（2）不提供人机交互能力，用户既不了解自己程序的运行情况，又不能控制计算机。</p><h2 id="分时操作系统"><a class="header-anchor" href="#分时操作系统">¶</a>分时操作系统</h2><p>  <strong>分时技术</strong>：把处理及的运行时间分成很短的时间片，按时间片把处理器分配给各联机作业使用。若某个作业在分配给它的时间片内不能完成其计算，则该作业暂时停止运行，把处理器让给其他的作业使用，等待下一次继续使用。<br>  <strong>分是操作系统</strong>：用户通过终端同时共享一台主机，这些终端连接在主机上，用户可以同时与主机进行交互操作而互不干扰。分时系统特征如下：<br>  （1）<strong>同时性</strong>：允许多个终端用户同时使用一台计算机。<br>  （2）<strong>交互性</strong>：用户可以方便地与系统进行人机对话。<br>  （3）<strong>独立性</strong>：多个用户可以彼此独立操作，互不干扰。<br>  （4）<strong>及时性</strong>：用户在请求计算机时可以很快得到响应。<br><strong>分时操作系统也是支持多道程序设计的，但是与多道批处理系统还是存在差别的</strong>，具体差别如下：<br>  多道程序系统是在计算机内存中同时存放几道相互独立的程序，使它们在管理程序控制之下，相互穿插的运行。 两个或两个以上程序在计算机系统中同处于开始和结束之间的状态。这就称为多道程序技术运行的特征：多道、宏观上并行、微观上串行。<br>  分时操作系统是使一台计算机同时为几个、几十个甚至几百个用户服务的一种操作系统。把计算机与许多终端用户连接起来，分时操作系统将系统处理机时间与内存空间按一定的时间间隔，轮流地切换给各终端用户的程序使用。由于时间间隔很短，每个用户的感觉就像他独占计算机一样。<br>  总之，分时操作系统主要是针对于多用户来说的，而多道程序系统主要是针对于多程序来说的，注意用户和程序之间的区别。</p><h2 id="实时操作系统"><a class="header-anchor" href="#实时操作系统">¶</a>实时操作系统</h2><p>  可以在某个时间限制内完成某些紧急任务而不需要时间片排队。其分为两种情况：<br>  （1）<strong>硬实时系统</strong>：某个动作必须要在规定的时刻或时间段内完成，如飞行自动控制系统。<br>  （2）<strong>软实时系统</strong>：偶尔违反时间规定且不会引起任何永久性的损害。如飞机订票系统。</p><h2 id="网络操作系统"><a class="header-anchor" href="#网络操作系统">¶</a>网络操作系统</h2><p>  通过计算机网络把各台计算机联合在一起，实现各台计算机之间数据的互相传送。<br>  <strong>特点</strong>：网络中各种资源的共享及计算机之间的通信。</p><h2 id="分布式计算机系统"><a class="header-anchor" href="#分布式计算机系统">¶</a>分布式计算机系统</h2><p>  属于分布式计算机系统需要满足以下条件：<br>  （1）系统中任意两台计算机通过通信方式交换信息。<br>  （2）系统中的每台计算机具有相同的地位。<br>  （3）每台计算机的资源为所有用户共享。<br>  （4）任意台计算机都可以构成子系统，并且可以重构。<br>  （5）任何工作都可以分布在几台计算机上，由他们协同完成。<br>  主要特点如下：<strong>分布性</strong>和<strong>并行性</strong>。</p><h2 id="个人操作系统"><a class="header-anchor" href="#个人操作系统">¶</a>个人操作系统</h2><p>  有常见的Windows、Linux、Macintosh等，应用最为广泛。</p><p>  还有嵌入式操作系统、服务器操作系统等。</p>]]></content>
      
      
      <categories>
          
          <category> 课本学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 操作系统 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>计算机系统概述（一）</title>
      <link href="/2021/03/20/ji-suan-ji-xi-tong-gai-shu-yi/"/>
      <url>/2021/03/20/ji-suan-ji-xi-tong-gai-shu-yi/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h1>操作系统的基本概念</h1><h2 id="什么是操作系统？"><a class="header-anchor" href="#什么是操作系统？">¶</a>什么是操作系统？</h2><p>  操作系统（Operating System， OS）是指：控制和管理整个计算机系统的硬件与软件资源，合理地组织、调度计算机的工作与资源的分配，进而为用户和其他软件提供方便的接口和环境的程序集合。<strong>操作系统是计算机系统中最基本的系统软件</strong>。<br>  其作用简单来说：<br>（1）控制和管理整个计算机系统的硬件与软件资源。<br>（2）组织和调度计算机工作和资源的分配。<br>（3）提供给用户和其他软件方便的接口和环境。</p><h2 id="操作系统的特征"><a class="header-anchor" href="#操作系统的特征">¶</a>操作系统的特征</h2><p>  操作系统是一种系统软件，其基本特征包括<strong>并发、共享、虚拟和异步</strong>。</p><h3 id="（1）并发（Concurrence）"><a class="header-anchor" href="#（1）并发（Concurrence）">¶</a>（1）并发（Concurrence）</h3><p>  并发是指两个或多个事件在同一时间间隔内发生。操作系统的并发性是指计算机系统中同时存在多个运行的程序。因此我们可以认为它有上述的作用（2）.同时在整个操作系统中，我们引入进程的目的，也是为了使程序能并发执行。<br>  在上面我们需要注意的是，并发和并行的区别，通俗来说，并发是指事情同一时间段内发生，例如在我写这篇博客的整个时间段内，我顺便去偷看了旁边几个漂亮的妹纸，在宏观上来说，这些事情都是在这个时间段上是同时发生的，但是在微观上的其中几分钟，几秒钟来说，这些事情其实是交替发生的，整个过程大致就是“写博客-看妹纸-写博客-看妹纸-看妹纸-写博客”（不好意思，其实我只看了一次😇 😇 😇）而并行是指在同一时刻发生的，例如在我偷看妹纸的时候，发现我对面的哥们（经典无中生友😂 😂 😂）其实也在和我一样，不过他在偷看旁边的男生😳 😳 😳，那么我们两个人在相同的时刻做的这件事情，这就叫做并行。<br>  在我们进行提到的多线程中，如果你的电脑为单核CPU，那么即使你加再多的线程，也是并发执行，因为一个核心，只能处理一个任务。一个核心处理多任务的方法，（1）排队，一个一个执行。（2）一个执行一小段时间，在多个任务间切换，没有被服务的任务只能等待，表现就是你的电脑会有些卡顿。如果你的CPU有两个核心，那么它的每个核心，在此时此刻，可以分别服务一个任务，这样就可以实现并行。</p><h3 id="（2）共享（Sharing）"><a class="header-anchor" href="#（2）共享（Sharing）">¶</a>（2）共享（Sharing）</h3><p>  即资源共享，是指操作系统中的资源可供内存中多个并发执行的进程共同使用。共享主要分为<strong>互斥共享方式</strong>和<strong>同时访问方式</strong>。</p><h4 id="互斥共享方式"><a class="header-anchor" href="#互斥共享方式">¶</a>互斥共享方式</h4><p>  操作系统中的资源虽然可以同时提供给多个进程使用，但一个时间段内只允许一个进程访问该资源。<br>  举一个通俗的例子来解释，如果有一个妹纸问我有没有时间陪她出去玩，但是同时我的平时一个很讨厌的人也在问我能否陪他出去，这两个进程同时请求使用我嘴的资源，这时我不可能同时回答他们两个人的问题，是或者不是都会让其中一个人误会我的意思（当然是想和妹纸出去，拒绝另外一个人）。这就是互斥共享。<br>  我们把上述的资源共享方式称为互斥共享，把一段时间内只允许一个进程访问的资源称为<strong>临界资源</strong>或者<strong>独占资源</strong>，例如：打印机、磁带机等。</p><h4 id="同时访问方式"><a class="header-anchor" href="#同时访问方式">¶</a>同时访问方式</h4><p>  允许一个时间段内由多个进程“同时”对他们进行访问<br>  “同时”通常上是宏观的，而在微观上，这些进程可能是交替地对该资源进行访问即“分时共享”的。比如说：今天在图书馆的这一个下午，看妹纸和写博客这两件事情都可以向我的眼睛和大脑来发送请求，使用这两个资源，在这段时间内，我可以看一眼妹子，然后写几行博客，交替进行这两个进程。在计算机中可供多个进程“同时”访问的典型资源是磁盘设备。<br>  互斥共享要求一种资源在一段时间内（哪怕是一段很小的时间）只能，满足一个请求，否则就会出现严重的问题（例如打字机一行打印A文档内容，一行打印B文档内容）而同时访问共享通常要求一个请求分几个时间片段间隔地完成。<br>  并发和共享之间互为存在的条件：<strong>（1）资源共享是以程序的并发为条件的，若系统不允许程序并发执行则不存在资源共享的问题。（2）若系统不能对资源共享实施有效的管理，则影响到程序的并发执行，甚至根本无法存在并发执行</strong>。</p><h3 id="（3）虚拟（Virtual）"><a class="header-anchor" href="#（3）虚拟（Virtual）">¶</a>（3）虚拟（Virtual）</h3><p>  虚拟是指把一个物理上的实体转变为若干逻辑上的对应物。操作系统的虚拟技术可归纳为：<strong>（1）时分复用技术：处理器的分时共享</strong>。<strong>（2）空分复用技术：虚拟存储器</strong>。</p><h3 id="（4）异步（Asynchronism）"><a class="header-anchor" href="#（4）异步（Asynchronism）">¶</a>（4）异步（Asynchronism）</h3><p>  在多道程序环境中，允许多个程序并发执行，但由于资源优先，进程的执行不是一贯到底，而是走走停停，以不可预知的速度向前推进。</p><h2 id="操作系统的目标和功能"><a class="header-anchor" href="#操作系统的目标和功能">¶</a>操作系统的目标和功能</h2><p>  （1）为了给多道程序提供良好的运行环境，操作系统应具有以下几方面的功能：<strong>处理机管理、存储器管理、设备管理和文件管理</strong>。<br>  （2）为了方便用户使用操作系统，还必须向用户提供接口。<br>  （3）操作系统可用来扩充机器，以提供更方便的服务、更高的资源利用率。</p><h3 id="操作系统作为计算机系统资源的管理者"><a class="header-anchor" href="#操作系统作为计算机系统资源的管理者">¶</a>操作系统作为计算机系统资源的管理者</h3><h4 id="处理机管理"><a class="header-anchor" href="#处理机管理">¶</a>处理机管理</h4><p>  在多道程序环境下，<strong>处理机的分配和运行都以进程、线程为基本单位</strong>，因而对处理机的管理可归结为对进程的管理。因此进程何时创建、何时撒销、如何管理、如何避免冲突、合理共享就是进程管理的最主要的任务。进程管理的主要功能包括<strong>进程控制、进程同步、进程通信、死锁处理、处理机调度等。</strong></p><h4 id="存储器管理"><a class="header-anchor" href="#存储器管理">¶</a>存储器管理</h4><p>  存储器管理是为了给多道程序的运行提供良好的环境，方便用户用及提高内存的利用率，主要包括<strong>内存分配与回收、地址映射、内存保护与共享和内存扩充等功能</strong>。</p><h4 id="文件管理"><a class="header-anchor" href="#文件管理">¶</a>文件管理</h4><p>  计算机中的信息都是以文件的形式存在的，操作系统中负责文件管理的部分称为文件系统，文件管理包括<strong>文件存储空间的管理、目录管理及文件读写管理和保护等</strong>。</p><h4 id="设备管理"><a class="header-anchor" href="#设备管理">¶</a>设备管理</h4><p>  设备管理的主要任务是完成用户的I/O请求，方便用户使用各种设备，并提高设备的利用率，主要包括<strong>缓冲管理、设备分配、设备处理和虚拟设备等功能</strong>。</p><h3 id="操作系统作为用户与计算机硬件系统之间的接口"><a class="header-anchor" href="#操作系统作为用户与计算机硬件系统之间的接口">¶</a>操作系统作为用户与计算机硬件系统之间的接口</h3><p>  为了方便用户对于计算机硬件的操作以及运行自己计算机上的程序，操作系统为用户提供了接口，接口分为两类，<strong>（1）命令接口，（2）程序接口</strong>。</p><h4 id="命令接口"><a class="header-anchor" href="#命令接口">¶</a>命令接口</h4><p>  命令接口的目的是，用户利用这些命令来组织和控制作业的执行。其分为<strong>联机命令接口</strong>和<strong>脱机命令接口</strong>。<br>  <strong>联机命令接口</strong>又称交互式命令接口，适用于分时或实时系统的接口，通常是用户在键盘通过控制台或者终端输入操作命令，操作系统的命令解释程序解释执行输入的命令后，完成功能后，将指挥权返回终端。<br>  <strong>脱机命令接口</strong>又称批处理命令接口，适用于批处理系统。用户不能直接干预作业的运行，而是事先用相应的作业控制命令以作业操作说明书的形式提交给操作系统，等待系统中命令解释器对作业进行逐条处理。</p><h4 id="程序接口（系统调用）"><a class="header-anchor" href="#程序接口（系统调用）">¶</a>程序接口（系统调用）</h4><p>  程序接口由一组系统调用命令(也称广义指令)组成。用户通过在程序中使用这些系统调用来请求操作系统为其提供服务。如使用各种外部设备、申请分配和回收内内存及其他用程序接口实现的，当前最为流行的是图形用户界面(GUT),即图形接口。</p><h3 id="操作系统用作扩充机器"><a class="header-anchor" href="#操作系统用作扩充机器">¶</a>操作系统用作扩充机器</h3><p>  裸机没有任何软件支持，而我们实际中的计算机系统是若干层软件改造之后的，在裸机的外层，就是操作系统，这也不难理解为什么我们称其为硬件与用户之间的中介，因为其提供了大量的资源管理功能和方便用户的各种服务功能。</p>]]></content>
      
      
      <categories>
          
          <category> 课本学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 操作系统 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>xbwcj的失败史</title>
      <link href="/2021/03/19/xbwcj-de-shi-bai-shi/"/>
      <url>/2021/03/19/xbwcj-de-shi-bai-shi/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="分享一句话"><a class="header-anchor" href="#分享一句话">¶</a>分享一句话</h2><p><font face="微软雅黑" size="4">你我都是内卷背景下不起眼的尘埃，被时代的洪流裹挟向前。</font></p><h2 id="个人失败历史"><a class="header-anchor" href="#个人失败历史">¶</a>个人失败历史</h2><p>入学以来大一参加计算机设计大赛失败。😟<br>入学以后第一场考试以及后面无数考试失利。😩<br>大一参加校内数学建模新生杯惨败。😞<br>大一参加华中赛数学建模无功而返。😑<br>大一参加互联网+因特殊原因退出。😔<br>……<br>大二开始接触DataFountain竞赛，一直徘徊在复赛圈，与决赛无缘。<br>开始学习acm算法，在各种比赛中被吊打。<br>开始学习一些简单的开发，但是没有完整做出一款属于自己的开发的应用。<br>大二参加华为软件精英挑战赛，被数据漏洞坑害（参赛经验不足，获取比赛信息时效性低），赛区64强止步。<br>大二参加全国k-code程序设计大赛，止步21名，倒在决赛圈（前二十名）门口。<br>参加MathorCup成功参赛。<br>大二暑期建模培训最终选拔全校第七名，特殊原因被淘汰。<br>……<br>大三参加天池Redis数据库中间件挑战赛止步决赛。<br>……<br>还有很多自己以及以往的失败历史。</p><h2 id="未来准备参加的比赛"><a class="header-anchor" href="#未来准备参加的比赛">¶</a>未来准备参加的比赛</h2><p>美赛成绩未出<br>继续准备参加计算机设计大赛、华为软件精英挑战赛、等之类的其他比赛。</p><h2 id="xbwcj的数学建模比赛的准备"><a class="header-anchor" href="#xbwcj的数学建模比赛的准备">¶</a>xbwcj的数学建模比赛的准备</h2><p>1、提高自己的编程能力，编程能力的提升是在平时就要锻炼的，平时写代码的时候不要马虎应付，多看博客等其他网站上的一些常用算法与伪代码，最重要的是了解算法的思维，严谨的思维+超强的编程能力才能轻松应对数模中的编程。<br>2、强烈推荐去看司守奎的《数学建模算法与应用》这本书，学习上面的一些比较常见的算法，自己一定要手动实现一次，自己不手动实现，就很难理解里面的编程思维。<br>3、多去准备一些比较新的智能优化算法，推荐公众号：数学算法实验室、智能优化算法。尝试自己改进一些算法，将这些算法做成接口的形式，在使用时只需要传参就可以。准备一些可以做出精美图片的数据分析的软件，例如Oracle提供的Data Visualization Desktop的软件，也可以使用Office的PPT功能。熟练使用可以在比赛过程中省时省力。或者准备一些Matlab或者Python的作图代码，例如简单的条形图、折线图、雷达图等，或者高级点的词云之类的，也以接口的形式封装。<br>4、如何上手？一定一定要自己独自去完成一次数模的全部流程，了解每个步骤应该做什么如何去做。只有全部接触才能知道自己的优势以及劣势，了解自己应该找什么样的队友来取长补短。<br>5、暑期培训前的准备：2020年由于疫情的原因，选拔赛是以组队的形式直接做题来选拔。在暑期培训前无论是校赛还是校外的数模比赛中，都可以自己去找队友，因此最好在暑期选拔培训前就确定好自己的队友，尽早开始磨合，利用其他的数模比赛找到队伍的不足，这样可以在选拔赛中发挥的更好一些。校外的数模比赛建议参加：华中赛、泰迪杯、数维杯、深圳杯、亚太赛。（挑选其中几个即可）</p><h2 id="xbwcj的校外比赛经验"><a class="header-anchor" href="#xbwcj的校外比赛经验">¶</a>xbwcj的校外比赛经验</h2><p>推荐几个我经常浪荡的比赛网站：<br><a href="https://tianchi.aliyun.com/competition/gameList/activeList">天池</a>、<a href="https://competition.huaweicloud.com/competitions">华为云</a>、<br><a href="https://www.datafountain.cn/competitions">DataFountain</a>、<a href="https://www.kaggle.com/competitions">kaggle</a>、<br>这些比赛有奖金，并且你会学习到很多平时在课堂里面学不到的知识技能。刚开始得奖会比较难，因为这些比赛中参加的不只是本科生，一般是面向所有在校学生，而且硕士生所占比例不低，所以开始大家可以去涨经验，后面能到何种地步就看大家自己的发挥了。🐶🐶🐶<br>如果想要提高自己的编程能力：<br><a href="https://ac.nowcoder.com/acm/problem/list">牛客</a>、<a href="https://www.luogu.com.cn/problem/list">洛谷</a>、<a href="http://acm.hdu.edu.cn/">hduOJ</a>、<a href="https://codeforces.com/problemset">CodeForces</a>、<a href="https://nanti.jisuanke.com/acm">计蒜客</a>。<br>去刷里面的题目，如果不是专门去训练acm，不需要去做太难的，达到中等就可以应付绝大多数公司的绝大多数笔试题了。也可以帮助你了解比较重要的基础算法。</p><h2 id="xbwcj个人大学总结经验（其实是自己的不足与后悔之处）"><a class="header-anchor" href="#xbwcj个人大学总结经验（其实是自己的不足与后悔之处）">¶</a>xbwcj个人大学总结经验（其实是自己的不足与后悔之处）</h2><p>1、一定要尽早规划，自己以后是找工作还是要读研，读研的话，是准备考研，还是A、B保，如果保研自己还欠缺什么，尽早做打算。<br>2、精心，静心去做一件事，贪多嚼不烂。<br>3、无论做什么，基础一定要扎实。</p>]]></content>
      
      
      <categories>
          
          <category> 个人分享 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 个人分享 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>xbwcj的失败史</title>
      <link href="/2021/03/19/xbwcj-de-shi-bai-shi-he-ccr-de-guang-hui-sui-yue/"/>
      <url>/2021/03/19/xbwcj-de-shi-bai-shi-he-ccr-de-guang-hui-sui-yue/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h1>XBW</h1><h2 id="分享一句话"><a class="header-anchor" href="#分享一句话">¶</a>分享一句话</h2><p><font face="微软雅黑" size="4">你我都是内卷背景下不起眼的尘埃，被时代的洪流裹挟向前。</font></p><h2 id="个人失败历史"><a class="header-anchor" href="#个人失败历史">¶</a>个人失败历史</h2><p>入学以来大一参加计算机设计大赛失败。😟<br>入学以后第一场考试以及后面无数考试失利。😩<br>大一参加校内数学建模新生杯惨败。😞<br>大一参加华中赛数学建模无功而返。😑<br>大一参加互联网+因特殊原因退出。😔<br>……<br>大二开始接触DataFountain竞赛，一直徘徊在复赛圈，与决赛无缘。<br>开始学习acm算法，在各种比赛中被吊打。<br>开始学习一些简单的开发，但是没有完整做出一款属于自己的开发的应用。<br>大二参加华为软件精英挑战赛，被数据漏洞坑害（参赛经验不足，获取比赛信息时效性低），赛区64强止步。<br>大二参加全国k-code程序设计大赛，止步21名，倒在决赛圈（前二十名）门口。<br>参加MathorCup成功参赛。<br>大二暑期建模培训最终选拔全校第七名，特殊原因被淘汰。<br>……<br>大三参加天池Redis数据库中间件挑战赛止步决赛。<br>……<br>还有很多自己以及以往的失败历史。</p><h2 id="未来准备参加的比赛"><a class="header-anchor" href="#未来准备参加的比赛">¶</a>未来准备参加的比赛</h2><p>美赛成绩未出<br>继续准备参加计算机设计大赛、华为软件精英挑战赛、等之类的其他比赛。</p><h2 id="xbwcj的数学建模比赛的准备"><a class="header-anchor" href="#xbwcj的数学建模比赛的准备">¶</a>xbwcj的数学建模比赛的准备</h2><p>1、提高自己的编程能力，编程能力的提升是在平时就要锻炼的，平时写代码的时候不要马虎应付，多看博客等其他网站上的一些常用算法与伪代码，最重要的是了解算法的思维，严谨的思维+超强的编程能力才能轻松应对数模中的编程。<br>2、强烈推荐去看司守奎的《数学建模算法与应用》这本书，学习上面的一些比较常见的算法，自己一定要手动实现一次，自己不手动实现，就很难理解里面的编程思维。<br>3、多去准备一些比较新的智能优化算法，推荐公众号：数学算法实验室、智能优化算法。尝试自己改进一些算法，将这些算法做成接口的形式，在使用时只需要传参就可以。准备一些可以做出精美图片的数据分析的软件，例如Oracle提供的Data Visualization Desktop的软件，也可以使用Office的PPT功能。熟练使用可以在比赛过程中省时省力。或者准备一些Matlab或者Python的作图代码，例如简单的条形图、折线图、雷达图等，或者高级点的词云之类的，也以接口的形式封装。<br>4、如何上手？一定一定要自己独自去完成一次数模的全部流程，了解每个步骤应该做什么如何去做。只有全部接触才能知道自己的优势以及劣势，了解自己应该找什么样的队友来取长补短。<br>5、暑期培训前的准备：2020年由于疫情的原因，选拔赛是以组队的形式直接做题来选拔。在暑期培训前无论是校赛还是校外的数模比赛中，都可以自己去找队友，因此最好在暑期选拔培训前就确定好自己的队友，尽早开始磨合，利用其他的数模比赛找到队伍的不足，这样可以在选拔赛中发挥的更好一些。校外的数模比赛建议参加：华中赛、泰迪杯、数维杯、深圳杯、亚太赛。（挑选其中几个即可）</p><h2 id="xbwcj的校外比赛经验"><a class="header-anchor" href="#xbwcj的校外比赛经验">¶</a>xbwcj的校外比赛经验</h2><p>推荐几个我经常浪荡的比赛网站：<br><a href="https://tianchi.aliyun.com/competition/gameList/activeList">天池</a>、<a href="https://competition.huaweicloud.com/competitions">华为云</a>、<br><a href="https://www.datafountain.cn/competitions">DataFountain</a>、<a href="https://www.kaggle.com/competitions">kaggle</a>、<br>这些比赛有奖金，并且你会学习到很多平时在课堂里面学不到的知识技能。刚开始得奖会比较难，因为这些比赛中参加的不只是本科生，一般是面向所有在校学生，而且硕士生所占比例不低，所以开始大家可以去涨经验，后面能到何种地步就看大家自己的发挥了。🐶🐶🐶<br>如果想要提高自己的编程能力：<br><a href="https://ac.nowcoder.com/acm/problem/list">牛客</a>、<a href="https://www.luogu.com.cn/problem/list">洛谷</a>、<a href="http://acm.hdu.edu.cn/">hduOJ</a>、<a href="https://codeforces.com/problemset">CodeForces</a>、<a href="https://nanti.jisuanke.com/acm">计蒜客</a>。<br>去刷里面的题目，如果不是专门去训练acm，不需要去做太难的，达到中等就可以应付绝大多数公司的绝大多数笔试题了。也可以帮助你了解比较重要的基础算法。</p><h2 id="xbwcj个人大学总结经验（其实是自己的不足与后悔之处）"><a class="header-anchor" href="#xbwcj个人大学总结经验（其实是自己的不足与后悔之处）">¶</a>xbwcj个人大学总结经验（其实是自己的不足与后悔之处）</h2><p>1、一定要尽早规划，自己以后是找工作还是要读研，读研的话，是准备考研，还是A、B保，如果保研自己还欠缺什么，尽早做打算。<br>2、精心，静心去做一件事，贪多嚼不烂。<br>3、无论做什么，基础一定要扎实。</p><h1>Terence</h1><h2 id="最近做的事"><a class="header-anchor" href="#最近做的事">¶</a>最近做的事</h2><ul><li><p>和好朋友一起参加  <code>DataWhale</code> 三月份的组队学习（推荐系统和区块链方面）</p></li><li><p>学习英语和编程</p></li><li><p>准备了解一下多模态这个领域，也顺便考虑一下自己以后的研究方向，准备读研也不得不面对这个问题，以后要做什么？</p></li></ul><p>就以这个 <code>以后要做什么？</code> 题简单跟大家分享下</p><h2 id="以后要做什么？"><a class="header-anchor" href="#以后要做什么？">¶</a>以后要做什么？</h2><p>留给我们的选择其实不多，读研 or 就业</p><p>读研</p><ul><li>绩点要稳住（大数据专业绝大部分专业必修都在前两年）走 A 保</li><li>比赛拿奖项走 B 保</li></ul><p>科研</p><ul><li>理论研究</li><li>数据挖掘</li><li>机器学习</li><li>NLP（自然语言处理）</li><li>CV（计算机视觉）</li><li>多模态</li></ul><p>职业</p><ul><li><p>数据科学家</p></li><li><p>数据分析师</p></li><li><p>算法工程师</p></li></ul><h2 id="参加的比赛"><a class="header-anchor" href="#参加的比赛">¶</a>参加的比赛</h2><ul><li>华中地区大学生数学建模邀请赛省赛</li><li>华为大学生ICT大赛湖北区域省赛</li><li>全国大学生数学建模省赛</li><li>美国大学生数学建模大赛国赛</li><li>全国大学生节能减排国赛</li><li>中国大学生计算机设计大赛国赛</li><li>中国大学生服务外包创新创业大赛国赛</li><li>Kaggle、天池、腾讯广告…</li></ul><h2 id="数学建模"><a class="header-anchor" href="#数学建模">¶</a>数学建模</h2><p>讲下数学建模，其实刚才雯雯也说了很多，也非常详细，跟我观点也蛮吻合的。有一个观点，就是在咱们学校成功进入校队，参加国赛和美赛，不是一件容易的事情。确实它收益很大，每一年都有很多同学通过数学建模成功保研。并不是很推荐大家去很功利地做一件事情，数模带给我们的不仅仅是一个保研的资格或者说门票，更多的是带来一种能力。</p><ul><li>分析问题，将实际问题建立数学模型的能力</li><li>解决问题，运用自己所学去求解模型的能力</li><li>撰写科技论文的能力</li><li>团队配合</li></ul><p>2019年数学建模选拔形式（具体细节可以在教务处官网查询）</p><ul><li><p>编程：开卷、不限语言（主要是 Matlab ）对 8 个题目进行求解，求解问题大多是线性方程、0-1背包、最优化、智能算法以及开放题。最终按成绩排名。</p></li><li><p>建模：这个我就不太清楚了</p></li></ul><h2 id="其他比赛"><a class="header-anchor" href="#其他比赛">¶</a>其他比赛</h2><ul><li><p>ACM</p></li><li><p><a href="http://i.whut.edu.cn/xxtg/znbm/jwc/202103/t20210317_484335.shtml">【教务处】关于组织参加 “2021 年（第14届）中国大学生计算机设计大赛”的通知</a></p></li><li><p><a href="http://i.whut.edu.cn/xxtg/znbm/jwc/202103/t20210304_483136.shtml">【教务处】关于组织参加第十二届中国大学生服务外包创新创业大赛的通知</a></p></li><li><p><a href="http://i.whut.edu.cn/xxtg/znbm/jwc/202103/t20210304_483133.shtml">【教务处】关于举办“2021年中国高校计算机大赛-团体程序设计天梯赛”校内选拔赛的通知</a></p></li></ul><p>怎么说，不断去接触去尝试才会有更多可能。你说比赛它难不难，要想拿到好名次确实很难，没有一帆风顺的路，当你觉得自己真的真的真的快撑不下去的时候，那就别撑了，我们又不是把伞，老撑着干嘛:happy:</p><h2 id="小结"><a class="header-anchor" href="#小结">¶</a>小结</h2><p>找到自己喜欢的事情，然后坚持下去就好了。如果有一件事（限学业方面），能够让你忘记时间地投入进去，那就是足够热爱的事情啦！</p>]]></content>
      
      
      <categories>
          
          <category> 个人分享 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 个人分享 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据结构--线性表</title>
      <link href="/2021/03/08/shu-ju-jie-gou-xian-xing-biao/"/>
      <url>/2021/03/08/shu-ju-jie-gou-xian-xing-biao/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="什么是线性表？"><a class="header-anchor" href="#什么是线性表？">¶</a>什么是线性表？</h2><p>线性表（linear_list）是最常用且最简单的一种数据结构，一个线性表相当于是n个数据元素的有限序列，其中每个数据元素表示的内容在不同的情况下并不相同，有可能是一个数字、一个符号之类的。若将线性表用集合表示为:<br>$$<br>(a_{1}, …, a_{i-1}, a_{i}, a_{i+1}, …, a_{n})<br>$$</p><p>由集合中可以看出，除第一个外，集合中的每个数据元素均只有一个前驱，除最后一个外，集合中的每个数据元素均只有一个后继。</p><h2 id="线性表的两种表示形式"><a class="header-anchor" href="#线性表的两种表示形式">¶</a>线性表的两种表示形式</h2><p>（1）顺序表示：用一组地址连续的存储单元依次存储线性表的数据元素，称为线性表的顺序存储结构，可随机存取表中任一元素（其实就是数组）<br>（2）链式表示：用一组任意的存储单元存储线性表中的数据元素，称为线性表的链式存储结构。它的存储单元可以是连续的，也可以是不连续的。在表示数据元素之间的逻辑关系时，除了存储其本身的信息之外，还需存储一个指示其直接后继的信息（即直接后继的存储位置），这两部分信息组成数据元素的存储映像，称为结点（node）。它包括两个域；存储数据元素信息的域称为数据域；存储直接后继存储位置的域称为指针域。指针域中存储的信息称为指针或链。<br>可以用下图来更加直观的表示顺序结构和链式结构</p><h2 id="线性表的一般定义"><a class="header-anchor" href="#线性表的一般定义">¶</a>线性表的一般定义</h2><p>线性表的抽象数据类型定义如下：<br>数据对象：$$D , = , {a_{i} , | , a_{i} , \in , ElemSet, ,i = 1,2,\cdots,n, , n \geq 0}$$<br>数据关系：$$R1 , = , {&lt; a_{i-1},a_{i} &gt; |a_{i-1},a{i} \in D,,i=2,\cdots,n}$$</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">ADT List &#123;    &#x2F;&#x2F;基本操作    InitList(&amp;L) &#x2F;&#x2F;构造空线性表L    DestoryList(&amp;L) &#x2F;&#x2F;销毁线性表L    ClearList(&amp;L) &#x2F;&#x2F;将线性表L变为空    ListEmpty(L) &#x2F;&#x2F;若线性表L为空表，返回TRUE，否则返回FASLSE    ListLength(L) &#x2F;&#x2F;返回线性表中的数据元素个数    GetElem(L, i, &amp;e) &#x2F;&#x2F;用e返回L中第i个数据元素的值    PriorElem(L, cur_e, &amp;pre_e) &#x2F;&#x2F; 如果cur_e是线性表中的元素，而且不是第一个，那么我们就可以返回该元素前一个元素的值    NextElem(L, cur_e, &amp;next_e) &#x2F;&#x2F; &#x2F;&#x2F;如果cur_e是线性表中的元素，而且不是最后一个，就返回它下一个元素的值    Listinsert(&amp;L, i, e)&#x2F;&#x2F;如果线性表存在了，而且i符合条件，则在i位置插入一个元素    ListDelete(&amp;L, i)&#x2F;&#x2F;删除i位置上的元素    ListDelete_data(&amp;L, e, order)&#x2F;&#x2F;删除指定的元素e，order决定是删除一个，还是全部。    Connect_two_List(L_a,L_b,&amp; L_c)&#x2F;&#x2F;连接两个线性表，除去重复的内容    print(L)&#x2F;&#x2F;打印线性表    &#x2F;*        此处省略部分其他常见的操作    *&#x2F;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在上述的函数中需要注意，如果要改变原本线性表的内容，则需要传入线性表的地址，如果只是对线性表进行查询等不改变原本的线性表内容的操作，则不需要以地址的形式传入。（这部分其实就是C/C++的对传入函数参数值改变的原理，在此做一个小的提醒）。</p><h2 id="顺序存储结构常见的结构体实现"><a class="header-anchor" href="#顺序存储结构常见的结构体实现">¶</a>顺序存储结构常见的结构体实现</h2><p>我们通过线性表顺序结构来实现线性表的一些基础操作。</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">#include&lt;bits&#x2F;stdc++.h&gt;#define SIZE 100#define SIZE_T 150using namespace std;typedef int ElemType;struct List &#123;    ElemType *data; &#x2F;&#x2F;数据    int length; &#x2F;&#x2F;长度    int size; &#x2F;&#x2F;线性表初始容量&#125;;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="顺序存储结构InitList-函数的实现"><a class="header-anchor" href="#顺序存储结构InitList-函数的实现">¶</a>顺序存储结构InitList()函数的实现</h2><p>线性表的顺序表示可以看作对数组进行一些操作，初始化函数就是将表的长度变为0，对data进行堆内存的分配和初始容量的初始化。</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">&#x2F;&#x2F;创建一个空的线性表void InitList(List &amp;newList) &#123;    &#x2F;&#x2F;初始容量为startsize    newList.size &#x3D; SIZE_T;    &#x2F;&#x2F;首先开辟空间    newList.data &#x3D; (int *)malloc(SIZE * sizeof(ElemType));    if(!newList.data)    &#123;        exit(OVERFLOW); &#x2F;&#x2F; 存储分配失败    &#125;    &#x2F;&#x2F;空表，长度是0    newList.length &#x3D; 0;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="顺序存储结构DestoryList-函数的实现"><a class="header-anchor" href="#顺序存储结构DestoryList-函数的实现">¶</a>顺序存储结构DestoryList()函数的实现</h2><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">&#x2F;&#x2F;前提是线性表已经存在void Destory (List &amp;newList)&#123;if (newList &#x3D;&#x3D; NULL)    &#123;        exit(OVERFLOW);&#x2F;&#x2F; 线性表不存在    &#125;    &#x2F;&#x2F;首先释放堆内存free(newList.data);    &#x2F;&#x2F;每次释放堆内存后，应将对应的指针指向NULL，这是一个比较好的编程习惯    newList.data &#x3D; NULL;    newList.length &#x3D; 0;    newList.size &#x3D; 0;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="顺序存储结构ClearList-函数的实现"><a class="header-anchor" href="#顺序存储结构ClearList-函数的实现">¶</a>顺序存储结构ClearList()函数的实现</h2><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">&#x2F;&#x2F;前提是线性表已经存在void ClearList(List &amp;newList) &#123;    if (newList &#x3D;&#x3D; NULL)    &#123;        exit(OVERFLOW);&#x2F;&#x2F; 线性表不存在    &#125;    newList.length &#x3D; 0;free(newList.data);    newList.data &#x3D; NULL;    &#x2F;&#x2F;重新开辟空间    newList.data &#x3D; (int *)malloc(SIZE * sizeof(ElemType));&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="顺序存储结构ListEmpty-函数的实现"><a class="header-anchor" href="#顺序存储结构ListEmpty-函数的实现">¶</a>顺序存储结构ListEmpty()函数的实现</h2><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">&#x2F;&#x2F; 判断线性表是否为空bool ListEmpty(List newList) &#123;    if(newList.length &#x3D;&#x3D; 0)    &#123;        return 1;    &#125;    else    &#123;        return 0;    &#125;    return 0;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="顺序存储结构ListLength-函数的实现"><a class="header-anchor" href="#顺序存储结构ListLength-函数的实现">¶</a>顺序存储结构ListLength()函数的实现</h2><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">&#x2F;&#x2F; 返回线性表的长度int ListEmpty(List newList) &#123;       return newList.length;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> 课本学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据结构 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>BAS算法</title>
      <link href="/2021/01/29/bas-suan-fa/"/>
      <url>/2021/01/29/bas-suan-fa/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="天牛须搜索-Beetle-Antennae-Search-BAS"><a class="header-anchor" href="#天牛须搜索-Beetle-Antennae-Search-BAS">¶</a>天牛须搜索(Beetle Antennae Search-BAS)</h2><p>又称甲壳虫须搜索，类似于遗传算法、粒子群算法、模拟退火等智能优化算法，天牛须搜索不需要知道函数的具体形式，不需要梯度信息，就可以实现高效寻优。<br><strong>优点</strong>：相比于粒子群算法，天牛须搜索只需要一个个体，即一只天牛，运算量大大降低。</p><h2 id="仿生学原理"><a class="header-anchor" href="#仿生学原理">¶</a>仿生学原理</h2><p>天牛须搜索算法模仿自然界中天牛觅食行为。在天牛觅食过程中，其并不知道食物在哪里，但食物会产生特殊气味，吸引天牛向着食物前进。天牛通过其两只触角对空气中的食物气味进行感知，且根据食物距离两只触角的距离远近不同，两只触角所感知的气味浓度也有所差异。当食物处于天牛左侧时，左侧触角感知的气味浓度强于右侧触角感知的气味浓度，天牛根据两只触角所感知的浓度差，向着浓度强的一侧随机前进。通过一次次迭代，最终找到食物的位置。</p><h2 id="行为启发"><a class="header-anchor" href="#行为启发">¶</a>行为启发</h2><p>食物的气味就相当于一个函数,这个函数在三维空间每个点值都不同,天牛两个须可以采集自身附近两点的气味值,天牛的目的是找到仝局气味值最大的点仿照天牛的行为,我们就可以高效的进行函数寻优。</p><h2 id="算法模型"><a class="header-anchor" href="#算法模型">¶</a>算法模型</h2><p>BAS算法主要是通过在不停的左右触角气味浓度比对中前进，同其他算法相比，原理十分简单。<br>在进行两只触角气味浓度计算之前，需要对其进行一系列准备工作，在$D$维空间中天牛的位置为$X = (x_{1}, x_{2}, … , x_{n})$,天牛左右两只触角的位置被定义为如下公式所示模型：</p><p>$$<br>\left{ \begin{array}{l}<br>{X_r} = X + l * \mathop d\limits^ \to  \<br>{X_l} = X - l * \mathop d\limits^ \to<br>\end{array} \right.<br>$$</p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 智能优化算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>前缀和、二维前缀和与差分个人理解</title>
      <link href="/2020/05/28/qian-zhui-he-er-wei-qian-zhui-he-yu-chai-fen-ge-ren-li-jie/"/>
      <url>/2020/05/28/qian-zhui-he-er-wei-qian-zhui-he-yu-chai-fen-ge-ren-li-jie/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h1>前缀和</h1><h2 id="什么是前缀和"><a class="header-anchor" href="#什么是前缀和">¶</a>什么是前缀和</h2><p>  前缀和顾名思义就是指一个数组的某一个下标的（包括该下标）之前的所有数组元素的和。现在我们假设有某一数组a = [1, 2, 3, 4, 5, 6, 7, 8, 9]。其前缀和数组为sum，那么sum数组与a数组对应的关系如下图所示。<br><img src="/2020/05/28/qian-zhui-he-er-wei-qian-zhui-he-yu-chai-fen-ge-ren-li-jie/1.png" alt="在这里插入图片描述"><br>  由上面的对应关系我们可以得到他们满足如下的公式。</p><p><img src="/2020/05/28/qian-zhui-he-er-wei-qian-zhui-he-yu-chai-fen-ge-ren-li-jie/2.png" alt="在这里插入图片描述"><br>  以上的公式即为<strong>一维前缀和</strong>一维前缀和的代码模板如下所示。</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">&#x2F;**     * 一维前缀和     *     * @a 表示原数组     * @sum 表示a数组的一维前缀和     *&#x2F;     const int maxn &#x3D; 1e5 + 10;     int a[9] &#x3D; &#123;1, 2, 3, 4, 5, 6, 7, 8, 9&#125;;      int sum[maxn];    void oneDimen(int num) &#123;&#x2F;&#x2F;num表示数组a的长度        sum[0] &#x3D; a[0];        for (int i &#x3D; 1; i &lt; num; i++) &#123;            sum[i] &#x3D; sum[i - 1] + a[i];        &#125;    &#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="前缀和使用情况"><a class="header-anchor" href="#前缀和使用情况">¶</a>前缀和使用情况</h2><p>  我们在做题的时候经常会遇到查询问题，例如给出一个数组a，再给出m次查询，每次查询都会给出两个数L，R，分表表示查询区间的左右范围。如果我们只是使用最简单的朴素查询的方法，每次遍历区间，进行m次的查询，这样在题目所给数据范围较小的情况下可以进行，但是当查询次数很大时，其时间复杂度为<strong>O（n*m）<strong>会使运行TLE，所以我们使用上述的前缀和可以使时间复杂度降低为</strong>O(m+n)</strong></p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">int query(int L, int R)&#123;retrun sum[R] - sum[L - 1];&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h1>差分</h1><h2 id="什么是差分"><a class="header-anchor" href="#什么是差分">¶</a>什么是差分</h2><p>  差分就是指相邻两个数的差，我们假设存在一个数组，如下图所示<br><img src="/2020/05/28/qian-zhui-he-er-wei-qian-zhui-he-yu-chai-fen-ge-ren-li-jie/3.png" alt="在这里插入图片描述"><br>  具体代码模板如下：</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">const int maxn &#x3D; 1e5 + 10;int a[9] &#x3D; &#123;1, 2, 3, 4, 5, 6, 7, 8, 9&#125;; int diff[maxn];&#x2F;&#x2F;求出差分数组void chafen(int num)&#123;&#x2F;&#x2F;num表示原数组的长度diff[0] &#x3D; a[0];for(int i &#x3D; 1; i &lt; num; i++)&#123;diff[i] &#x3D; a[i] - a[i - 1];&#125;&#125;&#x2F;&#x2F;对区间进行加操作void addarray(int L, int R, int k)&#123;&#x2F;&#x2F;L和R分别代表对加区间的左右范围，k表示在区间里每个元素加的数字diff[L] +&#x3D; k;diff[R + 1] -&#x3D; k;&#x2F;&#x2F;这里特别要注意，因为在前面进行区间加后，后面一个数与前面这个数的差变小了，所以要在后面这个数的差分数组减去前面区间所增加的数字。&#125;&#x2F;&#x2F;通过差分数组和原数组a推理得到进行区间加后数组中某一个元素的值void get_a()&#123;for(int i &#x3D; 1; i &lt;&#x3D; n; i++)&#123;a[i] &#x3D; doff[i] + a[i - 1];&#125; &#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="差分使用情况"><a class="header-anchor" href="#差分使用情况">¶</a>差分使用情况</h2><p>  区间加：把数组a[l]到a[r]都加上k，这种操作称为区间加。在进行区间加的操作后得到的数组b，我们对数组b进行查询，但可以发现如果是L——-R非常大的情况下，通过朴素的区间范围内主次累加求和这个操作执行的次数又很多，那时间复杂度会很高。所以可以使用差分的思想来降低复杂度。</p><h1>二维前缀和</h1><h2 id="什么是二维前缀和"><a class="header-anchor" href="#什么是二维前缀和">¶</a>什么是二维前缀和</h2><p><img src="/2020/05/28/qian-zhui-he-er-wei-qian-zhui-he-yu-chai-fen-ge-ren-li-jie/4.png" alt="在这里插入图片描述"><br>  在上图中深蓝色的部分代表的是二维数组的索引，浅蓝色的部分代表的是二维数组的每个元素的值。其二维前缀和如下图所示<br><img src="/2020/05/28/qian-zhui-he-er-wei-qian-zhui-he-yu-chai-fen-ge-ren-li-jie/5.png" alt="在这里插入图片描述"><br>  前缀和数组里每一个位置都表示原数组当前索引左上方的数字的和。<br>如上表中的而为前缀和数组：prefixSum[3, 3] = src[0~2, 0~2]的和;<br>二维前缀和数组的计算步骤如下所示。<br>可以分为四种情况</p><ol><li>i == 0 &amp;&amp; j ==0，只有一个直接赋值即可：prefixSum[0, 0] = a[0, 0]。</li><li>i == 0，最左边的一列，二维前缀和为元素上一行相同列的元素加该数字，公式为prefixSum[0, j] = prefixSum[0, j-1] + a[0, j]；</li><li>j == 0，最上面一排，与i == 0类似prefixSum[i, o] = prefixSum[i-1, 0] + a[i, 0];</li><li>i!=0 || j!=0，其公式为 prefixSum[i][j] = prefixSum[i - 1][j] + prefixSum[i][j - 1] + a[i][j] - prefixSum[i - 1][j - 1];<br>其代码模板如下所示</li></ol><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">&#x2F;** * 二维前缀和 * * @param src 原数组 * @return 二维前缀和 *&#x2F; const int maxn &#x3D; 100; int prefixSum[maxn][maxn];void twoDimen(int a[][], int n, int m) &#123;&#x2F;&#x2F;n和m分别代表二维原始数组的行列长度    for (int i &#x3D; 0; i &lt; n; i++) &#123;        for (int j &#x3D; 0; j &lt; m; j++) &#123;            if (i &#x3D;&#x3D; 0 &amp;&amp; j &#x3D;&#x3D; 0) &#123;&#x2F;&#x2F;第0个，最左上角                prefixSum[i][j] &#x3D; a[i][j];            &#125; else if (i &#x3D;&#x3D; 0) &#123;&#x2F;&#x2F;第一行，最顶部一行                prefixSum[i][j] &#x3D; prefixSum[i][j - 1] + a[i][j];            &#125; else if (j &#x3D;&#x3D; 0) &#123;&#x2F;&#x2F;第一列，最左边一列                prefixSum[i][j] &#x3D; prefixSum[i - 1][j] + a[i][j];            &#125; else &#123;&#x2F;&#x2F;其他                prefixSum[i][j] &#x3D; prefixSum[i - 1][j] + prefixSum[i][j - 1] + a[i][j] - prefixSum[i - 1][j - 1];            &#125;        &#125;    &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="二维前缀和的使用情况"><a class="header-anchor" href="#二维前缀和的使用情况">¶</a>二维前缀和的使用情况</h2><p>  一般使用二维前缀和可以求子矩阵的最大值。通过求解出整个矩阵的二维前缀和数组，然后对二位前缀和数组中的元素进行查询，找到其和最大的子矩阵。</p><h2 id="二维前缀和的差分"><a class="header-anchor" href="#二维前缀和的差分">¶</a>二维前缀和的差分</h2><p>  二维前缀和也可以使用差分的形式。方法是和一维类似的，我们也是需要另开一个数组记录修改操作，最后求前缀和时统计修改操作，只是二维每一次操作需要记录4个位置，一维只需要记录2个位置。具体模板代码如下所示。</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">void chafen()&#123;for(int i&#x3D;0;i&lt;m;i++)&#123;&#x2F;&#x2F;m是修改操作次数 int x1,y1,x2,y2,p;cin&gt;&gt;x1&gt;&gt;y1&gt;&gt;x2&gt;&gt;y2&gt;&gt;p;b[x1][y1]+&#x3D;p;b[x2+1][y2+1]+&#x3D;p;b[x2+1][y1]-&#x3D;p;b[x1][y2+1]-&#x3D;p;&#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>  以上部分来自个人理解以及从其他大佬的博客中领悟到的，有些内容可能与其他大佬相似，如有侵权，请及时指出，立马进行修正。有写的不好地方也请及时指出，本人菜鸡，勿喷。</p><p><font face="微软雅黑" size="4">你我都是内卷背景下不起眼的尘埃，被时代的洪流裹挟向前。</font></p>]]></content>
      
      
      <categories>
          
          <category> ACM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ACM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>线段树个人理解</title>
      <link href="/2020/05/27/xian-duan-shu-ge-ren-li-jie/"/>
      <url>/2020/05/27/xian-duan-shu-ge-ren-li-jie/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h1>线段树</h1><p><img src="/2020/05/27/xian-duan-shu-ge-ren-li-jie/1.png" alt="在这里插入图片描述"></p><h2 id="定义："><a class="header-anchor" href="#定义：">¶</a>定义：</h2><p>  线段树是一种二叉搜索树，与区间树相似，它将一个区间划分成一些单元区间，每个单元区间对应线段树中的一个叶结点。使用线段树可以快速的查找某一个节点在若干条线段中出现的次数，时间复杂度为O(logN)。而未优化的空间复杂度为2N，实际应用时一般还要开4N的数组以免越界，因此有时需要离散化让空间压缩。对于线段树中的每一个非叶子节点[a,b]，它的左儿子表示的区间为[a,(a+b)/2]，右儿子表示的区间为[(a+b)/2+1,b]。因此线段树是平衡二叉树，最后的子节点数目为N，即整个线段区间的长度。</p><h2 id="作用范围："><a class="header-anchor" href="#作用范围：">¶</a>作用范围：</h2><p>  线段树的适用范围很广，可以在线维护修改以及查询区间上的最值，求和。更可以扩充到二维线段树（矩阵树）和三维线段树（空间树）。对于一维线段树来说，每次更新以及查询的时间复杂度为O(logN)。还支持区间修改，单点修改等操作。</p><h2 id="实现原理："><a class="header-anchor" href="#实现原理：">¶</a>实现原理：</h2><p>  线段树主要是把一段大区间平均地划分成两段小区间进行维护，再用小区间的值来更新大区间。这样既能保证正确性，又能使时间保持在log级别（因为这棵线段树是平衡的）。也就是说，一个[L…R]的区间会被划分成[L…(L+R)/2]和[(L+R)/2+1…R]这两个小区间进行维护，直到L=R。但是在上述的过程中我们会遇到以下几个问题，就是我们该如何建树，建树的过程中每一个下标我们该如何去分配，分派到的每一个空间我们应该用来存放哪些数据。</p><p><img src="/2020/05/27/xian-duan-shu-ge-ren-li-jie/2.png" alt="在这幅图片中"><br>  在这里我们仅对线段树中常见的区间最大值问题进行解释讨论。假设所给的区间为F[1:6] = {1, 9, 7, 8, 2, 3}。那么其对应的线段树的结构就如上图所示。其中红色的圆圈就代表线段树对应的每一个结点的下标。蓝色方框中的Max就是我们每一个结点所存放的内容，即每一个区间存放的最大值。Max下面的内容是对这个区间范围的一个说明，并不需要存放在数组中。<br>  仔细看这幅图我们会发现，其中结点的下标并不连续（在图中结点的标号并没有10，11）。这是因为我们在用数组对线段树进行模拟的时候，必须要提前对整个树的空间进行提前的开辟，所开辟的空间虽然并没有使用到，但是其仍然真是存在，这也是为什么我们在对数组进行开辟空间时一般会选择4<em>n的大小以避免出现RE。<br>  通过观察上面的线段树结点标号我们可以发现，对于一个区间[L,R]来说，最重要的数据当然就是区间的左右端点L和R，但是大部分的情况我们并不会去存储这两个数值，而是通过递归的传参方式进行传递。这种方式用指针好实现，定义两个左右子树递归即可，按时指针表示过于繁琐，而且不方便各种操作，大部分的线段树都是使用数组进行表示，那这里怎么快速使用下标找到左右子树呢。这就会涉及到每个结点下表数字的规律。我们发现在线段树中每个非叶子结点的度都为2，且父亲节点的左右两个孩子分别存储父亲一半的区间，而每个父亲结点存放的欧式孩子的最大值，而且左孩子的下标都为偶数，右孩子的下标都是奇数且左孩子下标数+1，即：<br><strong><em><em>L = Father</em>2 （左子树下标为父亲下标的两倍）<br>R = Father</em>2+1（右子树下标为父亲下标的两倍+1）</strong></em>*<br>或<br><strong>k&lt;&lt;1（结点k的左子树下标）<br>k&lt;&lt;1|1（结点k的右子树下标）</strong><br>所以建树的操作可用如下代码实现</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">const int maxn &#x3D; 1e5+5;const int INF &#x3D; 0x3f3f3f3f;int tree[maxn&lt;&lt;2],temp[maxn];&#x2F;&#x2F;tree[]数组表示线段树数组，temp[]表示存放原始数据的数组void Build(int l,int r,int rt)&#123; &#x2F;&#x2F;l,r表示当前节点区间，rt表示当前节点编号      if(l&#x3D;&#x3D;r) &#123;&#x2F;&#x2F;若到达叶节点，即区间的左右值相等           tree[rt]&#x3D;temp[l];&#x2F;&#x2F;储存数组值           return;      &#125;      int mid &#x3D; (l+r)&gt;&gt;1;  &#x2F;&#x2F;mid表示中间点    &#x2F;&#x2F;左右递归       Build(l,mid,rt&lt;&lt;1);      Build(mid+1,r,rt&lt;&lt;1|1);      tree[rt] &#x3D; max(tree[rt&lt;&lt;1],tree[rt&lt;&lt;1|1];&#x2F;&#x2F;更新信息&#125;  <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="线段树的基本操作："><a class="header-anchor" href="#线段树的基本操作：">¶</a>线段树的基本操作：</h2><h3 id="一、点更新"><a class="header-anchor" href="#一、点更新">¶</a>一、点更新</h3><p><img src="/2020/05/27/xian-duan-shu-ge-ren-li-jie/3.png" alt="在这里插入图片描述"><br>  假设我们将上述的区间F[1:6] = {1, 9, 7, 8, 2, 3}中的F[3] = 7通过对其+3更改为10。那么我们应当对线段树进行如下的几个操作。</p><ol><li>我们通过线段树的根结点向下遍历，通过叶子结点所在的区间进行查询，在每一处根结点与我们改变的值相比较，如果F[3] = 10大于当前根结点中存储的Max值，那么将Max = 10，否则不变且继续向下遍历。</li><li>直至到L=R时，即为我们改变的叶子结点，将其中存储的值变为我们上述的F[3] = 10，退出。<br>  具体代码实现如下：</li></ol><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">&#x2F;&#x2F;递归方式更新 updata(p,v,1,n,1);void updata(int p,int v,int l,int r,int rt)&#123;    &#x2F;&#x2F;p为下标，v为要加上的值，l，r为结点区间，rt为结点下标    if(l &#x3D;&#x3D; r)&#123;    &#x2F;&#x2F;左端点等于右端点，即为叶子结点，直接加上v即可        temp[rt] +&#x3D; v;        tree[rt] +&#x3D; v;    &#x2F;&#x2F;原数组和线段树数组都得到更新        return ;    &#125;    int m &#x3D; l + ((r-l)&gt;&gt;1);    &#x2F;&#x2F;m则为中间点，左儿子的结点区间为[l,m],右儿子的结点区间为[m+1,r]    if(p &lt;&#x3D; m)    &#x2F;&#x2F;如果需要更新的结点在左子树区间        updata(p,v,l,m,rt&lt;&lt;1);    else    &#x2F;&#x2F;如果需要更新的结点在右子树区间        updata(p,v,m+1,r,rt&lt;&lt;1|1);    tree[rt] &#x3D; max(tree[rt&lt;&lt;1],tree[rt&lt;&lt;1|1];    &#x2F;&#x2F;更新父节点的值&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="二、区间查询"><a class="header-anchor" href="#二、区间查询">¶</a>二、区间查询</h3><p>  线段树的每个结点存储的都是一段区间的信息 ，这就意味着如果我们刚好要查询这个区间，那么则直接返回这个结点的信息即可，比如对于上面线段树，如果我直接查询[1,6]这个区间的最值，那么直接返回根节点信息10即可，查询[1,2]直接返回9。但是有时题目中为了设置难度并不会轻易让我们查询每个结点所表示的区间。比如现在我要查询[2,5]区间的最值，这时候我们会发现并不存在某个节点的区间是[2,5]，那么这时我们应该采取一些什么方法来进行区间信息的查询呢？<br><img src="/2020/05/27/xian-duan-shu-ge-ren-li-jie/4.png" alt="在这里插入图片描述"></p><ol><li>首先我们发现区间[2,5]在线段树中包括的节点有[2,2]，[3,3]，[4,4]，[5,5]，[4,5]。但是[4,4]，[5,5]这两个信息的区间已经被[4,5]区间所包含，所以我们真正需要查询的结点为[2,2]，[3,3]，[4,5]这三个区间所在的结点。</li><li>其次从根节点开始往下递归，如果当前结点是要查询的区间的真子集，则返回这个结点的信息且不需要再往下递归。<br>  具体代码如下</li></ol><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">&#x2F;&#x2F;递归方式区间查询 query(Ld,Rd,1,n,1);int query(int Ld,int Rd,int l,int r,int rt)&#123;    &#x2F;&#x2F;[Ld,Rd]即为要查询的区间，l，r为结点区间，rt为结点下标    if(Ld &lt;&#x3D; l &amp;&amp; r &lt;&#x3D; Rd)    &#x2F;&#x2F;如果当前结点的区间真包含于要查询的区间内，则返回结点信息且不需要往下递归        return tree[rt];    int ans &#x3D; -INF;    &#x2F;&#x2F;返回值变量，根据具体线段树查询的什么而自定义    int mid &#x3D; (l+r)&gt;&gt;1;    &#x2F;&#x2F;m则为中间点，左儿子的结点区间为[l,m],右儿子的结点区间为[m+1,r]    if(Ld &lt;&#x3D; m)    &#x2F;&#x2F;如果左子树和需要查询的区间交集非空        ans &#x3D; max(ans, query(L,R,l,m,k&lt;&lt;1));    if(Rd &gt; m)    &#x2F;&#x2F;如果右子树和需要查询的区间交集非空，注意这里不是else if，因为查询区间可能同时和左右区间都有交集        ans &#x3D; max(ans, query(L,R,m+1,r,k&lt;&lt;1|1));    return ans;    &#x2F;&#x2F;返回当前结点得到的信息    &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="三、区间更新"><a class="header-anchor" href="#三、区间更新">¶</a>三、区间更新</h3><p>  在线段树的区间更新中我们引进了一个新的思想，Lazy_tag，字面意思就是懒惰标记的意思，实际上它的功能也就是偷懒= =，因为对于一个区间[L,R]来说，我们每次都更新区间中的每一个值，那样的话更新的复杂度将会是O(NlogN)，这太高了，所以引进了Lazy_tag，这个标记一般用于处理线段树的区间更新。<br>　　线段树在进行区间更新的时候，为了提高更新的效率，所以每次更新只更新到更新区间完全覆盖线段树结点区间为止，这样就会导致被更新结点的子孙结点的区间得不到需要更新的信息，所以在被更新结点上打上一个标记，称为lazy-tag，等到下次访问这个结点的子结点时再将这个标记传递给子结点，所以也可以叫延迟标记。<br>　　也就是说递归更新的过程，更新到结点区间为需要更新的区间的真子集不再往下更新，下次若是遇到需要用这下面的结点的信息，再去更新这些结点，所以这样的话使得区间更新的操作和区间查询类似，复杂度为O(logN)。</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">void Pushdown(int rt)&#123;    &#x2F;&#x2F;更新子树的lazy值，这里是RMQ的函数，要实现区间和等则需要修改函数内容    if(lazy[rt])&#123;    &#x2F;&#x2F;如果有lazy标记        lazy[rt&lt;&lt;1] +&#x3D; lazy[rt];    &#x2F;&#x2F;更新左子树的lazy值        lazy[rt&lt;&lt;1|1] +&#x3D; lazy[rt];    &#x2F;&#x2F;更新右子树的lazy值        t[rt&lt;&lt;1] +&#x3D; lazy[rt];        &#x2F;&#x2F;左子树的最值加上lazy值        t[rt&lt;&lt;1|1] +&#x3D; lazy[rt];    &#x2F;&#x2F;右子树的最值加上lazy值        lazy[rt] &#x3D; 0;    &#x2F;&#x2F;lazy值归0    &#125;&#125;&#x2F;&#x2F;递归更新区间 updata(L,R,v,1,n,1);void updata(int Ld,int Rd,int v,int l,int r,int rt)&#123;    &#x2F;&#x2F;[Ld,Rd]即为要更新的区间，l，r为结点区间，k为结点下标    if(Ld &lt;&#x3D; l &amp;&amp; r &lt;&#x3D; Rd)&#123;    &#x2F;&#x2F;如果当前结点的区间真包含于要更新的区间内        lazy[rt] +&#x3D; v;    &#x2F;&#x2F;懒惰标记        t[rt] +&#x3D; v;    &#x2F;&#x2F;最大值加上v之后，此区间的最大值也肯定是加v    &#125;    else&#123;        Pushdown(k);    &#x2F;&#x2F;重难点，查询lazy标记，更新子树        int m &#x3D; l + ((r-l)&gt;&gt;1);        if(Ld &lt;&#x3D; m)    &#x2F;&#x2F;如果左子树和需要更新的区间交集非空            update(Ld,Rd,v,l,m,rt&lt;&lt;1);        if(m &lt; Rd)    &#x2F;&#x2F;如果右子树和需要更新的区间交集非空            update(Ld,Rd,v,m+1,r,rt&lt;&lt;1|1);        Pushup(rt);    &#x2F;&#x2F;更新父节点    &#125;&#125;&#x2F;&#x2F;递归方式区间查询 query(Ld,Rd,1,n,1);int query(int Ld,int Rd,int l,int r,int rt)&#123;    &#x2F;&#x2F;[L,R]即为要查询的区间，l，r为结点区间，k为结点下标    if(Ld &lt;&#x3D; l &amp;&amp; r &lt;&#x3D; Rd)    &#x2F;&#x2F;如果当前结点的区间真包含于要查询的区间内，则返回结点信息且不需要往下递归        return t[rt];    else&#123;        Pushdown(rt);    &#x2F;**每次都需要更新子树的Lazy标记*&#x2F;        int res &#x3D; -INF;    &#x2F;&#x2F;返回值变量，根据具体线段树查询的什么而自定义        int mid &#x3D; l + ((r-l)&gt;&gt;1);    &#x2F;&#x2F;m则为中间点，左儿子的结点区间为[l,m],右儿子的结点区间为[m+1,r]        if(Ld &lt;&#x3D; m)    &#x2F;&#x2F;如果左子树和需要查询的区间交集非空            res &#x3D; max(res, query(Ld,Rd,l,m,rt&lt;&lt;1));        if(Rd &gt; m)    &#x2F;&#x2F;如果右子树和需要查询的区间交集非空，注意这里不是else if，因为查询区间可能同时和左右区间都有交集            res &#x3D; max(res, query(Ld,Rd,m+1,r,rt&lt;&lt;1|1));        return res;    &#x2F;&#x2F;返回当前结点得到的信息    &#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><font face="微软雅黑" size="4">你我都是内卷背景下不起眼的尘埃，被时代的洪流裹挟向前。</font></p>]]></content>
      
      
      <categories>
          
          <category> ACM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ACM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>武汉理工大学《软件工程》复习总括三</title>
      <link href="/2019/11/05/wu-han-li-gong-da-xue-ruan-jian-gong-cheng-fu-xi-zong-gua-san/"/>
      <url>/2019/11/05/wu-han-li-gong-da-xue-ruan-jian-gong-cheng-fu-xi-zong-gua-san/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h1>第六章软件结构体系</h1><h2 id="软件结构体系的内容："><a class="header-anchor" href="#软件结构体系的内容：">¶</a><strong>软件结构体系的内容：</strong></h2><p><strong>1、构件</strong>：代表着一组基本的构成要素<br><strong>2、连接件</strong>：也就是构件之间的连接关系<br><strong>3、约束</strong>：是作用于构件或者连接关系上的一些限制条件<br><strong>4、质量</strong>：是系统的质量属性，如性能、可扩展性、可修改性、可重用性、安全性等。<br><strong>5、物理分布</strong>：代表着构件连接之后形成的拓扑结构，描述了软件到硬件之间的影射。</p><h2 id="软件结构体系发展的阶段："><a class="header-anchor" href="#软件结构体系发展的阶段：">¶</a>软件结构体系发展的阶段：</h2><p><img src="/2019/11/05/wu-han-li-gong-da-xue-ruan-jian-gong-cheng-fu-xi-zong-gua-san/1.png" alt="在这里插入图片描述"></p><h2 id="体系结构、软件框架、设计模式三者的联系和区别："><a class="header-anchor" href="#体系结构、软件框架、设计模式三者的联系和区别：">¶</a>体系结构、软件框架、设计模式三者的联系和区别：</h2><p><strong>体系结构</strong>：描述某一特定应用领域中系统组织的惯用模式，反映领域中众多系统所共有的结构和语义特性，例如：MVC<br><strong>软件框架</strong>：由开发人员定制的应用系统骨架，整个或部分系统的可重用设计，由一组抽象构件和构件实例之间的交互方式组成。例如Django就是一个开放源代码的应运框架，由Python写成。<br><strong>设计模式</strong>：描述软件系统设计过程中常见问题的一些解决方案，从大量的成功实践中总结出来的，且被广泛公认的实践和知识。<br><strong>软件框架和体系结构的区别及关系：</strong><br><img src="/2019/11/05/wu-han-li-gong-da-xue-ruan-jian-gong-cheng-fu-xi-zong-gua-san/2.png" alt="在这里插入图片描述"><br><strong>软件框架和实际模式的区别及关系：</strong><br><img src="/2019/11/05/wu-han-li-gong-da-xue-ruan-jian-gong-cheng-fu-xi-zong-gua-san/3.png" alt="在这里插入图片描述"></p><h2 id="软件工程问题中的关键的角色："><a class="header-anchor" href="#软件工程问题中的关键的角色：">¶</a>软件工程问题中的关键的角色：</h2><p><strong>用户</strong>：使用系统实现某种目标<br><strong>软件系统</strong>：待开发的系统<br><strong>环境</strong>：软件系统以外的任何事物</p><h2 id="软件设计的原则（高内聚低耦合）："><a class="header-anchor" href="#软件设计的原则（高内聚低耦合）：">¶</a>软件设计的原则（高内聚低耦合）：</h2><p><strong>内聚性</strong>：是一个模块或子系统内部的依赖程度。分为七种：功能内聚、信息内聚、通信内聚、过程内聚、时间内聚、逻辑内聚、巧合内聚。<br><img src="/2019/11/05/wu-han-li-gong-da-xue-ruan-jian-gong-cheng-fu-xi-zong-gua-san/4.png" alt="在这里插入图片描述"><br><strong>耦合性</strong>：是两个模块或者子系统之间依赖关系的强度，程序结构各个模块之间相互关联的度量。模块之间的联系方式一般有7中：非直接耦合、数据耦合、标记耦合、控制耦合、外部耦合、公共耦合、内容耦合。<br><img src="/2019/11/05/wu-han-li-gong-da-xue-ruan-jian-gong-cheng-fu-xi-zong-gua-san/5.png" alt="在这里插入图片描述"></p><h2 id="软件体系结构常见的风格："><a class="header-anchor" href="#软件体系结构常见的风格：">¶</a>软件体系结构常见的风格：</h2><p>可以根据文字描述判断出风格的种类即可。<br>**<img src="/2019/11/05/wu-han-li-gong-da-xue-ruan-jian-gong-cheng-fu-xi-zong-gua-san/6.png" alt="在这里插入图片描述"><br><strong>管道/过滤器风格</strong>：把系统任务分成若干连续的处理步骤，这些步骤由通过系统的数据流连接，一个步骤的输出是下一个步骤的输入。<br><strong><img src="/2019/11/05/wu-han-li-gong-da-xue-ruan-jian-gong-cheng-fu-xi-zong-gua-san/7.png" alt="在这里插入图片描述"><br>主程序—子程序风格</strong>：结构化程序设计的一种典型风格，从功能的观点设计系统，逐步分解和细化，形成整个系统的体系结构。<br><strong><img src="/2019/11/05/wu-han-li-gong-da-xue-ruan-jian-gong-cheng-fu-xi-zong-gua-san/8.png" alt="在这里插入图片描述"><br>面向对象风格</strong>：系统被看作是对象的集合，每个对象都有一个它自己的功能集合，数据及作用在数据上的操作被封装成抽象数据类型，只通过接口与外界交互，内部的设计决策则被封装起来。<br>**<img src="/2019/11/05/wu-han-li-gong-da-xue-ruan-jian-gong-cheng-fu-xi-zong-gua-san/9.png" alt="在这里插入图片描述"><br><strong>层次结构风格----客户机／服务器体系结构</strong>：一种分布式系统模型<br><strong>服务器</strong>：为客户机提供服务<br><strong>客户机</strong>：负责与用户的交互       类似于网络编程交互的情景。<br><strong><img src="/2019/11/05/wu-han-li-gong-da-xue-ruan-jian-gong-cheng-fu-xi-zong-gua-san/10.png" alt="在这里插入图片描述"><br>层次结构----浏览器/服务器结构</strong>：<br><img src="/2019/11/05/wu-han-li-gong-da-xue-ruan-jian-gong-cheng-fu-xi-zong-gua-san/11.png" alt="在这里插入图片描述"><br><strong>层次结构----模型/视图/控制器：（MVC）</strong><br>**<img src="/2019/11/05/wu-han-li-gong-da-xue-ruan-jian-gong-cheng-fu-xi-zong-gua-san/12.png" alt="在这里插入图片描述"><br><strong>基于事件的隐式调用风格</strong>：将应用看成是一个构件集合，每个构件直至发生对它有影响的事件时才有所动作<br>**<img src="/2019/11/05/wu-han-li-gong-da-xue-ruan-jian-gong-cheng-fu-xi-zong-gua-san/13.png" alt="在这里插入图片描述"><br><strong>仓库风格</strong>：以数据为中心，适合于数据由一个模块产生而由其他模块使用的情形<br><img src="/2019/11/05/wu-han-li-gong-da-xue-ruan-jian-gong-cheng-fu-xi-zong-gua-san/14.png" alt="在这里插入图片描述"></p><h1>第七章面向对象设计</h1><h2 id="面向对象的设计和分析中的三种类："><a class="header-anchor" href="#面向对象的设计和分析中的三种类：">¶</a>面向对象的设计和分析中的三种类：</h2><p><strong>1、实体类</strong>：对应系统需求中的每个实体，它们通常需要保存在永久的存储体中，一般使用数据库表和文件来记录，实体类包括存储和传递数据的类，也包括操作数据的类<br><strong>2、控制类</strong>：用于体现应用程序的执行逻辑，提供相应的业务操作，将控制类抽象出来可以降低界面和数据库之间的耦合度；<br><strong>3、边界类</strong>：用于对外部用户与系统之间的交互对象进行抽象。主要包括界面类。</p><h2 id="什么是领域模型："><a class="header-anchor" href="#什么是领域模型：">¶</a>什么是领域模型：</h2><p>在面向对象分析和设计的初级阶段，通常先识别出实体类，绘制初始类图，此时的类图称为领域模型，包括实体类和它们之间的相互关系。</p><h1>第八章编写高质量代码</h1><h2 id="程序复杂度怎么计算（重点是第二种，第二种有三种小方法）："><a class="header-anchor" href="#程序复杂度怎么计算（重点是第二种，第二种有三种小方法）：">¶</a>程序复杂度怎么计算（重点是第二种，第二种有三种小方法）：</h2><p><strong>基本思想</strong>：程序复杂性主要取决于程序控制流的复杂性，单一的顺序结构最简单，选择和循环结构构成的环路越多，程序越复杂。<br><strong>实质</strong>：度量程序拓扑结构的复杂性程序图：把程序看成是有一个入口、一个出口的有向图程序图的<br><strong>节点</strong>：每个语句、一个顺序流程的程序代码段、程序流程图中的每个处理符号程序图的<br><strong>有向弧</strong>：程序中的流程控制、程序流程图中连接不同处理符号的、带箭头的线段<br><strong>强连通图(Strongly Connected Graph)</strong>：是指一个有向图（Directed Graph）中任意两点v1、v2间存在v1到v2的路径（path）及v2到v1的路径的图。<br><strong>三种方法</strong>：如果程序图中每个节点都可以由入口节点到达，则<strong>图中环的个数 = 环路复杂度</strong><br>如果程序图是强连通图，则计算环路数V(G)的方法 <strong>方法一：V(G) = e–n + p（e: 弧数，n: 节点数，p: 分离部分的数目,V(G)有向图G中的环数）</strong> <strong>方法二：包括强连通域在内的环路数</strong> <strong>方法三：判定节点数 +  1</strong><br>V(G)与程序复杂性呈正比关系 一般一个模块V(G) ≤ 10</p><h1>第九章测试驱动的实现</h1><h2 id="软件测试的类型："><a class="header-anchor" href="#软件测试的类型：">¶</a><strong>软件测试的类型</strong>：</h2><p>1）从测试对象角度①单元测试 ②集成测试③功能测试 ④性能测试 ⑤安装测试2）测试技术角度①黑盒测试（功能测试）②白盒测试（结构测试）<br>3）是否运行程序角度①静态测试 ②动态测试<br>4）执行测试的方式①手工测试 ②自动化测试</p><h2 id="白盒测试："><a class="header-anchor" href="#白盒测试：">¶</a>白盒测试：</h2><p>在下一次更新中详写。</p><h2 id="自己会设计测试用例：路径覆盖："><a class="header-anchor" href="#自己会设计测试用例：路径覆盖：">¶</a>自己会设计测试用例：路径覆盖：</h2><p>在下一次更新中详写</p><h2 id="软件测试的几个阶段：-每个阶段的名称作用测试的对象"><a class="header-anchor" href="#软件测试的几个阶段：-每个阶段的名称作用测试的对象">¶</a>软件测试的几个阶段：(每个阶段的名称作用测试的对象)</h2><p><strong>1、单元测试</strong>：对软件中的最小可测试单元进行检查和验证 对象是单元。<br><strong>2、集成测试</strong>：在单元测试的基础上，将所有模块按照总体设计的要求组装成为子系统或系统进行的测试 对象是系统或者子系统<br><strong>3、确认测试</strong>：在开发过程中或结束时评估系统或组成部分的过程，目的是判断系统是否满足规定的要求。对象是系统<br><strong>4、系统测试</strong>：检测软件系统运行时与其他相关要素的协调工作情况是否满足要求。对象是系统。</p>]]></content>
      
      
      <categories>
          
          <category> 课本学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 软件工程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>武汉理工大学《软件工程》复习总括二</title>
      <link href="/2019/11/03/wu-han-li-gong-da-xue-ruan-jian-gong-cheng-fu-xi-zong-gua-er/"/>
      <url>/2019/11/03/wu-han-li-gong-da-xue-ruan-jian-gong-cheng-fu-xi-zong-gua-er/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h1>第四章需求获取</h1><p><strong>需求分析的实质</strong>：是对系统的理解与表达的过程，是一种软件工程的活动。</p><p><strong>需求分析之后建立模型的名称</strong>：分析模型或需求模型需求分析的过程：<br><strong>需求分析的过程</strong>：<img src="/2019/11/03/wu-han-li-gong-da-xue-ruan-jian-gong-cheng-fu-xi-zong-gua-er/1.png" alt="在这里插入图片描述"><br><strong>常用的需求分析的方法</strong>：<br>1：面向数据流的结构化分析方法（简称SA）<br>2：面向数据结构的Jackson方法（简称JSD）<br>3：面向对象的分析方法<br>4：建立动态模型的迁移图或Petri网等。<br><strong>软件需求的分类，根据分类的标准不同，结果也不同</strong>：<br><strong>1：按修饰对象的不同</strong>：<br><strong>Ø 产品需求</strong>：<br><strong>l 功能性需求</strong>：软件产品的功能特性<br><strong>l 非功能性需求</strong>：软件产品的质量属性，是在功能性需求满足情况下的进一步要求<br><strong>“FURPS“模型</strong>：<br><strong>功能性</strong>：需要考虑的额外的功能需求，如安全性；<br><strong>可用性</strong>：易用性、美观性、一致性和文档化；<br><strong>可靠性</strong>：指的是在特定操作环境下预期的系统故障频率、可恢复性、可预测性准确性以及平均故障时间；<br><strong>性能</strong>：响应时间、效率、资源利用率和吞吐量（在一个指定时间内系统可完成的工作量）<br><strong>可支持性</strong>：可测试性、适应性、可维护性、兼容性、可配置性、可扩展性和本地化<br><strong>Ø 过程需求</strong>——修饰或限制软件开发过程的要求<br><strong>2：按抽象层次详细程度</strong>：<br><strong>Ø 业务需求</strong><br><strong>Ø 用户需求</strong><br><strong>Ø 系统需求</strong><br><strong>Ø 软件设计规约</strong></p><p><strong>需求优先级的等级：</strong><br><strong>1、基本的</strong>：使得客户能够黑手系统并且必须实现的要求<br><strong>2：可取的</strong>：非常可取但却不是必须的那些需求<br><strong>3：可选的</strong>：在时间和资源允许的情况下，可能会实现的需求<br><strong>4：未来的</strong>：不会在系统当前版本中实现，但考虑到系统后续的版本应该记录下来的需求</p><p><strong>需求获取的技术有哪些</strong>：<br>1：面谈<br>2：问卷调查<br>3：群体诱导技术<br>4：头脑风暴<br>5：参与观察法<br>6：亲身实践<br>7：原型<br>8：情景分析<br>9：概念建模<br>10：A/B测试</p><p><strong>结构化分析的主要工具</strong>：<br>1：数据流图(DFD)<br>2:数据字典(DD)<br>3：结构化语言<br>4：判定树<br>5：判定表</p><p><strong>传统的软件建模中分析模型的核心及围绕核心的三个子模型</strong>：<br>分析模型的核心是数据字典，围绕数据字典3个层次的子模型有数据模型、功能模型和行为模型。<br><strong>数据字典</strong>：用于描述系统软件中使用或者产生的每一个数据元素，是系统数据信息定义的集合。<br><strong>数据模型</strong>：用于描述数据对象之间的关系。其应包含3种相关的信息，即数据对象、属性和关系<br><strong>功能模型</strong>：可以用数据流图描述（数据流图是一种图形化技术，可以表达软件系统必须完成的功能），所以又称数据流模型。<br><strong>行为模型</strong>：常用状态转化图（即状态图）来描述，又称状态机模型，可以理解为，在任一个时刻，系统处于有限可能的状态中的一个状态，当某一个激励条件到达时，它激发系统从一个状态转换到另一新状态。</p><h1>第五章</h1><p><strong>用例建模UML的九种图的画法，以及每种图的作用，在分析和设计的过程中怎么使用：</strong><br><img src="/2019/11/03/wu-han-li-gong-da-xue-ruan-jian-gong-cheng-fu-xi-zong-gua-er/2.png" alt="在这里插入图片描述"><br><strong>1、用例图：</strong><br><strong>作用</strong>：表示角色和用例之间的关系，其中用例代表的是一个系统或分类器的功能，外部交互者与这一分类器来进行交互呈现。<br><strong>组成/使用</strong>：由一些角色、一组用例，还可能有一些接口以及这些组成元素之间的关系构成的图，其中关系是指角色和用例之间的联系。用例通常用矩形框起来以表示系统或分类器的边界。<br><img src="/2019/11/03/wu-han-li-gong-da-xue-ruan-jian-gong-cheng-fu-xi-zong-gua-er/3.png" alt="在这里插入图片描述"><br><strong>2、类图：</strong><br><strong>作用</strong>：静态描述性模型元素相互连接的集合图，可以表示不同实体（人，事务和数据）的内部构成<br><strong>组成/使用</strong>：名称，属性和方法，他们之间的关系。#表示受保护成员，+表示公有成员，“-”表示私有成员。<br><strong><img src="/2019/11/03/wu-han-li-gong-da-xue-ruan-jian-gong-cheng-fu-xi-zong-gua-er/4.png" alt="在这里插入图片描述"><br>3、交互图</strong> ：包括顺序图和协作图，两种图在内容上是等效的，可以相互转换。<br><strong>顺序图</strong>：强调消息的时间排序<br><strong>作用</strong>：表示交互，指为得到一个期望的结果而在多个分类器角色之间进行的交互序列。<br><strong>组成/使用</strong>：顺序图有两维，垂直维代表时间，水平维代表对象。通常，垂直维自上至下代表时间向前推进。<br><strong>协作图</strong>：强调发送消息和接收消息的对象的结构组织<br><strong>作用</strong>：描述相互联系的对象之间的关系，或者分类器角色和关联角色之间的关系以下是同一个例子分别用两种图的表示<br><img src="/2019/11/03/wu-han-li-gong-da-xue-ruan-jian-gong-cheng-fu-xi-zong-gua-er/5.png" alt="在这里插入图片描述"> 顺序图<br><img src="/2019/11/03/wu-han-li-gong-da-xue-ruan-jian-gong-cheng-fu-xi-zong-gua-er/6.png" alt="在这里插入图片描述"><br>协作图<br><strong>4、状态图</strong>：描述模型元素在接收到事件后的动态行为。<br><strong>作用</strong>：描述一个类的对象在生命周期里如何从一个状态转移到另外一个状态，类的迁移由事件触发。<br><strong>组成/使用</strong>：图形中的状态和各种其他类型的顶点（伪状态）用适当的状态或者伪状态符号表示，状态之间的转换则用有向弧连接表示。<img src="/2019/11/03/wu-han-li-gong-da-xue-ruan-jian-gong-cheng-fu-xi-zong-gua-er/7.png" alt="在这里插入图片描述"></p><p><strong>5、活动图</strong>：是状态图的一种特殊的情况，其中绝大部分状态是动作或子活动状态，并且绝大部分甚至所有的转换是通过动作或者子活动的完成所触发的。<br><strong>作用</strong>：描述绝大多数甚至是所有的事件是由内部动作的完成所引起的情况。<br><strong>组成</strong>：由一条路径组成。包含并发和分叉，并发：两个活动同时发生。  分叉：选择性活动的发生<br><img src="/2019/11/03/wu-han-li-gong-da-xue-ruan-jian-gong-cheng-fu-xi-zong-gua-er/8.png" alt="在这里插入图片描述"><br><strong>6、构件图：</strong><br><strong>作用</strong>：表示构建之间的依赖关系组成：软件构建包括源代码构建、二进制代码构建和可执行构建，一些构建存在于编译时刻，一些存在于链接时刻，一些存在于运行时刻，还有一些可能存在于不止一个时刻。<br><strong>组成/使用</strong>：使用箭头表示依赖关系<br><img src="/2019/11/03/wu-han-li-gong-da-xue-ruan-jian-gong-cheng-fu-xi-zong-gua-er/9.png" alt="在这里插入图片描述"><br><strong>7、配置图：</strong><br><strong>作用</strong>：表示系统运行时的处理元素、软件构件以及基于它们的进程和对象的配置情况<br><strong>组成/使用</strong>：不处于运行状态的实体的软件构件不出现（在构件图中表示），结点可能包含构件实例，构件可能包含对象，构件与构件之间的依赖关系用箭线表示<br><strong><img src="/2019/11/03/wu-han-li-gong-da-xue-ruan-jian-gong-cheng-fu-xi-zong-gua-er/10.png" alt="在这里插入图片描述"><br>用例之间的关系</strong>：泛化、包含、扩展</p><p><strong>用例的场景：</strong><br>1：某个用例的一个实例，只描述完成给定的用例行为的若干可能途径中的一种  2：一个用例可能存在多个场景<br>3：系统会根据参与者提供的不同信息进入不同的场景<br>4：场景可以表达：正面行为需求，反面行为需求，不希望发生的交互，并行机制</p><p><strong>类与类之间的关系</strong>：<br>1：关联关系：包含自返关联、二元关联、N元关联<br>2：泛化关系<br>3：依赖关系<br>4：实现关系</p>]]></content>
      
      
      <categories>
          
          <category> 课本学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 软件工程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>武汉理工大学《软件工程》复习总括一</title>
      <link href="/2019/11/02/wu-han-li-gong-da-xue-ruan-jian-gong-cheng-fu-xi-zong-gua-yi/"/>
      <url>/2019/11/02/wu-han-li-gong-da-xue-ruan-jian-gong-cheng-fu-xi-zong-gua-yi/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h1>第一章软件工程概述</h1><p><strong>软件的本质特征：</strong><br>复杂性+一致性+可变性+不可见性<br><strong>软件危机的概念：</strong><br>软件危机是指在计算机软件的开发和维护过程中所遇到的一系列问题。<br><strong>软件工程的概念：</strong><br>1：将系统化的、规范的、可度量的方法应运与软件的开发、运行和维护的过程，即将工程化应运与软件中<br>2：对1中所述的方法的研究<br><strong>软件工程的关键元素</strong>：方法+工具+过程<br><strong>软件工程的开发策略</strong>：软件复用+分而治之+逐步演进+折中优化<br><strong>软件工程的基本原理：</strong><br>1：用分阶段的生命周期计划严格管理<br>2：坚持进行评审阶段<br>3：实行严格的产品控制<br>4：采用现代化程序设计技术<br>5：结果应能清楚的审查<br>6：开发小组人员应少而精<br>7：承认不断改进软件工程的必要性</p><h1>第二章软件过程</h1><p><strong>软件过程的定义及包含的活动：</strong><br>软件过程是指软件生成周期中的一系列相关过程，是为了获得高质量软件而实施的一系列活动。它包括问题定义、需求开发、软件设计、软件构造、软件测试等一系列软件开发的实现活动，而每一项都会产生相应的中间制品。<br><strong>软件过程常见的模型及其关系</strong>：<br>Ø <strong>瀑布模型</strong>——无法适应需求变化，计划驱动<br>特点：①. 阶段间具有顺序性和依赖性，便于分工合作；<br>②. 强调软件文档的重要性，要求每个阶段都进行仔细的验证；<br>③. 文档便于修改，并有复审质量保证。<br>缺陷：①. 划分固定，产生大量文档，增加了开发的工作量；<br>②. 开发是线性的，用户只有在整个程结束时才能看到开发成果；<br>③. 难以响应开发过程中用户的变更需求；<br>④. 早期错误难以发现适用于在软件需求明确，开发技术比较成熟，工程管理较严格的场合下使用（基本不会单独使用瀑布模型作为软件过程模型）<br>Ø <strong>原型化模型</strong>——需求不明确时选用<br>从用户需求出发快速建立一个原型，使用户通过这个原型初步表达出自己的需求，并通过反复修改完善逐渐靠近用户的全部需求，最终形成一个完全满足用户需求的新体系。<br>例：3D打印机<br>Ø <strong>迭代式开发</strong>——适应需求变化<br>开发被组织成一系列固定的短期小项目，称为一次迭代，每次迭代都包括完整的需求分析、设计、实现和测试活动。（理解：发布一系列版本）<br>增量开发：逐渐增加新的功能（缺点：增加功能的过程中可能破坏原有系统）<br>迭代开发：一次性开发出所有功能，后期再逐步完善各个功能<br>优点：①. 快速交付产品；<br>②. 快速响应需求变更；<br>③. 关注用户行为，很快得到用户反馈例：网上视频学习网站<br>Ø <strong>可转换模型</strong>——数学方法（安全、可靠、保密）<br>特点：需要一个精确表述的形式化的规格说明<br>例：汽车防抱系统，嵌入式控制系统<br><strong>关系</strong>：这些模型相互并不排斥，而且经常一起使用，尤其是对一些大型系统的开发。<br><strong>敏捷软件开发的核心价值</strong>：<br>1：“个体和交互”胜过“过程和工具”<br>2：“可以工作的软件”胜过“面面俱到的文档”<br>3：“客户合作”胜过“合同谈判”<br>4：“响应变化”胜过“遵循计划”</p><h1>第三章对象模型</h1><p><strong>面向对象方法的精华</strong>：面向对象=对象+分类+继承+消息通信<br><strong>1：对象</strong>：<br>客观世界都是由各种对象组成，任何事物都是对象，复杂的对象可以由比较简单的对象组合起来<br><strong>2：分类</strong>：<br>把所有的对象都划分为各种类，每个类都定义了一组数据和一组方法<br><strong>3：继承</strong>：<br>按照子类和父类的关系分类组成一个层次结构的系统，下层的子类与上层的父类有相同的特性<br><strong>4：消息通信</strong>：对象与对象之间只能通过传递消息进行通信。<br><strong>接口的概念</strong>：方法声明的集合<br><strong>对象属性和方法的三种访问权限</strong>：<br><strong>1：公有的（public）</strong>：其他对象可以直接访问<br><strong>2：私有的  (private)</strong>：只有特定的对象可以访问<br><strong>3：保护的(protected)</strong>：表示允许相关对象的访问<br><strong>对象的职责</strong>：即一个对象对其他对象的职责<br>1：“知道”型职责:知道各类数据和引用变量<br>2：“做”型职责：执行计算完成某项任务<br>3：“交流”型职责：和其他对象进行交流</p><p>此部分仅为第一到三章的内容，后续部分会依次更新，请您关注我的博客，在我的博客中寻找其他几章内容！！</p>]]></content>
      
      
      <categories>
          
          <category> 课本学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 软件工程 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
